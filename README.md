# FJSP-DRL

This repository is the official implementation of the paper â€œ[Flexible Job Shop Scheduling via Dual Attention Network Based Reinforcement Learning](https://doi.org/10.1109/TNNLS.2023.3306421)â€. *IEEE Transactions on Neural Networks and Learning Systems*, 2023.

## Quick Start

### requirements

- python $=$ 3.7.11
- argparse $=$ 1.4.0
- numpy $=$ 1.21.6
- ortools $=$ 9.3.10497
- pandas $=$ 1.3.5
- torch $=$ 1.11.0+cu113
- torchaudio $=$ 0.11.0+cu113
- torchvision $=$ 0.12.0+cu113
- tqdm $=$ 4.64.0

### introduction

- `data` saves the instance files including testing instances (in the subfolder `BenchData`, `SD1` and `SD2`) and validation instances (in the subfolder `data_train_vali`) .
- `model` contains the implementation of the proposed framework.
- `or_solution` saves the results solved by Google OR-Tools.
- `test_results`saves the results solved by priority dispatching rules and DRL models.
- `train_log` saves the training log of models, including information of the reward and validation makespan.
- `trained_network` saves the trained models.
- `common_utils.py` contains some useful functions (including the implementation of priority dispatching rules mentioned in the paper) .
- `data_utils.py` is used for data generation, reading and format conversion.
- `fjsp_env_same_op_nums.py` and `fjsp_env_various_op_nums.py` are implementations of fjsp environments, describing fjsp instances with the same number of operations and different number of operations, respectively.
- `ortools_solver.py` is used for solving the instances by Google OR-Tools.
- `params.py` defines parameters settings.
- `print_test_result.py` is used for printing the experimental results into an Excel file.
- `test_heuristic.py` is used for solving the instances by priority dispatching rules.
- `test_trained_model.py` is used for evaluating the models.
- `train.py` is used for training.

### train

```python
python train.py # train the model on 10x5 FJSP instances using SD2

# options (Validation instances of corresponding size should be prepared in ./data/data_train_vali/{data_source})
python train.py 	--n_j 10		# number of jobs for training/validation instances
			--n_m 5			# number of machines for training/validation instances
    			--data_source SD2	# data source (SD1 / SD2)
        		--data_suffix mix	# mode for SD2 data generation
            					# 'mix' is thedefault mode as defined in the paper
                				# 'nf' means 'no flexibility' (generating JSP data) 
        		--model_suffix demo	# annotations for the model
```

### evaluate

```python
python test_trained_model.py # evaluate the model trained on '10x5+mix' of SD2 using the testing instances of the same size using the greedy strategy

# options (Model files should be prepared in ./trained_network/{model_source})
python test_trained_model.py 	--data_source SD2	# source of testing instances
				--model_source SD2	# source of instances that the model trained on 
    				--test_data 10x5+mix	# list of instance names for testing
        			--test_model 10x5+mix	# list of model names for testing
            			--test_mode False	# whether using the sampling strategy
                		--sample_times 100	# set the number of sampling times
```

## Cite the paper

```
@ARTICLE{10246328,
  author={Wang, Runqing and Wang, Gang and Sun, Jian and Deng, Fang and Chen, Jie},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Flexible Job Shop Scheduling via Dual Attention Network-Based Reinforcement Learning}, 
  year={2023},
  volume={},
  number={},
  pages={1-12},
  doi={10.1109/TNNLS.2023.3306421}
}
```

## Reference

- https://github.com/songwenas12/fjsp-drl/
- https://github.com/zcaicaros/L2D
- https://github.com/google/or-tools
- https://github.com/Diego999/pyGAT


## Memoryæ•°æ®ç»“æ„
- ä¸“é—¨è®¾è®¡ç”¨äºPPOçš„è½¨è¿¹æ•°æ®å­˜å‚¨ä¸ç®¡ç†ã€‚
#### ä¸»è¦æ•°æ®ç»“æ„

|å±æ€§å|æ•°æ®ç±»å‹/ç»´åº¦|ä½œç”¨è¯´æ˜|
|---------|--------|--------|
|fea_j_seq	| [N, tensor[sz_b, N, 8]] |ä½œä¸šç‰¹å¾åºåˆ—|
|op_mask_seq|	[N, tensor[sz_b, N, 3]]	|æ“ä½œæ©ç ï¼Œç”¨äºçº¦æŸåˆæ³•åŠ¨ä½œ|
|fea_m_seq|	[N, tensor[sz_b, M, 6]]	|æœºå™¨ç‰¹å¾åºåˆ—|
|mch_mask_seq|	[N, tensor[sz_b, M, M]]	|æœºå™¨æ©ç ï¼Œè¡¨ç¤ºæœºå™¨çº¦æŸ|
|dynamic_pair_mask_seq	|[N, tensor[sz_b, J, M]]	|åŠ¨æ€æœºå™¨-ä½œä¸šé…å¯¹æ©ç |
|comp_idx_seq|	[N, tensor[sz_b, M, M, J]]|	ç«äº‰ç´¢å¼•ç‰¹å¾|
|candidate_seq|	[N, tensor[sz_b, J]]|	å€™é€‰ä½œä¸šåºåˆ—|
|fea_pairs_seq|	[N, tensor[sz_b, J]]|	ç‰¹å¾é…å¯¹ä¿¡æ¯|
|action_seq|	[N, tensor[sz_b]]|	åŠ¨ä½œç´¢å¼•|
|reward_seq|	[N, tensor[sz_b]]|	å¥–åŠ±å€¼|
|val_seq|	[N, tensor[sz_b]]|	çŠ¶æ€å€¼|
|done_seq|	[N, tensor[sz_b]]|	ç»“æŸæ ‡è®°ï¼Œç”¨äºPPOæ›´æ–°æ—¶ç»ˆæ­¢æ¡ä»¶|
|log_probs|	[N, tensor[sz_b]]|	åŠ¨ä½œæ¦‚ç‡çš„å¯¹æ•°å€¼|


#### ä¼˜åŠ¿ä¼°è®¡ï¼šget_gae_advantages

å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡é€šè¿‡æ—¶é—´å·®åˆ†è¯¯å·®é€’æ¨å¾—åˆ°ï¼Œå…¬å¼å¦‚ä¸‹ï¼š

$$
A_t^{\text{GAE}(\lambda)} = \sum_{l=0}^\infty (\gamma \lambda)^l \delta_{t+l}
$$
- å…¶ä¸­$\delta_t$æ˜¯æ—¶é—´å·®åˆ†è¯¯å·® 
$$ \delta_t = r_t + \gamma V(s_{t+1}) - V(s_t) $$
- $A_t^{\text{GAE}(\lambda)}$ åœ¨ ğœ† å‚æ•°æ§åˆ¶ä¸‹çš„ä¼˜åŠ¿ä¼°è®¡ã€‚
- Î³ æ˜¯æŠ˜æ‰£å› å­ï¼Œæ§åˆ¶æœªæ¥å¥–åŠ±çš„è¡°å‡ç¨‹åº¦ã€‚
- ğœ†æ˜¯å¹³æ»‘å‚æ•°ï¼Œå¹³è¡¡åå·®å’Œæ–¹å·®ã€‚
- ğ‘‰(ğ‘ ğ‘¡)æ˜¯çŠ¶æ€ $ğ‘ _ğ‘¡$çš„å€¼å‡½æ•°ä¼°è®¡ã€‚

ä¸ºäº†æ›´é«˜æ•ˆåœ°è®¡ç®—ï¼Œå¯ä»¥å°† GAE å†™æˆé€’æ¨å½¢å¼ï¼š

$$
A_t^{\text{GAE}(\lambda)} = \delta_t + \gamma \lambda A_{t+1}^{\text{GAE}(\lambda)}
$$
è¿™è¡¨ç¤ºä¼˜åŠ¿ä¼°è®¡å¯ä»¥é€šè¿‡æœªæ¥çš„ GAE å€¼ä¸å½“å‰æ—¶é—´å·®åˆ†è¯¯å·®é€æ­¥é€’æ¨è®¡ç®—å¾—åˆ°ã€‚

GAEæä¾›äº†å¹³æ»‘çš„ä¼˜åŠ¿ä¼°è®¡ï¼Œé€‚åº”äºé•¿æ—¶é—´æ­¥çš„ç­–ç•¥ä¼˜åŒ–ã€‚

## PPOä»£ç 

#### PPOçš„æ ¸å¿ƒç›®æ ‡ï¼š
ç¨³å®šåœ°æ›´æ–°ç­–ç•¥ï¼Œé™åˆ¶é‡è¦æ€§é‡‡æ ·æ¯”ç‡ï¼Œé˜²æ­¢ç­–ç•¥è¿‡å¤§å˜åŠ¨ã€‚
å¼•å…¥å€¼å‡½æ•°å’Œç†µæŸå¤±ï¼Œå¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ã€‚

#### Clipped Objectiveï¼š
é€šè¿‡ torch.clamp é™åˆ¶ç­–ç•¥æ›´æ–°å¹…åº¦ï¼Œé˜²æ­¢ç­–ç•¥â€œå´©åâ€ã€‚

#### GAEï¼ˆå¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ï¼‰ï¼š
æé«˜ä¼˜åŠ¿ä¼°è®¡çš„ç¨³å®šæ€§ï¼Œå‡å°‘é«˜æ–¹å·®çš„å½±å“ã€‚

#### è½¯æ›´æ–°ç­–ç•¥ï¼š
ä½¿ç”¨æƒé‡ tau ç¼“æ…¢æ›´æ–°æ—§ç­–ç•¥ç½‘ç»œï¼Œå¢å¼ºå­¦ä¹ ç¨³å®šæ€§ã€‚

## è®­ç»ƒè¿‡ç¨‹
DANTrainer ç±»è´Ÿè´£å°† PPO ç®—æ³•ä¸ FJSP ç¯å¢ƒç»“åˆï¼Œè¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚

#### è®­ç»ƒæµç¨‹
1. åˆå§‹åŒ– PPO ç®—æ³•å’Œ FJSP ç¯å¢ƒã€‚
2. ç¯å¢ƒäº¤äº’ï¼šé€šè¿‡ç­–ç•¥ç½‘ç»œé‡‡æ ·åŠ¨ä½œä¸ç¯å¢ƒäº¤äº’ï¼Œæ”¶é›†è½¨è¿¹æ•°æ®ã€‚
3. ç­–ç•¥æ›´æ–°ï¼šè°ƒç”¨ PPO æ›´æ–°ç½‘ç»œå‚æ•°ï¼Œä¼˜åŒ–ç­–ç•¥ä¸å€¼å‡½æ•°ã€‚
4. ç»“æœè®°å½•ï¼šç›‘æ§å¥–åŠ±ã€å·¥æœŸå’ŒæŸå¤±ï¼Œä¿å­˜æ¨¡å‹å‚æ•°ã€‚



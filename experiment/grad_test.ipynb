{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 3*a**3 - b**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:288\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    283\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly_inputs argument is deprecated and is ignored now \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(defaults to True). To accumulate gradient for other \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    285\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparts of the graph, please use torch.autograd.backward.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    287\u001b[0m grad_outputs_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_outputs, \u001b[38;5;28mlen\u001b[39m(t_outputs))\n\u001b[0;32m--> 288\u001b[0m grad_outputs_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_grads_batched\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    291\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:88\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 88\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     89\u001b[0m     new_grads\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mones_like(out, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format))\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "torch.autograd.grad(Q, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "external_grad = torch.tensor([1., 1.])\n",
    "Q.backward(gradient=external_grad)\n",
    "external_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "print(x.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x.grad_fn)  # we've created x ourselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x7f52126a5630>\n"
     ]
    }
   ],
   "source": [
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成的张量：\n",
      "[[[ 1  2  3]\n",
      "  [ 4  5  6]\n",
      "  [ 7  8  9]\n",
      "  [ 0 11  0]\n",
      "  [13 14 15]\n",
      "  [16 17 18]\n",
      "  [19 20 21]\n",
      "  [22 23 24]\n",
      "  [25 26 27]\n",
      "  [28 29 30]\n",
      "  [31 32 33]\n",
      "  [34 35 36]\n",
      "  [37 38 39]\n",
      "  [40 41 42]\n",
      "  [43 44 45]]\n",
      "\n",
      " [[46 47 48]\n",
      "  [49 50 51]\n",
      "  [52 53 54]\n",
      "  [55 56 57]\n",
      "  [58 59 60]\n",
      "  [ 0  0 63]\n",
      "  [64 65 66]\n",
      "  [67 68 69]\n",
      "  [70 71 72]\n",
      "  [73 74 75]\n",
      "  [76 77 78]\n",
      "  [79 80 81]\n",
      "  [82 83 84]\n",
      "  [85 86 87]\n",
      "  [88 89 90]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 假设选择的索引\n",
    "self_env_idxs = np.array([0, 1])\n",
    "chosen_job = np.array([3, 5])\n",
    "chosen_mch = np.array([1, 2])\n",
    "\n",
    "# 输入张量\n",
    "arr2 = np.array([[[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12], [13, 14, 15],\n",
    "                  [16, 17, 18], [19, 20, 21], [22, 23, 24], [25, 26, 27], [28, 29, 30],\n",
    "                  [31, 32, 33], [34, 35, 36], [37, 38, 39], [40, 41, 42], [43, 44, 45]],\n",
    "                 [[46, 47, 48], [49, 50, 51], [52, 53, 54], [55, 56, 57], [58, 59, 60],\n",
    "                  [61, 62, 63], [64, 65, 66], [67, 68, 69], [70, 71, 72], [73, 74, 75],\n",
    "                  [76, 77, 78], [79, 80, 81], [82, 83, 84], [85, 86, 87], [88, 89, 90]]])\n",
    "\n",
    "# 复制原始张量\n",
    "zero_tensor = np.copy(arr2)\n",
    "\n",
    "# 将指定位置修改为0，并填充相应的元素\n",
    "zero_tensor[self_env_idxs[:, np.newaxis], chosen_job[:, np.newaxis], :] = 0\n",
    "zero_tensor[self_env_idxs[:, np.newaxis], chosen_job[:, np.newaxis], chosen_mch[:, np.newaxis]] = \\\n",
    "    arr2[self_env_idxs[:, np.newaxis], chosen_job[:, np.newaxis], chosen_mch[:, np.newaxis]]\n",
    "\n",
    "print(\"生成的张量：\")\n",
    "print(zero_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每个环境的机器功耗：\n",
      "[[1.5015982 ]\n",
      " [1.22925678]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 给定的平均操作时间\n",
    "mch_mean_pt = np.array([[0.62626263, 0.67125803, 0.47785548],\n",
    "                        [0.42501942, 0.41792929, 0.61111111]])\n",
    "\n",
    "# 随机生成每个环境中每台机器的平均能耗（范围在0.5到1之间）\n",
    "mch_mean_energy = np.random.uniform(0.5, 1, size=mch_mean_pt.shape)\n",
    "\n",
    "# 计算每台机器的功耗\n",
    "mch_power_consumption = mch_mean_pt * mch_mean_energy\n",
    "\n",
    "# 计算每个环境的总功耗\n",
    "env_power_consumption = np.sum(mch_power_consumption, axis=1, keepdims=True)\n",
    "\n",
    "print(\"每个环境的机器功耗：\")\n",
    "print(env_power_consumption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每个环境的总待机时间：\n",
      "[0.73806071 0.52311109] [[0.19623284 0.19703245 0.12864908]\n",
      " [0.1549593  0.18843454 0.16935862]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 给定的最大完工时间\n",
    "max_endTime = np.array([1.41414141, 1.02020202])\n",
    "\n",
    "# 给定的平均操作时间\n",
    "mch_mean_pt = np.array([[0.62626263, 0.67125803, 0.47785548],\n",
    "                        [0.42501942, 0.41792929, 0.61111111]])\n",
    "\n",
    "# 随机生成每个环境中每台机器的待机功率（范围在0.1到0.2之间）\n",
    "mch_idle_power = np.random.uniform(0.1, 0.2, size=mch_mean_pt.shape)\n",
    "\n",
    "# 计算每个环境中所有机器的总待机时间\n",
    "env_idle_time = np.sum(mch_idle_power * max_endTime[:, np.newaxis], axis=1)\n",
    "\n",
    "print(\"每个环境的总待机时间：\")\n",
    "print(env_idle_time, mch_idle_power)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17 0.42 0.42\n"
     ]
    }
   ],
   "source": [
    "def calculate_dividends(total_value, product_prices):\n",
    "    # 计算第一个产品的总价值（打五折）\n",
    "    if len(product_prices) < 2: return \"\"\n",
    "    product_prices[0] = product_prices[0] * 0.5\n",
    "    product_prices[1] = 0\n",
    "    # 计算后面产品的总价值\n",
    "    remaining_value = sum(product_prices)\n",
    "    product_prices[1] = total_value - remaining_value\n",
    "    # 如果后面的产品总价值小于等于第一个产品的总价值\n",
    "    \n",
    "    dividend_ratios = [\"{:.2f}\".format(price / total_value) for price in product_prices]\n",
    "    return \" \".join(dividend_ratios)\n",
    "# 示例用法\n",
    "total_value = 30\n",
    "product_prices = [10, 30, 12.5]\n",
    "dividends = calculate_dividends(total_value, product_prices)\n",
    "print(dividends)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

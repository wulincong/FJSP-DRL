+ source /work/home/lxx_hzau/.bashrc
++ export CUDA_HOME=/usr/local/cuda-11.1/
++ CUDA_HOME=/usr/local/cuda-11.1/
++ export PATH=/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/.vscode-server/bin/af28b32d7e553898b2a91af498b1fb666fdebe0c/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
++ PATH=/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/.vscode-server/bin/af28b32d7e553898b2a91af498b1fb666fdebe0c/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
++ '[' -f /etc/bashrc ']'
++ . /etc/bashrc
+++ '[' '' ']'
+++ shopt -q login_shell
+++ '[' 5235 -gt 199 ']'
++++ /usr/bin/id -gn
++++ /usr/bin/id -un
+++ '[' ac6owqc808 = lxx_hzau ']'
+++ umask 022
+++ SHELL=/bin/bash
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/256term.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/256term.sh
++++ local256=truecolor
++++ '[' -n truecolor ']'
++++ case "$TERM" in
++++ export TERM
++++ '[' -n '' ']'
++++ unset local256
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/abrt-console-notification.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/abrt-console-notification.sh
++++ tty -s
++++ return 0
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/bash_completion.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/bash_completion.sh
++++ '[' -z '4.2.46(2)-release' -o -z '' -o -n '' ']'
++++ return
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/clusconf-env.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/clusconf-env.sh
++++ export CLUSCONF_HOME=/opt/clusconf
++++ CLUSCONF_HOME=/opt/clusconf
++++ export PATH=/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/.vscode-server/bin/af28b32d7e553898b2a91af498b1fb666fdebe0c/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
++++ PATH=/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/.vscode-server/bin/af28b32d7e553898b2a91af498b1fb666fdebe0c/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
++++ export PATH=/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/.vscode-server/bin/af28b32d7e553898b2a91af498b1fb666fdebe0c/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
++++ PATH=/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/.vscode-server/bin/af28b32d7e553898b2a91af498b1fb666fdebe0c/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
++++ export NFSCONF=/opt/clusconf/etc/nfs.cfg
++++ NFSCONF=/opt/clusconf/etc/nfs.cfg
++++ export IPMICONF=/opt/clusconf/etc/ipmi.cfg
++++ IPMICONF=/opt/clusconf/etc/ipmi.cfg
++++ export AUTOCLUSCONF=/opt/clusconf/etc/autoconf.cfg
++++ AUTOCLUSCONF=/opt/clusconf/etc/autoconf.cfg
++++ export STARTWAITTIME=300
++++ STARTWAITTIME=300
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/colorgrep.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/colorgrep.sh
++++ /usr/libexec/grepconf.sh -c
++++ alias 'grep=grep --color=auto'
++++ alias 'egrep=egrep --color=auto'
++++ alias 'fgrep=fgrep --color=auto'
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/colorls.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/colorls.sh
++++ '[' '!' -t 0 ']'
++++ return
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/dawning.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/dawning.sh
++++ export GRIDVIEW_HOME=/opt/gridview
++++ GRIDVIEW_HOME=/opt/gridview
++++ export MUNGE_HOME=/opt/gridview/munge
++++ MUNGE_HOME=/opt/gridview/munge
++++ export PATH=/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/.vscode-server/bin/af28b32d7e553898b2a91af498b1fb666fdebe0c/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
++++ PATH=/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/.vscode-server/bin/af28b32d7e553898b2a91af498b1fb666fdebe0c/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
++++ export LD_LIBRARY_PATH=/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/sharp/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/lib:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:
++++ LD_LIBRARY_PATH=/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/sharp/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/lib:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:
++++ export MANPATH=/opt/gridview/munge/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/share/man:/opt/rh/devtoolset-7/root/usr/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:
++++ MANPATH=/opt/gridview/munge/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/share/man:/opt/rh/devtoolset-7/root/usr/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:
++++ export C_INCLUDE_PATH=/opt/gridview/munge/include:/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/rh/devtoolset-7/root/usr/include/c++/7:/opt/rh/devtoolset-7/root/usr/include/c++/7/x86_64-redhat-linux:/opt/gridview/slurm/include:/opt/gridview/munge/include:
++++ C_INCLUDE_PATH=/opt/gridview/munge/include:/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/rh/devtoolset-7/root/usr/include/c++/7:/opt/rh/devtoolset-7/root/usr/include/c++/7/x86_64-redhat-linux:/opt/gridview/slurm/include:/opt/gridview/munge/include:
++++ export SLURM_HOME=/opt/gridview/slurm
++++ SLURM_HOME=/opt/gridview/slurm
++++ export PATH=/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/.vscode-server/bin/af28b32d7e553898b2a91af498b1fb666fdebe0c/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
++++ PATH=/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/.vscode-server/bin/af28b32d7e553898b2a91af498b1fb666fdebe0c/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
++++ export LD_LIBRARY_PATH=/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/sharp/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/lib:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:
++++ LD_LIBRARY_PATH=/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/sharp/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/lib:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:
++++ export MANPATH=/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/share/man:/opt/rh/devtoolset-7/root/usr/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:
++++ MANPATH=/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/share/man:/opt/rh/devtoolset-7/root/usr/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:
++++ export C_INCLUDE_PATH=/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/rh/devtoolset-7/root/usr/include/c++/7:/opt/rh/devtoolset-7/root/usr/include/c++/7/x86_64-redhat-linux:/opt/gridview/slurm/include:/opt/gridview/munge/include:
++++ C_INCLUDE_PATH=/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/rh/devtoolset-7/root/usr/include/c++/7:/opt/rh/devtoolset-7/root/usr/include/c++/7/x86_64-redhat-linux:/opt/gridview/slurm/include:/opt/gridview/munge/include:
++++ export SLURM_PMIX_DIRECT_CONN=true
++++ SLURM_PMIX_DIRECT_CONN=true
++++ export SLURM_PMIX_DIRECT_CONN_UCX=true
++++ SLURM_PMIX_DIRECT_CONN_UCX=true
++++ export SLURM_PMIX_DIRECT_CONN_EARLY=true
++++ SLURM_PMIX_DIRECT_CONN_EARLY=true
++++ export SLURM_PMIX_TIMEOUT=3000
++++ SLURM_PMIX_TIMEOUT=3000
++++ '[' lxx_hzau '!=' root ']'
++++ export SQUEUE_USERS=lxx_hzau
++++ SQUEUE_USERS=lxx_hzau
++++ export SCONTROL_JOB_USERS=lxx_hzau
++++ SCONTROL_JOB_USERS=lxx_hzau
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/flatpak.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/flatpak.sh
++++ '[' /exports/share:/var/lib/flatpak/exports/share:/usr/local/share:/usr/share = /work/home/lxx_hzau/.local/share/flatpak/exports/share:/var/lib/flatpak/exports/share:/usr/local/share:/usr/share ']'
++++ export XDG_DATA_DIRS
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/kde.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/kde.sh
++++ '[' -z '' ']'
++++ KDEDIRS=/usr
++++ export KDEDIRS
++++ '[' -z '' ']'
++++ grep --color=auto -qs '^PRELINKING=yes' /etc/sysconfig/prelink
++++ for libdir in /usr/lib64 /usr/lib
++++ '[' -n '' ']'
++++ QT_PLUGIN_PATH=/usr/lib64/kde4/plugins
++++ export QT_PLUGIN_PATH
++++ for libdir in /usr/lib64 /usr/lib
++++ '[' -n /usr/lib64/kde4/plugins ']'
++++ echo /usr/lib64/kde4/plugins
++++ /bin/grep -q /usr/lib/kde4/plugins
++++ QT_PLUGIN_PATH=/usr/lib64/kde4/plugins:/usr/lib/kde4/plugins
++++ export QT_PLUGIN_PATH
++++ '[' '!' -d /work/home/lxx_hzau/.local/share ']'
++++ unset libdir
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/lang.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/lang.sh
++++ sourced=0
++++ '[' -n en_US.UTF-8 ']'
++++ saved_lang=en_US.UTF-8
++++ '[' -f /work/home/lxx_hzau/.i18n ']'
++++ LANG=en_US.UTF-8
++++ unset saved_lang
++++ '[' 0 = 1 ']'
++++ unset sourced
++++ unset langfile
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/less.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/less.sh
++++ '[' -x /usr/bin/lesspipe.sh ']'
++++ export 'LESSOPEN=||/usr/bin/lesspipe.sh %s'
++++ LESSOPEN='||/usr/bin/lesspipe.sh %s'
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/modules.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/modules.sh
++++++ /bin/ps -p 22356 -ocomm=
+++++ /bin/basename slurm_script
++++ shell=slurm_script
++++ '[' -f /usr/share/Modules/init/slurm_script ']'
++++ . /usr/share/Modules/init/sh
+++++ MODULESHOME=/usr/share/Modules
+++++ export MODULESHOME
+++++ '[' compiler/devtoolset/7.3.1:mpi/hpcx/gcc-7.3.1 = '' ']'
+++++ '[' /public/software/modules/base:/public/software/modules/apps = '' ']'
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/mpi-selector.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/mpi-selector.sh
++++ mpi_selector_dir=/var/mpi-selector/data
++++ mpi_selector_homefile=/work/home/lxx_hzau/.mpi-selector
++++ mpi_selector_sysfile=/etc/sysconfig/mpi-selector
++++ mpi_selection=
++++ test -f /work/home/lxx_hzau/.mpi-selector
++++ test -f /etc/sysconfig/mpi-selector
++++ test '' '!=' '' -a -f /var/mpi-selector/data/.sh
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/PackageKit.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/PackageKit.sh
++++ [[ -n '' ]]
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/perl-homedir.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/perl-homedir.sh
++++ PERL_HOMEDIR=1
++++ '[' -f /etc/sysconfig/perl-homedir ']'
++++ '[' -f /work/home/lxx_hzau/.perl-homedir ']'
++++ alias 'perlll=eval `perl -Mlocal::lib`'
++++ '[' x1 = x1 ']'
+++++ perl -Mlocal::lib
++++ eval export 'PERL_LOCAL_LIB_ROOT="$PERL_LOCAL_LIB_ROOT:/work/home/lxx_hzau/perl5";' export 'PERL_MB_OPT="--install_base' '/work/home/lxx_hzau/perl5";' export 'PERL_MM_OPT="INSTALL_BASE=/work/home/lxx_hzau/perl5";' export 'PERL5LIB="/work/home/lxx_hzau/perl5/lib/perl5:$PERL5LIB";' export 'PATH="/work/home/lxx_hzau/perl5/bin:$PATH";'
+++++ export PERL_LOCAL_LIB_ROOT=:/work/home/lxx_hzau/perl5
+++++ PERL_LOCAL_LIB_ROOT=:/work/home/lxx_hzau/perl5
+++++ export 'PERL_MB_OPT=--install_base /work/home/lxx_hzau/perl5'
+++++ PERL_MB_OPT='--install_base /work/home/lxx_hzau/perl5'
+++++ export PERL_MM_OPT=INSTALL_BASE=/work/home/lxx_hzau/perl5
+++++ PERL_MM_OPT=INSTALL_BASE=/work/home/lxx_hzau/perl5
+++++ export PERL5LIB=/work/home/lxx_hzau/perl5/lib/perl5:/opt/rh/devtoolset-7/root/usr/lib64/perl5/vendor_perl:/opt/rh/devtoolset-7/root/usr/share/perl5/vendor_perl
+++++ PERL5LIB=/work/home/lxx_hzau/perl5/lib/perl5:/opt/rh/devtoolset-7/root/usr/lib64/perl5/vendor_perl:/opt/rh/devtoolset-7/root/usr/share/perl5/vendor_perl
+++++ export PATH=/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/.vscode-server/bin/af28b32d7e553898b2a91af498b1fb666fdebe0c/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
+++++ PATH=/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/.vscode-server/bin/af28b32d7e553898b2a91af498b1fb666fdebe0c/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/qt-graphicssystem.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/qt-graphicssystem.sh
++++ '[' -z 1 -a -z '' ']'
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/qt.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/qt.sh
++++ '[' -z /usr/lib64/qt-3.3 ']'
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/rocm-gcc-mpi-default.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/rocm-gcc-mpi-default.sh
++++ [[ lxx_hzau =~ root|xhadmin ]]
++++ module use /public/software/modules/apps
+++++ /usr/bin/modulecmd sh use /public/software/modules/apps
++++ eval
++++ module use /public/software/modules/base
+++++ /usr/bin/modulecmd sh use /public/software/modules/base
++++ eval
++++ module unuse /opt/hpc/software/modules
+++++ /usr/bin/modulecmd sh unuse /opt/hpc/software/modules
++++ eval
++++ module load compiler/devtoolset/7.3.1
+++++ /usr/bin/modulecmd sh load compiler/devtoolset/7.3.1
++++ eval
++++ module load mpi/hpcx/gcc-7.3.1
+++++ /usr/bin/modulecmd sh load mpi/hpcx/gcc-7.3.1
++++ eval
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/vim.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/vim.sh
++++ '[' -n '4.2.46(2)-release' -o -n '' -o -n '' ']'
++++ '[' -x /usr/bin/id ']'
+++++ /usr/bin/id -u
++++ ID=5235
++++ '[' -n 5235 -a 5235 -le 200 ']'
++++ alias vi
++++ alias vi=vim
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/vte.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/vte.sh
++++ '[' -n '4.2.46(2)-release' -o -n '' ']'
++++ [[ hxB == *i* ]]
++++ return 0
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/which2.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/which2.sh
++++ alias 'which=alias | /usr/bin/which --tty-only --read-alias --show-dot --show-tilde'
+++ unset i
+++ unset -f pathmunge
+++ /work/home/lxx_hzau/miniconda3/bin/conda shell.bash hook
++ __conda_setup='export CONDA_EXE='\''/work/home/lxx_hzau/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/work/home/lxx_hzau/miniconda3/bin/python'\''

# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
__conda_exe() (
    "$CONDA_EXE" $_CE_M $_CE_CONDA "$@"
)

__conda_hashr() {
    if [ -n "${ZSH_VERSION:+x}" ]; then
        \rehash
    elif [ -n "${POSH_VERSION:+x}" ]; then
        :  # pass
    else
        \hash -r
    fi
}

__conda_activate() {
    if [ -n "${CONDA_PS1_BACKUP:+x}" ]; then
        # Handle transition from shell activated with conda <= 4.3 to a subsequent activation
        # after conda updated to >= 4.4. See issue #6173.
        PS1="$CONDA_PS1_BACKUP"
        \unset CONDA_PS1_BACKUP
    fi
    \local ask_conda
    ask_conda="$(PS1="${PS1:-}" __conda_exe shell.posix "$@")" || \return
    \eval "$ask_conda"
    __conda_hashr
}

__conda_reactivate() {
    \local ask_conda
    ask_conda="$(PS1="${PS1:-}" __conda_exe shell.posix reactivate)" || \return
    \eval "$ask_conda"
    __conda_hashr
}

conda() {
    \local cmd="${1-__missing__}"
    case "$cmd" in
        activate|deactivate)
            __conda_activate "$@"
            ;;
        install|update|upgrade|remove|uninstall)
            __conda_exe "$@" || \return
            __conda_reactivate
            ;;
        *)
            __conda_exe "$@"
            ;;
    esac
}

if [ -z "${CONDA_SHLVL+x}" ]; then
    \export CONDA_SHLVL=0
    # In dev-mode CONDA_EXE is python.exe and on Windows
    # it is in a different relative location to condabin.
    if [ -n "${_CE_CONDA:+x}" ] && [ -n "${WINDIR+x}" ]; then
        PATH="$(\dirname "$CONDA_EXE")/condabin${PATH:+":${PATH}"}"
    else
        PATH="$(\dirname "$(\dirname "$CONDA_EXE")")/condabin${PATH:+":${PATH}"}"
    fi
    \export PATH

    # We'\''re not allowing PS1 to be unbound. It must at least be set.
    # However, we'\''re not exporting it, which can cause problems when starting a second shell
    # via a first shell (i.e. starting zsh from bash).
    if [ -z "${PS1+x}" ]; then
        PS1=
    fi
fi

conda activate base'
++ '[' 0 -eq 0 ']'
++ eval 'export CONDA_EXE='\''/work/home/lxx_hzau/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/work/home/lxx_hzau/miniconda3/bin/python'\''

# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
__conda_exe() (
    "$CONDA_EXE" $_CE_M $_CE_CONDA "$@"
)

__conda_hashr() {
    if [ -n "${ZSH_VERSION:+x}" ]; then
        \rehash
    elif [ -n "${POSH_VERSION:+x}" ]; then
        :  # pass
    else
        \hash -r
    fi
}

__conda_activate() {
    if [ -n "${CONDA_PS1_BACKUP:+x}" ]; then
        # Handle transition from shell activated with conda <= 4.3 to a subsequent activation
        # after conda updated to >= 4.4. See issue #6173.
        PS1="$CONDA_PS1_BACKUP"
        \unset CONDA_PS1_BACKUP
    fi
    \local ask_conda
    ask_conda="$(PS1="${PS1:-}" __conda_exe shell.posix "$@")" || \return
    \eval "$ask_conda"
    __conda_hashr
}

__conda_reactivate() {
    \local ask_conda
    ask_conda="$(PS1="${PS1:-}" __conda_exe shell.posix reactivate)" || \return
    \eval "$ask_conda"
    __conda_hashr
}

conda() {
    \local cmd="${1-__missing__}"
    case "$cmd" in
        activate|deactivate)
            __conda_activate "$@"
            ;;
        install|update|upgrade|remove|uninstall)
            __conda_exe "$@" || \return
            __conda_reactivate
            ;;
        *)
            __conda_exe "$@"
            ;;
    esac
}

if [ -z "${CONDA_SHLVL+x}" ]; then
    \export CONDA_SHLVL=0
    # In dev-mode CONDA_EXE is python.exe and on Windows
    # it is in a different relative location to condabin.
    if [ -n "${_CE_CONDA:+x}" ] && [ -n "${WINDIR+x}" ]; then
        PATH="$(\dirname "$CONDA_EXE")/condabin${PATH:+":${PATH}"}"
    else
        PATH="$(\dirname "$(\dirname "$CONDA_EXE")")/condabin${PATH:+":${PATH}"}"
    fi
    \export PATH

    # We'\''re not allowing PS1 to be unbound. It must at least be set.
    # However, we'\''re not exporting it, which can cause problems when starting a second shell
    # via a first shell (i.e. starting zsh from bash).
    if [ -z "${PS1+x}" ]; then
        PS1=
    fi
fi

conda activate base'
+++ export CONDA_EXE=/work/home/lxx_hzau/miniconda3/bin/conda
+++ CONDA_EXE=/work/home/lxx_hzau/miniconda3/bin/conda
+++ export _CE_M=
+++ _CE_M=
+++ export _CE_CONDA=
+++ _CE_CONDA=
+++ export CONDA_PYTHON_EXE=/work/home/lxx_hzau/miniconda3/bin/python
+++ CONDA_PYTHON_EXE=/work/home/lxx_hzau/miniconda3/bin/python
+++ '[' -z x ']'
+++ conda activate base
+++ local cmd=activate
+++ case "$cmd" in
+++ __conda_activate activate base
+++ '[' -n '' ']'
+++ local ask_conda
++++ PS1=
++++ __conda_exe shell.posix activate base
++++ /work/home/lxx_hzau/miniconda3/bin/conda shell.posix activate base
+++ ask_conda='PS1='\''(base) '\''
export PATH='\''/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/.vscode-server/bin/af28b32d7e553898b2a91af498b1fb666fdebe0c/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin'\''
export CONDA_SHLVL='\''1'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export PYTHONPATH='\''/work/home/lxx_hzau/project/FJSP-DRL-main'\'''
+++ eval 'PS1='\''(base) '\''
export PATH='\''/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/.vscode-server/bin/af28b32d7e553898b2a91af498b1fb666fdebe0c/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin'\''
export CONDA_SHLVL='\''1'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export PYTHONPATH='\''/work/home/lxx_hzau/project/FJSP-DRL-main'\'''
++++ PS1='(base) '
++++ export PATH=/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/.vscode-server/bin/af28b32d7e553898b2a91af498b1fb666fdebe0c/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin
++++ PATH=/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/.vscode-server/bin/af28b32d7e553898b2a91af498b1fb666fdebe0c/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin
++++ export CONDA_SHLVL=1
++++ CONDA_SHLVL=1
++++ export 'CONDA_PROMPT_MODIFIER=(base) '
++++ CONDA_PROMPT_MODIFIER='(base) '
++++ export PYTHONPATH=/work/home/lxx_hzau/project/FJSP-DRL-main
++++ PYTHONPATH=/work/home/lxx_hzau/project/FJSP-DRL-main
+++ __conda_hashr
+++ '[' -n '' ']'
+++ '[' -n '' ']'
+++ hash -r
++ unset __conda_setup
++ export PATH=/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/.vscode-server/bin/af28b32d7e553898b2a91af498b1fb666fdebe0c/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin
++ PATH=/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/.vscode-server/bin/af28b32d7e553898b2a91af498b1fb666fdebe0c/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin
+ module load nvidia/cuda/11.6
++ /usr/bin/modulecmd sh load nvidia/cuda/11.6
+ eval CUDA_HOME=/public/software/compiler/nvidia/cuda/11.6.0:/usr/local/cuda-11.1/ ';export' 'CUDA_HOME;CUDA_ROOT=/public/software/compiler/nvidia/cuda/11.6.0' ';export' 'CUDA_ROOT;INCLUDE=/public/software/compiler/nvidia/cuda/11.6.0/include:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/include:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/include:/opt/hpc/software/mpi/hpcx/v2.11.0/sharp/include:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/include' ';export' 'INCLUDE;LD_LIBRARY_PATH=/public/software/compiler/nvidia/cuda/11.6.0/lib64:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/sharp/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/lib:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:' ';export' 'LD_LIBRARY_PATH;LOADEDMODULES=compiler/devtoolset/7.3.1:mpi/hpcx/gcc-7.3.1:nvidia/cuda/11.6' ';export' 'LOADEDMODULES;PATH=/public/software/compiler/nvidia/cuda/11.6.0/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/.vscode-server/bin/af28b32d7e553898b2a91af498b1fb666fdebe0c/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin' ';export' 'PATH;_LMFILES_=/public/software/modules/base/compiler/devtoolset/7.3.1:/public/software/modules/base/mpi/hpcx/gcc-7.3.1:/public/software/modules/base/nvidia/cuda/11.6' ';export' '_LMFILES_;'
++ CUDA_HOME=/public/software/compiler/nvidia/cuda/11.6.0:/usr/local/cuda-11.1/
++ export CUDA_HOME
++ CUDA_ROOT=/public/software/compiler/nvidia/cuda/11.6.0
++ export CUDA_ROOT
++ INCLUDE=/public/software/compiler/nvidia/cuda/11.6.0/include:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/include:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/include:/opt/hpc/software/mpi/hpcx/v2.11.0/sharp/include:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/include
++ export INCLUDE
++ LD_LIBRARY_PATH=/public/software/compiler/nvidia/cuda/11.6.0/lib64:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/sharp/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/lib:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:
++ export LD_LIBRARY_PATH
++ LOADEDMODULES=compiler/devtoolset/7.3.1:mpi/hpcx/gcc-7.3.1:nvidia/cuda/11.6
++ export LOADEDMODULES
++ PATH=/public/software/compiler/nvidia/cuda/11.6.0/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/.vscode-server/bin/af28b32d7e553898b2a91af498b1fb666fdebe0c/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin
++ export PATH
++ _LMFILES_=/public/software/modules/base/compiler/devtoolset/7.3.1:/public/software/modules/base/mpi/hpcx/gcc-7.3.1:/public/software/modules/base/nvidia/cuda/11.6
++ export _LMFILES_
+ conda activate RL-torch
+ local cmd=activate
+ case "$cmd" in
+ __conda_activate activate RL-torch
+ '[' -n '' ']'
+ local ask_conda
++ PS1='(base) '
++ __conda_exe shell.posix activate RL-torch
++ /work/home/lxx_hzau/miniconda3/bin/conda shell.posix activate RL-torch
+ ask_conda='PS1='\''(RL-torch) '\''
export PATH='\''/public/software/compiler/nvidia/cuda/11.6.0/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/.vscode-server/bin/af28b32d7e553898b2a91af498b1fb666fdebe0c/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/envs/RL-torch/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin'\''
export CONDA_PREFIX='\''/work/home/lxx_hzau/miniconda3/envs/RL-torch'\''
export CONDA_SHLVL='\''2'\''
export CONDA_DEFAULT_ENV='\''RL-torch'\''
export CONDA_PROMPT_MODIFIER='\''(RL-torch) '\''
export PYTHONPATH='\''/work/home/lxx_hzau/project/FJSP-DRL-main'\''
export CONDA_PREFIX_1='\''/work/home/lxx_hzau/miniconda3'\''
export CONDA_EXE='\''/work/home/lxx_hzau/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/work/home/lxx_hzau/miniconda3/bin/python'\'''
+ eval 'PS1='\''(RL-torch) '\''
export PATH='\''/public/software/compiler/nvidia/cuda/11.6.0/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/.vscode-server/bin/af28b32d7e553898b2a91af498b1fb666fdebe0c/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/envs/RL-torch/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin'\''
export CONDA_PREFIX='\''/work/home/lxx_hzau/miniconda3/envs/RL-torch'\''
export CONDA_SHLVL='\''2'\''
export CONDA_DEFAULT_ENV='\''RL-torch'\''
export CONDA_PROMPT_MODIFIER='\''(RL-torch) '\''
export PYTHONPATH='\''/work/home/lxx_hzau/project/FJSP-DRL-main'\''
export CONDA_PREFIX_1='\''/work/home/lxx_hzau/miniconda3'\''
export CONDA_EXE='\''/work/home/lxx_hzau/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/work/home/lxx_hzau/miniconda3/bin/python'\'''
++ PS1='(RL-torch) '
++ export PATH=/public/software/compiler/nvidia/cuda/11.6.0/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/.vscode-server/bin/af28b32d7e553898b2a91af498b1fb666fdebe0c/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/envs/RL-torch/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin
++ PATH=/public/software/compiler/nvidia/cuda/11.6.0/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/.vscode-server/bin/af28b32d7e553898b2a91af498b1fb666fdebe0c/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/envs/RL-torch/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin
++ export CONDA_PREFIX=/work/home/lxx_hzau/miniconda3/envs/RL-torch
++ CONDA_PREFIX=/work/home/lxx_hzau/miniconda3/envs/RL-torch
++ export CONDA_SHLVL=2
++ CONDA_SHLVL=2
++ export CONDA_DEFAULT_ENV=RL-torch
++ CONDA_DEFAULT_ENV=RL-torch
++ export 'CONDA_PROMPT_MODIFIER=(RL-torch) '
++ CONDA_PROMPT_MODIFIER='(RL-torch) '
++ export PYTHONPATH=/work/home/lxx_hzau/project/FJSP-DRL-main
++ PYTHONPATH=/work/home/lxx_hzau/project/FJSP-DRL-main
++ export CONDA_PREFIX_1=/work/home/lxx_hzau/miniconda3
++ CONDA_PREFIX_1=/work/home/lxx_hzau/miniconda3
++ export CONDA_EXE=/work/home/lxx_hzau/miniconda3/bin/conda
++ CONDA_EXE=/work/home/lxx_hzau/miniconda3/bin/conda
++ export _CE_M=
++ _CE_M=
++ export _CE_CONDA=
++ _CE_CONDA=
++ export CONDA_PYTHON_EXE=/work/home/lxx_hzau/miniconda3/bin/python
++ CONDA_PYTHON_EXE=/work/home/lxx_hzau/miniconda3/bin/python
+ __conda_hashr
+ '[' -n '' ']'
+ '[' -n '' ']'
+ hash -r
+ t=train
+ ADAPT_NUMS=5
+ TEST_DIR=./test_script
+ exp=exp11
+ echo exp11
exp11
+ echo MAML
MAML
+ logdir_maml=./runs/exp11_maml
+ logdir=./runs/exp11
+ hidden_dim_actor=64
+ hidden_dim_critic=64
+ num_mlp_layers_actor=3
+ num_mlp_layers_critic=3
+ num_envs=5
+ meta_iterations=1000
+ max_updates_maml=1000
+ model_suffix=exp11_1000_64_3
+ n_j_options='15 15 15'
+ n_m_options='5  7  9  10'
+ num_tasks=6
+ max_updates_finetune=100
+ lr=0.003
+ data='15,5 15,7 15,9 15,10'
+ echo exp2 maml15x5 SD2
exp2 maml15x5 SD2
+ python train/multi_task_maml_exp11.py --logdir ./runs/exp11_maml --model_suffix exp11_1000_64_3 --maml_model True --meta_iterations 1000 --num_tasks 6 --max_updates 1000 --num_envs 5 --hidden_dim_actor 64 --hidden_dim_critic 64 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --n_j_options 15 15 15 --n_m_options 5 7 9 10
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/numpy/core/_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice
  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/numpy/core/_methods.py:163: RuntimeWarning: invalid value encountered in divide
  arrmean = um.true_divide(arrmean, div, out=arrmean,
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/numpy/core/_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
vali_data = ./data/data_train_vali/SD2/10x5+mix
save model name:  maml+exp11_1000_64_3
self.n_js [15, 15, 15]
[5, 7, 9, 10]
[(15, 9), (15, 10), (15, 5), (15, 10), (15, 9), (15, 5)]
[819.8, 915.4, 930.8, 985.0, 960.4, 795.2]
Episode 1	 reward: -7.95	 Mean_loss: 2.67753410,  training time: 21.85
[920.8, 925.8, 857.4, 927.6, 948.4, 824.6]
Episode 2	 reward: -8.44	 Mean_loss: 2.99389362,  training time: 20.75
[925.6, 911.4, 860.6, 957.0, 943.2, 804.6]
Episode 3	 reward: -8.51	 Mean_loss: 2.62393832,  training time: 20.82
[886.2, 986.8, 837.8, 972.8, 915.2, 829.6]
Episode 4	 reward: -8.25	 Mean_loss: 2.97434568,  training time: 20.76
[858.0, 913.4, 904.2, 939.6, 876.4, 851.0]
Episode 5	 reward: -8.19	 Mean_loss: 3.05974603,  training time: 20.74
[860.6, 945.2, 894.8, 956.6, 934.0, 852.4]
Episode 6	 reward: -8.29	 Mean_loss: 2.85191202,  training time: 20.74
[936.6, 958.8, 881.4, 938.4, 892.8, 801.2]
Episode 7	 reward: -7.76	 Mean_loss: 2.43856645,  training time: 21.09
[1003.0, 916.2, 852.4, 959.8, 906.0, 780.8]
Episode 8	 reward: -8.89	 Mean_loss: 2.18288469,  training time: 21.17
[866.2, 969.2, 866.6, 929.8, 911.8, 802.0]
Episode 9	 reward: -8.40	 Mean_loss: 2.63514495,  training time: 21.14
[887.8, 943.8, 842.8, 904.2, 897.6, 802.8]
Episode 10	 reward: -8.93	 Mean_loss: 2.57412505,  training time: 21.13
[879.0, 958.2, 793.2, 1013.2, 893.6, 814.0]
Episode 11	 reward: -8.49	 Mean_loss: 2.57968688,  training time: 20.87
[895.4, 941.4, 893.8, 961.0, 990.4, 822.0]
Episode 12	 reward: -8.10	 Mean_loss: 2.50467324,  training time: 20.72
[875.2, 900.4, 811.8, 982.6, 871.2, 817.0]
Episode 13	 reward: -8.21	 Mean_loss: 2.77858973,  training time: 20.70
[897.6, 914.2, 883.2, 900.0, 907.8, 853.4]
Episode 14	 reward: -9.03	 Mean_loss: 2.91801000,  training time: 20.86
[878.2, 906.8, 896.4, 998.4, 924.6, 795.6]
Episode 15	 reward: -9.00	 Mean_loss: 2.40979576,  training time: 20.72
[947.0, 918.8, 849.2, 954.6, 861.6, 839.8]
Episode 16	 reward: -8.67	 Mean_loss: 2.79683948,  training time: 20.75
[905.6, 940.0, 864.0, 983.2, 881.4, 847.8]
Episode 17	 reward: -8.55	 Mean_loss: 3.00202298,  training time: 20.70
[863.6, 888.8, 895.0, 979.6, 910.6, 828.8]
Episode 18	 reward: -7.89	 Mean_loss: 2.74533272,  training time: 20.71
[945.6, 924.2, 854.4, 974.0, 994.6, 815.8]
Episode 19	 reward: -8.45	 Mean_loss: 2.62396812,  training time: 20.76
[904.2, 916.8, 864.2, 925.8, 826.2, 852.4]
Episode 20	 reward: -8.51	 Mean_loss: 3.11600518,  training time: 20.70
[(15, 9), (15, 7), (15, 10), (15, 7), (15, 10), (15, 10)]
[973.4, 890.4, 931.4, 876.0, 957.6, 863.4]
Episode 21	 reward: -9.29	 Mean_loss: 0.95450091,  training time: 22.98
[920.0, 931.2, 915.6, 881.8, 893.8, 1004.0]
Episode 22	 reward: -9.26	 Mean_loss: 1.67306221,  training time: 22.91
[970.4, 878.0, 899.0, 906.8, 897.6, 825.0]
Episode 23	 reward: -9.19	 Mean_loss: 0.81106675,  training time: 22.96
[904.8, 840.4, 939.2, 930.2, 978.0, 876.2]
Episode 24	 reward: -9.09	 Mean_loss: 0.99029458,  training time: 22.91
[860.2, 887.6, 887.0, 889.2, 900.0, 875.4]
Episode 25	 reward: -8.84	 Mean_loss: 1.01886415,  training time: 22.90
[932.0, 831.6, 909.6, 956.0, 920.8, 902.2]
Episode 26	 reward: -8.72	 Mean_loss: 1.07987547,  training time: 22.90
[920.2, 854.6, 931.4, 878.2, 904.2, 859.8]
Episode 27	 reward: -9.28	 Mean_loss: 0.91100520,  training time: 23.05
[957.4, 840.8, 970.6, 902.4, 909.2, 948.2]
Episode 28	 reward: -9.09	 Mean_loss: 1.28410351,  training time: 22.92
[940.0, 894.0, 927.4, 908.2, 858.2, 876.8]
Episode 29	 reward: -8.98	 Mean_loss: 0.98725307,  training time: 22.91
[932.8, 851.2, 893.2, 946.4, 932.4, 961.0]
Episode 30	 reward: -9.57	 Mean_loss: 1.42933059,  training time: 22.90
[890.2, 873.2, 871.4, 910.6, 906.2, 924.8]
Episode 31	 reward: -9.87	 Mean_loss: 1.26499856,  training time: 22.92
[882.8, 837.4, 952.8, 878.8, 934.2, 906.8]
Episode 32	 reward: -9.20	 Mean_loss: 1.11816001,  training time: 22.94
[973.2, 837.8, 906.6, 955.0, 917.8, 878.2]
Episode 33	 reward: -9.42	 Mean_loss: 0.96646905,  training time: 23.07
[931.6, 834.8, 940.4, 874.8, 900.2, 907.0]
Episode 34	 reward: -8.60	 Mean_loss: 1.01530826,  training time: 22.92
[949.6, 837.6, 932.2, 875.0, 955.8, 885.6]
Episode 35	 reward: -9.00	 Mean_loss: 0.98733103,  training time: 22.91
[899.4, 838.0, 912.6, 922.0, 886.4, 864.2]
Episode 36	 reward: -9.44	 Mean_loss: 0.93055731,  training time: 22.91
[911.0, 861.0, 927.6, 895.8, 927.0, 861.2]
Episode 37	 reward: -9.09	 Mean_loss: 0.91031361,  training time: 22.92
[873.8, 876.6, 886.8, 856.4, 901.6, 887.2]
Episode 38	 reward: -9.12	 Mean_loss: 0.98416215,  training time: 22.92
[952.2, 845.8, 945.0, 913.2, 924.0, 936.8]
Episode 39	 reward: -8.94	 Mean_loss: 1.19265866,  training time: 23.00
[896.6, 837.8, 877.0, 948.2, 880.8, 905.0]
Episode 40	 reward: -9.28	 Mean_loss: 1.02994120,  training time: 23.06
[(15, 5), (15, 9), (15, 7), (15, 5), (15, 9), (15, 9)]
[836.0, 914.8, 866.0, 818.6, 895.0, 927.0]
Episode 41	 reward: -9.85	 Mean_loss: 1.38227344,  training time: 19.18
[845.8, 925.6, 788.4, 903.2, 951.0, 952.6]
Episode 42	 reward: -9.21	 Mean_loss: 1.54564023,  training time: 19.00
[791.6, 835.2, 860.6, 871.6, 952.0, 849.6]
Episode 43	 reward: -9.41	 Mean_loss: 1.06638169,  training time: 19.00
[812.8, 891.0, 861.0, 879.0, 927.0, 930.4]
Episode 44	 reward: -9.19	 Mean_loss: 1.38773584,  training time: 18.98
[829.6, 960.4, 827.0, 861.4, 903.8, 905.8]
Episode 45	 reward: -8.80	 Mean_loss: 1.19503224,  training time: 18.90
[834.0, 954.2, 870.6, 828.8, 924.6, 924.8]
Episode 46	 reward: -9.24	 Mean_loss: 1.33650720,  training time: 18.84
[848.4, 859.6, 879.2, 814.8, 936.6, 961.2]
Episode 47	 reward: -8.92	 Mean_loss: 1.55455184,  training time: 18.93
[799.4, 907.2, 851.6, 822.2, 913.6, 950.4]
Episode 48	 reward: -9.22	 Mean_loss: 1.47656524,  training time: 18.87
[815.4, 873.4, 849.6, 837.8, 938.2, 919.2]
Episode 49	 reward: -9.27	 Mean_loss: 1.35580611,  training time: 18.85
[813.2, 877.6, 838.2, 831.0, 896.4, 861.2]
Episode 50	 reward: -9.51	 Mean_loss: 1.02585816,  training time: 18.85
[868.2, 899.4, 810.0, 871.2, 927.0, 930.6]
Episode 51	 reward: -9.51	 Mean_loss: 1.31324852,  training time: 18.94
[820.4, 915.6, 867.6, 900.2, 901.4, 873.6]
Episode 52	 reward: -8.84	 Mean_loss: 0.99894404,  training time: 18.86
[842.2, 894.0, 828.2, 851.4, 899.0, 897.4]
Episode 53	 reward: -8.98	 Mean_loss: 1.11559510,  training time: 19.01
[864.4, 984.6, 904.8, 806.4, 900.2, 847.2]
Episode 54	 reward: -9.19	 Mean_loss: 0.91903573,  training time: 18.86
[871.0, 911.2, 922.6, 816.0, 916.4, 885.2]
Episode 55	 reward: -9.66	 Mean_loss: 1.11780822,  training time: 18.96
[841.8, 900.2, 873.2, 790.2, 866.8, 887.6]
Episode 56	 reward: -9.49	 Mean_loss: 1.12670314,  training time: 18.96
[816.4, 916.6, 826.6, 852.0, 934.8, 895.8]
Episode 57	 reward: -8.86	 Mean_loss: 1.15979624,  training time: 18.95
[824.4, 902.0, 902.6, 857.8, 918.8, 946.4]
Episode 58	 reward: -9.63	 Mean_loss: 1.34067369,  training time: 19.09
[895.4, 916.4, 892.6, 825.8, 944.2, 1022.6]
Episode 59	 reward: -9.32	 Mean_loss: 1.78875542,  training time: 18.96
[827.4, 948.2, 836.2, 822.8, 901.4, 849.2]
Episode 60	 reward: -8.90	 Mean_loss: 0.88805228,  training time: 18.80
[(15, 10), (15, 9), (15, 10), (15, 10), (15, 9), (15, 10)]
[901.8, 881.0, 957.0, 902.2, 876.2, 857.2]
Episode 61	 reward: -9.00	 Mean_loss: 0.62907171,  training time: 25.13
[897.0, 871.2, 933.0, 893.4, 924.0, 1047.0]
Episode 62	 reward: -9.51	 Mean_loss: 1.48225975,  training time: 25.10
[873.6, 926.8, 889.8, 908.8, 890.8, 991.2]
Episode 63	 reward: -9.31	 Mean_loss: 1.09393287,  training time: 25.12
[917.8, 912.4, 920.4, 898.8, 858.8, 930.6]
Episode 64	 reward: -10.16	 Mean_loss: 0.82658708,  training time: 25.14
[958.8, 867.0, 922.2, 922.0, 911.8, 901.4]
Episode 65	 reward: -9.82	 Mean_loss: 0.79766291,  training time: 25.20
[899.6, 859.6, 844.0, 857.2, 927.4, 931.8]
Episode 66	 reward: -9.47	 Mean_loss: 0.85217518,  training time: 25.10
[942.2, 877.0, 973.4, 861.2, 845.2, 926.0]
Episode 67	 reward: -8.94	 Mean_loss: 0.83899802,  training time: 25.15
[910.4, 894.4, 918.8, 865.2, 895.6, 924.6]
Episode 68	 reward: -9.07	 Mean_loss: 0.91415340,  training time: 25.13
[872.4, 882.6, 897.6, 881.6, 913.8, 944.8]
Episode 69	 reward: -9.49	 Mean_loss: 0.97286093,  training time: 25.09
[872.6, 886.4, 927.4, 869.4, 850.6, 931.2]
Episode 70	 reward: -9.61	 Mean_loss: 0.85755616,  training time: 25.12
[973.4, 909.0, 966.4, 896.8, 879.8, 939.6]
Episode 71	 reward: -10.16	 Mean_loss: 0.84114081,  training time: 25.10
[875.8, 902.0, 946.4, 898.8, 917.0, 927.0]
Episode 72	 reward: -9.69	 Mean_loss: 0.79478717,  training time: 25.09
[966.8, 854.6, 955.6, 898.8, 887.6, 979.0]
Episode 73	 reward: -9.64	 Mean_loss: 1.00040090,  training time: 25.34
[930.0, 925.4, 914.4, 904.6, 901.6, 925.2]
Episode 74	 reward: -10.06	 Mean_loss: 0.80823827,  training time: 25.45
[921.0, 889.6, 916.4, 868.4, 880.4, 910.6]
Episode 75	 reward: -9.64	 Mean_loss: 0.72198093,  training time: 25.37
[934.4, 961.0, 929.0, 947.2, 919.8, 962.2]
Episode 76	 reward: -9.81	 Mean_loss: 0.94746953,  training time: 25.35
[888.6, 832.2, 966.4, 886.0, 914.4, 967.2]
Episode 77	 reward: -9.42	 Mean_loss: 0.97143406,  training time: 25.30
[903.2, 867.0, 896.4, 894.2, 886.8, 878.0]
Episode 78	 reward: -9.18	 Mean_loss: 0.59563458,  training time: 25.09
[932.6, 834.8, 902.2, 960.8, 912.6, 916.0]
Episode 79	 reward: -9.97	 Mean_loss: 0.73874134,  training time: 25.10
[972.8, 937.6, 921.2, 874.8, 890.6, 964.2]
Episode 80	 reward: -10.34	 Mean_loss: 0.83926946,  training time: 25.18
[(15, 5), (15, 7), (15, 10), (15, 5), (15, 9), (15, 5)]
[877.0, 860.0, 902.6, 920.6, 857.8, 824.0]
Episode 81	 reward: -8.70	 Mean_loss: 2.05911040,  training time: 17.80
[824.8, 828.4, 901.8, 829.2, 902.6, 832.2]
Episode 82	 reward: -8.05	 Mean_loss: 2.06541538,  training time: 17.68
[886.4, 827.4, 927.6, 821.8, 839.6, 786.8]
Episode 83	 reward: -7.91	 Mean_loss: 1.57462382,  training time: 17.69
[813.2, 833.2, 923.4, 858.0, 910.4, 795.4]
Episode 84	 reward: -8.69	 Mean_loss: 1.70746291,  training time: 17.78
[825.8, 826.0, 867.4, 847.2, 878.8, 800.8]
Episode 85	 reward: -8.48	 Mean_loss: 1.72029769,  training time: 17.72
[829.0, 831.2, 871.4, 885.4, 844.8, 804.0]
Episode 86	 reward: -8.77	 Mean_loss: 1.75305009,  training time: 17.67
[825.4, 800.8, 908.0, 854.2, 861.6, 821.2]
Episode 87	 reward: -8.14	 Mean_loss: 1.91155577,  training time: 17.67
[835.8, 843.8, 882.4, 859.2, 856.2, 795.4]
Episode 88	 reward: -7.93	 Mean_loss: 1.61870539,  training time: 17.67
[890.8, 848.8, 867.4, 834.2, 961.8, 793.2]
Episode 89	 reward: -8.23	 Mean_loss: 1.52534294,  training time: 17.65
[842.2, 841.8, 886.0, 807.8, 893.2, 792.6]
Episode 90	 reward: -8.51	 Mean_loss: 1.57276058,  training time: 17.71
[851.0, 844.6, 925.6, 836.8, 891.6, 820.8]
Episode 91	 reward: -9.06	 Mean_loss: 1.79973602,  training time: 17.67
[864.8, 841.6, 851.6, 865.2, 894.2, 779.6]
Episode 92	 reward: -8.63	 Mean_loss: 1.50476432,  training time: 17.66
[822.2, 827.6, 869.4, 792.0, 852.6, 790.6]
Episode 93	 reward: -8.41	 Mean_loss: 1.55544174,  training time: 17.66
[798.0, 793.2, 925.6, 849.2, 886.8, 797.2]
Episode 94	 reward: -8.44	 Mean_loss: 1.65170860,  training time: 17.67
[821.4, 895.2, 845.4, 856.4, 892.6, 851.4]
Episode 95	 reward: -8.60	 Mean_loss: 1.94285476,  training time: 17.66
[829.6, 798.8, 920.4, 833.2, 873.0, 768.4]
Episode 96	 reward: -9.03	 Mean_loss: 1.39467514,  training time: 17.73
[827.2, 831.0, 894.6, 871.0, 874.6, 797.4]
Episode 97	 reward: -8.23	 Mean_loss: 1.54298091,  training time: 17.71
[799.0, 825.4, 946.8, 897.2, 897.4, 777.8]
Episode 98	 reward: -8.52	 Mean_loss: 1.31933284,  training time: 17.67
[809.6, 868.0, 876.0, 844.8, 865.8, 819.0]
Episode 99	 reward: -8.46	 Mean_loss: 1.49872708,  training time: 17.67
[849.4, 805.0, 858.8, 843.0, 869.8, 756.6]
Episode 100	 reward: -8.25	 Mean_loss: 1.19524741,  training time: 17.67
[(15, 5), (15, 10), (15, 7), (15, 10), (15, 7), (15, 5)]
[846.0, 854.4, 895.6, 941.2, 878.4, 813.8]
Episode 101	 reward: -8.64	 Mean_loss: 1.45757163,  training time: 19.10
[841.0, 855.4, 909.0, 889.6, 832.2, 812.8]
Episode 102	 reward: -8.56	 Mean_loss: 1.49562788,  training time: 19.12
[816.6, 844.8, 860.4, 874.6, 785.8, 819.6]
Episode 103	 reward: -8.10	 Mean_loss: 1.42154372,  training time: 19.06
[837.4, 833.8, 901.4, 866.4, 832.0, 849.2]
Episode 104	 reward: -8.74	 Mean_loss: 1.62886786,  training time: 19.03
[884.2, 920.6, 846.2, 877.8, 800.8, 817.2]
Episode 105	 reward: -8.52	 Mean_loss: 1.31764984,  training time: 19.02
[842.4, 937.4, 837.4, 917.6, 853.4, 845.4]
Episode 106	 reward: -8.10	 Mean_loss: 1.55847895,  training time: 19.02
[842.2, 843.6, 835.8, 931.8, 784.6, 874.8]
Episode 107	 reward: -8.66	 Mean_loss: 1.75017548,  training time: 19.03
[822.2, 902.0, 882.2, 891.8, 760.0, 842.6]
Episode 108	 reward: -8.84	 Mean_loss: 1.57021415,  training time: 19.05
[851.8, 846.6, 878.2, 979.4, 812.2, 781.8]
Episode 109	 reward: -9.35	 Mean_loss: 1.03874028,  training time: 19.02
[838.8, 885.8, 900.6, 886.8, 797.8, 815.8]
Episode 110	 reward: -8.78	 Mean_loss: 1.27824008,  training time: 19.02
[894.8, 886.8, 906.8, 873.2, 805.2, 871.4]
Episode 111	 reward: -8.82	 Mean_loss: 1.64739382,  training time: 19.02
[837.2, 813.8, 862.6, 871.8, 775.6, 818.4]
Episode 112	 reward: -9.12	 Mean_loss: 1.21222830,  training time: 19.03
[806.2, 871.8, 925.6, 839.8, 827.2, 784.8]
Episode 113	 reward: -8.47	 Mean_loss: 0.95297348,  training time: 19.08
[868.0, 804.8, 849.6, 857.8, 812.4, 780.2]
Episode 114	 reward: -8.46	 Mean_loss: 0.97798508,  training time: 19.02
[845.6, 859.8, 852.8, 918.4, 826.2, 800.8]
Episode 115	 reward: -8.65	 Mean_loss: 1.09989607,  training time: 19.02
[822.4, 857.8, 897.0, 818.8, 805.0, 868.0]
Episode 116	 reward: -8.61	 Mean_loss: 1.38169920,  training time: 19.03
[842.8, 836.8, 915.0, 869.0, 835.2, 814.4]
Episode 117	 reward: -8.96	 Mean_loss: 1.12369335,  training time: 19.14
[817.2, 849.2, 894.6, 842.0, 830.6, 836.0]
Episode 118	 reward: -8.89	 Mean_loss: 1.11843777,  training time: 19.07
[829.0, 957.0, 839.4, 838.4, 830.0, 874.4]
Episode 119	 reward: -8.51	 Mean_loss: 1.36632586,  training time: 19.05
[878.6, 861.4, 856.0, 886.0, 791.0, 845.0]
Episode 120	 reward: -8.71	 Mean_loss: 1.14751756,  training time: 19.03
[(15, 7), (15, 10), (15, 9), (15, 5), (15, 5), (15, 9)]
[841.2, 937.2, 898.2, 840.8, 780.4, 854.4]
Episode 121	 reward: -10.14	 Mean_loss: 0.15236841,  training time: 19.34
[820.8, 896.8, 844.2, 781.4, 796.0, 931.2]
Episode 122	 reward: -9.54	 Mean_loss: 0.28141290,  training time: 19.25
[819.6, 896.2, 933.8, 866.4, 802.2, 926.0]
Episode 123	 reward: -9.13	 Mean_loss: 0.27895507,  training time: 19.24
[772.8, 878.2, 881.4, 784.6, 804.0, 977.2]
Episode 124	 reward: -9.75	 Mean_loss: 0.36310735,  training time: 19.31
[825.2, 863.8, 897.2, 799.6, 817.4, 888.2]
Episode 125	 reward: -9.93	 Mean_loss: 0.15627936,  training time: 19.24
[848.6, 966.0, 892.4, 781.8, 808.8, 943.4]
Episode 126	 reward: -10.02	 Mean_loss: 0.25985682,  training time: 19.26
[843.8, 854.6, 853.0, 783.6, 823.4, 946.4]
Episode 127	 reward: -9.57	 Mean_loss: 0.28926176,  training time: 19.25
[834.8, 971.2, 946.6, 806.6, 823.8, 919.6]
Episode 128	 reward: -9.30	 Mean_loss: 0.24249582,  training time: 19.25
[907.0, 959.0, 886.2, 787.6, 840.6, 999.8]
Episode 129	 reward: -9.52	 Mean_loss: 0.42542335,  training time: 19.26
[844.2, 954.8, 907.8, 847.0, 832.0, 1020.0]
Episode 130	 reward: -10.34	 Mean_loss: 0.45468944,  training time: 19.26
[828.2, 896.6, 919.2, 809.8, 737.0, 893.2]
Episode 131	 reward: -9.83	 Mean_loss: 0.18368718,  training time: 19.35
[788.6, 876.6, 933.0, 806.8, 841.0, 924.6]
Episode 132	 reward: -9.58	 Mean_loss: 0.18990664,  training time: 19.26
[811.8, 826.0, 938.2, 782.4, 797.6, 885.8]
Episode 133	 reward: -9.53	 Mean_loss: 0.19424000,  training time: 19.26
[852.8, 871.2, 889.2, 791.0, 861.2, 889.8]
Episode 134	 reward: -10.54	 Mean_loss: 0.14885564,  training time: 19.30
[858.6, 872.6, 881.8, 782.6, 846.2, 900.0]
Episode 135	 reward: -9.72	 Mean_loss: 0.14444382,  training time: 19.33
[821.4, 900.4, 923.8, 765.4, 793.6, 991.2]
Episode 136	 reward: -9.69	 Mean_loss: 0.38972679,  training time: 19.33
[808.6, 866.2, 915.6, 762.2, 789.2, 977.8]
Episode 137	 reward: -9.92	 Mean_loss: 0.23932013,  training time: 19.34
[874.4, 897.8, 851.4, 750.0, 790.4, 946.2]
Episode 138	 reward: -10.69	 Mean_loss: 0.16357289,  training time: 19.33
[846.2, 877.0, 943.8, 774.0, 801.8, 972.2]
Episode 139	 reward: -10.22	 Mean_loss: 0.20699482,  training time: 19.35
[896.4, 863.4, 839.8, 846.0, 821.8, 973.4]
Episode 140	 reward: -9.95	 Mean_loss: 0.27583280,  training time: 19.36
[(15, 5), (15, 5), (15, 5), (15, 10), (15, 10), (15, 7)]
[814.0, 844.4, 744.6, 927.2, 902.6, 841.8]
Episode 141	 reward: -9.04	 Mean_loss: 0.37072098,  training time: 18.12
[841.4, 803.6, 773.6, 908.4, 898.8, 814.6]
Episode 142	 reward: -8.71	 Mean_loss: 0.24859436,  training time: 18.07
[796.4, 759.6, 807.2, 864.0, 938.2, 823.2]
Episode 143	 reward: -9.04	 Mean_loss: 0.32318619,  training time: 18.05
[829.8, 780.6, 813.4, 886.2, 912.6, 829.0]
Episode 144	 reward: -9.20	 Mean_loss: 0.30950183,  training time: 18.05
[786.8, 807.2, 727.4, 899.4, 923.2, 842.6]
Episode 145	 reward: -8.96	 Mean_loss: 0.29476738,  training time: 18.06
[806.0, 805.6, 790.4, 894.4, 815.2, 903.4]
Episode 146	 reward: -8.73	 Mean_loss: 0.37227359,  training time: 18.12
[825.6, 801.8, 790.0, 889.0, 884.4, 813.6]
Episode 147	 reward: -9.05	 Mean_loss: 0.23194824,  training time: 18.05
[785.0, 815.8, 789.0, 951.8, 910.4, 840.6]
Episode 148	 reward: -9.51	 Mean_loss: 0.22416301,  training time: 18.04
[788.2, 781.8, 764.8, 915.4, 935.8, 842.0]
Episode 149	 reward: -9.02	 Mean_loss: 0.24550147,  training time: 18.04
[850.8, 769.4, 811.2, 888.2, 926.6, 793.6]
Episode 150	 reward: -8.54	 Mean_loss: 0.23590279,  training time: 18.12
[838.8, 772.8, 733.8, 872.4, 886.0, 859.0]
Episode 151	 reward: -8.44	 Mean_loss: 0.28279978,  training time: 18.12
[782.8, 767.8, 805.4, 881.2, 966.0, 819.4]
Episode 152	 reward: -9.00	 Mean_loss: 0.27011505,  training time: 18.05
[768.2, 774.4, 782.6, 895.8, 865.8, 846.4]
Episode 153	 reward: -9.59	 Mean_loss: 0.27013305,  training time: 18.07
[785.8, 812.2, 757.0, 892.6, 899.4, 893.8]
Episode 154	 reward: -8.71	 Mean_loss: 0.26793364,  training time: 18.15
[780.4, 755.6, 724.6, 870.2, 909.0, 815.6]
Episode 155	 reward: -8.78	 Mean_loss: 0.32543340,  training time: 18.14
[872.0, 800.0, 731.6, 879.6, 891.8, 824.2]
Episode 156	 reward: -8.59	 Mean_loss: 0.21331984,  training time: 18.14
[747.0, 795.6, 744.8, 921.0, 863.8, 836.6]
Episode 157	 reward: -9.73	 Mean_loss: 0.25486618,  training time: 18.19
[837.4, 784.0, 737.8, 899.4, 854.8, 818.2]
Episode 158	 reward: -8.89	 Mean_loss: 0.23354538,  training time: 18.14
[829.2, 814.8, 774.8, 871.0, 918.6, 831.8]
Episode 159	 reward: -9.01	 Mean_loss: 0.26622599,  training time: 18.15
[776.6, 774.4, 721.8, 898.4, 883.0, 825.4]
Episode 160	 reward: -8.58	 Mean_loss: 0.24825323,  training time: 18.10
[(15, 9), (15, 10), (15, 9), (15, 10), (15, 7), (15, 5)]
[869.4, 924.0, 902.0, 838.0, 848.2, 743.0]
Episode 161	 reward: -8.71	 Mean_loss: 0.41366845,  training time: 21.54
[888.4, 885.4, 809.6, 878.0, 824.0, 731.8]
Episode 162	 reward: -8.31	 Mean_loss: 0.37397802,  training time: 21.59
[792.6, 931.6, 895.0, 860.0, 824.6, 750.6]
Episode 163	 reward: -8.27	 Mean_loss: 0.45630556,  training time: 21.63
[848.8, 925.0, 873.2, 833.8, 856.4, 730.0]
Episode 164	 reward: -8.03	 Mean_loss: 0.38099664,  training time: 21.67
[840.0, 848.0, 875.0, 839.8, 871.0, 750.6]
Episode 165	 reward: -7.94	 Mean_loss: 0.36439404,  training time: 22.10
[831.2, 873.8, 815.4, 903.6, 873.8, 732.6]
Episode 166	 reward: -8.23	 Mean_loss: 0.33105069,  training time: 21.58
[877.6, 915.8, 842.2, 886.0, 817.2, 732.2]
Episode 167	 reward: -8.37	 Mean_loss: 0.38820735,  training time: 21.56
[838.6, 895.8, 868.2, 907.0, 877.2, 753.2]
Episode 168	 reward: -7.90	 Mean_loss: 0.43666041,  training time: 21.52
[844.2, 843.6, 828.4, 845.0, 845.4, 753.0]
Episode 169	 reward: -7.80	 Mean_loss: 0.32874742,  training time: 21.52
[850.4, 884.6, 868.2, 871.6, 851.8, 735.4]
Episode 170	 reward: -8.10	 Mean_loss: 0.39609733,  training time: 21.53
[872.6, 886.6, 866.2, 950.4, 912.8, 751.4]
Episode 171	 reward: -7.88	 Mean_loss: 0.43566805,  training time: 21.51
[800.2, 896.4, 847.2, 877.2, 836.0, 727.4]
Episode 172	 reward: -8.36	 Mean_loss: 0.40988752,  training time: 21.55
[833.8, 861.2, 852.2, 828.4, 858.8, 694.0]
Episode 173	 reward: -8.15	 Mean_loss: 0.37443408,  training time: 21.51
[875.6, 897.8, 880.6, 856.8, 830.2, 797.4]
Episode 174	 reward: -7.85	 Mean_loss: 0.43233243,  training time: 21.50
[854.2, 911.6, 862.4, 911.8, 845.4, 740.2]
Episode 175	 reward: -7.85	 Mean_loss: 0.39775202,  training time: 21.50
[858.2, 832.2, 838.2, 858.6, 900.2, 748.0]
Episode 176	 reward: -8.39	 Mean_loss: 0.32432422,  training time: 21.51
[864.4, 940.8, 885.0, 853.8, 848.8, 729.6]
Episode 177	 reward: -7.88	 Mean_loss: 0.34415373,  training time: 21.54
[793.2, 904.0, 820.0, 828.2, 871.4, 713.0]
Episode 178	 reward: -8.19	 Mean_loss: 0.32842508,  training time: 21.49
[840.0, 891.0, 864.4, 855.0, 893.4, 752.2]
Episode 179	 reward: -8.03	 Mean_loss: 0.37833890,  training time: 21.51
[802.4, 886.8, 866.6, 849.4, 801.4, 778.8]
Episode 180	 reward: -8.33	 Mean_loss: 0.50380313,  training time: 21.57
[(15, 5), (15, 7), (15, 5), (15, 10), (15, 10), (15, 10)]
[817.4, 787.8, 835.0, 957.8, 829.0, 858.6]
Episode 181	 reward: -9.50	 Mean_loss: 0.18401483,  training time: 20.26
[793.6, 805.8, 821.8, 923.6, 822.6, 913.8]
Episode 182	 reward: -9.89	 Mean_loss: 0.13794129,  training time: 20.18
[819.4, 827.0, 820.2, 892.2, 867.2, 854.4]
Episode 183	 reward: -9.75	 Mean_loss: 0.19157173,  training time: 20.20
[856.4, 798.4, 791.0, 961.0, 829.0, 787.4]
Episode 184	 reward: -9.52	 Mean_loss: 0.24680294,  training time: 20.16
[803.2, 813.8, 840.0, 933.6, 847.4, 884.4]
Episode 185	 reward: -9.59	 Mean_loss: 0.19386739,  training time: 20.17
[791.4, 832.4, 789.2, 911.8, 852.4, 833.8]
Episode 186	 reward: -9.13	 Mean_loss: 0.14368181,  training time: 20.16
[896.0, 768.8, 835.8, 884.2, 854.2, 878.8]
Episode 187	 reward: -9.36	 Mean_loss: 0.15908748,  training time: 20.20
[785.4, 798.6, 816.6, 939.0, 859.6, 861.6]
Episode 188	 reward: -9.34	 Mean_loss: 0.18978259,  training time: 20.16
[859.8, 804.4, 845.8, 923.2, 831.0, 867.0]
Episode 189	 reward: -9.77	 Mean_loss: 0.21156238,  training time: 20.15
[838.4, 785.2, 810.0, 912.6, 845.6, 836.0]
Episode 190	 reward: -9.65	 Mean_loss: 0.19324671,  training time: 20.15
[810.4, 751.0, 797.4, 861.8, 878.2, 894.4]
Episode 191	 reward: -8.99	 Mean_loss: 0.18485321,  training time: 20.15
[768.0, 789.4, 770.2, 905.0, 848.6, 842.2]
Episode 192	 reward: -9.37	 Mean_loss: 0.18144856,  training time: 20.18
[830.0, 791.2, 776.2, 832.4, 893.8, 870.8]
Episode 193	 reward: -9.52	 Mean_loss: 0.16901027,  training time: 20.24
[790.2, 734.6, 744.0, 866.0, 862.4, 911.6]
Episode 194	 reward: -9.60	 Mean_loss: 0.10411173,  training time: 20.13
[827.6, 758.0, 835.2, 909.6, 863.0, 842.0]
Episode 195	 reward: -9.95	 Mean_loss: 0.16200203,  training time: 20.17
[814.8, 772.4, 819.6, 923.4, 800.6, 848.2]
Episode 196	 reward: -9.24	 Mean_loss: 0.19735394,  training time: 20.71
[804.6, 799.6, 886.0, 878.0, 832.0, 888.8]
Episode 197	 reward: -10.11	 Mean_loss: 0.13497362,  training time: 20.28
[809.2, 855.0, 793.2, 873.2, 796.8, 851.4]
Episode 198	 reward: -9.46	 Mean_loss: 0.17464375,  training time: 20.54
[861.4, 811.6, 816.8, 825.0, 832.0, 870.0]
Episode 199	 reward: -9.49	 Mean_loss: 0.16658634,  training time: 20.78
[841.8, 793.4, 725.6, 913.4, 765.0, 839.4]
Episode 200	 reward: -9.74	 Mean_loss: 0.18731661,  training time: 20.24
[(15, 9), (15, 9), (15, 10), (15, 5), (15, 9), (15, 7)]
[816.6, 830.4, 858.8, 791.6, 873.6, 808.8]
Episode 201	 reward: -8.88	 Mean_loss: 0.18559654,  training time: 21.05
[827.0, 875.8, 847.6, 769.4, 853.2, 850.6]
Episode 202	 reward: -9.54	 Mean_loss: 0.27460793,  training time: 21.37
[853.0, 889.0, 920.6, 751.2, 826.6, 818.6]
Episode 203	 reward: -8.87	 Mean_loss: 0.19285072,  training time: 21.43
[791.0, 810.0, 859.2, 813.0, 838.0, 820.2]
Episode 204	 reward: -9.11	 Mean_loss: 0.24853452,  training time: 21.37
[863.6, 824.2, 856.6, 777.2, 810.8, 822.4]
Episode 205	 reward: -8.94	 Mean_loss: 0.19925535,  training time: 21.20
[822.0, 856.0, 917.4, 793.8, 829.6, 889.2]
Episode 206	 reward: -9.04	 Mean_loss: 0.24146760,  training time: 21.10
[790.8, 831.2, 852.8, 805.8, 821.0, 808.0]
Episode 207	 reward: -9.10	 Mean_loss: 0.18715884,  training time: 21.00
[875.6, 856.0, 923.4, 783.6, 847.4, 810.2]
Episode 208	 reward: -9.42	 Mean_loss: 0.17206785,  training time: 20.98
[832.0, 868.2, 880.4, 770.2, 778.0, 842.8]
Episode 209	 reward: -9.42	 Mean_loss: 0.23392719,  training time: 20.97
[890.8, 861.2, 890.6, 781.0, 840.8, 784.6]
Episode 210	 reward: -9.51	 Mean_loss: 0.21305367,  training time: 20.97
[811.2, 774.2, 839.6, 778.0, 851.2, 827.2]
Episode 211	 reward: -8.94	 Mean_loss: 0.18679132,  training time: 20.98
[806.4, 836.8, 850.8, 801.6, 856.4, 812.4]
Episode 212	 reward: -9.73	 Mean_loss: 0.23910831,  training time: 21.13
[807.4, 835.4, 863.6, 751.4, 852.2, 780.0]
Episode 213	 reward: -9.53	 Mean_loss: 0.18213077,  training time: 21.14
[793.8, 821.6, 898.8, 783.0, 908.4, 813.0]
Episode 214	 reward: -9.23	 Mean_loss: 0.21470717,  training time: 21.15
[840.4, 853.4, 891.0, 749.6, 896.2, 892.4]
Episode 215	 reward: -9.64	 Mean_loss: 0.32977882,  training time: 21.14
[886.4, 829.0, 892.6, 760.8, 859.8, 835.0]
Episode 216	 reward: -9.08	 Mean_loss: 0.37607333,  training time: 21.03
[803.6, 896.6, 848.8, 799.0, 851.8, 815.8]
Episode 217	 reward: -9.08	 Mean_loss: 0.19411735,  training time: 20.91
[793.2, 838.4, 852.6, 798.2, 822.8, 873.4]
Episode 218	 reward: -8.77	 Mean_loss: 0.23149636,  training time: 20.90
[849.6, 878.6, 896.2, 812.0, 871.0, 820.6]
Episode 219	 reward: -8.60	 Mean_loss: 0.14898020,  training time: 20.91
[801.8, 822.8, 908.4, 805.2, 841.0, 835.2]
Episode 220	 reward: -9.43	 Mean_loss: 0.14854358,  training time: 20.99
[(15, 9), (15, 10), (15, 10), (15, 10), (15, 7), (15, 7)]
[848.8, 871.8, 879.0, 856.8, 844.0, 845.8]
Episode 221	 reward: -8.65	 Mean_loss: 0.18972626,  training time: 22.81
[836.8, 898.2, 865.6, 814.2, 837.4, 788.2]
Episode 222	 reward: -9.08	 Mean_loss: 0.23602848,  training time: 22.73
[901.4, 947.6, 825.4, 860.0, 805.0, 866.0]
Episode 223	 reward: -9.14	 Mean_loss: 0.22850725,  training time: 22.70
[850.0, 840.0, 846.2, 838.0, 810.0, 903.0]
Episode 224	 reward: -8.68	 Mean_loss: 0.28915998,  training time: 22.90
[804.2, 898.8, 938.0, 768.0, 904.6, 828.0]
Episode 225	 reward: -9.00	 Mean_loss: 0.19856566,  training time: 22.94
[837.8, 870.2, 852.6, 817.6, 836.2, 814.2]
Episode 226	 reward: -9.55	 Mean_loss: 0.18201320,  training time: 22.93
[837.2, 802.0, 879.6, 843.4, 801.8, 811.8]
Episode 227	 reward: -9.24	 Mean_loss: 0.27324027,  training time: 23.36
[836.2, 915.6, 871.6, 875.2, 820.8, 798.8]
Episode 228	 reward: -9.08	 Mean_loss: 0.22132982,  training time: 23.39
[932.2, 859.0, 854.4, 838.0, 870.4, 827.2]
Episode 229	 reward: -9.39	 Mean_loss: 0.27737531,  training time: 22.79
[785.6, 911.6, 822.2, 834.0, 809.4, 846.0]
Episode 230	 reward: -9.14	 Mean_loss: 0.33952978,  training time: 22.77
[819.8, 848.2, 813.6, 798.4, 817.4, 794.0]
Episode 231	 reward: -10.09	 Mean_loss: 0.17089929,  training time: 22.76
[816.4, 896.4, 832.8, 872.8, 821.0, 795.4]
Episode 232	 reward: -8.87	 Mean_loss: 0.22896068,  training time: 22.74
[825.4, 850.2, 827.2, 829.6, 828.6, 841.4]
Episode 233	 reward: -9.25	 Mean_loss: 0.24609488,  training time: 22.83
[849.8, 907.2, 855.8, 760.8, 815.0, 805.8]
Episode 234	 reward: -9.32	 Mean_loss: 0.21392398,  training time: 22.76
[829.0, 927.0, 819.6, 793.2, 810.4, 819.0]
Episode 235	 reward: -9.30	 Mean_loss: 0.24125516,  training time: 23.15
[887.4, 874.8, 867.6, 815.6, 804.8, 823.8]
Episode 236	 reward: -9.28	 Mean_loss: 0.22652189,  training time: 23.54
[854.6, 877.8, 834.8, 790.4, 814.0, 803.8]
Episode 237	 reward: -9.07	 Mean_loss: 0.25591934,  training time: 23.50
[840.2, 830.8, 821.4, 807.4, 856.4, 858.4]
Episode 238	 reward: -9.73	 Mean_loss: 0.23341364,  training time: 22.66
[835.0, 898.0, 844.6, 833.0, 834.6, 848.0]
Episode 239	 reward: -9.14	 Mean_loss: 0.31551346,  training time: 22.78
[805.6, 839.6, 853.0, 814.8, 783.8, 862.2]
Episode 240	 reward: -9.28	 Mean_loss: 0.32736167,  training time: 22.97
[(15, 5), (15, 5), (15, 9), (15, 10), (15, 10), (15, 7)]
[719.0, 724.2, 840.8, 858.4, 852.4, 783.0]
Episode 241	 reward: -8.88	 Mean_loss: 0.31253806,  training time: 19.89
[751.2, 716.8, 895.2, 871.6, 811.6, 837.4]
Episode 242	 reward: -8.91	 Mean_loss: 0.23010641,  training time: 19.74
[777.6, 681.2, 867.8, 870.0, 819.8, 759.8]
Episode 243	 reward: -9.23	 Mean_loss: 0.17472126,  training time: 19.73
[763.2, 706.2, 870.8, 799.0, 798.8, 835.4]
Episode 244	 reward: -8.40	 Mean_loss: 0.27436307,  training time: 19.73
[758.8, 757.6, 852.8, 917.8, 829.6, 778.2]
Episode 245	 reward: -8.15	 Mean_loss: 0.21815060,  training time: 19.78
[768.8, 721.0, 880.0, 933.2, 898.4, 842.4]
Episode 246	 reward: -8.30	 Mean_loss: 0.25873563,  training time: 19.60
[749.8, 739.8, 832.6, 851.2, 793.0, 855.4]
Episode 247	 reward: -8.35	 Mean_loss: 0.40951839,  training time: 19.65
[802.8, 740.2, 799.6, 827.4, 824.8, 763.6]
Episode 248	 reward: -9.23	 Mean_loss: 0.16642453,  training time: 19.61
[743.6, 694.2, 894.6, 861.8, 800.8, 725.6]
Episode 249	 reward: -8.60	 Mean_loss: 0.12799969,  training time: 19.61
[808.2, 718.2, 828.0, 901.0, 865.2, 755.2]
Episode 250	 reward: -8.86	 Mean_loss: 0.17066982,  training time: 19.62
[783.4, 708.0, 851.0, 803.6, 838.8, 744.6]
Episode 251	 reward: -8.21	 Mean_loss: 0.21848929,  training time: 19.65
[768.4, 701.0, 796.6, 864.4, 840.4, 795.2]
Episode 252	 reward: -8.89	 Mean_loss: 0.21687327,  training time: 19.62
[784.2, 741.2, 807.4, 834.4, 846.2, 732.6]
Episode 253	 reward: -8.46	 Mean_loss: 0.13985601,  training time: 19.60
[757.6, 715.4, 830.0, 884.4, 868.6, 818.8]
Episode 254	 reward: -8.67	 Mean_loss: 0.24145563,  training time: 19.59
[754.8, 768.0, 828.2, 850.6, 843.0, 819.0]
Episode 255	 reward: -9.03	 Mean_loss: 0.20661765,  training time: 19.58
[778.8, 717.4, 813.8, 875.8, 831.2, 756.4]
Episode 256	 reward: -8.55	 Mean_loss: 0.16600282,  training time: 19.69
[794.0, 712.2, 892.8, 842.4, 823.8, 789.8]
Episode 257	 reward: -8.39	 Mean_loss: 0.19180690,  training time: 19.70
[758.8, 731.6, 790.6, 916.8, 869.6, 776.6]
Episode 258	 reward: -8.61	 Mean_loss: 0.18519010,  training time: 19.71
[762.8, 679.2, 829.4, 923.0, 815.6, 783.2]
Episode 259	 reward: -9.01	 Mean_loss: 0.21644324,  training time: 19.70
[755.8, 702.8, 841.4, 852.0, 906.4, 767.8]
Episode 260	 reward: -8.38	 Mean_loss: 0.17632507,  training time: 19.71
[(15, 10), (15, 5), (15, 7), (15, 10), (15, 7), (15, 10)]
[823.0, 754.4, 821.4, 865.8, 811.4, 817.0]
Episode 261	 reward: -9.31	 Mean_loss: 0.09523166,  training time: 21.11
[867.8, 803.6, 802.8, 842.0, 783.4, 845.2]
Episode 262	 reward: -9.04	 Mean_loss: 0.12924749,  training time: 21.03
[821.4, 843.8, 770.4, 839.0, 834.2, 835.4]
Episode 263	 reward: -8.85	 Mean_loss: 0.12296928,  training time: 20.89
[866.8, 784.8, 840.6, 854.0, 765.4, 890.8]
Episode 264	 reward: -10.12	 Mean_loss: 0.12074506,  training time: 20.96
[885.2, 758.4, 750.0, 860.4, 817.8, 764.6]
Episode 265	 reward: -8.87	 Mean_loss: 0.15747491,  training time: 20.87
[858.4, 765.6, 839.8, 809.4, 812.8, 807.8]
Episode 266	 reward: -9.13	 Mean_loss: 0.13744991,  training time: 20.87
[906.6, 781.6, 762.0, 828.2, 796.4, 832.2]
Episode 267	 reward: -9.15	 Mean_loss: 0.11894912,  training time: 20.88
[847.8, 734.4, 801.8, 850.4, 766.0, 796.4]
Episode 268	 reward: -8.63	 Mean_loss: 0.13792366,  training time: 20.86
[805.0, 785.2, 802.4, 858.6, 777.6, 792.8]
Episode 269	 reward: -9.61	 Mean_loss: 0.15448691,  training time: 20.85
[820.2, 765.2, 767.2, 821.0, 767.2, 835.0]
Episode 270	 reward: -8.94	 Mean_loss: 0.11256465,  training time: 20.86
[811.2, 783.6, 791.6, 832.2, 796.8, 882.6]
Episode 271	 reward: -9.38	 Mean_loss: 0.12609963,  training time: 20.90
[792.6, 769.8, 782.6, 812.8, 854.6, 819.2]
Episode 272	 reward: -9.51	 Mean_loss: 0.12287561,  training time: 20.87
[830.8, 797.6, 784.8, 822.8, 782.0, 829.6]
Episode 273	 reward: -9.41	 Mean_loss: 0.10337984,  training time: 20.85
[877.6, 808.8, 740.8, 888.4, 761.8, 846.4]
Episode 274	 reward: -9.20	 Mean_loss: 0.10500492,  training time: 20.85
[895.8, 789.2, 755.8, 857.0, 736.0, 774.6]
Episode 275	 reward: -8.88	 Mean_loss: 0.15867926,  training time: 20.86
[805.2, 815.6, 744.0, 845.8, 798.6, 818.2]
Episode 276	 reward: -9.97	 Mean_loss: 0.08520940,  training time: 20.98
[865.2, 754.2, 721.2, 812.6, 797.4, 838.4]
Episode 277	 reward: -8.74	 Mean_loss: 0.08420934,  training time: 20.86
[867.8, 745.6, 734.2, 792.2, 808.0, 798.4]
Episode 278	 reward: -8.73	 Mean_loss: 0.10181233,  training time: 20.88
[821.8, 754.0, 762.2, 831.0, 815.2, 801.6]
Episode 279	 reward: -9.02	 Mean_loss: 0.12735043,  training time: 20.89
[870.0, 731.0, 738.0, 788.6, 793.4, 812.6]
Episode 280	 reward: -9.46	 Mean_loss: 0.10399394,  training time: 20.89
[(15, 10), (15, 9), (15, 10), (15, 5), (15, 7), (15, 10)]
[764.0, 818.6, 831.6, 762.4, 741.8, 834.6]
Episode 281	 reward: -9.53	 Mean_loss: 0.14866139,  training time: 21.87
[786.8, 818.8, 851.0, 772.6, 748.4, 815.4]
Episode 282	 reward: -8.87	 Mean_loss: 0.12917878,  training time: 22.00
[796.4, 838.8, 861.0, 732.8, 764.2, 870.6]
Episode 283	 reward: -9.24	 Mean_loss: 0.12772650,  training time: 22.02
[756.0, 806.8, 819.4, 730.6, 732.2, 800.4]
Episode 284	 reward: -9.51	 Mean_loss: 0.14005442,  training time: 22.49
[797.0, 844.8, 814.8, 719.2, 782.2, 839.8]
Episode 285	 reward: -9.07	 Mean_loss: 0.14236665,  training time: 22.01
[857.0, 807.0, 865.6, 765.2, 793.8, 882.6]
Episode 286	 reward: -9.25	 Mean_loss: 0.12667343,  training time: 21.93
[847.8, 875.8, 805.6, 757.6, 745.8, 757.0]
Episode 287	 reward: -9.16	 Mean_loss: 0.20187920,  training time: 21.93
[804.6, 817.6, 846.8, 775.8, 713.2, 798.2]
Episode 288	 reward: -9.40	 Mean_loss: 0.15699854,  training time: 21.90
[804.4, 796.6, 806.4, 721.8, 751.6, 817.8]
Episode 289	 reward: -9.87	 Mean_loss: 0.18465105,  training time: 21.91
[822.4, 790.0, 848.8, 727.8, 724.0, 805.0]
Episode 290	 reward: -8.67	 Mean_loss: 0.17333840,  training time: 21.96
[787.0, 823.0, 842.0, 746.4, 733.0, 834.2]
Episode 291	 reward: -9.69	 Mean_loss: 0.14819916,  training time: 21.94
[815.2, 788.0, 856.2, 678.4, 759.0, 828.4]
Episode 292	 reward: -9.56	 Mean_loss: 0.13024180,  training time: 21.93
[817.8, 882.8, 830.8, 777.2, 789.4, 793.6]
Episode 293	 reward: -8.94	 Mean_loss: 0.15246467,  training time: 21.92
[792.6, 820.2, 819.6, 708.6, 788.6, 841.2]
Episode 294	 reward: -9.48	 Mean_loss: 0.09391227,  training time: 21.92
[853.8, 835.2, 877.8, 700.0, 755.2, 793.6]
Episode 295	 reward: -8.94	 Mean_loss: 0.14392613,  training time: 21.97
[762.8, 802.2, 848.0, 737.0, 777.4, 853.6]
Episode 296	 reward: -9.15	 Mean_loss: 0.17311476,  training time: 22.00
[826.4, 856.4, 800.0, 717.2, 787.2, 857.2]
Episode 297	 reward: -9.20	 Mean_loss: 0.15414090,  training time: 21.92
[823.6, 829.0, 813.8, 724.2, 798.0, 844.6]
Episode 298	 reward: -8.84	 Mean_loss: 0.11377020,  training time: 21.93
[841.6, 820.4, 839.2, 752.0, 727.2, 772.2]
Episode 299	 reward: -9.16	 Mean_loss: 0.14515148,  training time: 21.93
[829.4, 803.6, 839.4, 755.4, 756.2, 779.0]
Episode 300	 reward: -8.88	 Mean_loss: 0.15067242,  training time: 21.94
[(15, 7), (15, 10), (15, 5), (15, 7), (15, 9), (15, 7)]
[797.2, 790.8, 734.2, 764.4, 787.8, 779.0]
Episode 301	 reward: -8.71	 Mean_loss: 0.13481966,  training time: 19.32
[776.0, 836.4, 761.2, 820.4, 762.4, 767.8]
Episode 302	 reward: -9.17	 Mean_loss: 0.15590364,  training time: 19.15
[740.4, 827.8, 800.8, 769.0, 771.2, 786.6]
Episode 303	 reward: -9.18	 Mean_loss: 0.15583606,  training time: 19.08
[767.2, 826.4, 790.4, 768.8, 808.8, 804.8]
Episode 304	 reward: -9.57	 Mean_loss: 0.18955360,  training time: 19.12
[751.8, 797.4, 747.8, 766.4, 792.0, 752.6]
Episode 305	 reward: -9.16	 Mean_loss: 0.19038504,  training time: 19.10
[718.0, 804.8, 805.2, 800.6, 815.0, 757.2]
Episode 306	 reward: -8.65	 Mean_loss: 0.17390253,  training time: 19.09
[747.8, 818.0, 769.8, 760.0, 842.6, 758.8]
Episode 307	 reward: -8.32	 Mean_loss: 0.12403116,  training time: 19.08
[756.2, 762.4, 747.4, 796.6, 814.8, 722.8]
Episode 308	 reward: -9.76	 Mean_loss: 0.16958557,  training time: 19.09
[771.8, 853.6, 763.0, 795.2, 766.4, 746.8]
Episode 309	 reward: -9.14	 Mean_loss: 0.23765549,  training time: 19.09
[748.8, 782.6, 767.0, 766.0, 811.0, 766.6]
Episode 310	 reward: -9.23	 Mean_loss: 0.26223919,  training time: 19.15
[817.4, 793.0, 758.8, 876.8, 842.8, 744.4]
Episode 311	 reward: -8.54	 Mean_loss: 0.12319486,  training time: 19.20
[726.6, 793.8, 742.8, 791.0, 788.6, 781.6]
Episode 312	 reward: -8.78	 Mean_loss: 0.11452165,  training time: 19.07
[801.8, 786.4, 761.0, 717.0, 857.4, 732.6]
Episode 313	 reward: -9.18	 Mean_loss: 0.23972851,  training time: 19.07
[698.4, 790.4, 748.0, 817.4, 882.0, 792.0]
Episode 314	 reward: -8.38	 Mean_loss: 0.29296529,  training time: 19.20
[801.8, 859.0, 763.6, 786.8, 821.4, 741.2]
Episode 315	 reward: -8.52	 Mean_loss: 0.10423569,  training time: 19.21
[770.4, 787.8, 750.8, 785.0, 803.4, 753.2]
Episode 316	 reward: -8.76	 Mean_loss: 0.16493854,  training time: 19.25
[764.6, 813.4, 730.0, 758.0, 806.2, 738.2]
Episode 317	 reward: -9.27	 Mean_loss: 0.10777236,  training time: 19.21
[783.0, 863.8, 731.2, 758.2, 799.4, 742.2]
Episode 318	 reward: -8.95	 Mean_loss: 0.13286558,  training time: 19.20
[744.4, 813.2, 805.8, 806.0, 839.2, 773.6]
Episode 319	 reward: -8.77	 Mean_loss: 0.17776327,  training time: 19.25
[708.0, 793.8, 771.0, 750.8, 763.8, 758.0]
Episode 320	 reward: -8.96	 Mean_loss: 0.16401507,  training time: 19.20
[(15, 7), (15, 9), (15, 5), (15, 5), (15, 9), (15, 5)]
[707.2, 804.0, 762.8, 790.4, 781.4, 759.6]
Episode 321	 reward: -8.96	 Mean_loss: 0.44040161,  training time: 17.11
[674.0, 823.4, 766.2, 739.2, 857.0, 804.8]
Episode 322	 reward: -8.93	 Mean_loss: 0.42779508,  training time: 16.90
[723.0, 823.4, 822.8, 778.4, 833.6, 767.2]
Episode 323	 reward: -9.23	 Mean_loss: 0.44192749,  training time: 16.89
[716.8, 795.4, 747.8, 747.0, 797.6, 774.4]
Episode 324	 reward: -9.12	 Mean_loss: 0.41503453,  training time: 16.90
[759.4, 830.2, 723.2, 749.6, 855.4, 752.2]
Episode 325	 reward: -8.98	 Mean_loss: 0.49635765,  training time: 16.91
[755.2, 837.0, 750.8, 735.4, 810.4, 791.6]
Episode 326	 reward: -9.21	 Mean_loss: 0.42613190,  training time: 16.91
[733.8, 757.6, 734.6, 796.2, 820.2, 756.6]
Episode 327	 reward: -8.86	 Mean_loss: 0.38554361,  training time: 16.97
[696.4, 781.6, 718.0, 764.6, 789.0, 785.2]
Episode 328	 reward: -9.53	 Mean_loss: 0.37600899,  training time: 16.93
[756.8, 802.2, 755.4, 722.6, 780.0, 787.6]
Episode 329	 reward: -9.31	 Mean_loss: 0.49838048,  training time: 16.90
[720.0, 789.6, 752.6, 730.8, 833.8, 751.4]
Episode 330	 reward: -8.99	 Mean_loss: 0.35731405,  training time: 16.90
[734.8, 808.6, 751.2, 746.8, 799.2, 795.6]
Episode 331	 reward: -8.64	 Mean_loss: 0.50126123,  training time: 17.51
[728.6, 847.2, 735.8, 758.6, 757.2, 763.4]
Episode 332	 reward: -9.37	 Mean_loss: 0.45291388,  training time: 17.51
[714.8, 770.8, 771.2, 735.2, 769.8, 804.8]
Episode 333	 reward: -8.94	 Mean_loss: 0.38319442,  training time: 17.25
[715.2, 811.6, 808.4, 742.4, 774.0, 798.0]
Episode 334	 reward: -9.03	 Mean_loss: 0.44096518,  training time: 16.80
[703.4, 831.6, 722.6, 729.0, 789.6, 753.0]
Episode 335	 reward: -8.56	 Mean_loss: 0.43937531,  training time: 16.82
[751.2, 754.6, 742.8, 750.2, 862.8, 734.2]
Episode 336	 reward: -8.89	 Mean_loss: 0.24328662,  training time: 17.02
[713.2, 806.6, 732.2, 823.2, 845.0, 775.2]
Episode 337	 reward: -8.92	 Mean_loss: 0.37011367,  training time: 17.01
[753.8, 853.8, 739.8, 789.2, 819.0, 792.0]
Episode 338	 reward: -8.40	 Mean_loss: 0.43669119,  training time: 17.02
[753.6, 804.8, 737.6, 787.8, 769.4, 822.2]
Episode 339	 reward: -8.94	 Mean_loss: 0.55148977,  training time: 17.06
[713.2, 792.8, 747.8, 770.0, 770.0, 771.0]
Episode 340	 reward: -8.85	 Mean_loss: 0.44250566,  training time: 17.12
[(15, 5), (15, 7), (15, 10), (15, 7), (15, 5), (15, 7)]
[686.2, 744.0, 825.0, 727.0, 687.4, 747.0]
Episode 341	 reward: -8.76	 Mean_loss: 0.13951384,  training time: 17.53
[752.2, 795.0, 806.6, 755.6, 686.2, 718.0]
Episode 342	 reward: -9.04	 Mean_loss: 0.18407112,  training time: 17.52
[766.0, 723.0, 831.8, 752.4, 714.4, 732.2]
Episode 343	 reward: -9.27	 Mean_loss: 0.14002757,  training time: 17.45
[716.8, 746.4, 822.6, 782.0, 794.0, 765.4]
Episode 344	 reward: -8.68	 Mean_loss: 0.17649627,  training time: 17.45
[745.6, 736.2, 796.2, 782.8, 696.2, 732.2]
Episode 345	 reward: -8.82	 Mean_loss: 0.16289163,  training time: 17.50
[734.4, 740.2, 841.4, 794.0, 709.2, 768.2]
Episode 346	 reward: -8.61	 Mean_loss: 0.14065968,  training time: 17.46
[765.8, 713.8, 827.6, 756.6, 698.2, 741.6]
Episode 347	 reward: -8.76	 Mean_loss: 0.15276442,  training time: 17.46
[725.0, 711.4, 842.0, 761.0, 738.0, 695.6]
Episode 348	 reward: -8.53	 Mean_loss: 0.19919334,  training time: 17.47
[731.2, 731.6, 880.6, 764.6, 710.6, 726.2]
Episode 349	 reward: -9.06	 Mean_loss: 0.17930789,  training time: 17.35
[762.6, 742.6, 778.2, 771.0, 743.0, 785.2]
Episode 350	 reward: -9.16	 Mean_loss: 0.15282306,  training time: 17.30
[729.8, 711.8, 758.8, 720.6, 717.4, 732.6]
Episode 351	 reward: -8.70	 Mean_loss: 0.14390831,  training time: 17.31
[723.2, 715.2, 793.8, 748.6, 719.2, 722.6]
Episode 352	 reward: -8.74	 Mean_loss: 0.18612993,  training time: 17.38
[725.6, 708.8, 786.2, 749.0, 693.4, 746.2]
Episode 353	 reward: -8.64	 Mean_loss: 0.15781148,  training time: 17.30
[709.4, 729.0, 787.0, 729.4, 686.4, 728.6]
Episode 354	 reward: -8.79	 Mean_loss: 0.18756841,  training time: 17.31
[753.6, 735.4, 827.4, 739.8, 709.0, 744.8]
Episode 355	 reward: -8.78	 Mean_loss: 0.10875510,  training time: 17.29
[700.4, 731.0, 841.6, 739.4, 719.2, 717.0]
Episode 356	 reward: -9.51	 Mean_loss: 0.11018716,  training time: 17.31
[772.2, 717.0, 798.0, 750.2, 698.2, 756.0]
Episode 357	 reward: -9.41	 Mean_loss: 0.15944792,  training time: 17.40
[743.8, 766.4, 797.2, 750.2, 690.4, 720.2]
Episode 358	 reward: -8.45	 Mean_loss: 0.15455297,  training time: 17.45
[759.0, 742.4, 829.2, 715.0, 695.4, 722.8]
Episode 359	 reward: -8.61	 Mean_loss: 0.14634101,  training time: 17.44
[730.0, 778.8, 858.4, 781.2, 738.8, 739.6]
Episode 360	 reward: -8.64	 Mean_loss: 0.16052684,  training time: 17.44
[(15, 9), (15, 10), (15, 10), (15, 9), (15, 10), (15, 10)]
[872.4, 798.4, 789.2, 769.8, 777.8, 817.6]
Episode 361	 reward: -9.63	 Mean_loss: 0.09805936,  training time: 25.09
[771.2, 818.8, 781.0, 763.0, 773.8, 809.8]
Episode 362	 reward: -9.90	 Mean_loss: 0.11972945,  training time: 25.19
[720.6, 833.6, 774.0, 772.0, 795.4, 835.2]
Episode 363	 reward: -9.82	 Mean_loss: 0.17944777,  training time: 25.06
[723.0, 848.4, 839.4, 821.0, 853.4, 798.2]
Episode 364	 reward: -9.97	 Mean_loss: 0.16681783,  training time: 25.06
[691.4, 860.0, 842.8, 777.0, 829.8, 804.2]
Episode 365	 reward: -9.32	 Mean_loss: 0.12801594,  training time: 25.05
[720.6, 876.0, 803.6, 689.2, 789.0, 816.4]
Episode 366	 reward: -9.24	 Mean_loss: 0.09561311,  training time: 25.11
[730.2, 824.8, 802.4, 750.6, 793.8, 779.6]
Episode 367	 reward: -9.55	 Mean_loss: 0.12137838,  training time: 25.08
[813.8, 826.0, 822.6, 739.4, 822.6, 784.8]
Episode 368	 reward: -10.13	 Mean_loss: 0.13952741,  training time: 25.05
[755.6, 872.0, 802.0, 756.0, 799.2, 826.0]
Episode 369	 reward: -9.42	 Mean_loss: 0.12874235,  training time: 24.76
[761.4, 790.0, 791.8, 714.2, 803.8, 801.0]
Episode 370	 reward: -9.47	 Mean_loss: 0.12143307,  training time: 25.02
[768.4, 785.6, 818.4, 786.2, 782.2, 807.6]
Episode 371	 reward: -10.52	 Mean_loss: 0.08751897,  training time: 24.80
[722.6, 799.0, 813.8, 780.4, 820.2, 803.2]
Episode 372	 reward: -9.45	 Mean_loss: 0.09270745,  training time: 24.78
[727.2, 835.4, 819.0, 721.2, 769.4, 806.8]
Episode 373	 reward: -9.89	 Mean_loss: 0.11379322,  training time: 24.92
[765.2, 807.2, 822.2, 752.8, 789.6, 740.6]
Episode 374	 reward: -9.72	 Mean_loss: 0.13866161,  training time: 25.06
[751.0, 755.2, 825.4, 777.6, 839.8, 812.0]
Episode 375	 reward: -9.69	 Mean_loss: 0.06223211,  training time: 25.08
[772.8, 820.0, 773.6, 746.4, 852.0, 818.4]
Episode 376	 reward: -9.11	 Mean_loss: 0.09025805,  training time: 25.03
[752.8, 842.0, 790.0, 785.4, 823.0, 889.6]
Episode 377	 reward: -9.29	 Mean_loss: 0.16200632,  training time: 25.05
[709.2, 849.2, 776.0, 785.0, 815.2, 829.4]
Episode 378	 reward: -9.59	 Mean_loss: 0.11520823,  training time: 25.06
[773.8, 853.0, 754.6, 751.6, 811.2, 820.0]
Episode 379	 reward: -9.24	 Mean_loss: 0.11891695,  training time: 25.07
[770.8, 817.2, 796.8, 787.0, 788.6, 845.8]
Episode 380	 reward: -9.30	 Mean_loss: 0.10732885,  training time: 25.08
[(15, 5), (15, 9), (15, 9), (15, 9), (15, 7), (15, 7)]
[697.8, 798.6, 721.4, 785.8, 777.2, 805.6]
Episode 381	 reward: -8.71	 Mean_loss: 0.20045775,  training time: 19.71
[716.6, 791.4, 736.8, 758.4, 740.4, 781.8]
Episode 382	 reward: -8.70	 Mean_loss: 0.15706144,  training time: 19.50
[737.0, 756.2, 748.6, 750.6, 775.2, 766.2]
Episode 383	 reward: -8.63	 Mean_loss: 0.13689570,  training time: 19.51
[749.8, 765.4, 770.6, 761.4, 774.0, 716.8]
Episode 384	 reward: -9.34	 Mean_loss: 0.08675450,  training time: 19.51
[720.6, 785.8, 742.8, 755.4, 742.4, 807.0]
Episode 385	 reward: -9.58	 Mean_loss: 0.27222556,  training time: 19.57
[719.4, 790.2, 741.4, 806.8, 749.4, 771.2]
Episode 386	 reward: -8.67	 Mean_loss: 0.21240509,  training time: 19.52
[698.6, 744.0, 761.8, 727.0, 768.4, 771.6]
Episode 387	 reward: -8.99	 Mean_loss: 0.16983706,  training time: 19.60
[727.2, 791.2, 768.8, 755.8, 757.0, 760.2]
Episode 388	 reward: -9.38	 Mean_loss: 0.15193868,  training time: 19.52
[739.4, 751.8, 751.0, 763.4, 764.4, 827.0]
Episode 389	 reward: -9.21	 Mean_loss: 0.23750532,  training time: 19.58
[786.4, 779.0, 784.2, 762.6, 741.6, 736.6]
Episode 390	 reward: -8.70	 Mean_loss: 0.09919370,  training time: 19.55
[758.4, 785.6, 767.8, 756.8, 730.6, 796.6]
Episode 391	 reward: -8.87	 Mean_loss: 0.20074441,  training time: 19.53
[740.8, 798.8, 742.6, 740.2, 771.6, 815.6]
Episode 392	 reward: -9.19	 Mean_loss: 0.16020653,  training time: 19.52
[717.2, 770.4, 775.2, 763.4, 761.2, 777.2]
Episode 393	 reward: -9.32	 Mean_loss: 0.16657664,  training time: 19.53
[752.8, 787.0, 745.8, 747.0, 715.8, 760.6]
Episode 394	 reward: -8.95	 Mean_loss: 0.12899278,  training time: 19.54
[754.8, 726.6, 803.0, 788.6, 734.4, 812.6]
Episode 395	 reward: -9.23	 Mean_loss: 0.16651079,  training time: 19.54
[670.8, 794.8, 785.2, 712.2, 715.2, 791.0]
Episode 396	 reward: -9.01	 Mean_loss: 0.16975594,  training time: 19.52
[687.4, 754.0, 780.0, 726.6, 747.4, 755.6]
Episode 397	 reward: -8.61	 Mean_loss: 0.15662329,  training time: 19.55
[730.0, 755.4, 735.8, 747.8, 782.6, 741.4]
Episode 398	 reward: -8.66	 Mean_loss: 0.17440380,  training time: 19.66
[768.6, 771.2, 757.8, 775.2, 778.6, 749.2]
Episode 399	 reward: -8.94	 Mean_loss: 0.10339623,  training time: 19.54
[717.8, 791.6, 735.0, 751.4, 768.6, 789.8]
Episode 400	 reward: -8.58	 Mean_loss: 0.15063584,  training time: 19.54
[(15, 7), (15, 10), (15, 9), (15, 10), (15, 9), (15, 10)]
[728.2, 778.4, 733.4, 846.8, 753.8, 780.6]
Episode 401	 reward: -9.23	 Mean_loss: 0.20202446,  training time: 23.62
[787.8, 792.0, 789.2, 819.0, 736.2, 802.2]
Episode 402	 reward: -9.40	 Mean_loss: 0.10045059,  training time: 23.74
[749.0, 797.0, 746.4, 816.2, 766.2, 768.8]
Episode 403	 reward: -9.74	 Mean_loss: 0.12374160,  training time: 23.73
[795.8, 794.8, 744.4, 808.2, 729.6, 816.8]
Episode 404	 reward: -9.58	 Mean_loss: 0.08412880,  training time: 23.73
[756.8, 784.6, 767.6, 772.2, 807.6, 829.4]
Episode 405	 reward: -8.97	 Mean_loss: 0.07541918,  training time: 23.75
[718.6, 815.4, 779.0, 857.4, 739.4, 812.2]
Episode 406	 reward: -9.66	 Mean_loss: 0.12381712,  training time: 23.72
[722.0, 805.2, 753.4, 820.0, 728.8, 781.4]
Episode 407	 reward: -10.02	 Mean_loss: 0.07571393,  training time: 23.71
[738.2, 751.4, 733.0, 826.0, 758.8, 800.0]
Episode 408	 reward: -8.92	 Mean_loss: 0.08171165,  training time: 23.71
[760.0, 796.4, 732.8, 843.0, 739.6, 804.2]
Episode 409	 reward: -9.22	 Mean_loss: 0.08326725,  training time: 23.71
[741.8, 778.2, 765.8, 788.4, 773.8, 807.4]
Episode 410	 reward: -8.81	 Mean_loss: 0.08886098,  training time: 23.71
[749.6, 792.2, 761.4, 792.2, 740.8, 780.4]
Episode 411	 reward: -9.81	 Mean_loss: 0.11756782,  training time: 23.78
[773.0, 798.4, 764.2, 854.2, 719.4, 781.0]
Episode 412	 reward: -8.89	 Mean_loss: 0.11023947,  training time: 23.67
[726.6, 798.4, 781.4, 848.0, 739.2, 767.8]
Episode 413	 reward: -9.37	 Mean_loss: 0.07366357,  training time: 23.70
[754.6, 795.2, 749.2, 836.6, 750.0, 815.6]
Episode 414	 reward: -9.48	 Mean_loss: 0.05643433,  training time: 23.69
[766.6, 795.6, 776.2, 777.6, 757.4, 752.8]
Episode 415	 reward: -9.37	 Mean_loss: 0.08831979,  training time: 23.54
[750.8, 826.2, 752.0, 779.6, 737.4, 805.2]
Episode 416	 reward: -8.97	 Mean_loss: 0.09891506,  training time: 23.40
[697.4, 774.0, 741.6, 875.6, 821.2, 789.2]
Episode 417	 reward: -8.79	 Mean_loss: 0.09775285,  training time: 23.41
[760.4, 741.6, 779.2, 806.4, 706.4, 835.2]
Episode 418	 reward: -9.36	 Mean_loss: 0.08123073,  training time: 23.46
[721.6, 823.8, 744.4, 832.0, 782.4, 817.0]
Episode 419	 reward: -9.69	 Mean_loss: 0.08603674,  training time: 23.39
[753.4, 778.0, 750.2, 866.4, 755.0, 749.0]
Episode 420	 reward: -9.42	 Mean_loss: 0.10613725,  training time: 23.40
[(15, 5), (15, 5), (15, 7), (15, 10), (15, 7), (15, 5)]
[754.6, 771.2, 816.4, 740.2, 759.0, 744.6]
Episode 421	 reward: -9.26	 Mean_loss: 0.39905176,  training time: 16.49
[722.8, 819.0, 761.8, 760.2, 717.2, 761.6]
Episode 422	 reward: -8.83	 Mean_loss: 0.41604304,  training time: 16.38
[674.8, 810.8, 739.8, 791.2, 740.4, 724.2]
Episode 423	 reward: -8.32	 Mean_loss: 0.34693140,  training time: 16.40
[733.4, 751.4, 768.6, 810.0, 701.8, 737.2]
Episode 424	 reward: -8.34	 Mean_loss: 0.35649520,  training time: 16.39
[701.0, 750.0, 744.8, 746.0, 736.2, 725.0]
Episode 425	 reward: -8.82	 Mean_loss: 0.24459879,  training time: 16.40
[783.2, 757.8, 731.6, 803.4, 727.6, 738.6]
Episode 426	 reward: -8.86	 Mean_loss: 0.32013074,  training time: 16.46
[709.8, 768.6, 772.8, 838.2, 726.6, 724.2]
Episode 427	 reward: -9.36	 Mean_loss: 0.30487981,  training time: 16.39
[762.4, 788.8, 730.0, 780.0, 727.8, 757.6]
Episode 428	 reward: -8.69	 Mean_loss: 0.43358812,  training time: 16.40
[730.2, 753.6, 746.8, 796.8, 760.4, 735.6]
Episode 429	 reward: -9.26	 Mean_loss: 0.29817912,  training time: 16.51
[747.0, 794.6, 777.8, 781.4, 737.4, 777.6]
Episode 430	 reward: -8.33	 Mean_loss: 0.36308047,  training time: 16.40
[719.8, 785.8, 695.8, 760.2, 769.0, 742.6]
Episode 431	 reward: -8.64	 Mean_loss: 0.34172809,  training time: 16.39
[722.0, 775.2, 770.0, 797.8, 778.0, 730.6]
Episode 432	 reward: -9.45	 Mean_loss: 0.33081913,  training time: 16.39
[771.0, 742.6, 732.2, 760.8, 742.2, 715.2]
Episode 433	 reward: -8.63	 Mean_loss: 0.28810489,  training time: 16.40
[732.4, 754.2, 748.8, 758.2, 737.0, 773.6]
Episode 434	 reward: -8.78	 Mean_loss: 0.38497844,  training time: 16.40
[723.0, 744.2, 751.8, 785.2, 732.4, 694.0]
Episode 435	 reward: -8.65	 Mean_loss: 0.27541271,  training time: 16.40
[750.6, 730.6, 751.0, 756.6, 764.4, 756.2]
Episode 436	 reward: -8.73	 Mean_loss: 0.27531219,  training time: 16.47
[716.2, 771.0, 749.8, 759.2, 762.2, 711.0]
Episode 437	 reward: -8.88	 Mean_loss: 0.24434607,  training time: 16.40
[725.8, 709.8, 758.2, 785.0, 710.8, 731.8]
Episode 438	 reward: -8.35	 Mean_loss: 0.24510901,  training time: 16.40
[682.8, 761.0, 730.8, 800.0, 744.8, 676.2]
Episode 439	 reward: -9.00	 Mean_loss: 0.18974571,  training time: 16.41
[744.8, 815.2, 756.8, 816.6, 741.6, 702.0]
Episode 440	 reward: -8.93	 Mean_loss: 0.25010926,  training time: 16.39
[(15, 7), (15, 7), (15, 7), (15, 5), (15, 7), (15, 5)]
[770.6, 694.0, 696.2, 687.4, 764.8, 738.6]
Episode 441	 reward: -8.45	 Mean_loss: 0.27548221,  training time: 15.98
[785.8, 727.4, 767.6, 713.8, 709.0, 697.0]
Episode 442	 reward: -8.56	 Mean_loss: 0.13276073,  training time: 15.93
[750.0, 715.0, 732.6, 755.4, 765.8, 699.0]
Episode 443	 reward: -8.72	 Mean_loss: 0.17642474,  training time: 15.90
[760.2, 693.4, 747.8, 735.8, 721.8, 718.2]
Episode 444	 reward: -8.65	 Mean_loss: 0.17964934,  training time: 15.91
[773.8, 687.2, 717.0, 736.6, 758.0, 777.8]
Episode 445	 reward: -9.22	 Mean_loss: 0.28450108,  training time: 15.92
[787.0, 692.6, 757.8, 716.2, 715.6, 703.4]
Episode 446	 reward: -8.63	 Mean_loss: 0.21373022,  training time: 15.99
[768.6, 751.8, 712.8, 718.6, 759.2, 703.8]
Episode 447	 reward: -8.88	 Mean_loss: 0.21516544,  training time: 15.99
[768.6, 763.4, 743.8, 716.2, 752.6, 724.8]
Episode 448	 reward: -8.64	 Mean_loss: 0.24423926,  training time: 16.08
[814.6, 731.0, 751.0, 744.8, 749.6, 729.6]
Episode 449	 reward: -8.34	 Mean_loss: 0.15938058,  training time: 16.11
[780.4, 753.0, 735.4, 697.0, 783.4, 747.6]
Episode 450	 reward: -8.31	 Mean_loss: 0.34938890,  training time: 16.08
[763.8, 728.2, 656.2, 723.8, 765.8, 738.6]
Episode 451	 reward: -8.56	 Mean_loss: 0.17781706,  training time: 16.09
[774.6, 718.2, 748.4, 704.4, 752.4, 702.4]
Episode 452	 reward: -8.61	 Mean_loss: 0.18671021,  training time: 16.08
[757.4, 704.8, 730.4, 729.6, 764.2, 702.6]
Episode 453	 reward: -8.57	 Mean_loss: 0.17411059,  training time: 16.08
[755.4, 697.8, 701.4, 698.2, 777.2, 732.0]
Episode 454	 reward: -8.56	 Mean_loss: 0.19790827,  training time: 16.09
[701.6, 709.4, 689.0, 696.2, 790.0, 726.2]
Episode 455	 reward: -8.13	 Mean_loss: 0.28171599,  training time: 16.15
[805.8, 735.0, 733.2, 719.8, 774.0, 790.8]
Episode 456	 reward: -8.78	 Mean_loss: 0.38168043,  training time: 15.97
[733.0, 714.2, 688.6, 785.8, 745.8, 737.6]
Episode 457	 reward: -8.63	 Mean_loss: 0.26485097,  training time: 15.96
[806.6, 695.0, 750.4, 781.4, 720.8, 681.6]
Episode 458	 reward: -8.38	 Mean_loss: 0.17241870,  training time: 15.97
[761.0, 697.8, 725.8, 731.2, 799.0, 693.0]
Episode 459	 reward: -8.51	 Mean_loss: 0.19989246,  training time: 15.94
[745.4, 707.6, 715.6, 700.8, 758.6, 725.8]
Episode 460	 reward: -8.83	 Mean_loss: 0.14897063,  training time: 15.95
[(15, 5), (15, 9), (15, 5), (15, 10), (15, 7), (15, 5)]
[705.2, 795.0, 745.0, 764.6, 728.6, 778.2]
Episode 461	 reward: -9.07	 Mean_loss: 0.25948250,  training time: 17.44
[696.8, 757.4, 717.4, 780.0, 714.4, 751.2]
Episode 462	 reward: -8.78	 Mean_loss: 0.17519478,  training time: 17.36
[699.8, 793.4, 696.6, 806.2, 719.0, 768.4]
Episode 463	 reward: -8.94	 Mean_loss: 0.29467264,  training time: 17.57
[710.8, 789.2, 701.6, 774.6, 708.4, 721.0]
Episode 464	 reward: -8.24	 Mean_loss: 0.22144012,  training time: 17.36
[700.4, 770.0, 704.2, 791.6, 761.0, 762.2]
Episode 465	 reward: -8.56	 Mean_loss: 0.23998569,  training time: 17.35
[710.8, 744.4, 715.2, 839.8, 723.0, 673.4]
Episode 466	 reward: -8.81	 Mean_loss: 0.23131335,  training time: 17.36
[690.4, 752.8, 720.6, 781.8, 721.4, 732.4]
Episode 467	 reward: -9.02	 Mean_loss: 0.18636769,  training time: 17.44
[696.8, 803.4, 765.8, 767.4, 728.0, 745.6]
Episode 468	 reward: -8.28	 Mean_loss: 0.24574649,  training time: 17.34
[706.2, 813.4, 705.4, 801.2, 764.2, 703.2]
Episode 469	 reward: -8.80	 Mean_loss: 0.16659020,  training time: 17.34
[729.4, 712.8, 725.2, 807.6, 719.6, 726.2]
Episode 470	 reward: -8.77	 Mean_loss: 0.25486740,  training time: 17.35
[735.2, 753.4, 718.6, 796.2, 701.8, 742.4]
Episode 471	 reward: -8.60	 Mean_loss: 0.24186748,  training time: 17.35
[731.2, 790.6, 763.6, 796.8, 738.0, 759.0]
Episode 472	 reward: -9.00	 Mean_loss: 0.29081640,  training time: 17.35
[724.2, 813.8, 738.2, 774.8, 708.0, 731.2]
Episode 473	 reward: -9.24	 Mean_loss: 0.22918078,  training time: 17.35
[710.8, 771.4, 730.6, 798.4, 755.4, 710.6]
Episode 474	 reward: -8.82	 Mean_loss: 0.18229431,  training time: 17.35
[700.2, 761.2, 735.8, 799.4, 673.8, 746.6]
Episode 475	 reward: -8.84	 Mean_loss: 0.27465189,  training time: 17.34
[700.8, 786.2, 717.6, 780.4, 747.8, 728.6]
Episode 476	 reward: -8.60	 Mean_loss: 0.24943241,  training time: 17.36
[689.0, 755.8, 731.6, 754.4, 713.2, 761.0]
Episode 477	 reward: -8.39	 Mean_loss: 0.33526999,  training time: 17.36
[683.6, 749.2, 719.0, 822.8, 713.0, 750.4]
Episode 478	 reward: -8.98	 Mean_loss: 0.20154411,  training time: 17.36
[697.2, 773.0, 716.2, 748.4, 738.2, 767.8]
Episode 479	 reward: -8.73	 Mean_loss: 0.31043324,  training time: 17.36
[698.4, 753.8, 720.2, 820.0, 716.8, 714.8]
Episode 480	 reward: -9.10	 Mean_loss: 0.26263237,  training time: 17.34
[(15, 9), (15, 10), (15, 9), (15, 9), (15, 9), (15, 9)]
[829.8, 849.8, 710.4, 781.4, 799.0, 796.4]
Episode 481	 reward: -9.11	 Mean_loss: 0.08477209,  training time: 23.54
[810.6, 833.4, 806.8, 758.0, 808.4, 742.4]
Episode 482	 reward: -9.45	 Mean_loss: 0.12229672,  training time: 23.51
[793.4, 794.6, 781.2, 798.2, 749.6, 727.8]
Episode 483	 reward: -8.85	 Mean_loss: 0.13283104,  training time: 23.57
[759.4, 854.8, 796.6, 759.4, 785.4, 761.2]
Episode 484	 reward: -8.78	 Mean_loss: 0.10578177,  training time: 23.55
[813.8, 869.2, 732.6, 829.4, 770.2, 773.0]
Episode 485	 reward: -9.45	 Mean_loss: 0.08964653,  training time: 23.50
[775.2, 835.6, 795.2, 772.0, 757.8, 768.0]
Episode 486	 reward: -8.63	 Mean_loss: 0.11028264,  training time: 23.64
[786.0, 782.8, 819.0, 745.2, 704.6, 719.8]
Episode 487	 reward: -9.17	 Mean_loss: 0.12989952,  training time: 23.67
[821.8, 828.2, 760.2, 740.2, 749.4, 788.2]
Episode 488	 reward: -9.44	 Mean_loss: 0.07819172,  training time: 23.67
[802.6, 770.0, 794.0, 764.4, 777.2, 784.4]
Episode 489	 reward: -9.21	 Mean_loss: 0.07985207,  training time: 23.73
[768.8, 785.0, 748.8, 764.0, 771.4, 731.0]
Episode 490	 reward: -9.12	 Mean_loss: 0.07993510,  training time: 23.73
[731.2, 861.6, 715.8, 808.8, 726.0, 737.4]
Episode 491	 reward: -8.73	 Mean_loss: 0.10800982,  training time: 23.60
[828.6, 841.4, 750.8, 819.8, 776.4, 738.8]
Episode 492	 reward: -9.54	 Mean_loss: 0.08183941,  training time: 23.52
[807.6, 836.8, 743.8, 759.6, 734.4, 766.8]
Episode 493	 reward: -8.84	 Mean_loss: 0.09085286,  training time: 23.66
[835.0, 797.0, 748.0, 758.2, 745.6, 742.4]
Episode 494	 reward: -9.78	 Mean_loss: 0.07400755,  training time: 23.66
[845.6, 922.8, 746.4, 777.4, 745.8, 750.0]
Episode 495	 reward: -8.67	 Mean_loss: 0.07569763,  training time: 23.66
[766.2, 857.4, 781.4, 752.0, 766.2, 739.6]
Episode 496	 reward: -8.81	 Mean_loss: 0.09034115,  training time: 23.67
[827.4, 819.4, 729.4, 755.6, 747.4, 751.0]
Episode 497	 reward: -9.50	 Mean_loss: 0.04606485,  training time: 23.72
[842.6, 841.0, 745.6, 752.4, 798.0, 800.2]
Episode 498	 reward: -8.73	 Mean_loss: 0.13832246,  training time: 23.66
[740.8, 821.8, 740.2, 817.8, 725.8, 795.8]
Episode 499	 reward: -9.27	 Mean_loss: 0.07957981,  training time: 23.66
[783.4, 797.0, 769.4, 774.4, 763.4, 756.6]
Episode 500	 reward: -9.13	 Mean_loss: 0.09014783,  training time: 23.73
[(15, 9), (15, 9), (15, 9), (15, 7), (15, 5), (15, 7)]
[776.8, 724.6, 759.0, 733.0, 760.8, 720.2]
Episode 501	 reward: -9.03	 Mean_loss: 0.12168077,  training time: 19.80
[754.4, 747.2, 818.6, 745.2, 767.2, 762.0]
Episode 502	 reward: -9.10	 Mean_loss: 0.14562438,  training time: 19.58
[774.6, 731.4, 747.0, 769.2, 733.8, 725.6]
Episode 503	 reward: -8.86	 Mean_loss: 0.08612449,  training time: 19.63
[841.8, 710.8, 741.0, 731.6, 741.0, 709.8]
Episode 504	 reward: -8.35	 Mean_loss: 0.08782560,  training time: 19.65
[816.6, 741.4, 753.6, 738.6, 766.0, 764.8]
Episode 505	 reward: -9.27	 Mean_loss: 0.15035135,  training time: 19.64
[791.0, 739.2, 773.8, 761.8, 731.8, 712.0]
Episode 506	 reward: -9.04	 Mean_loss: 0.06988951,  training time: 19.65
[788.8, 725.0, 743.8, 790.4, 745.8, 727.6]
Episode 507	 reward: -8.36	 Mean_loss: 0.13245629,  training time: 19.66
[785.2, 723.4, 746.0, 723.6, 751.0, 708.4]
Episode 508	 reward: -8.39	 Mean_loss: 0.08557149,  training time: 19.64
[771.0, 754.6, 761.6, 745.6, 727.8, 720.2]
Episode 509	 reward: -8.00	 Mean_loss: 0.12021681,  training time: 19.64
[772.4, 759.0, 774.6, 754.8, 739.4, 715.0]
Episode 510	 reward: -9.14	 Mean_loss: 0.08209201,  training time: 19.63
[746.8, 746.2, 758.6, 795.8, 723.0, 684.8]
Episode 511	 reward: -8.52	 Mean_loss: 0.09439226,  training time: 19.64
[731.2, 726.8, 741.6, 757.0, 780.6, 776.2]
Episode 512	 reward: -8.90	 Mean_loss: 0.18609665,  training time: 19.70
[811.8, 804.0, 779.0, 718.8, 782.4, 714.0]
Episode 513	 reward: -8.94	 Mean_loss: 0.06991699,  training time: 19.75
[744.4, 730.8, 805.0, 742.8, 755.2, 689.0]
Episode 514	 reward: -8.84	 Mean_loss: 0.13155669,  training time: 19.64
[775.2, 758.6, 741.6, 752.8, 759.2, 721.0]
Episode 515	 reward: -9.58	 Mean_loss: 0.10046553,  training time: 19.65
[748.4, 726.4, 707.2, 775.4, 726.2, 692.2]
Episode 516	 reward: -8.35	 Mean_loss: 0.13317409,  training time: 19.65
[817.0, 784.6, 727.2, 752.8, 779.6, 710.0]
Episode 517	 reward: -8.47	 Mean_loss: 0.09997251,  training time: 19.66
[776.4, 706.6, 759.8, 778.4, 819.6, 694.0]
Episode 518	 reward: -9.13	 Mean_loss: 0.10389288,  training time: 19.66
[731.4, 748.4, 781.6, 735.6, 726.4, 784.0]
Episode 519	 reward: -8.63	 Mean_loss: 0.28983593,  training time: 19.66
[726.0, 687.4, 767.6, 716.2, 709.2, 704.2]
Episode 520	 reward: -8.33	 Mean_loss: 0.11690766,  training time: 19.66
[(15, 7), (15, 5), (15, 5), (15, 9), (15, 7), (15, 5)]
[732.4, 732.6, 726.2, 771.6, 725.6, 676.0]
Episode 521	 reward: -8.33	 Mean_loss: 0.14300427,  training time: 16.21
[760.2, 687.4, 767.2, 805.4, 753.8, 704.0]
Episode 522	 reward: -8.27	 Mean_loss: 0.38262615,  training time: 16.12
[763.0, 725.0, 757.6, 807.8, 711.8, 659.6]
Episode 523	 reward: -8.05	 Mean_loss: 0.17244354,  training time: 16.15
[733.6, 747.4, 717.2, 725.0, 745.0, 660.2]
Episode 524	 reward: -8.01	 Mean_loss: 0.16023548,  training time: 16.13
[784.8, 710.4, 805.6, 794.2, 756.0, 685.0]
Episode 525	 reward: -8.68	 Mean_loss: 0.20992792,  training time: 16.15
[774.0, 736.8, 707.4, 772.6, 748.8, 719.8]
Episode 526	 reward: -8.45	 Mean_loss: 0.26656660,  training time: 16.21
[754.2, 686.4, 731.6, 759.4, 721.2, 651.2]
Episode 527	 reward: -8.37	 Mean_loss: 0.12032254,  training time: 16.10
[740.2, 687.6, 753.4, 754.2, 701.0, 670.8]
Episode 528	 reward: -8.25	 Mean_loss: 0.18755841,  training time: 16.11
[777.0, 701.4, 733.0, 754.2, 725.6, 717.8]
Episode 529	 reward: -8.06	 Mean_loss: 0.25907171,  training time: 16.12
[794.4, 686.4, 759.2, 757.6, 759.4, 663.0]
Episode 530	 reward: -8.43	 Mean_loss: 0.16403148,  training time: 16.12
[780.8, 713.4, 756.2, 790.4, 766.8, 658.8]
Episode 531	 reward: -9.27	 Mean_loss: 0.14638074,  training time: 16.12
[812.2, 717.4, 727.4, 764.4, 698.2, 699.0]
Episode 532	 reward: -8.90	 Mean_loss: 0.23040433,  training time: 16.12
[751.0, 665.4, 745.6, 744.8, 708.0, 659.0]
Episode 533	 reward: -8.17	 Mean_loss: 0.15505542,  training time: 16.13
[774.8, 743.2, 750.8, 773.6, 727.8, 624.0]
Episode 534	 reward: -8.16	 Mean_loss: 0.10400680,  training time: 16.12
[778.0, 718.6, 734.8, 781.4, 734.4, 696.0]
Episode 535	 reward: -8.23	 Mean_loss: 0.20446517,  training time: 16.12
[783.2, 701.8, 756.6, 756.6, 746.6, 680.4]
Episode 536	 reward: -8.48	 Mean_loss: 0.12364874,  training time: 16.13
[779.2, 705.0, 699.8, 744.8, 715.2, 683.6]
Episode 537	 reward: -8.83	 Mean_loss: 0.17052713,  training time: 16.17
[755.2, 730.8, 712.8, 772.8, 732.2, 678.4]
Episode 538	 reward: -8.74	 Mean_loss: 0.12692706,  training time: 16.12
[758.6, 708.8, 711.8, 754.0, 779.8, 717.0]
Episode 539	 reward: -8.14	 Mean_loss: 0.16265066,  training time: 16.12
[736.8, 702.6, 700.0, 767.8, 720.0, 677.0]
Episode 540	 reward: -8.16	 Mean_loss: 0.19223648,  training time: 16.11
[(15, 7), (15, 7), (15, 7), (15, 10), (15, 10), (15, 10)]
[746.6, 754.0, 713.6, 791.8, 783.6, 835.4]
Episode 541	 reward: -10.10	 Mean_loss: 0.13749455,  training time: 21.93
[723.2, 756.0, 695.8, 772.4, 813.4, 741.0]
Episode 542	 reward: -9.14	 Mean_loss: 0.25761852,  training time: 22.01
[741.6, 768.8, 684.6, 747.8, 836.0, 847.6]
Episode 543	 reward: -9.48	 Mean_loss: 0.19663946,  training time: 21.89
[756.6, 723.0, 742.6, 739.2, 806.6, 825.6]
Episode 544	 reward: -9.10	 Mean_loss: 0.14749192,  training time: 21.90
[737.4, 764.0, 732.8, 764.4, 812.6, 877.6]
Episode 545	 reward: -9.67	 Mean_loss: 0.15350282,  training time: 21.90
[714.0, 802.4, 711.6, 781.0, 773.6, 792.8]
Episode 546	 reward: -9.81	 Mean_loss: 0.18781391,  training time: 21.89
[730.6, 800.2, 684.8, 776.8, 821.2, 820.0]
Episode 547	 reward: -9.27	 Mean_loss: 0.13392000,  training time: 21.96
[725.2, 780.8, 714.0, 752.4, 803.4, 796.0]
Episode 548	 reward: -8.63	 Mean_loss: 0.14724673,  training time: 21.88
[762.4, 748.6, 706.2, 752.4, 874.2, 797.6]
Episode 549	 reward: -9.52	 Mean_loss: 0.19961776,  training time: 21.91
[735.0, 755.6, 750.8, 767.0, 803.8, 810.4]
Episode 550	 reward: -9.05	 Mean_loss: 0.12244890,  training time: 21.89
[737.0, 721.6, 705.6, 809.2, 835.0, 860.2]
Episode 551	 reward: -9.80	 Mean_loss: 0.14289929,  training time: 21.91
[685.6, 732.6, 747.6, 739.4, 809.2, 795.6]
Episode 552	 reward: -9.06	 Mean_loss: 0.12199914,  training time: 21.95
[762.4, 805.4, 715.2, 769.0, 791.0, 825.2]
Episode 553	 reward: -9.44	 Mean_loss: 0.11210132,  training time: 21.89
[755.8, 792.2, 766.8, 740.0, 751.4, 832.0]
Episode 554	 reward: -9.31	 Mean_loss: 0.10277056,  training time: 21.84
[795.6, 738.2, 722.4, 800.8, 802.8, 752.8]
Episode 555	 reward: -9.62	 Mean_loss: 0.14785253,  training time: 21.89
[733.0, 762.2, 709.0, 769.6, 829.8, 820.8]
Episode 556	 reward: -9.01	 Mean_loss: 0.16305640,  training time: 21.86
[772.6, 785.6, 686.4, 747.8, 767.8, 821.8]
Episode 557	 reward: -9.60	 Mean_loss: 0.11673937,  training time: 21.83
[739.0, 776.4, 721.8, 770.8, 809.8, 769.6]
Episode 558	 reward: -9.93	 Mean_loss: 0.16309518,  training time: 21.83
[764.2, 762.2, 774.2, 807.8, 793.6, 795.0]
Episode 559	 reward: -9.54	 Mean_loss: 0.17001536,  training time: 21.86
[707.2, 732.2, 731.6, 766.8, 759.0, 744.4]
Episode 560	 reward: -8.66	 Mean_loss: 0.16816182,  training time: 21.83
[(15, 5), (15, 5), (15, 5), (15, 7), (15, 10), (15, 7)]
[723.0, 697.0, 726.0, 741.8, 747.6, 707.0]
Episode 561	 reward: -9.26	 Mean_loss: 0.12949130,  training time: 16.68
[717.2, 711.8, 708.2, 771.6, 781.0, 734.8]
Episode 562	 reward: -9.29	 Mean_loss: 0.12925546,  training time: 16.56
[702.6, 717.2, 798.4, 726.0, 797.4, 706.6]
Episode 563	 reward: -9.03	 Mean_loss: 0.07795484,  training time: 16.54
[725.6, 769.4, 762.0, 737.6, 802.4, 680.6]
Episode 564	 reward: -8.99	 Mean_loss: 0.15297453,  training time: 16.55
[702.8, 731.0, 728.8, 753.6, 797.8, 741.8]
Episode 565	 reward: -8.92	 Mean_loss: 0.14364113,  training time: 16.56
[700.2, 714.8, 720.2, 739.8, 762.0, 714.8]
Episode 566	 reward: -9.17	 Mean_loss: 0.09498159,  training time: 16.55
[713.2, 714.4, 735.8, 745.0, 821.8, 784.0]
Episode 567	 reward: -8.85	 Mean_loss: 0.13512693,  training time: 16.56
[715.8, 728.2, 760.6, 748.4, 749.4, 694.0]
Episode 568	 reward: -8.84	 Mean_loss: 0.09650472,  training time: 16.57
[735.6, 718.6, 738.2, 735.8, 783.4, 697.2]
Episode 569	 reward: -9.19	 Mean_loss: 0.08152638,  training time: 16.68
[717.6, 759.4, 719.0, 740.2, 792.6, 767.0]
Episode 570	 reward: -8.99	 Mean_loss: 0.11851801,  training time: 16.55
[698.0, 729.4, 722.6, 720.6, 820.0, 718.6]
Episode 571	 reward: -9.51	 Mean_loss: 0.09083943,  training time: 16.55
[732.2, 719.0, 815.2, 743.8, 734.4, 701.2]
Episode 572	 reward: -9.54	 Mean_loss: 0.08237647,  training time: 16.41
[708.8, 705.6, 735.0, 723.2, 753.2, 745.8]
Episode 573	 reward: -9.31	 Mean_loss: 0.13791047,  training time: 16.41
[696.2, 686.4, 725.8, 776.4, 789.0, 748.6]
Episode 574	 reward: -8.68	 Mean_loss: 0.12501656,  training time: 16.50
[691.4, 782.0, 699.8, 747.4, 782.0, 699.0]
Episode 575	 reward: -8.58	 Mean_loss: 0.08062860,  training time: 16.42
[689.4, 710.6, 745.6, 790.6, 817.0, 722.6]
Episode 576	 reward: -9.66	 Mean_loss: 0.06192947,  training time: 16.56
[701.6, 707.2, 728.0, 717.6, 790.2, 757.0]
Episode 577	 reward: -9.02	 Mean_loss: 0.13469860,  training time: 16.58
[740.2, 732.2, 732.2, 756.2, 851.8, 725.4]
Episode 578	 reward: -8.67	 Mean_loss: 0.10892192,  training time: 16.58
[676.2, 686.0, 723.0, 803.4, 769.2, 703.6]
Episode 579	 reward: -8.85	 Mean_loss: 0.10669351,  training time: 16.57
[648.6, 749.8, 723.4, 780.2, 805.8, 717.2]
Episode 580	 reward: -9.62	 Mean_loss: 0.07877154,  training time: 16.58
[(15, 5), (15, 7), (15, 7), (15, 9), (15, 7), (15, 5)]
[722.0, 783.6, 774.6, 737.2, 730.8, 700.4]
Episode 581	 reward: -8.84	 Mean_loss: 0.19080342,  training time: 17.09
[736.8, 728.4, 717.4, 768.2, 705.8, 718.2]
Episode 582	 reward: -8.91	 Mean_loss: 0.18546152,  training time: 16.98
[722.0, 752.8, 751.4, 733.0, 743.0, 712.2]
Episode 583	 reward: -8.37	 Mean_loss: 0.17430054,  training time: 16.99
[744.4, 724.0, 701.2, 771.0, 734.2, 751.8]
Episode 584	 reward: -8.67	 Mean_loss: 0.20767443,  training time: 16.99
[698.6, 706.2, 782.2, 724.6, 699.0, 722.6]
Episode 585	 reward: -8.77	 Mean_loss: 0.19867282,  training time: 17.05
[729.4, 768.8, 728.4, 720.8, 765.2, 716.2]
Episode 586	 reward: -8.65	 Mean_loss: 0.19538261,  training time: 17.02
[706.4, 755.8, 734.4, 736.6, 764.0, 707.8]
Episode 587	 reward: -8.31	 Mean_loss: 0.12175217,  training time: 16.99
[673.6, 748.0, 713.6, 722.0, 723.2, 711.6]
Episode 588	 reward: -8.42	 Mean_loss: 0.13216026,  training time: 16.99
[708.4, 810.6, 731.0, 724.8, 698.6, 698.0]
Episode 589	 reward: -8.75	 Mean_loss: 0.14778376,  training time: 16.98
[741.0, 681.2, 731.6, 738.6, 736.6, 718.4]
Episode 590	 reward: -8.37	 Mean_loss: 0.21537642,  training time: 16.98
[674.8, 704.2, 721.0, 765.8, 730.6, 709.8]
Episode 591	 reward: -8.61	 Mean_loss: 0.12728556,  training time: 16.98
[693.4, 721.8, 732.2, 744.4, 726.4, 696.6]
Episode 592	 reward: -8.89	 Mean_loss: 0.18004102,  training time: 16.98
[696.6, 715.6, 745.8, 718.0, 731.8, 706.6]
Episode 593	 reward: -8.39	 Mean_loss: 0.20560059,  training time: 17.00
[703.6, 724.2, 743.2, 741.2, 757.6, 698.6]
Episode 594	 reward: -8.58	 Mean_loss: 0.13020864,  training time: 16.98
[692.0, 740.2, 794.6, 774.6, 717.4, 663.4]
Episode 595	 reward: -8.60	 Mean_loss: 0.13712177,  training time: 16.98
[693.2, 738.0, 738.4, 747.0, 765.6, 733.0]
Episode 596	 reward: -8.46	 Mean_loss: 0.18708418,  training time: 16.98
[715.4, 731.6, 740.6, 725.6, 723.8, 726.2]
Episode 597	 reward: -8.34	 Mean_loss: 0.16079569,  training time: 16.98
[713.6, 756.8, 732.8, 733.4, 704.4, 696.2]
Episode 598	 reward: -8.78	 Mean_loss: 0.15202276,  training time: 17.01
[694.6, 758.6, 768.4, 744.8, 734.0, 751.8]
Episode 599	 reward: -8.23	 Mean_loss: 0.17518021,  training time: 17.05
[700.4, 735.0, 744.6, 734.2, 742.4, 727.0]
Episode 600	 reward: -8.75	 Mean_loss: 0.23566961,  training time: 16.98
[(15, 10), (15, 9), (15, 7), (15, 7), (15, 10), (15, 10)]
[757.0, 770.6, 766.0, 703.6, 766.2, 722.0]
Episode 601	 reward: -9.52	 Mean_loss: 0.16178924,  training time: 22.83
[789.6, 716.8, 777.0, 689.0, 783.6, 695.6]
Episode 602	 reward: -9.09	 Mean_loss: 0.15819322,  training time: 22.79
[738.2, 733.2, 755.0, 717.2, 769.8, 745.4]
Episode 603	 reward: -9.15	 Mean_loss: 0.10163432,  training time: 22.80
[791.8, 742.8, 745.8, 720.2, 747.8, 742.4]
Episode 604	 reward: -9.05	 Mean_loss: 0.07981507,  training time: 22.90
[745.6, 706.0, 789.2, 740.8, 797.6, 758.2]
Episode 605	 reward: -9.37	 Mean_loss: 0.08819163,  training time: 22.80
[725.4, 746.4, 766.2, 740.4, 826.8, 760.2]
Episode 606	 reward: -9.85	 Mean_loss: 0.09177945,  training time: 22.80
[779.8, 782.4, 774.0, 677.4, 791.8, 736.0]
Episode 607	 reward: -9.06	 Mean_loss: 0.11048226,  training time: 22.80
[731.0, 766.4, 772.8, 764.0, 795.8, 730.6]
Episode 608	 reward: -9.57	 Mean_loss: 0.08269322,  training time: 22.85
[759.4, 793.4, 755.8, 724.0, 822.8, 766.4]
Episode 609	 reward: -8.71	 Mean_loss: 0.06227595,  training time: 22.82
[796.4, 745.2, 785.8, 741.2, 810.0, 753.6]
Episode 610	 reward: -9.75	 Mean_loss: 0.09462111,  training time: 22.80
[800.0, 751.6, 779.2, 704.6, 758.8, 755.4]
Episode 611	 reward: -9.01	 Mean_loss: 0.06989990,  training time: 22.82
[783.4, 708.6, 841.8, 695.6, 737.4, 777.4]
Episode 612	 reward: -9.41	 Mean_loss: 0.06128671,  training time: 22.82
[790.6, 770.2, 779.4, 711.8, 782.0, 755.8]
Episode 613	 reward: -9.59	 Mean_loss: 0.07013596,  training time: 22.88
[783.2, 727.6, 810.4, 738.2, 706.4, 746.2]
Episode 614	 reward: -9.10	 Mean_loss: 0.07051010,  training time: 22.83
[777.0, 745.8, 756.0, 695.4, 754.8, 769.4]
Episode 615	 reward: -9.37	 Mean_loss: 0.06472814,  training time: 22.83
[766.8, 778.8, 772.6, 686.0, 752.2, 731.6]
Episode 616	 reward: -8.95	 Mean_loss: 0.06884696,  training time: 22.83
[770.6, 764.6, 786.6, 732.4, 775.0, 765.6]
Episode 617	 reward: -8.49	 Mean_loss: 0.09565741,  training time: 22.92
[792.4, 734.2, 791.4, 732.6, 787.2, 737.8]
Episode 618	 reward: -9.26	 Mean_loss: 0.08204064,  training time: 22.91
[824.6, 807.8, 765.8, 750.4, 799.2, 746.6]
Episode 619	 reward: -8.92	 Mean_loss: 0.10036458,  training time: 22.81
[732.8, 768.6, 775.0, 699.4, 729.8, 731.6]
Episode 620	 reward: -9.87	 Mean_loss: 0.06455164,  training time: 22.82
[(15, 5), (15, 5), (15, 10), (15, 9), (15, 7), (15, 5)]
[725.4, 740.4, 778.0, 734.4, 785.6, 662.8]
Episode 621	 reward: -9.12	 Mean_loss: 0.15564631,  training time: 17.61
[761.2, 695.6, 812.8, 725.6, 780.8, 641.4]
Episode 622	 reward: -8.46	 Mean_loss: 0.16392018,  training time: 17.51
[753.6, 701.4, 778.2, 754.4, 775.6, 679.0]
Episode 623	 reward: -8.85	 Mean_loss: 0.12751925,  training time: 17.53
[731.2, 710.0, 803.8, 757.6, 712.8, 709.8]
Episode 624	 reward: -8.46	 Mean_loss: 0.23702356,  training time: 17.50
[736.4, 675.4, 792.0, 728.0, 705.4, 684.6]
Episode 625	 reward: -9.21	 Mean_loss: 0.14499760,  training time: 17.50
[734.8, 713.0, 776.8, 730.4, 754.2, 646.0]
Episode 626	 reward: -8.85	 Mean_loss: 0.13075361,  training time: 17.49
[765.4, 703.4, 785.4, 730.4, 738.8, 696.0]
Episode 627	 reward: -8.94	 Mean_loss: 0.22744523,  training time: 17.49
[737.8, 716.8, 803.2, 724.8, 722.6, 680.4]
Episode 628	 reward: -8.49	 Mean_loss: 0.14673264,  training time: 17.50
[745.0, 753.0, 770.2, 741.0, 703.0, 693.8]
Episode 629	 reward: -8.77	 Mean_loss: 0.17901652,  training time: 17.49
[740.2, 724.6, 779.4, 792.6, 770.8, 664.6]
Episode 630	 reward: -8.91	 Mean_loss: 0.09938820,  training time: 17.49
[737.4, 715.0, 766.2, 749.0, 740.2, 675.2]
Episode 631	 reward: -8.09	 Mean_loss: 0.13884613,  training time: 17.50
[770.4, 690.4, 804.6, 724.2, 733.0, 683.8]
Episode 632	 reward: -8.67	 Mean_loss: 0.12968636,  training time: 17.57
[750.4, 703.4, 809.2, 751.8, 739.6, 691.8]
Episode 633	 reward: -8.85	 Mean_loss: 0.13179292,  training time: 17.47
[756.2, 760.8, 744.6, 742.4, 736.6, 718.0]
Episode 634	 reward: -8.31	 Mean_loss: 0.18955489,  training time: 17.41
[735.8, 750.6, 800.8, 735.4, 722.2, 696.2]
Episode 635	 reward: -8.66	 Mean_loss: 0.19677664,  training time: 17.28
[720.4, 697.6, 809.2, 752.0, 759.8, 688.2]
Episode 636	 reward: -8.47	 Mean_loss: 0.22080834,  training time: 17.28
[740.8, 722.4, 822.6, 738.2, 753.2, 683.2]
Episode 637	 reward: -8.96	 Mean_loss: 0.14487620,  training time: 17.29
[751.4, 687.2, 785.6, 759.0, 763.2, 668.0]
Episode 638	 reward: -8.59	 Mean_loss: 0.11676110,  training time: 17.29
[740.0, 746.2, 815.6, 730.4, 747.2, 713.8]
Episode 639	 reward: -8.41	 Mean_loss: 0.21047013,  training time: 17.28
[763.6, 703.4, 808.2, 773.8, 706.8, 657.8]
Episode 640	 reward: -8.96	 Mean_loss: 0.12933105,  training time: 17.29
[(15, 7), (15, 10), (15, 9), (15, 5), (15, 7), (15, 9)]
[754.6, 754.0, 804.0, 695.8, 722.0, 787.6]
Episode 641	 reward: -9.33	 Mean_loss: 0.28186348,  training time: 19.92
[737.8, 747.2, 800.2, 769.0, 703.6, 727.2]
Episode 642	 reward: -8.68	 Mean_loss: 0.06551106,  training time: 19.86
[735.4, 743.0, 816.4, 731.4, 701.2, 738.2]
Episode 643	 reward: -9.35	 Mean_loss: 0.10795331,  training time: 19.90
[714.2, 745.8, 804.0, 707.0, 722.2, 775.6]
Episode 644	 reward: -8.65	 Mean_loss: 0.10932404,  training time: 19.87
[762.0, 737.0, 764.2, 739.4, 713.6, 756.8]
Episode 645	 reward: -9.16	 Mean_loss: 0.07779805,  training time: 19.98
[724.6, 770.4, 749.0, 702.4, 691.2, 740.6]
Episode 646	 reward: -9.21	 Mean_loss: 0.09067255,  training time: 20.10
[753.4, 710.8, 763.4, 739.0, 688.8, 707.2]
Episode 647	 reward: -9.47	 Mean_loss: 0.09024631,  training time: 20.16
[739.2, 727.4, 821.8, 759.0, 692.0, 726.6]
Episode 648	 reward: -8.99	 Mean_loss: 0.11242162,  training time: 20.09
[803.4, 719.0, 797.6, 737.8, 715.0, 746.8]
Episode 649	 reward: -9.61	 Mean_loss: 0.07000866,  training time: 20.08
[720.8, 795.2, 738.0, 697.0, 671.8, 747.6]
Episode 650	 reward: -8.81	 Mean_loss: 0.07925421,  training time: 20.08
[766.0, 747.4, 731.6, 735.2, 738.4, 758.0]
Episode 651	 reward: -9.17	 Mean_loss: 0.11288682,  training time: 20.09
[784.0, 790.4, 803.8, 745.8, 707.6, 761.8]
Episode 652	 reward: -9.68	 Mean_loss: 0.15775928,  training time: 20.09
[721.2, 752.6, 783.4, 748.0, 676.0, 761.4]
Episode 653	 reward: -8.99	 Mean_loss: 0.11144877,  training time: 20.09
[751.6, 816.8, 805.4, 737.0, 710.0, 768.8]
Episode 654	 reward: -9.25	 Mean_loss: 0.05929998,  training time: 20.08
[784.8, 725.8, 774.0, 720.0, 727.6, 785.2]
Episode 655	 reward: -9.59	 Mean_loss: 0.12729135,  training time: 20.09
[688.2, 763.8, 743.4, 687.4, 705.8, 763.2]
Episode 656	 reward: -9.07	 Mean_loss: 0.06341686,  training time: 20.14
[755.4, 766.4, 757.6, 733.4, 740.2, 781.8]
Episode 657	 reward: -9.57	 Mean_loss: 0.17208181,  training time: 20.09
[774.2, 753.0, 744.6, 798.4, 721.2, 760.6]
Episode 658	 reward: -9.19	 Mean_loss: 0.05148077,  training time: 20.09
[770.8, 746.8, 708.6, 751.0, 674.0, 784.6]
Episode 659	 reward: -9.34	 Mean_loss: 0.06995669,  training time: 20.09
[782.8, 741.2, 861.2, 748.2, 741.6, 711.4]
Episode 660	 reward: -9.18	 Mean_loss: 0.07916158,  training time: 20.10
[(15, 9), (15, 7), (15, 9), (15, 9), (15, 9), (15, 7)]
[723.4, 736.8, 745.2, 774.0, 751.4, 773.8]
Episode 661	 reward: -9.62	 Mean_loss: 0.10358933,  training time: 21.60
[727.2, 744.8, 745.2, 724.2, 748.0, 744.2]
Episode 662	 reward: -9.19	 Mean_loss: 0.11379704,  training time: 21.44
[724.2, 742.8, 757.2, 745.0, 749.6, 743.4]
Episode 663	 reward: -9.97	 Mean_loss: 0.10864635,  training time: 21.42
[722.6, 727.2, 782.6, 757.8, 746.4, 816.8]
Episode 664	 reward: -9.85	 Mean_loss: 0.28744927,  training time: 21.44
[754.0, 708.2, 775.4, 762.6, 743.6, 767.0]
Episode 665	 reward: -9.22	 Mean_loss: 0.11964171,  training time: 21.51
[753.6, 741.8, 788.4, 758.0, 765.8, 786.2]
Episode 666	 reward: -9.47	 Mean_loss: 0.20099217,  training time: 21.42
[740.8, 741.0, 716.6, 720.0, 734.6, 753.2]
Episode 667	 reward: -9.15	 Mean_loss: 0.15534328,  training time: 21.41
[756.2, 734.4, 760.4, 761.8, 773.0, 764.6]
Episode 668	 reward: -9.35	 Mean_loss: 0.14272885,  training time: 21.42
[739.0, 738.2, 784.0, 783.2, 745.2, 755.0]
Episode 669	 reward: -8.84	 Mean_loss: 0.20687631,  training time: 21.42
[746.0, 729.4, 739.8, 749.0, 815.0, 737.6]
Episode 670	 reward: -9.13	 Mean_loss: 0.10119516,  training time: 21.44
[748.4, 699.2, 810.4, 743.0, 760.8, 760.6]
Episode 671	 reward: -9.39	 Mean_loss: 0.17249747,  training time: 21.51
[771.8, 718.0, 781.8, 731.2, 721.2, 804.8]
Episode 672	 reward: -8.91	 Mean_loss: 0.17926873,  training time: 21.42
[777.2, 773.6, 760.4, 785.4, 773.6, 813.8]
Episode 673	 reward: -9.04	 Mean_loss: 0.31759781,  training time: 21.43
[777.2, 770.6, 786.4, 767.8, 792.4, 784.0]
Episode 674	 reward: -9.67	 Mean_loss: 0.17944583,  training time: 21.36
[724.0, 737.4, 770.4, 770.2, 809.2, 729.4]
Episode 675	 reward: -8.87	 Mean_loss: 0.11550514,  training time: 21.40
[708.6, 705.2, 784.8, 827.4, 790.6, 762.4]
Episode 676	 reward: -8.68	 Mean_loss: 0.15587229,  training time: 21.43
[730.8, 787.0, 758.8, 768.8, 752.0, 742.4]
Episode 677	 reward: -9.16	 Mean_loss: 0.23488386,  training time: 21.49
[735.6, 738.6, 765.6, 741.0, 768.4, 763.0]
Episode 678	 reward: -9.00	 Mean_loss: 0.09889570,  training time: 21.42
[728.2, 772.0, 743.6, 762.6, 833.8, 710.6]
Episode 679	 reward: -9.49	 Mean_loss: 0.07507493,  training time: 21.42
[770.4, 746.4, 794.0, 737.0, 766.2, 773.4]
Episode 680	 reward: -9.48	 Mean_loss: 0.17115031,  training time: 21.42
[(15, 10), (15, 10), (15, 7), (15, 5), (15, 10), (15, 7)]
[770.2, 765.6, 713.6, 750.0, 846.8, 672.2]
Episode 681	 reward: -8.38	 Mean_loss: 0.12173895,  training time: 21.10
[814.8, 818.2, 674.2, 706.0, 780.0, 683.4]
Episode 682	 reward: -8.27	 Mean_loss: 0.07400042,  training time: 21.03
[787.4, 831.0, 664.2, 670.2, 792.4, 655.2]
Episode 683	 reward: -8.44	 Mean_loss: 0.07277863,  training time: 21.03
[776.6, 825.4, 687.2, 674.0, 763.6, 704.4]
Episode 684	 reward: -9.11	 Mean_loss: 0.07172845,  training time: 21.03
[810.8, 813.4, 694.0, 690.2, 792.2, 693.8]
Episode 685	 reward: -8.35	 Mean_loss: 0.16130134,  training time: 21.02
[782.2, 753.0, 661.8, 700.6, 772.2, 710.2]
Episode 686	 reward: -8.47	 Mean_loss: 0.06862446,  training time: 20.81
[817.4, 779.8, 705.2, 718.6, 775.6, 664.6]
Episode 687	 reward: -8.67	 Mean_loss: 0.10873375,  training time: 20.80
[794.8, 822.0, 699.6, 695.6, 805.4, 649.8]
Episode 688	 reward: -8.29	 Mean_loss: 0.07761753,  training time: 20.81
[795.8, 817.0, 707.4, 686.2, 771.8, 708.8]
Episode 689	 reward: -8.50	 Mean_loss: 0.07856370,  training time: 20.81
[776.2, 800.6, 691.8, 715.4, 784.6, 699.2]
Episode 690	 reward: -8.69	 Mean_loss: 0.09829082,  training time: 20.85
[759.0, 828.8, 665.2, 715.0, 748.4, 720.6]
Episode 691	 reward: -8.39	 Mean_loss: 0.08095353,  training time: 20.81
[818.4, 774.6, 689.4, 721.2, 815.8, 686.6]
Episode 692	 reward: -9.20	 Mean_loss: 0.05565264,  training time: 20.80
[770.4, 798.0, 700.0, 747.4, 783.8, 687.4]
Episode 693	 reward: -8.28	 Mean_loss: 0.07305822,  training time: 20.86
[758.8, 794.8, 681.4, 683.2, 792.8, 669.0]
Episode 694	 reward: -8.42	 Mean_loss: 0.09066358,  training time: 20.79
[779.4, 820.2, 700.8, 699.2, 738.0, 687.2]
Episode 695	 reward: -8.82	 Mean_loss: 0.11859449,  training time: 20.79
[796.4, 771.0, 687.0, 719.2, 799.0, 692.6]
Episode 696	 reward: -8.60	 Mean_loss: 0.09094758,  training time: 20.83
[770.4, 761.2, 680.0, 682.6, 801.6, 665.0]
Episode 697	 reward: -8.74	 Mean_loss: 0.05699979,  training time: 21.01
[769.4, 810.2, 651.4, 643.4, 741.6, 692.4]
Episode 698	 reward: -8.26	 Mean_loss: 0.11759152,  training time: 21.04
[767.8, 819.8, 676.4, 663.0, 799.4, 690.0]
Episode 699	 reward: -8.20	 Mean_loss: 0.09984555,  training time: 21.03
[801.4, 775.0, 703.8, 708.0, 768.8, 692.4]
Episode 700	 reward: -9.45	 Mean_loss: 0.09176978,  training time: 21.09
[(15, 9), (15, 9), (15, 7), (15, 5), (15, 9), (15, 9)]
[787.0, 726.4, 710.2, 664.0, 740.6, 766.6]
Episode 701	 reward: -9.28	 Mean_loss: 0.09395237,  training time: 20.68
[762.2, 734.4, 728.2, 693.4, 728.8, 774.2]
Episode 702	 reward: -9.31	 Mean_loss: 0.05195278,  training time: 20.54
[765.2, 742.8, 715.8, 704.8, 782.6, 813.4]
Episode 703	 reward: -9.30	 Mean_loss: 0.12496926,  training time: 20.53
[794.6, 762.0, 717.4, 714.0, 759.0, 750.0]
Episode 704	 reward: -9.17	 Mean_loss: 0.06483759,  training time: 20.42
[724.2, 745.2, 728.2, 704.8, 720.6, 729.8]
Episode 705	 reward: -9.39	 Mean_loss: 0.06538641,  training time: 20.44
[754.4, 713.0, 727.6, 743.2, 810.4, 832.6]
Episode 706	 reward: -9.56	 Mean_loss: 0.06471287,  training time: 20.48
[763.6, 696.2, 692.8, 697.0, 712.6, 780.0]
Episode 707	 reward: -9.70	 Mean_loss: 0.08486378,  training time: 20.38
[771.6, 745.0, 724.2, 722.8, 780.0, 800.4]
Episode 708	 reward: -9.05	 Mean_loss: 0.09762544,  training time: 20.38
[711.2, 746.2, 712.4, 701.8, 755.4, 796.0]
Episode 709	 reward: -9.65	 Mean_loss: 0.12982850,  training time: 20.42
[759.0, 776.0, 672.6, 725.8, 758.8, 822.0]
Episode 710	 reward: -9.95	 Mean_loss: 0.10131255,  training time: 20.38
[752.0, 733.4, 685.6, 688.2, 756.2, 748.8]
Episode 711	 reward: -9.47	 Mean_loss: 0.09224888,  training time: 20.40
[767.8, 743.8, 668.0, 716.2, 750.2, 785.8]
Episode 712	 reward: -9.20	 Mean_loss: 0.09517619,  training time: 20.39
[777.0, 725.4, 670.8, 700.8, 705.4, 748.6]
Episode 713	 reward: -9.03	 Mean_loss: 0.08433637,  training time: 20.39
[776.6, 753.6, 719.2, 700.4, 793.4, 801.6]
Episode 714	 reward: -9.75	 Mean_loss: 0.09608256,  training time: 20.39
[785.4, 732.2, 682.0, 738.6, 709.4, 773.6]
Episode 715	 reward: -9.40	 Mean_loss: 0.07461642,  training time: 20.44
[789.0, 738.2, 694.8, 721.2, 727.2, 773.6]
Episode 716	 reward: -9.65	 Mean_loss: 0.07836377,  training time: 20.44
[786.6, 744.8, 684.8, 675.0, 735.8, 763.2]
Episode 717	 reward: -9.03	 Mean_loss: 0.07258383,  training time: 20.38
[746.4, 727.2, 721.6, 724.2, 704.2, 786.4]
Episode 718	 reward: -9.41	 Mean_loss: 0.07611534,  training time: 20.38
[761.6, 749.2, 753.4, 685.4, 786.2, 752.4]
Episode 719	 reward: -9.31	 Mean_loss: 0.08968425,  training time: 20.38
[754.8, 729.0, 682.0, 722.4, 707.2, 818.6]
Episode 720	 reward: -9.96	 Mean_loss: 0.11502924,  training time: 20.47
[(15, 7), (15, 9), (15, 7), (15, 7), (15, 10), (15, 7)]
[731.0, 753.0, 700.2, 718.8, 809.6, 726.2]
Episode 721	 reward: -9.81	 Mean_loss: 0.10372587,  training time: 20.02
[741.6, 754.4, 713.4, 739.2, 860.2, 761.8]
Episode 722	 reward: -8.96	 Mean_loss: 0.16171904,  training time: 19.97
[755.2, 772.0, 772.4, 735.8, 805.6, 703.8]
Episode 723	 reward: -9.06	 Mean_loss: 0.08069708,  training time: 19.96
[725.2, 787.8, 698.0, 764.0, 784.2, 726.8]
Episode 724	 reward: -9.13	 Mean_loss: 0.06484239,  training time: 19.97
[719.4, 743.6, 738.2, 697.4, 810.0, 740.2]
Episode 725	 reward: -8.45	 Mean_loss: 0.11320258,  training time: 19.97
[711.4, 757.6, 711.2, 737.0, 796.8, 735.6]
Episode 726	 reward: -9.05	 Mean_loss: 0.09046793,  training time: 20.01
[712.0, 717.6, 739.2, 702.6, 797.8, 785.2]
Episode 727	 reward: -9.15	 Mean_loss: 0.11284338,  training time: 19.97
[693.0, 706.0, 685.2, 760.8, 802.8, 696.6]
Episode 728	 reward: -9.00	 Mean_loss: 0.07444572,  training time: 20.09
[797.2, 721.8, 723.8, 752.6, 796.2, 712.0]
Episode 729	 reward: -8.85	 Mean_loss: 0.12196922,  training time: 20.09
[765.0, 741.4, 775.6, 735.2, 842.2, 731.2]
Episode 730	 reward: -8.79	 Mean_loss: 0.10436484,  training time: 20.09
[738.8, 736.2, 739.6, 731.4, 798.2, 729.4]
Episode 731	 reward: -9.59	 Mean_loss: 0.06572053,  training time: 20.12
[771.6, 794.2, 711.2, 684.6, 802.4, 711.4]
Episode 732	 reward: -9.29	 Mean_loss: 0.12269819,  training time: 20.02
[748.8, 726.8, 742.0, 770.4, 796.8, 699.6]
Episode 733	 reward: -8.66	 Mean_loss: 0.05633510,  training time: 19.87
[745.2, 739.0, 722.4, 712.4, 861.4, 754.8]
Episode 734	 reward: -8.79	 Mean_loss: 0.12741528,  training time: 20.15
[738.0, 734.8, 692.0, 723.8, 806.4, 747.8]
Episode 735	 reward: -8.71	 Mean_loss: 0.10265512,  training time: 20.64
[724.6, 744.6, 697.4, 680.8, 799.8, 778.6]
Episode 736	 reward: -8.91	 Mean_loss: 0.16672741,  training time: 20.08
[740.6, 704.2, 690.6, 752.8, 796.6, 739.6]
Episode 737	 reward: -9.31	 Mean_loss: 0.07439934,  training time: 19.93
[776.0, 732.0, 717.2, 753.0, 865.6, 697.2]
Episode 738	 reward: -8.93	 Mean_loss: 0.12731995,  training time: 19.93
[721.0, 772.8, 690.8, 729.6, 821.4, 736.0]
Episode 739	 reward: -8.78	 Mean_loss: 0.09854618,  training time: 19.98
[767.6, 708.8, 717.0, 735.6, 850.6, 745.0]
Episode 740	 reward: -8.86	 Mean_loss: 0.12103247,  training time: 20.06
[(15, 9), (15, 10), (15, 9), (15, 10), (15, 5), (15, 10)]
[742.4, 794.2, 767.0, 728.0, 703.6, 744.8]
Episode 741	 reward: -9.31	 Mean_loss: 0.08860540,  training time: 22.89
[777.2, 771.6, 770.8, 753.4, 692.2, 736.4]
Episode 742	 reward: -9.00	 Mean_loss: 0.07420020,  training time: 22.78
[791.2, 732.8, 782.4, 812.2, 689.0, 763.6]
Episode 743	 reward: -9.34	 Mean_loss: 0.05763932,  training time: 22.77
[749.0, 744.2, 712.6, 791.6, 740.2, 767.4]
Episode 744	 reward: -9.80	 Mean_loss: 0.08349852,  training time: 22.53
[798.2, 710.2, 723.8, 762.4, 688.8, 751.0]
Episode 745	 reward: -9.13	 Mean_loss: 0.05643660,  training time: 22.53
[764.6, 762.4, 783.4, 750.8, 746.0, 700.4]
Episode 746	 reward: -9.76	 Mean_loss: 0.08365504,  training time: 22.58
[827.0, 750.4, 743.6, 728.2, 702.4, 745.4]
Episode 747	 reward: -9.15	 Mean_loss: 0.08123554,  training time: 22.54
[749.8, 729.8, 779.2, 755.0, 758.6, 740.2]
Episode 748	 reward: -9.04	 Mean_loss: 0.05352021,  training time: 22.63
[770.0, 767.4, 790.2, 701.4, 657.4, 723.4]
Episode 749	 reward: -9.30	 Mean_loss: 0.06259948,  training time: 22.52
[758.6, 804.4, 759.4, 765.2, 699.4, 766.8]
Episode 750	 reward: -9.18	 Mean_loss: 0.05058770,  training time: 22.78
[805.8, 808.2, 763.2, 766.6, 662.0, 819.4]
Episode 751	 reward: -9.19	 Mean_loss: 0.07320885,  training time: 22.79
[796.0, 762.2, 811.8, 769.2, 699.0, 746.6]
Episode 752	 reward: -8.89	 Mean_loss: 0.04309804,  training time: 22.79
[767.0, 798.4, 738.8, 764.6, 699.4, 750.4]
Episode 753	 reward: -9.07	 Mean_loss: 0.05443683,  training time: 22.81
[809.0, 742.6, 758.4, 754.2, 717.2, 785.6]
Episode 754	 reward: -9.68	 Mean_loss: 0.07173754,  training time: 22.76
[772.0, 768.0, 762.6, 755.4, 742.8, 737.0]
Episode 755	 reward: -9.43	 Mean_loss: 0.07782575,  training time: 22.84
[757.8, 768.8, 731.4, 737.0, 716.4, 768.4]
Episode 756	 reward: -9.28	 Mean_loss: 0.05047876,  training time: 22.79
[776.6, 761.6, 756.4, 731.4, 717.2, 802.0]
Episode 757	 reward: -9.20	 Mean_loss: 0.07722507,  training time: 22.81
[777.4, 748.8, 744.6, 757.0, 735.6, 732.0]
Episode 758	 reward: -9.34	 Mean_loss: 0.05107977,  training time: 22.78
[773.0, 749.2, 785.2, 771.0, 692.0, 768.2]
Episode 759	 reward: -9.19	 Mean_loss: 0.05708888,  training time: 22.80
[829.4, 785.4, 816.0, 764.8, 710.8, 732.8]
Episode 760	 reward: -9.15	 Mean_loss: 0.06102446,  training time: 22.80
[(15, 5), (15, 5), (15, 9), (15, 9), (15, 10), (15, 7)]
[646.2, 758.0, 756.6, 748.6, 795.6, 715.8]
Episode 761	 reward: -9.39	 Mean_loss: 0.06953438,  training time: 19.20
[677.8, 693.6, 745.8, 756.2, 762.8, 725.0]
Episode 762	 reward: -8.91	 Mean_loss: 0.07341249,  training time: 19.00
[661.2, 693.4, 781.0, 749.8, 763.8, 760.6]
Episode 763	 reward: -8.82	 Mean_loss: 0.14953849,  training time: 19.02
[696.4, 699.6, 753.0, 742.0, 751.4, 776.4]
Episode 764	 reward: -9.35	 Mean_loss: 0.13135682,  training time: 19.02
[673.6, 720.6, 754.2, 723.6, 812.6, 742.2]
Episode 765	 reward: -9.20	 Mean_loss: 0.08984939,  training time: 19.06
[708.0, 717.0, 698.8, 718.0, 764.6, 710.8]
Episode 766	 reward: -8.99	 Mean_loss: 0.14099546,  training time: 19.00
[717.0, 737.2, 733.4, 689.6, 757.4, 704.8]
Episode 767	 reward: -9.23	 Mean_loss: 0.14705849,  training time: 19.00
[694.8, 728.8, 717.2, 780.4, 739.4, 718.0]
Episode 768	 reward: -9.00	 Mean_loss: 0.10285973,  training time: 19.01
[705.4, 695.2, 718.4, 796.2, 761.6, 706.8]
Episode 769	 reward: -9.32	 Mean_loss: 0.10229111,  training time: 19.01
[718.0, 714.4, 712.4, 725.0, 742.8, 670.6]
Episode 770	 reward: -9.11	 Mean_loss: 0.10788815,  training time: 19.01
[726.0, 693.0, 746.6, 769.6, 783.4, 745.2]
Episode 771	 reward: -9.45	 Mean_loss: 0.10773196,  training time: 19.01
[711.8, 708.6, 729.2, 707.6, 797.4, 762.4]
Episode 772	 reward: -9.30	 Mean_loss: 0.13910833,  training time: 19.01
[681.4, 687.8, 775.0, 745.4, 811.2, 717.6]
Episode 773	 reward: -8.69	 Mean_loss: 0.07725188,  training time: 19.01
[736.8, 674.8, 752.4, 764.8, 733.6, 722.0]
Episode 774	 reward: -9.18	 Mean_loss: 0.11993760,  training time: 19.02
[660.2, 695.0, 719.2, 698.8, 806.6, 748.8]
Episode 775	 reward: -9.32	 Mean_loss: 0.06816974,  training time: 19.02
[666.8, 673.4, 769.0, 733.0, 745.2, 784.8]
Episode 776	 reward: -8.55	 Mean_loss: 0.14333142,  training time: 19.11
[683.6, 691.8, 724.4, 727.6, 766.2, 715.4]
Episode 777	 reward: -9.30	 Mean_loss: 0.08560032,  training time: 19.02
[751.2, 676.2, 744.2, 752.4, 768.2, 767.4]
Episode 778	 reward: -8.44	 Mean_loss: 0.08720652,  training time: 19.08
[687.4, 703.2, 736.4, 768.0, 761.0, 786.6]
Episode 779	 reward: -9.21	 Mean_loss: 0.17668407,  training time: 19.02
[664.2, 716.6, 734.4, 739.4, 783.4, 759.0]
Episode 780	 reward: -9.43	 Mean_loss: 0.16581517,  training time: 19.01
[(15, 5), (15, 10), (15, 7), (15, 10), (15, 9), (15, 5)]
[709.6, 740.8, 696.6, 741.6, 801.8, 695.4]
Episode 781	 reward: -8.16	 Mean_loss: 0.25068736,  training time: 19.57
[693.4, 784.2, 694.6, 785.4, 759.0, 682.8]
Episode 782	 reward: -8.45	 Mean_loss: 0.19353430,  training time: 19.47
[739.2, 787.8, 709.2, 753.6, 761.4, 642.8]
Episode 783	 reward: -9.33	 Mean_loss: 0.09909734,  training time: 19.47
[710.0, 744.4, 671.2, 770.8, 760.6, 652.8]
Episode 784	 reward: -8.54	 Mean_loss: 0.12998469,  training time: 19.46
[710.8, 782.2, 784.8, 734.0, 753.2, 715.0]
Episode 785	 reward: -8.08	 Mean_loss: 0.19108537,  training time: 19.47
[730.6, 775.6, 726.0, 719.6, 760.8, 677.4]
Episode 786	 reward: -8.79	 Mean_loss: 0.19786116,  training time: 19.50
[707.8, 839.6, 702.4, 748.8, 740.6, 700.4]
Episode 787	 reward: -8.38	 Mean_loss: 0.27769950,  training time: 19.48
[708.6, 760.6, 718.8, 742.4, 756.0, 684.4]
Episode 788	 reward: -8.59	 Mean_loss: 0.13473991,  training time: 19.47
[696.8, 774.6, 713.4, 715.0, 753.2, 700.8]
Episode 789	 reward: -8.12	 Mean_loss: 0.21128023,  training time: 19.48
[719.0, 781.8, 724.2, 747.0, 809.8, 702.0]
Episode 790	 reward: -8.74	 Mean_loss: 0.19130057,  training time: 19.55
[666.6, 812.2, 731.2, 677.0, 772.8, 677.6]
Episode 791	 reward: -8.17	 Mean_loss: 0.13617931,  training time: 19.47
[686.2, 784.6, 758.0, 713.8, 759.0, 681.8]
Episode 792	 reward: -8.45	 Mean_loss: 0.13768774,  training time: 19.52
[697.8, 778.8, 717.0, 710.2, 790.6, 693.0]
Episode 793	 reward: -8.07	 Mean_loss: 0.19397008,  training time: 19.50
[740.2, 765.6, 741.2, 689.8, 748.6, 707.8]
Episode 794	 reward: -8.40	 Mean_loss: 0.19415551,  training time: 19.49
[732.6, 802.2, 714.8, 736.6, 767.2, 639.0]
Episode 795	 reward: -8.92	 Mean_loss: 0.16034372,  training time: 19.50
[691.0, 725.2, 712.4, 755.6, 755.2, 675.2]
Episode 796	 reward: -8.52	 Mean_loss: 0.16671532,  training time: 19.49
[766.4, 773.6, 714.0, 773.0, 749.4, 676.2]
Episode 797	 reward: -8.16	 Mean_loss: 0.18323423,  training time: 19.51
[731.4, 767.4, 697.4, 738.2, 730.6, 676.8]
Episode 798	 reward: -8.16	 Mean_loss: 0.16634579,  training time: 19.62
[766.2, 800.6, 729.2, 783.2, 783.4, 660.8]
Episode 799	 reward: -8.10	 Mean_loss: 0.12550873,  training time: 19.70
[701.2, 801.6, 728.0, 777.6, 763.2, 702.4]
Episode 800	 reward: -8.50	 Mean_loss: 0.16180092,  training time: 19.70
[(15, 9), (15, 7), (15, 9), (15, 10), (15, 5), (15, 9)]
[767.6, 763.8, 775.0, 750.2, 737.6, 737.6]
Episode 801	 reward: -9.15	 Mean_loss: 0.05854503,  training time: 21.05
[752.2, 754.4, 782.2, 833.2, 699.2, 730.0]
Episode 802	 reward: -9.42	 Mean_loss: 0.04458421,  training time: 20.99
[745.4, 737.0, 745.2, 772.8, 724.6, 661.6]
Episode 803	 reward: -8.95	 Mean_loss: 0.09582668,  training time: 21.04
[787.0, 731.2, 730.8, 756.2, 701.2, 729.0]
Episode 804	 reward: -8.86	 Mean_loss: 0.07034793,  training time: 21.01
[771.2, 753.8, 742.0, 792.0, 716.6, 676.4]
Episode 805	 reward: -9.03	 Mean_loss: 0.09863581,  training time: 21.01
[793.0, 701.4, 766.8, 786.6, 707.6, 713.0]
Episode 806	 reward: -8.88	 Mean_loss: 0.05415918,  training time: 21.02
[757.0, 738.0, 795.4, 832.0, 711.2, 751.6]
Episode 807	 reward: -8.82	 Mean_loss: 0.07166818,  training time: 21.01
[746.0, 692.4, 748.6, 768.6, 729.2, 707.6]
Episode 808	 reward: -8.61	 Mean_loss: 0.06143449,  training time: 21.08
[751.0, 722.0, 774.4, 817.6, 743.4, 699.2]
Episode 809	 reward: -9.03	 Mean_loss: 0.06685265,  training time: 21.00
[734.2, 729.8, 712.4, 721.2, 720.8, 740.6]
Episode 810	 reward: -8.34	 Mean_loss: 0.04672433,  training time: 21.00
[809.2, 742.2, 755.4, 789.2, 719.0, 724.6]
Episode 811	 reward: -8.80	 Mean_loss: 0.05687361,  training time: 21.01
[738.6, 717.6, 727.4, 839.4, 726.0, 715.4]
Episode 812	 reward: -8.93	 Mean_loss: 0.06754439,  training time: 21.01
[752.6, 696.8, 733.6, 800.4, 726.8, 706.0]
Episode 813	 reward: -8.89	 Mean_loss: 0.06397411,  training time: 21.01
[745.4, 711.6, 731.8, 776.4, 705.0, 694.0]
Episode 814	 reward: -8.75	 Mean_loss: 0.07173913,  training time: 21.01
[746.4, 703.2, 701.0, 754.6, 763.6, 709.8]
Episode 815	 reward: -8.81	 Mean_loss: 0.07741518,  training time: 21.01
[743.4, 694.8, 780.8, 759.2, 671.8, 706.6]
Episode 816	 reward: -8.87	 Mean_loss: 0.08073016,  training time: 21.00
[799.2, 714.0, 692.6, 802.8, 726.2, 670.0]
Episode 817	 reward: -9.23	 Mean_loss: 0.07325108,  training time: 21.05
[731.6, 732.6, 748.6, 694.4, 699.2, 692.4]
Episode 818	 reward: -8.50	 Mean_loss: 0.07876569,  training time: 20.88
[766.4, 720.0, 729.2, 778.0, 709.8, 705.6]
Episode 819	 reward: -9.30	 Mean_loss: 0.06467208,  training time: 20.88
[746.2, 730.0, 752.0, 751.2, 725.8, 710.2]
Episode 820	 reward: -9.29	 Mean_loss: 0.04955795,  training time: 20.88
[(15, 7), (15, 7), (15, 10), (15, 10), (15, 10), (15, 10)]
[823.4, 772.2, 770.0, 785.6, 773.4, 742.0]
Episode 821	 reward: -9.05	 Mean_loss: 0.07146639,  training time: 23.20
[742.8, 760.6, 747.4, 750.0, 775.8, 795.4]
Episode 822	 reward: -8.96	 Mean_loss: 0.07013445,  training time: 23.20
[748.4, 720.6, 769.2, 752.8, 763.2, 766.0]
Episode 823	 reward: -10.15	 Mean_loss: 0.08112539,  training time: 23.10
[781.2, 701.2, 729.2, 839.6, 761.8, 762.4]
Episode 824	 reward: -9.02	 Mean_loss: 0.06837325,  training time: 23.08
[773.2, 749.2, 730.2, 722.0, 813.2, 821.0]
Episode 825	 reward: -8.93	 Mean_loss: 0.08271099,  training time: 23.09
[778.4, 760.4, 752.6, 743.6, 720.0, 789.8]
Episode 826	 reward: -9.37	 Mean_loss: 0.08352780,  training time: 23.10
[758.4, 737.8, 752.0, 736.6, 782.2, 812.2]
Episode 827	 reward: -9.22	 Mean_loss: 0.09912726,  training time: 23.23
[766.6, 718.8, 740.2, 799.0, 761.2, 773.8]
Episode 828	 reward: -9.95	 Mean_loss: 0.06755743,  training time: 23.10
[742.0, 736.8, 737.4, 742.2, 752.8, 765.4]
Episode 829	 reward: -9.41	 Mean_loss: 0.08531781,  training time: 23.11
[754.2, 746.2, 725.2, 749.4, 747.8, 837.8]
Episode 830	 reward: -9.30	 Mean_loss: 0.07525956,  training time: 23.10
[737.2, 714.6, 717.4, 726.4, 728.4, 772.8]
Episode 831	 reward: -9.41	 Mean_loss: 0.07699641,  training time: 23.14
[773.2, 741.4, 743.4, 772.4, 763.0, 778.4]
Episode 832	 reward: -9.33	 Mean_loss: 0.08015005,  training time: 23.11
[747.2, 812.4, 765.2, 757.0, 747.8, 812.0]
Episode 833	 reward: -10.20	 Mean_loss: 0.04714899,  training time: 23.11
[741.8, 698.2, 778.0, 794.8, 787.0, 789.8]
Episode 834	 reward: -9.75	 Mean_loss: 0.11023986,  training time: 23.11
[770.8, 749.0, 767.8, 765.8, 771.4, 780.2]
Episode 835	 reward: -8.93	 Mean_loss: 0.07306895,  training time: 23.20
[766.4, 702.8, 719.4, 735.2, 763.0, 758.2]
Episode 836	 reward: -9.17	 Mean_loss: 0.05456341,  training time: 23.12
[754.2, 721.8, 749.4, 752.8, 739.4, 766.0]
Episode 837	 reward: -9.20	 Mean_loss: 0.05839851,  training time: 23.10
[705.2, 763.6, 739.4, 768.4, 806.6, 775.2]
Episode 838	 reward: -8.99	 Mean_loss: 0.05253460,  training time: 23.13
[756.8, 708.4, 748.2, 750.4, 776.6, 775.8]
Episode 839	 reward: -9.76	 Mean_loss: 0.07330240,  training time: 23.11
[759.2, 764.6, 731.4, 783.0, 762.6, 779.2]
Episode 840	 reward: -9.62	 Mean_loss: 0.07113563,  training time: 23.19
[(15, 5), (15, 7), (15, 5), (15, 7), (15, 10), (15, 9)]
[732.2, 734.0, 747.8, 718.4, 765.4, 728.4]
Episode 841	 reward: -9.43	 Mean_loss: 0.10593110,  training time: 18.31
[750.2, 740.4, 753.8, 705.6, 760.4, 723.4]
Episode 842	 reward: -10.04	 Mean_loss: 0.10180064,  training time: 18.22
[686.6, 757.6, 756.2, 714.4, 812.4, 743.8]
Episode 843	 reward: -9.26	 Mean_loss: 0.13622849,  training time: 18.22
[718.2, 747.8, 792.6, 761.2, 740.6, 747.0]
Episode 844	 reward: -9.01	 Mean_loss: 0.11097930,  training time: 18.23
[700.2, 727.0, 799.2, 698.6, 789.6, 722.4]
Episode 845	 reward: -8.87	 Mean_loss: 0.06172485,  training time: 18.23
[705.2, 694.4, 771.0, 718.4, 767.8, 698.0]
Episode 846	 reward: -9.43	 Mean_loss: 0.11092093,  training time: 18.27
[717.8, 718.0, 769.8, 794.6, 764.4, 740.0]
Episode 847	 reward: -8.66	 Mean_loss: 0.09233081,  training time: 18.23
[679.6, 765.2, 763.6, 744.8, 784.0, 739.6]
Episode 848	 reward: -9.31	 Mean_loss: 0.06053640,  training time: 18.23
[695.0, 738.8, 735.4, 715.4, 796.4, 707.6]
Episode 849	 reward: -8.90	 Mean_loss: 0.10535991,  training time: 18.23
[724.4, 755.2, 724.0, 748.4, 738.6, 784.4]
Episode 850	 reward: -9.17	 Mean_loss: 0.16703609,  training time: 18.23
[698.8, 707.0, 812.2, 730.6, 734.8, 752.0]
Episode 851	 reward: -8.81	 Mean_loss: 0.10790242,  training time: 18.25
[745.8, 691.2, 729.2, 738.0, 775.0, 739.4]
Episode 852	 reward: -8.92	 Mean_loss: 0.06098520,  training time: 18.32
[704.8, 705.4, 765.0, 718.2, 801.2, 707.2]
Episode 853	 reward: -9.41	 Mean_loss: 0.09345623,  training time: 18.23
[722.4, 733.4, 760.4, 741.8, 796.4, 720.0]
Episode 854	 reward: -9.33	 Mean_loss: 0.08883790,  training time: 18.24
[700.6, 733.8, 810.6, 720.0, 780.4, 724.2]
Episode 855	 reward: -8.55	 Mean_loss: 0.09858979,  training time: 18.22
[700.4, 739.2, 725.2, 710.0, 756.2, 748.0]
Episode 856	 reward: -9.09	 Mean_loss: 0.07951523,  training time: 18.23
[708.2, 731.0, 718.8, 755.2, 775.2, 718.2]
Episode 857	 reward: -9.09	 Mean_loss: 0.08710810,  training time: 18.25
[762.8, 727.8, 759.2, 776.0, 780.6, 720.2]
Episode 858	 reward: -9.22	 Mean_loss: 0.09045774,  training time: 18.24
[721.6, 704.0, 749.8, 731.2, 760.0, 716.6]
Episode 859	 reward: -9.01	 Mean_loss: 0.07841440,  training time: 18.23
[724.4, 782.4, 762.2, 706.0, 746.2, 736.4]
Episode 860	 reward: -8.92	 Mean_loss: 0.10541814,  training time: 18.24
[(15, 5), (15, 10), (15, 5), (15, 9), (15, 9), (15, 7)]
[693.0, 823.8, 690.0, 746.6, 810.8, 724.8]
Episode 861	 reward: -9.52	 Mean_loss: 0.08296952,  training time: 19.21
[708.6, 762.8, 672.8, 754.8, 751.0, 708.8]
Episode 862	 reward: -8.76	 Mean_loss: 0.17968127,  training time: 19.14
[703.6, 794.2, 726.6, 788.4, 719.6, 764.8]
Episode 863	 reward: -8.74	 Mean_loss: 0.12203866,  training time: 19.15
[729.8, 782.4, 630.6, 754.8, 743.0, 692.6]
Episode 864	 reward: -9.15	 Mean_loss: 0.13320620,  training time: 19.14
[661.4, 766.8, 669.4, 770.6, 741.0, 678.6]
Episode 865	 reward: -9.10	 Mean_loss: 0.10779741,  training time: 19.15
[715.6, 735.0, 619.8, 747.2, 766.0, 733.8]
Episode 866	 reward: -8.62	 Mean_loss: 0.19862938,  training time: 19.15
[735.2, 778.4, 678.6, 736.6, 747.4, 664.4]
Episode 867	 reward: -8.70	 Mean_loss: 0.12414285,  training time: 19.14
[698.6, 760.4, 634.8, 720.0, 806.8, 719.8]
Episode 868	 reward: -9.00	 Mean_loss: 0.10241400,  training time: 19.20
[666.8, 751.6, 643.4, 772.4, 737.0, 701.0]
Episode 869	 reward: -8.94	 Mean_loss: 0.13415016,  training time: 19.14
[706.6, 786.2, 679.0, 817.6, 760.8, 721.4]
Episode 870	 reward: -9.04	 Mean_loss: 0.13524234,  training time: 19.14
[691.2, 775.6, 733.0, 759.4, 765.4, 719.8]
Episode 871	 reward: -9.12	 Mean_loss: 0.13810259,  training time: 19.22
[699.6, 781.8, 689.4, 783.0, 764.6, 703.6]
Episode 872	 reward: -8.36	 Mean_loss: 0.10680778,  training time: 19.12
[708.8, 767.6, 651.0, 778.4, 748.6, 688.2]
Episode 873	 reward: -8.61	 Mean_loss: 0.10588685,  training time: 19.20
[761.4, 791.4, 692.8, 786.6, 786.0, 725.8]
Episode 874	 reward: -9.50	 Mean_loss: 0.13614947,  training time: 19.21
[712.6, 783.0, 670.4, 767.8, 735.2, 752.8]
Episode 875	 reward: -9.20	 Mean_loss: 0.09868357,  training time: 19.13
[672.0, 778.0, 665.4, 761.2, 743.2, 707.0]
Episode 876	 reward: -9.07	 Mean_loss: 0.11539055,  training time: 19.13
[744.2, 782.4, 640.8, 738.4, 724.4, 690.0]
Episode 877	 reward: -8.62	 Mean_loss: 0.08109365,  training time: 19.12
[721.6, 775.6, 636.6, 725.2, 763.6, 715.0]
Episode 878	 reward: -8.90	 Mean_loss: 0.11784343,  training time: 19.12
[730.6, 753.0, 635.0, 728.6, 761.2, 729.4]
Episode 879	 reward: -9.69	 Mean_loss: 0.12965663,  training time: 19.17
[685.2, 767.2, 660.6, 793.8, 746.6, 716.4]
Episode 880	 reward: -9.08	 Mean_loss: 0.11320775,  training time: 19.12
[(15, 10), (15, 5), (15, 5), (15, 9), (15, 9), (15, 5)]
[782.6, 746.2, 735.8, 704.8, 735.8, 673.4]
Episode 881	 reward: -8.16	 Mean_loss: 0.14410180,  training time: 18.35
[786.2, 742.6, 747.0, 735.4, 718.4, 657.4]
Episode 882	 reward: -8.90	 Mean_loss: 0.11715528,  training time: 18.28
[790.4, 730.8, 725.4, 807.8, 717.4, 666.8]
Episode 883	 reward: -8.35	 Mean_loss: 0.11850478,  training time: 18.28
[807.2, 730.6, 699.8, 735.8, 693.4, 701.8]
Episode 884	 reward: -8.59	 Mean_loss: 0.15884557,  training time: 18.32
[866.0, 734.0, 735.4, 718.6, 729.0, 643.8]
Episode 885	 reward: -8.16	 Mean_loss: 0.10117538,  training time: 18.29
[749.4, 726.6, 736.2, 760.0, 750.0, 675.4]
Episode 886	 reward: -8.92	 Mean_loss: 0.17877653,  training time: 18.27
[826.6, 754.8, 772.6, 723.4, 723.2, 658.8]
Episode 887	 reward: -8.02	 Mean_loss: 0.13116162,  training time: 18.35
[756.4, 797.6, 705.0, 678.8, 764.0, 657.0]
Episode 888	 reward: -8.60	 Mean_loss: 0.12211021,  training time: 18.26
[822.2, 740.0, 774.2, 715.0, 741.8, 654.0]
Episode 889	 reward: -8.40	 Mean_loss: 0.11994587,  training time: 18.27
[761.0, 683.4, 749.2, 719.0, 717.0, 628.8]
Episode 890	 reward: -8.14	 Mean_loss: 0.12977064,  training time: 18.33
[751.6, 727.2, 727.0, 712.2, 732.0, 667.2]
Episode 891	 reward: -8.91	 Mean_loss: 0.22929628,  training time: 18.28
[782.4, 734.0, 691.0, 743.8, 744.8, 672.8]
Episode 892	 reward: -8.58	 Mean_loss: 0.13866846,  training time: 18.27
[778.0, 736.4, 725.6, 677.8, 737.2, 676.8]
Episode 893	 reward: -8.34	 Mean_loss: 0.14575507,  training time: 18.28
[792.4, 746.2, 756.6, 775.2, 724.6, 657.6]
Episode 894	 reward: -8.42	 Mean_loss: 0.08882792,  training time: 18.31
[796.8, 774.8, 734.4, 730.0, 731.0, 677.6]
Episode 895	 reward: -8.42	 Mean_loss: 0.15872312,  training time: 18.28
[792.4, 710.2, 688.4, 765.0, 780.4, 692.2]
Episode 896	 reward: -8.97	 Mean_loss: 0.20969434,  training time: 18.32
[841.8, 736.2, 717.0, 709.8, 725.6, 639.4]
Episode 897	 reward: -8.79	 Mean_loss: 0.11055025,  training time: 18.29
[815.6, 745.4, 753.8, 739.4, 717.8, 680.4]
Episode 898	 reward: -8.86	 Mean_loss: 0.15143611,  training time: 18.28
[798.2, 761.2, 714.2, 701.8, 724.6, 681.2]
Episode 899	 reward: -8.44	 Mean_loss: 0.09761997,  training time: 18.30
[786.0, 770.0, 749.0, 755.0, 735.0, 688.4]
Episode 900	 reward: -8.69	 Mean_loss: 0.15371440,  training time: 18.29
[(15, 10), (15, 10), (15, 7), (15, 9), (15, 5), (15, 10)]
[794.2, 756.0, 682.4, 767.2, 697.0, 723.4]
Episode 901	 reward: -9.54	 Mean_loss: 0.07558136,  training time: 22.06
[795.6, 746.8, 697.2, 745.6, 656.4, 756.2]
Episode 902	 reward: -9.78	 Mean_loss: 0.06137948,  training time: 21.77
[760.2, 743.8, 697.2, 777.0, 665.8, 718.2]
Episode 903	 reward: -9.71	 Mean_loss: 0.07756177,  training time: 21.78
[792.0, 726.2, 698.2, 733.0, 703.4, 766.0]
Episode 904	 reward: -9.11	 Mean_loss: 0.07296629,  training time: 21.80
[790.4, 699.4, 727.2, 755.8, 682.6, 737.8]
Episode 905	 reward: -8.76	 Mean_loss: 0.06884247,  training time: 21.77
[744.6, 717.6, 684.0, 721.6, 673.8, 748.6]
Episode 906	 reward: -8.77	 Mean_loss: 0.06564233,  training time: 21.80
[737.8, 752.2, 635.2, 753.6, 675.4, 751.6]
Episode 907	 reward: -9.52	 Mean_loss: 0.07377958,  training time: 21.78
[704.2, 777.8, 727.0, 769.4, 703.2, 708.0]
Episode 908	 reward: -9.65	 Mean_loss: 0.08497966,  training time: 21.77
[768.6, 737.8, 729.4, 741.6, 696.8, 709.0]
Episode 909	 reward: -9.80	 Mean_loss: 0.10013054,  training time: 21.78
[765.6, 699.0, 650.8, 766.6, 687.4, 722.8]
Episode 910	 reward: -9.63	 Mean_loss: 0.06630073,  training time: 21.77
[732.6, 731.2, 701.8, 827.6, 692.4, 756.8]
Episode 911	 reward: -9.02	 Mean_loss: 0.04280281,  training time: 21.86
[766.4, 725.2, 692.6, 748.6, 694.2, 774.6]
Episode 912	 reward: -9.69	 Mean_loss: 0.04894628,  training time: 21.78
[781.8, 735.4, 716.4, 787.8, 698.6, 762.6]
Episode 913	 reward: -8.65	 Mean_loss: 0.06799700,  training time: 21.80
[726.0, 728.2, 679.4, 760.4, 698.8, 756.0]
Episode 914	 reward: -9.40	 Mean_loss: 0.05939698,  training time: 21.86
[740.4, 704.8, 740.4, 785.8, 704.0, 769.2]
Episode 915	 reward: -9.74	 Mean_loss: 0.05010519,  training time: 21.83
[743.2, 707.6, 692.4, 730.6, 674.0, 782.0]
Episode 916	 reward: -9.46	 Mean_loss: 0.07543972,  training time: 21.79
[757.6, 725.4, 741.8, 737.2, 692.2, 716.6]
Episode 917	 reward: -9.32	 Mean_loss: 0.05973077,  training time: 21.79
[765.6, 715.0, 729.8, 766.2, 720.8, 748.0]
Episode 918	 reward: -9.29	 Mean_loss: 0.05424827,  training time: 21.82
[720.2, 718.8, 698.8, 730.0, 685.0, 710.6]
Episode 919	 reward: -9.08	 Mean_loss: 0.07090253,  training time: 21.90
[749.8, 697.6, 709.2, 768.2, 714.2, 765.4]
Episode 920	 reward: -9.21	 Mean_loss: 0.05295632,  training time: 21.98
[(15, 5), (15, 10), (15, 9), (15, 5), (15, 5), (15, 7)]
[649.0, 770.6, 725.8, 700.2, 730.2, 712.2]
Episode 921	 reward: -8.58	 Mean_loss: 0.04813137,  training time: 17.53
[640.0, 750.8, 767.6, 686.0, 680.6, 693.8]
Episode 922	 reward: -9.27	 Mean_loss: 0.08141919,  training time: 17.44
[671.4, 761.8, 715.2, 702.4, 676.4, 660.8]
Episode 923	 reward: -9.16	 Mean_loss: 0.09247624,  training time: 17.45
[609.0, 814.2, 758.2, 671.8, 676.8, 675.0]
Episode 924	 reward: -9.14	 Mean_loss: 0.08921198,  training time: 17.45
[643.0, 738.0, 747.8, 668.0, 683.6, 664.8]
Episode 925	 reward: -9.20	 Mean_loss: 0.06604347,  training time: 17.44
[607.8, 692.2, 702.4, 704.8, 734.0, 686.4]
Episode 926	 reward: -9.42	 Mean_loss: 0.05428159,  training time: 17.47
[656.2, 802.0, 807.2, 708.6, 722.2, 685.2]
Episode 927	 reward: -8.71	 Mean_loss: 0.06294327,  training time: 17.54
[663.0, 847.0, 735.4, 640.6, 730.4, 639.0]
Episode 928	 reward: -8.76	 Mean_loss: 0.10218716,  training time: 17.43
[670.2, 798.6, 746.6, 678.0, 684.4, 688.8]
Episode 929	 reward: -8.93	 Mean_loss: 0.07501869,  training time: 17.44
[647.2, 781.6, 718.2, 670.2, 715.4, 701.6]
Episode 930	 reward: -9.36	 Mean_loss: 0.06313380,  training time: 17.44
[647.4, 800.0, 765.8, 708.8, 727.6, 651.2]
Episode 931	 reward: -9.14	 Mean_loss: 0.10520157,  training time: 17.45
[658.2, 777.6, 771.8, 674.0, 709.4, 692.0]
Episode 932	 reward: -8.82	 Mean_loss: 0.06478070,  training time: 17.48
[656.6, 828.0, 752.6, 666.4, 737.6, 672.8]
Episode 933	 reward: -8.95	 Mean_loss: 0.08172863,  training time: 17.44
[668.8, 795.6, 740.2, 688.4, 712.8, 668.2]
Episode 934	 reward: -8.91	 Mean_loss: 0.08143257,  training time: 17.44
[677.8, 750.0, 783.2, 647.0, 659.0, 659.8]
Episode 935	 reward: -9.15	 Mean_loss: 0.09587613,  training time: 17.45
[694.8, 759.8, 741.6, 668.6, 722.8, 702.2]
Episode 936	 reward: -9.49	 Mean_loss: 0.09534504,  training time: 17.45
[630.0, 744.6, 710.2, 666.6, 724.2, 700.2]
Episode 937	 reward: -8.78	 Mean_loss: 0.06851178,  training time: 17.46
[681.8, 725.0, 752.0, 680.8, 752.2, 680.0]
Episode 938	 reward: -8.95	 Mean_loss: 0.11055280,  training time: 17.50
[656.0, 776.4, 736.6, 706.4, 752.6, 704.4]
Episode 939	 reward: -8.40	 Mean_loss: 0.07281418,  training time: 17.45
[663.6, 768.6, 747.8, 701.6, 697.8, 713.8]
Episode 940	 reward: -9.45	 Mean_loss: 0.08454202,  training time: 17.46
[(15, 10), (15, 10), (15, 9), (15, 7), (15, 7), (15, 9)]
[747.0, 783.2, 805.6, 750.0, 727.2, 716.2]
Episode 941	 reward: -9.47	 Mean_loss: 0.08815146,  training time: 22.39
[826.0, 715.4, 751.2, 745.8, 716.6, 697.4]
Episode 942	 reward: -8.73	 Mean_loss: 0.06939031,  training time: 22.34
[766.2, 698.2, 748.6, 743.2, 698.0, 728.6]
Episode 943	 reward: -8.93	 Mean_loss: 0.05014004,  training time: 22.36
[783.2, 755.4, 751.8, 734.0, 719.2, 704.6]
Episode 944	 reward: -9.48	 Mean_loss: 0.08295634,  training time: 22.39
[784.6, 779.8, 711.6, 741.0, 698.8, 728.4]
Episode 945	 reward: -9.37	 Mean_loss: 0.06363426,  training time: 22.33
[752.4, 797.6, 802.8, 754.8, 684.4, 747.6]
Episode 946	 reward: -9.45	 Mean_loss: 0.05287823,  training time: 22.32
[774.6, 784.4, 777.0, 701.2, 708.6, 760.8]
Episode 947	 reward: -9.34	 Mean_loss: 0.06877279,  training time: 22.32
[792.2, 760.4, 743.4, 778.8, 746.6, 699.8]
Episode 948	 reward: -9.04	 Mean_loss: 0.05278734,  training time: 22.33
[805.6, 756.6, 777.6, 737.4, 698.2, 715.2]
Episode 949	 reward: -9.45	 Mean_loss: 0.04867373,  training time: 22.31
[758.4, 756.2, 733.2, 727.4, 731.8, 704.8]
Episode 950	 reward: -9.47	 Mean_loss: 0.05154977,  training time: 22.34
[760.0, 778.4, 773.6, 703.6, 702.0, 727.6]
Episode 951	 reward: -9.23	 Mean_loss: 0.08498432,  training time: 22.30
[747.0, 744.8, 791.6, 698.4, 686.4, 673.2]
Episode 952	 reward: -8.84	 Mean_loss: 0.08681062,  training time: 22.32
[781.6, 725.4, 772.0, 678.4, 684.8, 725.2]
Episode 953	 reward: -9.51	 Mean_loss: 0.05643708,  training time: 22.35
[761.0, 813.8, 779.2, 750.4, 705.2, 725.8]
Episode 954	 reward: -10.38	 Mean_loss: 0.03773300,  training time: 22.32
[780.2, 763.8, 758.8, 723.6, 712.8, 714.8]
Episode 955	 reward: -9.45	 Mean_loss: 0.05185687,  training time: 22.31
[765.4, 771.8, 754.2, 659.6, 710.2, 678.8]
Episode 956	 reward: -9.11	 Mean_loss: 0.07135309,  training time: 22.31
[788.8, 735.6, 781.8, 724.8, 726.8, 759.4]
Episode 957	 reward: -8.80	 Mean_loss: 0.05445646,  training time: 22.36
[762.0, 739.6, 762.0, 729.2, 715.4, 668.2]
Episode 958	 reward: -9.13	 Mean_loss: 0.07374768,  training time: 22.39
[798.6, 795.0, 814.6, 702.4, 729.0, 733.2]
Episode 959	 reward: -9.49	 Mean_loss: 0.05151032,  training time: 22.31
[742.0, 780.8, 790.0, 727.4, 724.8, 735.8]
Episode 960	 reward: -9.18	 Mean_loss: 0.09229530,  training time: 22.31
[(15, 9), (15, 10), (15, 9), (15, 7), (15, 5), (15, 7)]
[741.0, 748.0, 758.2, 685.6, 707.0, 726.2]
Episode 961	 reward: -9.17	 Mean_loss: 0.07944705,  training time: 20.09
[726.6, 709.4, 695.8, 715.2, 716.8, 683.6]
Episode 962	 reward: -8.55	 Mean_loss: 0.07012602,  training time: 20.04
[753.8, 760.0, 723.0, 736.0, 695.2, 739.2]
Episode 963	 reward: -8.78	 Mean_loss: 0.08131889,  training time: 20.00
[772.6, 771.4, 716.2, 729.2, 688.4, 692.8]
Episode 964	 reward: -9.42	 Mean_loss: 0.07925479,  training time: 20.00
[790.6, 705.8, 792.8, 725.4, 720.6, 720.4]
Episode 965	 reward: -9.47	 Mean_loss: 0.10857859,  training time: 19.99
[740.4, 748.8, 715.0, 742.0, 705.2, 729.6]
Episode 966	 reward: -8.85	 Mean_loss: 0.13653268,  training time: 19.99
[745.2, 716.4, 718.2, 751.6, 708.8, 717.0]
Episode 967	 reward: -8.75	 Mean_loss: 0.08098631,  training time: 20.01
[746.0, 737.8, 760.8, 695.4, 711.6, 677.4]
Episode 968	 reward: -8.98	 Mean_loss: 0.07283275,  training time: 20.01
[776.8, 770.2, 742.6, 740.0, 692.4, 704.0]
Episode 969	 reward: -9.39	 Mean_loss: 0.07020707,  training time: 20.01
[746.8, 748.8, 714.2, 674.8, 686.4, 760.2]
Episode 970	 reward: -9.09	 Mean_loss: 0.12072098,  training time: 20.00
[794.0, 775.4, 723.2, 709.2, 720.4, 707.2]
Episode 971	 reward: -8.75	 Mean_loss: 0.09214416,  training time: 20.01
[746.4, 705.4, 730.4, 722.0, 714.2, 705.0]
Episode 972	 reward: -8.86	 Mean_loss: 0.06859975,  training time: 20.10
[740.6, 787.4, 737.0, 685.4, 681.8, 733.0]
Episode 973	 reward: -8.40	 Mean_loss: 0.15437840,  training time: 19.99
[747.6, 701.8, 753.4, 706.8, 678.2, 734.0]
Episode 974	 reward: -9.32	 Mean_loss: 0.11744925,  training time: 19.99
[724.6, 705.0, 736.0, 755.8, 678.8, 730.6]
Episode 975	 reward: -9.10	 Mean_loss: 0.07306890,  training time: 19.99
[707.2, 777.0, 752.4, 711.6, 666.2, 705.6]
Episode 976	 reward: -9.48	 Mean_loss: 0.08913645,  training time: 19.99
[729.8, 748.8, 701.8, 701.6, 678.4, 678.8]
Episode 977	 reward: -8.43	 Mean_loss: 0.08175450,  training time: 20.01
[750.8, 767.6, 733.2, 716.0, 709.8, 680.2]
Episode 978	 reward: -8.63	 Mean_loss: 0.08046127,  training time: 20.02
[744.4, 743.0, 688.4, 717.0, 680.6, 741.0]
Episode 979	 reward: -8.63	 Mean_loss: 0.08749451,  training time: 19.97
[747.4, 759.6, 688.0, 715.4, 725.4, 718.2]
Episode 980	 reward: -8.66	 Mean_loss: 0.08088898,  training time: 19.99
[(15, 10), (15, 10), (15, 7), (15, 5), (15, 9), (15, 9)]
[718.2, 760.2, 681.4, 748.0, 718.6, 690.8]
Episode 981	 reward: -9.16	 Mean_loss: 0.08468236,  training time: 21.39
[758.4, 763.2, 690.4, 747.2, 662.6, 704.2]
Episode 982	 reward: -8.71	 Mean_loss: 0.06563878,  training time: 21.33
[747.2, 792.4, 663.4, 759.2, 665.6, 720.4]
Episode 983	 reward: -9.37	 Mean_loss: 0.04794995,  training time: 21.33
[738.4, 813.0, 741.2, 753.2, 721.4, 698.6]
Episode 984	 reward: -8.78	 Mean_loss: 0.07677571,  training time: 21.35
[721.2, 755.6, 706.6, 784.2, 732.2, 679.6]
Episode 985	 reward: -8.85	 Mean_loss: 0.07625988,  training time: 21.35
[718.2, 736.6, 724.2, 751.0, 729.8, 686.8]
Episode 986	 reward: -8.74	 Mean_loss: 0.06179236,  training time: 21.34
[744.8, 724.6, 658.2, 786.2, 730.6, 676.8]
Episode 987	 reward: -9.32	 Mean_loss: 0.08235971,  training time: 21.38
[743.0, 757.2, 645.4, 794.8, 737.8, 707.4]
Episode 988	 reward: -8.79	 Mean_loss: 0.07247593,  training time: 21.42
[747.2, 808.4, 722.8, 767.6, 732.4, 692.6]
Episode 989	 reward: -9.23	 Mean_loss: 0.08656969,  training time: 21.34
[731.0, 760.0, 696.0, 734.2, 726.6, 735.0]
Episode 990	 reward: -9.57	 Mean_loss: 0.08198689,  training time: 21.33
[710.0, 798.4, 686.0, 779.4, 717.8, 678.4]
Episode 991	 reward: -9.40	 Mean_loss: 0.10987277,  training time: 21.34
[762.4, 860.8, 622.0, 822.4, 717.8, 691.8]
Episode 992	 reward: -9.16	 Mean_loss: 0.08592071,  training time: 21.34
[742.0, 774.2, 655.6, 798.6, 721.0, 674.2]
Episode 993	 reward: -8.96	 Mean_loss: 0.07108204,  training time: 21.41
[722.8, 789.4, 680.6, 774.6, 699.6, 705.8]
Episode 994	 reward: -9.13	 Mean_loss: 0.06495223,  training time: 21.32
[741.8, 789.2, 677.0, 790.8, 715.6, 691.0]
Episode 995	 reward: -8.95	 Mean_loss: 0.08150253,  training time: 21.34
[733.8, 790.0, 687.6, 807.6, 734.6, 711.8]
Episode 996	 reward: -9.19	 Mean_loss: 0.11141421,  training time: 21.32
[755.6, 769.0, 697.2, 743.6, 731.2, 684.2]
Episode 997	 reward: -9.04	 Mean_loss: 0.06514110,  training time: 21.34
[684.0, 779.6, 692.6, 764.6, 732.8, 711.6]
Episode 998	 reward: -9.00	 Mean_loss: 0.06967579,  training time: 21.33
[780.2, 748.0, 706.4, 792.6, 707.4, 686.0]
Episode 999	 reward: -9.10	 Mean_loss: 0.05247233,  training time: 21.33
[714.2, 727.6, 659.8, 805.4, 694.2, 696.2]
Episode 1000	 reward: -8.89	 Mean_loss: 0.06821088,  training time: 21.33
[]
+ for model in 'maml+$model_suffix'
+ echo 15,5 15,7 15,9 15,10
+ tr ' ' '\n'
+ IFS=,
+ read n_j n_m
+ python train/DAN_finetuning.py --logdir ./runs/exp11/transfer_maml+exp11_1000_64_3_15x5 --model_suffix exp11_maml+exp11_1000_64_3_15x5 --finetuning_model maml+exp11_1000_64_3 --max_updates 100 --n_j 15 --n_m 5 --num_envs 5 --hidden_dim_actor 64 --hidden_dim_critic 64 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x5+mix
save model name:  15x5+mix+exp11_maml+exp11_1000_64_3_15x5
./trained_network/SD2/maml+exp11_1000_64_3.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/maml+exp11_1000_64_3.pth
vali data :./data/data_train_vali/SD2/15x5+mix


progress:   0%|[34m          [0m| 0/100 [00:00<?, ?it/s]                                                 Episode 1	 reward: -6.38	 makespan: 631.80	 Mean_loss: 0.27525520,  training time: 2.22
progress:   0%|[34m          [0m| 0/100 [00:02<?, ?it/s]progress:   1%|[34m          [0m| 1/100 [00:02<03:39,  2.22s/it]                                                         Episode 2	 reward: -6.44	 makespan: 637.60	 Mean_loss: 0.19872075,  training time: 1.19
progress:   1%|[34m          [0m| 1/100 [00:03<03:39,  2.22s/it]progress:   2%|[34m         [0m| 2/100 [00:03<02:38,  1.61s/it]                                                         Episode 3	 reward: -6.49	 makespan: 642.20	 Mean_loss: 0.31483650,  training time: 1.19
progress:   2%|[34m         [0m| 2/100 [00:04<02:38,  1.61s/it]progress:   3%|[34m         [0m| 3/100 [00:04<02:17,  1.42s/it]                                                         Episode 4	 reward: -6.89	 makespan: 682.00	 Mean_loss: 0.18874359,  training time: 1.19
progress:   3%|[34m         [0m| 3/100 [00:05<02:17,  1.42s/it]progress:   4%|[34m         [0m| 4/100 [00:05<02:07,  1.33s/it]                                                         Episode 5	 reward: -6.73	 makespan: 666.00	 Mean_loss: 0.26134735,  training time: 1.19
progress:   4%|[34m         [0m| 4/100 [00:06<02:07,  1.33s/it]progress:   5%|[34m         [0m| 5/100 [00:06<02:01,  1.28s/it]                                                         Episode 6	 reward: -6.43	 makespan: 637.00	 Mean_loss: 0.23000409,  training time: 1.19
progress:   5%|[34m         [0m| 5/100 [00:08<02:01,  1.28s/it]progress:   6%|[34m         [0m| 6/100 [00:08<01:57,  1.25s/it]                                                         Episode 7	 reward: -6.32	 makespan: 625.40	 Mean_loss: 0.11791686,  training time: 1.19
progress:   6%|[34m         [0m| 6/100 [00:09<01:57,  1.25s/it]progress:   7%|[34m         [0m| 7/100 [00:09<01:54,  1.23s/it]                                                         Episode 8	 reward: -6.00	 makespan: 594.00	 Mean_loss: 0.14136471,  training time: 1.19
progress:   7%|[34m         [0m| 7/100 [00:10<01:54,  1.23s/it]progress:   8%|[34m         [0m| 8/100 [00:10<01:51,  1.21s/it]                                                         Episode 9	 reward: -5.88	 makespan: 582.20	 Mean_loss: 0.08541124,  training time: 1.19
progress:   8%|[34m         [0m| 8/100 [00:11<01:51,  1.21s/it]progress:   9%|[34m         [0m| 9/100 [00:11<01:49,  1.21s/it]                                                         Episode 10	 reward: -6.24	 makespan: 617.40	 Mean_loss: 0.11831366,  training time: 1.19
progress:   9%|[34m         [0m| 9/100 [00:12<01:49,  1.21s/it]progress:  10%|[34m         [0m| 10/100 [00:12<01:48,  1.20s/it]                                                          Episode 11	 reward: -6.02	 makespan: 596.20	 Mean_loss: 0.06327330,  training time: 1.19
progress:  10%|[34m         [0m| 10/100 [00:14<01:48,  1.20s/it]progress:  11%|[34m         [0m| 11/100 [00:14<01:46,  1.20s/it]                                                          Episode 12	 reward: -6.08	 makespan: 602.40	 Mean_loss: 0.07377556,  training time: 1.29
progress:  11%|[34m         [0m| 11/100 [00:15<01:46,  1.20s/it]progress:  12%|[34m        [0m| 12/100 [00:15<01:47,  1.23s/it]                                                          Episode 13	 reward: -5.78	 makespan: 572.60	 Mean_loss: 0.06967679,  training time: 1.19
progress:  12%|[34m        [0m| 12/100 [00:16<01:47,  1.23s/it]progress:  13%|[34m        [0m| 13/100 [00:16<01:45,  1.22s/it]                                                          Episode 14	 reward: -5.80	 makespan: 573.80	 Mean_loss: 0.06113537,  training time: 1.19
progress:  13%|[34m        [0m| 13/100 [00:17<01:45,  1.22s/it]progress:  14%|[34m        [0m| 14/100 [00:17<01:43,  1.21s/it]                                                          Episode 15	 reward: -5.41	 makespan: 535.20	 Mean_loss: 0.06275664,  training time: 1.18
progress:  14%|[34m        [0m| 14/100 [00:18<01:43,  1.21s/it]progress:  15%|[34m        [0m| 15/100 [00:18<01:42,  1.20s/it]                                                          Episode 16	 reward: -5.94	 makespan: 588.40	 Mean_loss: 0.13201749,  training time: 1.19
progress:  15%|[34m        [0m| 15/100 [00:20<01:42,  1.20s/it]progress:  16%|[34m        [0m| 16/100 [00:20<01:40,  1.20s/it]                                                          Episode 17	 reward: -5.67	 makespan: 561.40	 Mean_loss: 0.07543496,  training time: 1.18
progress:  16%|[34m        [0m| 16/100 [00:21<01:40,  1.20s/it]progress:  17%|[34m        [0m| 17/100 [00:21<01:39,  1.19s/it]                                                          Episode 18	 reward: -5.62	 makespan: 556.40	 Mean_loss: 0.07397322,  training time: 1.18
progress:  17%|[34m        [0m| 17/100 [00:22<01:39,  1.19s/it]progress:  18%|[34m        [0m| 18/100 [00:22<01:37,  1.19s/it]                                                          Episode 19	 reward: -5.76	 makespan: 570.60	 Mean_loss: 0.07596675,  training time: 1.18
progress:  18%|[34m        [0m| 18/100 [00:23<01:37,  1.19s/it]progress:  19%|[34m        [0m| 19/100 [00:23<01:36,  1.19s/it]                                                          Episode 20	 reward: -5.76	 makespan: 570.60	 Mean_loss: 0.05941030,  training time: 1.18
progress:  19%|[34m        [0m| 19/100 [00:24<01:36,  1.19s/it]progress:  20%|[34m        [0m| 20/100 [00:24<01:34,  1.19s/it]                                                          Episode 21	 reward: -6.01	 makespan: 595.20	 Mean_loss: 0.07290694,  training time: 1.18
progress:  20%|[34m        [0m| 20/100 [00:26<01:34,  1.19s/it]progress:  21%|[34m        [0m| 21/100 [00:26<01:33,  1.19s/it]                                                          Episode 22	 reward: -5.81	 makespan: 575.00	 Mean_loss: 0.07585892,  training time: 1.18
progress:  21%|[34m        [0m| 21/100 [00:27<01:33,  1.19s/it]progress:  22%|[34m       [0m| 22/100 [00:27<01:32,  1.19s/it]                                                          Episode 23	 reward: -5.34	 makespan: 529.00	 Mean_loss: 0.04727345,  training time: 1.18
progress:  22%|[34m       [0m| 22/100 [00:28<01:32,  1.19s/it]progress:  23%|[34m       [0m| 23/100 [00:28<01:31,  1.19s/it]                                                          Episode 24	 reward: -5.87	 makespan: 581.60	 Mean_loss: 0.03857032,  training time: 1.18
progress:  23%|[34m       [0m| 23/100 [00:29<01:31,  1.19s/it]progress:  24%|[34m       [0m| 24/100 [00:29<01:30,  1.19s/it]                                                          Episode 25	 reward: -5.47	 makespan: 541.60	 Mean_loss: 0.03706475,  training time: 1.18
progress:  24%|[34m       [0m| 24/100 [00:30<01:30,  1.19s/it]progress:  25%|[34m       [0m| 25/100 [00:30<01:28,  1.19s/it]                                                          Episode 26	 reward: -5.38	 makespan: 532.40	 Mean_loss: 0.03864684,  training time: 1.18
progress:  25%|[34m       [0m| 25/100 [00:31<01:28,  1.19s/it]progress:  26%|[34m       [0m| 26/100 [00:31<01:27,  1.19s/it]                                                          Episode 27	 reward: -5.47	 makespan: 541.40	 Mean_loss: 0.06044839,  training time: 1.19
progress:  26%|[34m       [0m| 26/100 [00:33<01:27,  1.19s/it]progress:  27%|[34m       [0m| 27/100 [00:33<01:26,  1.19s/it]                                                          Episode 28	 reward: -6.09	 makespan: 603.40	 Mean_loss: 0.05311633,  training time: 1.18
progress:  27%|[34m       [0m| 27/100 [00:34<01:26,  1.19s/it]progress:  28%|[34m       [0m| 28/100 [00:34<01:25,  1.19s/it]                                                          Episode 29	 reward: -5.82	 makespan: 575.80	 Mean_loss: 0.05378812,  training time: 1.19
progress:  28%|[34m       [0m| 28/100 [00:35<01:25,  1.19s/it]progress:  29%|[34m       [0m| 29/100 [00:35<01:24,  1.19s/it]                                                          Episode 30	 reward: -5.80	 makespan: 574.20	 Mean_loss: 0.02951327,  training time: 1.18
progress:  29%|[34m       [0m| 29/100 [00:36<01:24,  1.19s/it]progress:  30%|[34m       [0m| 30/100 [00:36<01:22,  1.18s/it]                                                          Episode 31	 reward: -6.22	 makespan: 615.60	 Mean_loss: 0.05187718,  training time: 1.19
progress:  30%|[34m       [0m| 30/100 [00:37<01:22,  1.18s/it]progress:  31%|[34m       [0m| 31/100 [00:37<01:21,  1.19s/it]                                                          Episode 32	 reward: -5.81	 makespan: 574.80	 Mean_loss: 0.05640658,  training time: 1.18
progress:  31%|[34m       [0m| 31/100 [00:39<01:21,  1.19s/it]progress:  32%|[34m      [0m| 32/100 [00:39<01:20,  1.19s/it]                                                          Episode 33	 reward: -6.13	 makespan: 607.00	 Mean_loss: 0.06757525,  training time: 1.18
progress:  32%|[34m      [0m| 32/100 [00:40<01:20,  1.19s/it]progress:  33%|[34m      [0m| 33/100 [00:40<01:19,  1.19s/it]                                                          Episode 34	 reward: -6.07	 makespan: 600.60	 Mean_loss: 0.05947667,  training time: 1.18
progress:  33%|[34m      [0m| 33/100 [00:41<01:19,  1.19s/it]progress:  34%|[34m      [0m| 34/100 [00:41<01:18,  1.18s/it]                                                          Episode 35	 reward: -5.90	 makespan: 584.00	 Mean_loss: 0.05192693,  training time: 1.18
progress:  34%|[34m      [0m| 34/100 [00:42<01:18,  1.18s/it]progress:  35%|[34m      [0m| 35/100 [00:42<01:16,  1.18s/it]                                                          Episode 36	 reward: -5.90	 makespan: 584.00	 Mean_loss: 0.02681789,  training time: 1.18
progress:  35%|[34m      [0m| 35/100 [00:43<01:16,  1.18s/it]progress:  36%|[34m      [0m| 36/100 [00:43<01:15,  1.18s/it]                                                          Episode 37	 reward: -5.50	 makespan: 544.60	 Mean_loss: 0.03363961,  training time: 1.18
progress:  36%|[34m      [0m| 36/100 [00:45<01:15,  1.18s/it]progress:  37%|[34m      [0m| 37/100 [00:45<01:14,  1.18s/it]                                                          Episode 38	 reward: -5.47	 makespan: 541.60	 Mean_loss: 0.02794982,  training time: 1.18
progress:  37%|[34m      [0m| 37/100 [00:46<01:14,  1.18s/it]progress:  38%|[34m      [0m| 38/100 [00:46<01:13,  1.18s/it]                                                          Episode 39	 reward: -5.50	 makespan: 544.40	 Mean_loss: 0.05596623,  training time: 1.18
progress:  38%|[34m      [0m| 38/100 [00:47<01:13,  1.18s/it]progress:  39%|[34m      [0m| 39/100 [00:47<01:12,  1.18s/it]                                                          Episode 40	 reward: -5.66	 makespan: 560.60	 Mean_loss: 0.03411002,  training time: 1.18
progress:  39%|[34m      [0m| 39/100 [00:48<01:12,  1.18s/it]progress:  40%|[34m      [0m| 40/100 [00:48<01:10,  1.18s/it]                                                          Episode 41	 reward: -5.65	 makespan: 559.00	 Mean_loss: 0.03295432,  training time: 1.18
progress:  40%|[34m      [0m| 40/100 [00:49<01:10,  1.18s/it]progress:  41%|[34m      [0m| 41/100 [00:49<01:09,  1.18s/it]                                                          Episode 42	 reward: -5.63	 makespan: 557.60	 Mean_loss: 0.05610739,  training time: 1.19
progress:  41%|[34m      [0m| 41/100 [00:50<01:09,  1.18s/it]progress:  42%|[34m     [0m| 42/100 [00:50<01:08,  1.18s/it]                                                          Episode 43	 reward: -5.79	 makespan: 573.40	 Mean_loss: 0.05274948,  training time: 1.19
progress:  42%|[34m     [0m| 42/100 [00:52<01:08,  1.18s/it]progress:  43%|[34m     [0m| 43/100 [00:52<01:07,  1.19s/it]                                                          Episode 44	 reward: -5.83	 makespan: 577.40	 Mean_loss: 0.03930245,  training time: 1.19
progress:  43%|[34m     [0m| 43/100 [00:53<01:07,  1.19s/it]progress:  44%|[34m     [0m| 44/100 [00:53<01:06,  1.19s/it]                                                          Episode 45	 reward: -5.61	 makespan: 555.60	 Mean_loss: 0.03161948,  training time: 1.19
progress:  44%|[34m     [0m| 44/100 [00:54<01:06,  1.19s/it]progress:  45%|[34m     [0m| 45/100 [00:54<01:05,  1.19s/it]                                                          Episode 46	 reward: -5.66	 makespan: 560.20	 Mean_loss: 0.04130197,  training time: 1.19
progress:  45%|[34m     [0m| 45/100 [00:55<01:05,  1.19s/it]progress:  46%|[34m     [0m| 46/100 [00:55<01:04,  1.19s/it]                                                          Episode 47	 reward: -5.76	 makespan: 570.00	 Mean_loss: 0.03674798,  training time: 1.19
progress:  46%|[34m     [0m| 46/100 [00:56<01:04,  1.19s/it]progress:  47%|[34m     [0m| 47/100 [00:56<01:03,  1.19s/it]                                                          Episode 48	 reward: -5.72	 makespan: 566.40	 Mean_loss: 0.02984311,  training time: 1.19
progress:  47%|[34m     [0m| 47/100 [00:58<01:03,  1.19s/it]progress:  48%|[34m     [0m| 48/100 [00:58<01:02,  1.19s/it]                                                          Episode 49	 reward: -5.37	 makespan: 531.60	 Mean_loss: 0.02785035,  training time: 1.19
progress:  48%|[34m     [0m| 48/100 [00:59<01:02,  1.19s/it]progress:  49%|[34m     [0m| 49/100 [00:59<01:00,  1.19s/it]                                                          Episode 50	 reward: -5.59	 makespan: 553.60	 Mean_loss: 0.02119755,  training time: 1.19
progress:  49%|[34m     [0m| 49/100 [01:00<01:00,  1.19s/it]progress:  50%|[34m     [0m| 50/100 [01:00<00:59,  1.19s/it]                                                          Episode 51	 reward: -5.54	 makespan: 548.20	 Mean_loss: 0.03248643,  training time: 1.20
progress:  50%|[34m     [0m| 50/100 [01:01<00:59,  1.19s/it]progress:  51%|[34m     [0m| 51/100 [01:01<00:58,  1.19s/it]                                                          Episode 52	 reward: -5.55	 makespan: 549.80	 Mean_loss: 0.04516403,  training time: 1.19
progress:  51%|[34m     [0m| 51/100 [01:02<00:58,  1.19s/it]progress:  52%|[34m    [0m| 52/100 [01:02<00:57,  1.19s/it]                                                          Episode 53	 reward: -5.52	 makespan: 546.80	 Mean_loss: 0.02468021,  training time: 1.20
progress:  52%|[34m    [0m| 52/100 [01:04<00:57,  1.19s/it]progress:  53%|[34m    [0m| 53/100 [01:04<00:56,  1.20s/it]                                                          Episode 54	 reward: -5.78	 makespan: 572.40	 Mean_loss: 0.02980217,  training time: 1.19
progress:  53%|[34m    [0m| 53/100 [01:05<00:56,  1.20s/it]progress:  54%|[34m    [0m| 54/100 [01:05<00:54,  1.19s/it]                                                          Episode 55	 reward: -5.86	 makespan: 580.20	 Mean_loss: 0.03138656,  training time: 1.20
progress:  54%|[34m    [0m| 54/100 [01:06<00:54,  1.19s/it]progress:  55%|[34m    [0m| 55/100 [01:06<00:53,  1.20s/it]                                                          Episode 56	 reward: -5.69	 makespan: 563.40	 Mean_loss: 0.02836453,  training time: 1.19
progress:  55%|[34m    [0m| 55/100 [01:07<00:53,  1.20s/it]progress:  56%|[34m    [0m| 56/100 [01:07<00:52,  1.20s/it]                                                          Episode 57	 reward: -5.49	 makespan: 543.60	 Mean_loss: 0.03296076,  training time: 1.20
progress:  56%|[34m    [0m| 56/100 [01:08<00:52,  1.20s/it]progress:  57%|[34m    [0m| 57/100 [01:08<00:51,  1.20s/it]                                                          Episode 58	 reward: -5.45	 makespan: 539.80	 Mean_loss: 0.04034105,  training time: 1.20
progress:  57%|[34m    [0m| 57/100 [01:10<00:51,  1.20s/it]progress:  58%|[34m    [0m| 58/100 [01:10<00:50,  1.20s/it]                                                          Episode 59	 reward: -5.37	 makespan: 531.20	 Mean_loss: 0.02511030,  training time: 1.19
progress:  58%|[34m    [0m| 58/100 [01:11<00:50,  1.20s/it]progress:  59%|[34m    [0m| 59/100 [01:11<00:49,  1.20s/it]                                                          Episode 60	 reward: -5.64	 makespan: 558.40	 Mean_loss: 0.02204203,  training time: 1.19
progress:  59%|[34m    [0m| 59/100 [01:12<00:49,  1.20s/it]progress:  60%|[34m    [0m| 60/100 [01:12<00:47,  1.20s/it]                                                          Episode 61	 reward: -5.56	 makespan: 550.20	 Mean_loss: 0.01447513,  training time: 1.19
progress:  60%|[34m    [0m| 60/100 [01:13<00:47,  1.20s/it]progress:  61%|[34m    [0m| 61/100 [01:13<00:46,  1.19s/it]                                                          Episode 62	 reward: -5.71	 makespan: 564.80	 Mean_loss: 0.04326260,  training time: 1.19
progress:  61%|[34m    [0m| 61/100 [01:14<00:46,  1.19s/it]progress:  62%|[34m   [0m| 62/100 [01:14<00:45,  1.19s/it]                                                          Episode 63	 reward: -5.69	 makespan: 563.20	 Mean_loss: 0.02820675,  training time: 1.19
progress:  62%|[34m   [0m| 62/100 [01:16<00:45,  1.19s/it]progress:  63%|[34m   [0m| 63/100 [01:16<00:44,  1.19s/it]                                                          Episode 64	 reward: -5.71	 makespan: 565.60	 Mean_loss: 0.05416434,  training time: 1.19
progress:  63%|[34m   [0m| 63/100 [01:17<00:44,  1.19s/it]progress:  64%|[34m   [0m| 64/100 [01:17<00:42,  1.19s/it]                                                          Episode 65	 reward: -5.95	 makespan: 589.40	 Mean_loss: 0.03050232,  training time: 1.19
progress:  64%|[34m   [0m| 64/100 [01:18<00:42,  1.19s/it]progress:  65%|[34m   [0m| 65/100 [01:18<00:41,  1.19s/it]                                                          Episode 66	 reward: -5.41	 makespan: 536.00	 Mean_loss: 0.05688005,  training time: 1.19
progress:  65%|[34m   [0m| 65/100 [01:19<00:41,  1.19s/it]progress:  66%|[34m   [0m| 66/100 [01:19<00:40,  1.19s/it]                                                          Episode 67	 reward: -5.81	 makespan: 575.00	 Mean_loss: 0.04772411,  training time: 1.19
progress:  66%|[34m   [0m| 66/100 [01:20<00:40,  1.19s/it]progress:  67%|[34m   [0m| 67/100 [01:20<00:39,  1.19s/it]                                                          Episode 68	 reward: -5.64	 makespan: 558.80	 Mean_loss: 0.04903794,  training time: 1.20
progress:  67%|[34m   [0m| 67/100 [01:22<00:39,  1.19s/it]progress:  68%|[34m   [0m| 68/100 [01:22<00:38,  1.20s/it]                                                          Episode 69	 reward: -5.40	 makespan: 534.80	 Mean_loss: 0.02307684,  training time: 1.19
progress:  68%|[34m   [0m| 68/100 [01:23<00:38,  1.20s/it]progress:  69%|[34m   [0m| 69/100 [01:23<00:37,  1.19s/it]                                                          Episode 70	 reward: -5.76	 makespan: 569.80	 Mean_loss: 0.02744175,  training time: 1.19
progress:  69%|[34m   [0m| 69/100 [01:24<00:37,  1.19s/it]progress:  70%|[34m   [0m| 70/100 [01:24<00:35,  1.19s/it]                                                          Episode 71	 reward: -5.58	 makespan: 552.00	 Mean_loss: 0.01563809,  training time: 1.19
progress:  70%|[34m   [0m| 70/100 [01:25<00:35,  1.19s/it]progress:  71%|[34m   [0m| 71/100 [01:25<00:34,  1.19s/it]                                                          Episode 72	 reward: -5.72	 makespan: 566.60	 Mean_loss: 0.02636984,  training time: 1.19
progress:  71%|[34m   [0m| 71/100 [01:26<00:34,  1.19s/it]progress:  72%|[34m  [0m| 72/100 [01:26<00:33,  1.19s/it]                                                          Episode 73	 reward: -5.84	 makespan: 577.80	 Mean_loss: 0.04594259,  training time: 1.19
progress:  72%|[34m  [0m| 72/100 [01:27<00:33,  1.19s/it]progress:  73%|[34m  [0m| 73/100 [01:27<00:32,  1.19s/it]                                                          Episode 74	 reward: -5.72	 makespan: 566.20	 Mean_loss: 0.01901980,  training time: 1.19
progress:  73%|[34m  [0m| 73/100 [01:29<00:32,  1.19s/it]progress:  74%|[34m  [0m| 74/100 [01:29<00:31,  1.19s/it]                                                          Episode 75	 reward: -5.54	 makespan: 548.40	 Mean_loss: 0.02020098,  training time: 1.19
progress:  74%|[34m  [0m| 74/100 [01:30<00:31,  1.19s/it]progress:  75%|[34m  [0m| 75/100 [01:30<00:29,  1.19s/it]                                                          Episode 76	 reward: -6.03	 makespan: 597.40	 Mean_loss: 0.08876790,  training time: 1.19
progress:  75%|[34m  [0m| 75/100 [01:31<00:29,  1.19s/it]progress:  76%|[34m  [0m| 76/100 [01:31<00:28,  1.19s/it]                                                          Episode 77	 reward: -5.85	 makespan: 579.40	 Mean_loss: 0.04606820,  training time: 1.19
progress:  76%|[34m  [0m| 76/100 [01:32<00:28,  1.19s/it]progress:  77%|[34m  [0m| 77/100 [01:32<00:27,  1.19s/it]                                                          Episode 78	 reward: -5.93	 makespan: 586.80	 Mean_loss: 0.02576764,  training time: 1.19
progress:  77%|[34m  [0m| 77/100 [01:33<00:27,  1.19s/it]progress:  78%|[34m  [0m| 78/100 [01:33<00:26,  1.19s/it]                                                          Episode 79	 reward: -5.54	 makespan: 548.40	 Mean_loss: 0.05707933,  training time: 1.19
progress:  78%|[34m  [0m| 78/100 [01:35<00:26,  1.19s/it]progress:  79%|[34m  [0m| 79/100 [01:35<00:25,  1.19s/it]                                                          Episode 80	 reward: -6.05	 makespan: 598.80	 Mean_loss: 0.07220121,  training time: 1.20
progress:  79%|[34m  [0m| 79/100 [01:36<00:25,  1.19s/it]progress:  80%|[34m  [0m| 80/100 [01:36<00:23,  1.20s/it]                                                          Episode 81	 reward: -6.06	 makespan: 599.80	 Mean_loss: 0.05230003,  training time: 1.19
progress:  80%|[34m  [0m| 80/100 [01:37<00:23,  1.20s/it]progress:  81%|[34m  [0m| 81/100 [01:37<00:22,  1.19s/it]                                                          Episode 82	 reward: -5.90	 makespan: 584.20	 Mean_loss: 0.06445422,  training time: 1.19
progress:  81%|[34m  [0m| 81/100 [01:38<00:22,  1.19s/it]progress:  82%|[34m [0m| 82/100 [01:38<00:21,  1.19s/it]                                                          Episode 83	 reward: -6.29	 makespan: 623.20	 Mean_loss: 0.05323978,  training time: 1.19
progress:  82%|[34m [0m| 82/100 [01:39<00:21,  1.19s/it]progress:  83%|[34m [0m| 83/100 [01:39<00:20,  1.19s/it]                                                          Episode 84	 reward: -5.97	 makespan: 590.60	 Mean_loss: 0.05617335,  training time: 1.18
progress:  83%|[34m [0m| 83/100 [01:41<00:20,  1.19s/it]progress:  84%|[34m [0m| 84/100 [01:41<00:19,  1.19s/it]                                                          Episode 85	 reward: -5.71	 makespan: 565.20	 Mean_loss: 0.04879602,  training time: 1.18
progress:  84%|[34m [0m| 84/100 [01:42<00:19,  1.19s/it]progress:  85%|[34m [0m| 85/100 [01:42<00:17,  1.19s/it]                                                          Episode 86	 reward: -5.91	 makespan: 585.00	 Mean_loss: 0.03794633,  training time: 1.19
progress:  85%|[34m [0m| 85/100 [01:43<00:17,  1.19s/it]progress:  86%|[34m [0m| 86/100 [01:43<00:16,  1.19s/it]                                                          Episode 87	 reward: -5.70	 makespan: 564.00	 Mean_loss: 0.05838383,  training time: 1.19
progress:  86%|[34m [0m| 86/100 [01:44<00:16,  1.19s/it]progress:  87%|[34m [0m| 87/100 [01:44<00:15,  1.19s/it]                                                          Episode 88	 reward: -5.57	 makespan: 551.40	 Mean_loss: 0.01239732,  training time: 1.19
progress:  87%|[34m [0m| 87/100 [01:45<00:15,  1.19s/it]progress:  88%|[34m [0m| 88/100 [01:45<00:14,  1.19s/it]                                                          Episode 89	 reward: -5.57	 makespan: 551.00	 Mean_loss: 0.03021492,  training time: 1.19
progress:  88%|[34m [0m| 88/100 [01:47<00:14,  1.19s/it]progress:  89%|[34m [0m| 89/100 [01:47<00:13,  1.19s/it]                                                          Episode 90	 reward: -5.52	 makespan: 546.00	 Mean_loss: 0.01689588,  training time: 1.19
progress:  89%|[34m [0m| 89/100 [01:48<00:13,  1.19s/it]progress:  90%|[34m [0m| 90/100 [01:48<00:11,  1.19s/it]                                                          Episode 91	 reward: -5.90	 makespan: 584.40	 Mean_loss: 0.04681483,  training time: 1.19
progress:  90%|[34m [0m| 90/100 [01:49<00:11,  1.19s/it]progress:  91%|[34m [0m| 91/100 [01:49<00:10,  1.19s/it]                                                          Episode 92	 reward: -5.59	 makespan: 553.40	 Mean_loss: 0.03208558,  training time: 1.19
progress:  91%|[34m [0m| 91/100 [01:50<00:10,  1.19s/it]progress:  92%|[34m[0m| 92/100 [01:50<00:09,  1.19s/it]                                                          Episode 93	 reward: -5.86	 makespan: 579.80	 Mean_loss: 0.03466796,  training time: 1.20
progress:  92%|[34m[0m| 92/100 [01:51<00:09,  1.19s/it]progress:  93%|[34m[0m| 93/100 [01:51<00:08,  1.19s/it]                                                          Episode 94	 reward: -5.77	 makespan: 571.20	 Mean_loss: 0.04578935,  training time: 1.19
progress:  93%|[34m[0m| 93/100 [01:53<00:08,  1.19s/it]progress:  94%|[34m[0m| 94/100 [01:53<00:07,  1.19s/it]                                                          Episode 95	 reward: -5.89	 makespan: 582.80	 Mean_loss: 0.04567865,  training time: 1.19
progress:  94%|[34m[0m| 94/100 [01:54<00:07,  1.19s/it]progress:  95%|[34m[0m| 95/100 [01:54<00:05,  1.19s/it]                                                          Episode 96	 reward: -5.50	 makespan: 544.40	 Mean_loss: 0.01975143,  training time: 1.19
progress:  95%|[34m[0m| 95/100 [01:55<00:05,  1.19s/it]progress:  96%|[34m[0m| 96/100 [01:55<00:04,  1.19s/it]                                                          Episode 97	 reward: -5.48	 makespan: 542.40	 Mean_loss: 0.02896543,  training time: 1.19
progress:  96%|[34m[0m| 96/100 [01:56<00:04,  1.19s/it]progress:  97%|[34m[0m| 97/100 [01:56<00:03,  1.19s/it]                                                          Episode 98	 reward: -5.81	 makespan: 575.60	 Mean_loss: 0.03049143,  training time: 1.19
progress:  97%|[34m[0m| 97/100 [01:57<00:03,  1.19s/it]progress:  98%|[34m[0m| 98/100 [01:57<00:02,  1.19s/it]                                                          Episode 99	 reward: -5.57	 makespan: 551.20	 Mean_loss: 0.01700324,  training time: 1.19
progress:  98%|[34m[0m| 98/100 [01:58<00:02,  1.19s/it]progress:  99%|[34m[0m| 99/100 [01:58<00:01,  1.19s/it]                                                          Episode 100	 reward: -5.68	 makespan: 562.00	 Mean_loss: 0.01767367,  training time: 1.19
progress:  99%|[34m[0m| 99/100 [02:00<00:01,  1.19s/it]progress: 100%|[34m[0m| 100/100 [02:00<00:00,  1.19s/it]progress: 100%|[34m[0m| 100/100 [02:00<00:00,  1.20s/it]
+ IFS=,
+ read n_j n_m
+ python train/DAN_finetuning.py --logdir ./runs/exp11/transfer_maml+exp11_1000_64_3_15x7 --model_suffix exp11_maml+exp11_1000_64_3_15x7 --finetuning_model maml+exp11_1000_64_3 --max_updates 100 --n_j 15 --n_m 7 --num_envs 5 --hidden_dim_actor 64 --hidden_dim_critic 64 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x7+mix
save model name:  15x7+mix+exp11_maml+exp11_1000_64_3_15x7
./trained_network/SD2/maml+exp11_1000_64_3.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/maml+exp11_1000_64_3.pth
vali data :./data/data_train_vali/SD2/15x7+mix


progress:   0%|[34m          [0m| 0/100 [00:00<?, ?it/s]                                                 Episode 1	 reward: -6.62	 makespan: 655.00	 Mean_loss: 0.26315430,  training time: 2.68
progress:   0%|[34m          [0m| 0/100 [00:02<?, ?it/s]progress:   1%|[34m          [0m| 1/100 [00:02<04:25,  2.68s/it]                                                         Episode 2	 reward: -7.20	 makespan: 712.40	 Mean_loss: 0.20951042,  training time: 1.62
progress:   1%|[34m          [0m| 1/100 [00:04<04:25,  2.68s/it]progress:   2%|[34m         [0m| 2/100 [00:04<03:21,  2.06s/it]                                                         Episode 3	 reward: -6.98	 makespan: 691.00	 Mean_loss: 0.29686269,  training time: 1.62
progress:   2%|[34m         [0m| 2/100 [00:05<03:21,  2.06s/it]progress:   3%|[34m         [0m| 3/100 [00:05<03:00,  1.86s/it]                                                         Episode 4	 reward: -6.55	 makespan: 648.80	 Mean_loss: 0.26963583,  training time: 1.62
progress:   3%|[34m         [0m| 3/100 [00:07<03:00,  1.86s/it]progress:   4%|[34m         [0m| 4/100 [00:07<02:49,  1.76s/it]                                                         Episode 5	 reward: -6.49	 makespan: 643.00	 Mean_loss: 0.18664330,  training time: 1.62
progress:   4%|[34m         [0m| 4/100 [00:09<02:49,  1.76s/it]progress:   5%|[34m         [0m| 5/100 [00:09<02:42,  1.71s/it]                                                         Episode 6	 reward: -6.74	 makespan: 667.40	 Mean_loss: 0.18951975,  training time: 1.61
progress:   5%|[34m         [0m| 5/100 [00:10<02:42,  1.71s/it]progress:   6%|[34m         [0m| 6/100 [00:10<02:37,  1.68s/it]                                                         Episode 7	 reward: -6.98	 makespan: 690.60	 Mean_loss: 0.22359183,  training time: 1.61
progress:   6%|[34m         [0m| 6/100 [00:12<02:37,  1.68s/it]progress:   7%|[34m         [0m| 7/100 [00:12<02:34,  1.66s/it]                                                         Episode 8	 reward: -6.81	 makespan: 674.20	 Mean_loss: 0.15603219,  training time: 1.62
progress:   7%|[34m         [0m| 7/100 [00:14<02:34,  1.66s/it]progress:   8%|[34m         [0m| 8/100 [00:14<02:31,  1.65s/it]                                                         Episode 9	 reward: -6.49	 makespan: 642.80	 Mean_loss: 0.19322011,  training time: 1.62
progress:   8%|[34m         [0m| 8/100 [00:15<02:31,  1.65s/it]progress:   9%|[34m         [0m| 9/100 [00:15<02:28,  1.64s/it]                                                         Episode 10	 reward: -6.99	 makespan: 691.60	 Mean_loss: 0.17882508,  training time: 1.62
progress:   9%|[34m         [0m| 9/100 [00:17<02:28,  1.64s/it]progress:  10%|[34m         [0m| 10/100 [00:17<02:26,  1.63s/it]                                                          Episode 11	 reward: -6.78	 makespan: 670.80	 Mean_loss: 0.12545991,  training time: 1.62
progress:  10%|[34m         [0m| 10/100 [00:18<02:26,  1.63s/it]progress:  11%|[34m         [0m| 11/100 [00:18<02:24,  1.63s/it]                                                          Episode 12	 reward: -6.72	 makespan: 665.00	 Mean_loss: 0.16202380,  training time: 1.61
progress:  11%|[34m         [0m| 11/100 [00:20<02:24,  1.63s/it]progress:  12%|[34m        [0m| 12/100 [00:20<02:22,  1.62s/it]                                                          Episode 13	 reward: -6.54	 makespan: 647.20	 Mean_loss: 0.13797882,  training time: 1.74
progress:  12%|[34m        [0m| 12/100 [00:22<02:22,  1.62s/it]progress:  13%|[34m        [0m| 13/100 [00:22<02:24,  1.66s/it]                                                          Episode 14	 reward: -6.90	 makespan: 683.40	 Mean_loss: 0.16604581,  training time: 1.63
progress:  13%|[34m        [0m| 13/100 [00:23<02:24,  1.66s/it]progress:  14%|[34m        [0m| 14/100 [00:23<02:22,  1.65s/it]                                                          Episode 15	 reward: -6.77	 makespan: 670.60	 Mean_loss: 0.08219139,  training time: 1.64
progress:  14%|[34m        [0m| 14/100 [00:25<02:22,  1.65s/it]progress:  15%|[34m        [0m| 15/100 [00:25<02:20,  1.65s/it]                                                          Episode 16	 reward: -6.56	 makespan: 649.80	 Mean_loss: 0.16717671,  training time: 1.64
progress:  15%|[34m        [0m| 15/100 [00:27<02:20,  1.65s/it]progress:  16%|[34m        [0m| 16/100 [00:27<02:18,  1.64s/it]                                                          Episode 17	 reward: -6.23	 makespan: 616.40	 Mean_loss: 0.08227121,  training time: 1.62
progress:  16%|[34m        [0m| 16/100 [00:28<02:18,  1.64s/it]progress:  17%|[34m        [0m| 17/100 [00:28<02:15,  1.64s/it]                                                          Episode 18	 reward: -6.51	 makespan: 644.80	 Mean_loss: 0.09655316,  training time: 1.61
progress:  17%|[34m        [0m| 17/100 [00:30<02:15,  1.64s/it]progress:  18%|[34m        [0m| 18/100 [00:30<02:13,  1.63s/it]                                                          Episode 19	 reward: -6.95	 makespan: 688.20	 Mean_loss: 0.11652277,  training time: 1.64
progress:  18%|[34m        [0m| 18/100 [00:31<02:13,  1.63s/it]progress:  19%|[34m        [0m| 19/100 [00:31<02:12,  1.63s/it]                                                          Episode 20	 reward: -6.73	 makespan: 666.60	 Mean_loss: 0.09751947,  training time: 1.64
progress:  19%|[34m        [0m| 19/100 [00:33<02:12,  1.63s/it]progress:  20%|[34m        [0m| 20/100 [00:33<02:10,  1.63s/it]                                                          Episode 21	 reward: -7.14	 makespan: 707.20	 Mean_loss: 0.12791088,  training time: 1.64
progress:  20%|[34m        [0m| 20/100 [00:35<02:10,  1.63s/it]progress:  21%|[34m        [0m| 21/100 [00:35<02:09,  1.63s/it]                                                          Episode 22	 reward: -6.78	 makespan: 671.60	 Mean_loss: 0.13207032,  training time: 1.64
progress:  21%|[34m        [0m| 21/100 [00:36<02:09,  1.63s/it]progress:  22%|[34m       [0m| 22/100 [00:36<02:07,  1.63s/it]                                                          Episode 23	 reward: -6.95	 makespan: 688.40	 Mean_loss: 0.16544671,  training time: 1.64
progress:  22%|[34m       [0m| 22/100 [00:38<02:07,  1.63s/it]progress:  23%|[34m       [0m| 23/100 [00:38<02:06,  1.64s/it]                                                          Episode 24	 reward: -7.09	 makespan: 702.40	 Mean_loss: 0.10464756,  training time: 1.64
progress:  23%|[34m       [0m| 23/100 [00:40<02:06,  1.64s/it]progress:  24%|[34m       [0m| 24/100 [00:40<02:04,  1.64s/it]                                                          Episode 25	 reward: -7.06	 makespan: 699.00	 Mean_loss: 0.14197318,  training time: 1.64
progress:  24%|[34m       [0m| 24/100 [00:41<02:04,  1.64s/it]progress:  25%|[34m       [0m| 25/100 [00:41<02:02,  1.64s/it]                                                          Episode 26	 reward: -6.35	 makespan: 629.00	 Mean_loss: 0.10404470,  training time: 1.63
progress:  25%|[34m       [0m| 25/100 [00:43<02:02,  1.64s/it]progress:  26%|[34m       [0m| 26/100 [00:43<02:01,  1.64s/it]                                                          Episode 27	 reward: -6.82	 makespan: 675.00	 Mean_loss: 0.10476951,  training time: 1.63
progress:  26%|[34m       [0m| 26/100 [00:45<02:01,  1.64s/it]progress:  27%|[34m       [0m| 27/100 [00:45<01:59,  1.64s/it]                                                          Episode 28	 reward: -6.76	 makespan: 669.40	 Mean_loss: 0.07039476,  training time: 1.63
progress:  27%|[34m       [0m| 27/100 [00:46<01:59,  1.64s/it]progress:  28%|[34m       [0m| 28/100 [00:46<01:57,  1.64s/it]                                                          Episode 29	 reward: -6.59	 makespan: 652.00	 Mean_loss: 0.06367947,  training time: 1.63
progress:  28%|[34m       [0m| 28/100 [00:48<01:57,  1.64s/it]progress:  29%|[34m       [0m| 29/100 [00:48<01:56,  1.64s/it]                                                          Episode 30	 reward: -7.06	 makespan: 698.60	 Mean_loss: 0.15591517,  training time: 1.63
progress:  29%|[34m       [0m| 29/100 [00:50<01:56,  1.64s/it]progress:  30%|[34m       [0m| 30/100 [00:50<01:54,  1.64s/it]                                                          Episode 31	 reward: -6.52	 makespan: 645.80	 Mean_loss: 0.05611363,  training time: 1.64
progress:  30%|[34m       [0m| 30/100 [00:51<01:54,  1.64s/it]progress:  31%|[34m       [0m| 31/100 [00:51<01:52,  1.64s/it]                                                          Episode 32	 reward: -6.43	 makespan: 636.20	 Mean_loss: 0.08304901,  training time: 1.63
progress:  31%|[34m       [0m| 31/100 [00:53<01:52,  1.64s/it]progress:  32%|[34m      [0m| 32/100 [00:53<01:51,  1.64s/it]                                                          Episode 33	 reward: -7.24	 makespan: 717.20	 Mean_loss: 0.09085154,  training time: 1.63
progress:  32%|[34m      [0m| 32/100 [00:54<01:51,  1.64s/it]progress:  33%|[34m      [0m| 33/100 [00:54<01:49,  1.63s/it]                                                          Episode 34	 reward: -6.50	 makespan: 643.20	 Mean_loss: 0.06444494,  training time: 1.63
progress:  33%|[34m      [0m| 33/100 [00:56<01:49,  1.63s/it]progress:  34%|[34m      [0m| 34/100 [00:56<01:47,  1.63s/it]                                                          Episode 35	 reward: -6.71	 makespan: 663.80	 Mean_loss: 0.07534818,  training time: 1.63
progress:  34%|[34m      [0m| 34/100 [00:58<01:47,  1.63s/it]progress:  35%|[34m      [0m| 35/100 [00:58<01:46,  1.63s/it]                                                          Episode 36	 reward: -6.78	 makespan: 671.60	 Mean_loss: 0.06529434,  training time: 1.63
progress:  35%|[34m      [0m| 35/100 [00:59<01:46,  1.63s/it]progress:  36%|[34m      [0m| 36/100 [00:59<01:44,  1.63s/it]                                                          Episode 37	 reward: -6.69	 makespan: 662.60	 Mean_loss: 0.10177595,  training time: 1.63
progress:  36%|[34m      [0m| 36/100 [01:01<01:44,  1.63s/it]progress:  37%|[34m      [0m| 37/100 [01:01<01:42,  1.63s/it]                                                          Episode 38	 reward: -6.58	 makespan: 651.60	 Mean_loss: 0.07808203,  training time: 1.63
progress:  37%|[34m      [0m| 37/100 [01:03<01:42,  1.63s/it]progress:  38%|[34m      [0m| 38/100 [01:03<01:41,  1.63s/it]                                                          Episode 39	 reward: -6.86	 makespan: 679.00	 Mean_loss: 0.06762530,  training time: 1.63
progress:  38%|[34m      [0m| 38/100 [01:04<01:41,  1.63s/it]progress:  39%|[34m      [0m| 39/100 [01:04<01:39,  1.63s/it]                                                          Episode 40	 reward: -6.21	 makespan: 615.00	 Mean_loss: 0.05326806,  training time: 1.63
progress:  39%|[34m      [0m| 39/100 [01:06<01:39,  1.63s/it]progress:  40%|[34m      [0m| 40/100 [01:06<01:37,  1.63s/it]                                                          Episode 41	 reward: -6.69	 makespan: 662.40	 Mean_loss: 0.07568628,  training time: 1.64
progress:  40%|[34m      [0m| 40/100 [01:07<01:37,  1.63s/it]progress:  41%|[34m      [0m| 41/100 [01:07<01:36,  1.63s/it]                                                          Episode 42	 reward: -6.26	 makespan: 619.80	 Mean_loss: 0.05862786,  training time: 1.62
progress:  41%|[34m      [0m| 41/100 [01:09<01:36,  1.63s/it]progress:  42%|[34m     [0m| 42/100 [01:09<01:34,  1.63s/it]                                                          Episode 43	 reward: -6.42	 makespan: 635.20	 Mean_loss: 0.05998078,  training time: 1.63
progress:  42%|[34m     [0m| 42/100 [01:11<01:34,  1.63s/it]progress:  43%|[34m     [0m| 43/100 [01:11<01:32,  1.63s/it]                                                          Episode 44	 reward: -6.45	 makespan: 639.00	 Mean_loss: 0.04282553,  training time: 1.62
progress:  43%|[34m     [0m| 43/100 [01:12<01:32,  1.63s/it]progress:  44%|[34m     [0m| 44/100 [01:12<01:31,  1.63s/it]                                                          Episode 45	 reward: -6.74	 makespan: 667.00	 Mean_loss: 0.06027547,  training time: 1.62
progress:  44%|[34m     [0m| 44/100 [01:14<01:31,  1.63s/it]progress:  45%|[34m     [0m| 45/100 [01:14<01:29,  1.63s/it]                                                          Episode 46	 reward: -6.45	 makespan: 638.80	 Mean_loss: 0.09062418,  training time: 1.61
progress:  45%|[34m     [0m| 45/100 [01:16<01:29,  1.63s/it]progress:  46%|[34m     [0m| 46/100 [01:16<01:27,  1.62s/it]                                                          Episode 47	 reward: -6.20	 makespan: 613.80	 Mean_loss: 0.06673840,  training time: 1.61
progress:  46%|[34m     [0m| 46/100 [01:17<01:27,  1.62s/it]progress:  47%|[34m     [0m| 47/100 [01:17<01:25,  1.62s/it]                                                          Episode 48	 reward: -6.64	 makespan: 657.00	 Mean_loss: 0.05340438,  training time: 1.61
progress:  47%|[34m     [0m| 47/100 [01:19<01:25,  1.62s/it]progress:  48%|[34m     [0m| 48/100 [01:19<01:24,  1.62s/it]                                                          Episode 49	 reward: -6.14	 makespan: 607.60	 Mean_loss: 0.04315935,  training time: 1.61
progress:  48%|[34m     [0m| 48/100 [01:20<01:24,  1.62s/it]progress:  49%|[34m     [0m| 49/100 [01:20<01:22,  1.62s/it]                                                          Episode 50	 reward: -6.33	 makespan: 626.60	 Mean_loss: 0.05079338,  training time: 1.61
progress:  49%|[34m     [0m| 49/100 [01:22<01:22,  1.62s/it]progress:  50%|[34m     [0m| 50/100 [01:22<01:20,  1.62s/it]                                                          Episode 51	 reward: -6.48	 makespan: 641.60	 Mean_loss: 0.03765418,  training time: 1.61
progress:  50%|[34m     [0m| 50/100 [01:24<01:20,  1.62s/it]progress:  51%|[34m     [0m| 51/100 [01:24<01:19,  1.62s/it]                                                          Episode 52	 reward: -6.71	 makespan: 664.40	 Mean_loss: 0.03835648,  training time: 1.61
progress:  51%|[34m     [0m| 51/100 [01:25<01:19,  1.62s/it]progress:  52%|[34m    [0m| 52/100 [01:25<01:17,  1.61s/it]                                                          Episode 53	 reward: -6.42	 makespan: 635.20	 Mean_loss: 0.04738824,  training time: 1.61
progress:  52%|[34m    [0m| 52/100 [01:27<01:17,  1.61s/it]progress:  53%|[34m    [0m| 53/100 [01:27<01:15,  1.61s/it]                                                          Episode 54	 reward: -6.57	 makespan: 650.40	 Mean_loss: 0.04256857,  training time: 1.61
progress:  53%|[34m    [0m| 53/100 [01:28<01:15,  1.61s/it]progress:  54%|[34m    [0m| 54/100 [01:28<01:14,  1.61s/it]                                                          Episode 55	 reward: -6.09	 makespan: 603.00	 Mean_loss: 0.04486672,  training time: 1.61
progress:  54%|[34m    [0m| 54/100 [01:30<01:14,  1.61s/it]progress:  55%|[34m    [0m| 55/100 [01:30<01:12,  1.61s/it]                                                          Episode 56	 reward: -6.27	 makespan: 621.20	 Mean_loss: 0.03975914,  training time: 1.61
progress:  55%|[34m    [0m| 55/100 [01:32<01:12,  1.61s/it]progress:  56%|[34m    [0m| 56/100 [01:32<01:10,  1.61s/it]                                                          Episode 57	 reward: -6.00	 makespan: 593.60	 Mean_loss: 0.04231700,  training time: 1.61
progress:  56%|[34m    [0m| 56/100 [01:33<01:10,  1.61s/it]progress:  57%|[34m    [0m| 57/100 [01:33<01:09,  1.61s/it]                                                          Episode 58	 reward: -6.26	 makespan: 619.40	 Mean_loss: 0.06226926,  training time: 1.61
progress:  57%|[34m    [0m| 57/100 [01:35<01:09,  1.61s/it]progress:  58%|[34m    [0m| 58/100 [01:35<01:07,  1.61s/it]                                                          Episode 59	 reward: -6.11	 makespan: 605.00	 Mean_loss: 0.02535300,  training time: 1.61
progress:  58%|[34m    [0m| 58/100 [01:37<01:07,  1.61s/it]progress:  59%|[34m    [0m| 59/100 [01:37<01:06,  1.61s/it]                                                          Episode 60	 reward: -6.34	 makespan: 628.00	 Mean_loss: 0.02728460,  training time: 1.61
progress:  59%|[34m    [0m| 59/100 [01:38<01:06,  1.61s/it]progress:  60%|[34m    [0m| 60/100 [01:38<01:04,  1.61s/it]                                                          Episode 61	 reward: -6.32	 makespan: 625.40	 Mean_loss: 0.03622150,  training time: 1.62
progress:  60%|[34m    [0m| 60/100 [01:40<01:04,  1.61s/it]progress:  61%|[34m    [0m| 61/100 [01:40<01:03,  1.62s/it]                                                          Episode 62	 reward: -6.15	 makespan: 608.60	 Mean_loss: 0.03941952,  training time: 1.61
progress:  61%|[34m    [0m| 61/100 [01:41<01:03,  1.62s/it]progress:  62%|[34m   [0m| 62/100 [01:41<01:01,  1.61s/it]                                                          Episode 63	 reward: -6.05	 makespan: 598.80	 Mean_loss: 0.02764734,  training time: 1.61
progress:  62%|[34m   [0m| 62/100 [01:43<01:01,  1.61s/it]progress:  63%|[34m   [0m| 63/100 [01:43<00:59,  1.61s/it]                                                          Episode 64	 reward: -5.96	 makespan: 590.20	 Mean_loss: 0.02304739,  training time: 1.61
progress:  63%|[34m   [0m| 63/100 [01:45<00:59,  1.61s/it]progress:  64%|[34m   [0m| 64/100 [01:45<00:58,  1.61s/it]                                                          Episode 65	 reward: -6.13	 makespan: 606.60	 Mean_loss: 0.02394434,  training time: 1.61
progress:  64%|[34m   [0m| 64/100 [01:46<00:58,  1.61s/it]progress:  65%|[34m   [0m| 65/100 [01:46<00:56,  1.61s/it]                                                          Episode 66	 reward: -5.85	 makespan: 579.40	 Mean_loss: 0.02341618,  training time: 1.61
progress:  65%|[34m   [0m| 65/100 [01:48<00:56,  1.61s/it]progress:  66%|[34m   [0m| 66/100 [01:48<00:54,  1.61s/it]                                                          Episode 67	 reward: -6.27	 makespan: 620.40	 Mean_loss: 0.03155764,  training time: 1.61
progress:  66%|[34m   [0m| 66/100 [01:49<00:54,  1.61s/it]progress:  67%|[34m   [0m| 67/100 [01:49<00:53,  1.61s/it]                                                          Episode 68	 reward: -5.97	 makespan: 590.60	 Mean_loss: 0.01842782,  training time: 1.61
progress:  67%|[34m   [0m| 67/100 [01:51<00:53,  1.61s/it]progress:  68%|[34m   [0m| 68/100 [01:51<00:51,  1.61s/it]                                                          Episode 69	 reward: -6.14	 makespan: 607.80	 Mean_loss: 0.02950214,  training time: 1.61
progress:  68%|[34m   [0m| 68/100 [01:53<00:51,  1.61s/it]progress:  69%|[34m   [0m| 69/100 [01:53<00:50,  1.61s/it]                                                          Episode 70	 reward: -6.19	 makespan: 612.80	 Mean_loss: 0.01773567,  training time: 1.61
progress:  69%|[34m   [0m| 69/100 [01:54<00:50,  1.61s/it]progress:  70%|[34m   [0m| 70/100 [01:54<00:48,  1.61s/it]                                                          Episode 71	 reward: -6.01	 makespan: 594.60	 Mean_loss: 0.03726555,  training time: 1.61
progress:  70%|[34m   [0m| 70/100 [01:56<00:48,  1.61s/it]progress:  71%|[34m   [0m| 71/100 [01:56<00:46,  1.61s/it]                                                          Episode 72	 reward: -6.35	 makespan: 628.80	 Mean_loss: 0.02263662,  training time: 1.61
progress:  71%|[34m   [0m| 71/100 [01:58<00:46,  1.61s/it]progress:  72%|[34m  [0m| 72/100 [01:58<00:45,  1.61s/it]                                                          Episode 73	 reward: -5.78	 makespan: 572.00	 Mean_loss: 0.03473585,  training time: 1.61
progress:  72%|[34m  [0m| 72/100 [01:59<00:45,  1.61s/it]progress:  73%|[34m  [0m| 73/100 [01:59<00:43,  1.61s/it]                                                          Episode 74	 reward: -6.01	 makespan: 595.40	 Mean_loss: 0.02449160,  training time: 1.61
progress:  73%|[34m  [0m| 73/100 [02:01<00:43,  1.61s/it]progress:  74%|[34m  [0m| 74/100 [02:01<00:41,  1.61s/it]                                                          Episode 75	 reward: -6.37	 makespan: 630.60	 Mean_loss: 0.03127509,  training time: 1.61
progress:  74%|[34m  [0m| 74/100 [02:02<00:41,  1.61s/it]progress:  75%|[34m  [0m| 75/100 [02:02<00:40,  1.61s/it]                                                          Episode 76	 reward: -6.04	 makespan: 598.20	 Mean_loss: 0.02470238,  training time: 1.61
progress:  75%|[34m  [0m| 75/100 [02:04<00:40,  1.61s/it]progress:  76%|[34m  [0m| 76/100 [02:04<00:38,  1.61s/it]                                                          Episode 77	 reward: -6.06	 makespan: 600.00	 Mean_loss: 0.03554913,  training time: 1.61
progress:  76%|[34m  [0m| 76/100 [02:06<00:38,  1.61s/it]progress:  77%|[34m  [0m| 77/100 [02:06<00:37,  1.61s/it]                                                          Episode 78	 reward: -6.09	 makespan: 603.40	 Mean_loss: 0.01703322,  training time: 1.61
progress:  77%|[34m  [0m| 77/100 [02:07<00:37,  1.61s/it]progress:  78%|[34m  [0m| 78/100 [02:07<00:35,  1.61s/it]                                                          Episode 79	 reward: -6.08	 makespan: 602.20	 Mean_loss: 0.01767446,  training time: 1.61
progress:  78%|[34m  [0m| 78/100 [02:09<00:35,  1.61s/it]progress:  79%|[34m  [0m| 79/100 [02:09<00:33,  1.61s/it]                                                          Episode 80	 reward: -6.06	 makespan: 599.80	 Mean_loss: 0.05203836,  training time: 1.61
progress:  79%|[34m  [0m| 79/100 [02:10<00:33,  1.61s/it]progress:  80%|[34m  [0m| 80/100 [02:10<00:32,  1.61s/it]                                                          Episode 81	 reward: -5.84	 makespan: 578.40	 Mean_loss: 0.02192239,  training time: 1.61
progress:  80%|[34m  [0m| 80/100 [02:12<00:32,  1.61s/it]progress:  81%|[34m  [0m| 81/100 [02:12<00:30,  1.61s/it]                                                          Episode 82	 reward: -5.94	 makespan: 587.60	 Mean_loss: 0.02884809,  training time: 1.61
progress:  81%|[34m  [0m| 81/100 [02:14<00:30,  1.61s/it]progress:  82%|[34m [0m| 82/100 [02:14<00:28,  1.61s/it]                                                          Episode 83	 reward: -5.94	 makespan: 587.60	 Mean_loss: 0.02837417,  training time: 1.61
progress:  82%|[34m [0m| 82/100 [02:15<00:28,  1.61s/it]progress:  83%|[34m [0m| 83/100 [02:15<00:27,  1.61s/it]                                                          Episode 84	 reward: -5.88	 makespan: 582.40	 Mean_loss: 0.01978404,  training time: 1.61
progress:  83%|[34m [0m| 83/100 [02:17<00:27,  1.61s/it]progress:  84%|[34m [0m| 84/100 [02:17<00:25,  1.61s/it]                                                          Episode 85	 reward: -5.90	 makespan: 584.20	 Mean_loss: 0.02346272,  training time: 1.61
progress:  84%|[34m [0m| 84/100 [02:18<00:25,  1.61s/it]progress:  85%|[34m [0m| 85/100 [02:18<00:24,  1.61s/it]                                                          Episode 86	 reward: -6.10	 makespan: 604.00	 Mean_loss: 0.01481515,  training time: 1.61
progress:  85%|[34m [0m| 85/100 [02:20<00:24,  1.61s/it]progress:  86%|[34m [0m| 86/100 [02:20<00:22,  1.61s/it]                                                          Episode 87	 reward: -6.03	 makespan: 597.00	 Mean_loss: 0.01518147,  training time: 1.61
progress:  86%|[34m [0m| 86/100 [02:22<00:22,  1.61s/it]progress:  87%|[34m [0m| 87/100 [02:22<00:20,  1.61s/it]                                                          Episode 88	 reward: -5.93	 makespan: 587.20	 Mean_loss: 0.00600050,  training time: 1.61
progress:  87%|[34m [0m| 87/100 [02:23<00:20,  1.61s/it]progress:  88%|[34m [0m| 88/100 [02:23<00:19,  1.61s/it]                                                          Episode 89	 reward: -5.81	 makespan: 575.40	 Mean_loss: 0.01423294,  training time: 1.61
progress:  88%|[34m [0m| 88/100 [02:25<00:19,  1.61s/it]progress:  89%|[34m [0m| 89/100 [02:25<00:17,  1.61s/it]                                                          Episode 90	 reward: -5.76	 makespan: 570.60	 Mean_loss: 0.02985853,  training time: 1.61
progress:  89%|[34m [0m| 89/100 [02:27<00:17,  1.61s/it]progress:  90%|[34m [0m| 90/100 [02:27<00:16,  1.61s/it]                                                          Episode 91	 reward: -5.91	 makespan: 585.20	 Mean_loss: 0.01468433,  training time: 1.61
progress:  90%|[34m [0m| 90/100 [02:28<00:16,  1.61s/it]progress:  91%|[34m [0m| 91/100 [02:28<00:14,  1.61s/it]                                                          Episode 92	 reward: -6.02	 makespan: 596.40	 Mean_loss: 0.01605393,  training time: 1.61
progress:  91%|[34m [0m| 91/100 [02:30<00:14,  1.61s/it]progress:  92%|[34m[0m| 92/100 [02:30<00:12,  1.61s/it]                                                          Episode 93	 reward: -6.07	 makespan: 601.20	 Mean_loss: 0.02916550,  training time: 1.61
progress:  92%|[34m[0m| 92/100 [02:31<00:12,  1.61s/it]progress:  93%|[34m[0m| 93/100 [02:31<00:11,  1.61s/it]                                                          Episode 94	 reward: -6.07	 makespan: 601.00	 Mean_loss: 0.01868223,  training time: 1.61
progress:  93%|[34m[0m| 93/100 [02:33<00:11,  1.61s/it]progress:  94%|[34m[0m| 94/100 [02:33<00:09,  1.61s/it]                                                          Episode 95	 reward: -6.15	 makespan: 608.80	 Mean_loss: 0.01737720,  training time: 1.61
progress:  94%|[34m[0m| 94/100 [02:35<00:09,  1.61s/it]progress:  95%|[34m[0m| 95/100 [02:35<00:08,  1.61s/it]                                                          Episode 96	 reward: -5.88	 makespan: 582.20	 Mean_loss: 0.01186372,  training time: 1.61
progress:  95%|[34m[0m| 95/100 [02:36<00:08,  1.61s/it]progress:  96%|[34m[0m| 96/100 [02:36<00:06,  1.61s/it]                                                          Episode 97	 reward: -5.98	 makespan: 592.20	 Mean_loss: 0.01988778,  training time: 1.61
progress:  96%|[34m[0m| 96/100 [02:38<00:06,  1.61s/it]progress:  97%|[34m[0m| 97/100 [02:38<00:04,  1.61s/it]                                                          Episode 98	 reward: -5.95	 makespan: 589.40	 Mean_loss: 0.01603356,  training time: 1.61
progress:  97%|[34m[0m| 97/100 [02:39<00:04,  1.61s/it]progress:  98%|[34m[0m| 98/100 [02:39<00:03,  1.61s/it]                                                          Episode 99	 reward: -5.76	 makespan: 570.20	 Mean_loss: 0.02232750,  training time: 1.61
progress:  98%|[34m[0m| 98/100 [02:41<00:03,  1.61s/it]progress:  99%|[34m[0m| 99/100 [02:41<00:01,  1.61s/it]                                                          Episode 100	 reward: -5.88	 makespan: 582.60	 Mean_loss: 0.00892831,  training time: 1.61
progress:  99%|[34m[0m| 99/100 [02:43<00:01,  1.61s/it]progress: 100%|[34m[0m| 100/100 [02:43<00:00,  1.61s/it]progress: 100%|[34m[0m| 100/100 [02:43<00:00,  1.63s/it]
+ IFS=,
+ read n_j n_m
+ python train/DAN_finetuning.py --logdir ./runs/exp11/transfer_maml+exp11_1000_64_3_15x9 --model_suffix exp11_maml+exp11_1000_64_3_15x9 --finetuning_model maml+exp11_1000_64_3 --max_updates 100 --n_j 15 --n_m 9 --num_envs 5 --hidden_dim_actor 64 --hidden_dim_critic 64 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x9+mix
save model name:  15x9+mix+exp11_maml+exp11_1000_64_3_15x9
./trained_network/SD2/maml+exp11_1000_64_3.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/maml+exp11_1000_64_3.pth
vali data :./data/data_train_vali/SD2/15x9+mix


progress:   0%|[34m          [0m| 0/100 [00:00<?, ?it/s]                                                 Episode 1	 reward: -7.21	 makespan: 713.40	 Mean_loss: 0.23218283,  training time: 3.11
progress:   0%|[34m          [0m| 0/100 [00:03<?, ?it/s]progress:   1%|[34m          [0m| 1/100 [00:03<05:08,  3.12s/it]                                                         Episode 2	 reward: -7.53	 makespan: 745.20	 Mean_loss: 0.22636172,  training time: 2.07
progress:   1%|[34m          [0m| 1/100 [00:05<05:08,  3.12s/it]progress:   2%|[34m         [0m| 2/100 [00:05<04:05,  2.50s/it]                                                         Episode 3	 reward: -7.57	 makespan: 749.60	 Mean_loss: 0.32679018,  training time: 2.07
progress:   2%|[34m         [0m| 2/100 [00:07<04:05,  2.50s/it]progress:   3%|[34m         [0m| 3/100 [00:07<03:43,  2.30s/it]                                                         Episode 4	 reward: -7.63	 makespan: 755.20	 Mean_loss: 0.35315219,  training time: 2.06
progress:   3%|[34m         [0m| 3/100 [00:09<03:43,  2.30s/it]progress:   4%|[34m         [0m| 4/100 [00:09<03:32,  2.21s/it]                                                         Episode 5	 reward: -7.76	 makespan: 768.40	 Mean_loss: 0.49244699,  training time: 2.06
progress:   4%|[34m         [0m| 4/100 [00:11<03:32,  2.21s/it]progress:   5%|[34m         [0m| 5/100 [00:11<03:24,  2.16s/it]                                                         Episode 6	 reward: -7.45	 makespan: 738.00	 Mean_loss: 0.37858182,  training time: 2.18
progress:   5%|[34m         [0m| 5/100 [00:13<03:24,  2.16s/it]progress:   6%|[34m         [0m| 6/100 [00:13<03:23,  2.17s/it]                                                         Episode 7	 reward: -7.32	 makespan: 724.80	 Mean_loss: 0.21874925,  training time: 2.07
progress:   6%|[34m         [0m| 6/100 [00:15<03:23,  2.17s/it]progress:   7%|[34m         [0m| 7/100 [00:15<03:18,  2.14s/it]                                                         Episode 8	 reward: -7.11	 makespan: 703.80	 Mean_loss: 0.17079672,  training time: 2.07
progress:   7%|[34m         [0m| 7/100 [00:17<03:18,  2.14s/it]progress:   8%|[34m         [0m| 8/100 [00:17<03:14,  2.12s/it]                                                         Episode 9	 reward: -6.48	 makespan: 641.20	 Mean_loss: 0.13094611,  training time: 2.06
progress:   8%|[34m         [0m| 8/100 [00:19<03:14,  2.12s/it]progress:   9%|[34m         [0m| 9/100 [00:19<03:10,  2.10s/it]                                                         Episode 10	 reward: -6.73	 makespan: 665.80	 Mean_loss: 0.07804418,  training time: 2.06
progress:   9%|[34m         [0m| 9/100 [00:21<03:10,  2.10s/it]progress:  10%|[34m         [0m| 10/100 [00:21<03:07,  2.09s/it]                                                          Episode 11	 reward: -6.60	 makespan: 653.60	 Mean_loss: 0.12503363,  training time: 2.06
progress:  10%|[34m         [0m| 10/100 [00:23<03:07,  2.09s/it]progress:  11%|[34m         [0m| 11/100 [00:23<03:04,  2.08s/it]                                                          Episode 12	 reward: -5.98	 makespan: 592.40	 Mean_loss: 0.09173337,  training time: 2.06
progress:  11%|[34m         [0m| 11/100 [00:25<03:04,  2.08s/it]progress:  12%|[34m        [0m| 12/100 [00:25<03:02,  2.07s/it]                                                          Episode 13	 reward: -6.41	 makespan: 634.40	 Mean_loss: 0.05615997,  training time: 2.06
progress:  12%|[34m        [0m| 12/100 [00:28<03:02,  2.07s/it]progress:  13%|[34m        [0m| 13/100 [00:28<02:59,  2.07s/it]                                                          Episode 14	 reward: -6.51	 makespan: 644.60	 Mean_loss: 0.04730256,  training time: 2.06
progress:  13%|[34m        [0m| 13/100 [00:30<02:59,  2.07s/it]progress:  14%|[34m        [0m| 14/100 [00:30<02:57,  2.06s/it]                                                          Episode 15	 reward: -6.24	 makespan: 617.80	 Mean_loss: 0.06986915,  training time: 2.06
progress:  14%|[34m        [0m| 14/100 [00:32<02:57,  2.06s/it]progress:  15%|[34m        [0m| 15/100 [00:32<02:55,  2.06s/it]                                                          Episode 16	 reward: -6.31	 makespan: 624.60	 Mean_loss: 0.07163921,  training time: 2.06
progress:  15%|[34m        [0m| 15/100 [00:34<02:55,  2.06s/it]progress:  16%|[34m        [0m| 16/100 [00:34<02:53,  2.06s/it]                                                          Episode 17	 reward: -6.42	 makespan: 635.20	 Mean_loss: 0.06255058,  training time: 2.08
progress:  16%|[34m        [0m| 16/100 [00:36<02:53,  2.06s/it]progress:  17%|[34m        [0m| 17/100 [00:36<02:51,  2.07s/it]                                                          Episode 18	 reward: -6.50	 makespan: 643.80	 Mean_loss: 0.04400992,  training time: 2.07
progress:  17%|[34m        [0m| 17/100 [00:38<02:51,  2.07s/it]progress:  18%|[34m        [0m| 18/100 [00:38<02:49,  2.07s/it]                                                          Episode 19	 reward: -6.42	 makespan: 635.60	 Mean_loss: 0.06314083,  training time: 2.06
progress:  18%|[34m        [0m| 18/100 [00:40<02:49,  2.07s/it]progress:  19%|[34m        [0m| 19/100 [00:40<02:47,  2.07s/it]                                                          Episode 20	 reward: -5.91	 makespan: 584.60	 Mean_loss: 0.06448001,  training time: 2.05
progress:  19%|[34m        [0m| 19/100 [00:42<02:47,  2.07s/it]progress:  20%|[34m        [0m| 20/100 [00:42<02:45,  2.06s/it]                                                          Episode 21	 reward: -6.12	 makespan: 606.20	 Mean_loss: 0.03831135,  training time: 2.05
progress:  20%|[34m        [0m| 20/100 [00:44<02:45,  2.06s/it]progress:  21%|[34m        [0m| 21/100 [00:44<02:42,  2.06s/it]                                                          Episode 22	 reward: -6.17	 makespan: 610.80	 Mean_loss: 0.02622661,  training time: 2.05
progress:  21%|[34m        [0m| 21/100 [00:46<02:42,  2.06s/it]progress:  22%|[34m       [0m| 22/100 [00:46<02:40,  2.06s/it]                                                          Episode 23	 reward: -6.24	 makespan: 618.00	 Mean_loss: 0.02745461,  training time: 2.05
progress:  22%|[34m       [0m| 22/100 [00:48<02:40,  2.06s/it]progress:  23%|[34m       [0m| 23/100 [00:48<02:38,  2.06s/it]                                                          Episode 24	 reward: -6.38	 makespan: 632.00	 Mean_loss: 0.03398358,  training time: 2.05
progress:  23%|[34m       [0m| 23/100 [00:50<02:38,  2.06s/it]progress:  24%|[34m       [0m| 24/100 [00:50<02:36,  2.05s/it]                                                          Episode 25	 reward: -6.13	 makespan: 606.80	 Mean_loss: 0.03932818,  training time: 2.05
progress:  24%|[34m       [0m| 24/100 [00:52<02:36,  2.05s/it]progress:  25%|[34m       [0m| 25/100 [00:52<02:34,  2.05s/it]                                                          Episode 26	 reward: -5.91	 makespan: 584.80	 Mean_loss: 0.03147576,  training time: 2.06
progress:  25%|[34m       [0m| 25/100 [00:54<02:34,  2.05s/it]progress:  26%|[34m       [0m| 26/100 [00:54<02:32,  2.06s/it]                                                          Episode 27	 reward: -6.15	 makespan: 608.40	 Mean_loss: 0.02269753,  training time: 2.08
progress:  26%|[34m       [0m| 26/100 [00:56<02:32,  2.06s/it]progress:  27%|[34m       [0m| 27/100 [00:56<02:30,  2.06s/it]                                                          Episode 28	 reward: -6.05	 makespan: 598.60	 Mean_loss: 0.01893442,  training time: 2.08
progress:  27%|[34m       [0m| 27/100 [00:58<02:30,  2.06s/it]progress:  28%|[34m       [0m| 28/100 [00:58<02:29,  2.07s/it]                                                          Episode 29	 reward: -6.26	 makespan: 619.60	 Mean_loss: 0.02758196,  training time: 2.09
progress:  28%|[34m       [0m| 28/100 [01:01<02:29,  2.07s/it]progress:  29%|[34m       [0m| 29/100 [01:01<02:27,  2.08s/it]                                                          Episode 30	 reward: -6.29	 makespan: 623.00	 Mean_loss: 0.03285938,  training time: 2.08
progress:  29%|[34m       [0m| 29/100 [01:03<02:27,  2.08s/it]progress:  30%|[34m       [0m| 30/100 [01:03<02:25,  2.08s/it]                                                          Episode 31	 reward: -5.87	 makespan: 580.80	 Mean_loss: 0.02949186,  training time: 2.05
progress:  30%|[34m       [0m| 30/100 [01:05<02:25,  2.08s/it]progress:  31%|[34m       [0m| 31/100 [01:05<02:22,  2.07s/it]                                                          Episode 32	 reward: -6.04	 makespan: 598.40	 Mean_loss: 0.03166252,  training time: 2.04
progress:  31%|[34m       [0m| 31/100 [01:07<02:22,  2.07s/it]progress:  32%|[34m      [0m| 32/100 [01:07<02:20,  2.06s/it]                                                          Episode 33	 reward: -6.04	 makespan: 598.40	 Mean_loss: 0.03142442,  training time: 2.04
progress:  32%|[34m      [0m| 32/100 [01:09<02:20,  2.06s/it]progress:  33%|[34m      [0m| 33/100 [01:09<02:17,  2.05s/it]                                                          Episode 34	 reward: -6.31	 makespan: 625.00	 Mean_loss: 0.02409536,  training time: 2.04
progress:  33%|[34m      [0m| 33/100 [01:11<02:17,  2.05s/it]progress:  34%|[34m      [0m| 34/100 [01:11<02:15,  2.05s/it]                                                          Episode 35	 reward: -6.26	 makespan: 620.00	 Mean_loss: 0.03132509,  training time: 2.04
progress:  34%|[34m      [0m| 34/100 [01:13<02:15,  2.05s/it]progress:  35%|[34m      [0m| 35/100 [01:13<02:13,  2.05s/it]                                                          Episode 36	 reward: -6.13	 makespan: 607.00	 Mean_loss: 0.03294846,  training time: 2.04
progress:  35%|[34m      [0m| 35/100 [01:15<02:13,  2.05s/it]progress:  36%|[34m      [0m| 36/100 [01:15<02:10,  2.04s/it]                                                          Episode 37	 reward: -5.93	 makespan: 587.00	 Mean_loss: 0.02992120,  training time: 2.04
progress:  36%|[34m      [0m| 36/100 [01:17<02:10,  2.04s/it]progress:  37%|[34m      [0m| 37/100 [01:17<02:08,  2.04s/it]                                                          Episode 38	 reward: -6.22	 makespan: 616.20	 Mean_loss: 0.03674170,  training time: 2.04
progress:  37%|[34m      [0m| 37/100 [01:19<02:08,  2.04s/it]progress:  38%|[34m      [0m| 38/100 [01:19<02:06,  2.04s/it]                                                          Episode 39	 reward: -6.05	 makespan: 598.80	 Mean_loss: 0.03603515,  training time: 2.06
progress:  38%|[34m      [0m| 38/100 [01:21<02:06,  2.04s/it]progress:  39%|[34m      [0m| 39/100 [01:21<02:04,  2.05s/it]                                                          Episode 40	 reward: -5.93	 makespan: 587.00	 Mean_loss: 0.02445148,  training time: 2.06
progress:  39%|[34m      [0m| 39/100 [01:23<02:04,  2.05s/it]progress:  40%|[34m      [0m| 40/100 [01:23<02:03,  2.05s/it]                                                          Episode 41	 reward: -5.89	 makespan: 583.40	 Mean_loss: 0.03263899,  training time: 2.08
progress:  40%|[34m      [0m| 40/100 [01:25<02:03,  2.05s/it]progress:  41%|[34m      [0m| 41/100 [01:25<02:01,  2.06s/it]                                                          Episode 42	 reward: -6.15	 makespan: 609.00	 Mean_loss: 0.02376362,  training time: 2.06
progress:  41%|[34m      [0m| 41/100 [01:27<02:01,  2.06s/it]progress:  42%|[34m     [0m| 42/100 [01:27<01:59,  2.06s/it]                                                          Episode 43	 reward: -6.23	 makespan: 617.00	 Mean_loss: 0.03698102,  training time: 2.05
progress:  42%|[34m     [0m| 42/100 [01:29<01:59,  2.06s/it]progress:  43%|[34m     [0m| 43/100 [01:29<01:57,  2.06s/it]                                                          Episode 44	 reward: -6.03	 makespan: 596.60	 Mean_loss: 0.02301320,  training time: 2.05
progress:  43%|[34m     [0m| 43/100 [01:31<01:57,  2.06s/it]progress:  44%|[34m     [0m| 44/100 [01:31<01:55,  2.06s/it]                                                          Episode 45	 reward: -5.63	 makespan: 557.20	 Mean_loss: 0.03166497,  training time: 2.05
progress:  44%|[34m     [0m| 44/100 [01:33<01:55,  2.06s/it]progress:  45%|[34m     [0m| 45/100 [01:33<01:53,  2.05s/it]                                                          Episode 46	 reward: -5.86	 makespan: 580.20	 Mean_loss: 0.02756583,  training time: 2.04
progress:  45%|[34m     [0m| 45/100 [01:35<01:53,  2.05s/it]progress:  46%|[34m     [0m| 46/100 [01:35<01:50,  2.05s/it]                                                          Episode 47	 reward: -6.05	 makespan: 598.60	 Mean_loss: 0.01413031,  training time: 2.04
progress:  46%|[34m     [0m| 46/100 [01:37<01:50,  2.05s/it]progress:  47%|[34m     [0m| 47/100 [01:37<01:48,  2.05s/it]                                                          Episode 48	 reward: -5.79	 makespan: 573.20	 Mean_loss: 0.03252941,  training time: 2.04
progress:  47%|[34m     [0m| 47/100 [01:39<01:48,  2.05s/it]progress:  48%|[34m     [0m| 48/100 [01:39<01:46,  2.05s/it]                                                          Episode 49	 reward: -6.07	 makespan: 601.20	 Mean_loss: 0.01943741,  training time: 2.04
progress:  48%|[34m     [0m| 48/100 [01:42<01:46,  2.05s/it]progress:  49%|[34m     [0m| 49/100 [01:42<01:44,  2.05s/it]                                                          Episode 50	 reward: -5.96	 makespan: 589.80	 Mean_loss: 0.02301141,  training time: 2.04
progress:  49%|[34m     [0m| 49/100 [01:44<01:44,  2.05s/it]progress:  50%|[34m     [0m| 50/100 [01:44<01:42,  2.05s/it]                                                          Episode 51	 reward: -6.05	 makespan: 599.40	 Mean_loss: 0.03514969,  training time: 2.05
progress:  50%|[34m     [0m| 50/100 [01:46<01:42,  2.05s/it]progress:  51%|[34m     [0m| 51/100 [01:46<01:40,  2.05s/it]                                                          Episode 52	 reward: -5.91	 makespan: 584.60	 Mean_loss: 0.02599158,  training time: 2.04
progress:  51%|[34m     [0m| 51/100 [01:48<01:40,  2.05s/it]progress:  52%|[34m    [0m| 52/100 [01:48<01:38,  2.05s/it]                                                          Episode 53	 reward: -5.89	 makespan: 583.60	 Mean_loss: 0.02080560,  training time: 2.04
progress:  52%|[34m    [0m| 52/100 [01:50<01:38,  2.05s/it]progress:  53%|[34m    [0m| 53/100 [01:50<01:36,  2.05s/it]                                                          Episode 54	 reward: -6.04	 makespan: 598.40	 Mean_loss: 0.02330580,  training time: 2.04
progress:  53%|[34m    [0m| 53/100 [01:52<01:36,  2.05s/it]progress:  54%|[34m    [0m| 54/100 [01:52<01:34,  2.05s/it]                                                          Episode 55	 reward: -5.74	 makespan: 568.60	 Mean_loss: 0.01545024,  training time: 2.04
progress:  54%|[34m    [0m| 54/100 [01:54<01:34,  2.05s/it]progress:  55%|[34m    [0m| 55/100 [01:54<01:32,  2.04s/it]                                                          Episode 56	 reward: -5.73	 makespan: 567.20	 Mean_loss: 0.02089589,  training time: 2.04
progress:  55%|[34m    [0m| 55/100 [01:56<01:32,  2.04s/it]progress:  56%|[34m    [0m| 56/100 [01:56<01:29,  2.04s/it]                                                          Episode 57	 reward: -5.70	 makespan: 564.00	 Mean_loss: 0.02873326,  training time: 2.05
progress:  56%|[34m    [0m| 56/100 [01:58<01:29,  2.04s/it]progress:  57%|[34m    [0m| 57/100 [01:58<01:27,  2.05s/it]                                                          Episode 58	 reward: -6.05	 makespan: 599.40	 Mean_loss: 0.03101780,  training time: 2.05
progress:  57%|[34m    [0m| 57/100 [02:00<01:27,  2.05s/it]progress:  58%|[34m    [0m| 58/100 [02:00<01:25,  2.05s/it]                                                          Episode 59	 reward: -5.83	 makespan: 577.40	 Mean_loss: 0.02404453,  training time: 2.04
progress:  58%|[34m    [0m| 58/100 [02:02<01:25,  2.05s/it]progress:  59%|[34m    [0m| 59/100 [02:02<01:23,  2.04s/it]                                                          Episode 60	 reward: -5.96	 makespan: 589.60	 Mean_loss: 0.02705688,  training time: 2.04
progress:  59%|[34m    [0m| 59/100 [02:04<01:23,  2.04s/it]progress:  60%|[34m    [0m| 60/100 [02:04<01:21,  2.05s/it]                                                          Episode 61	 reward: -6.14	 makespan: 607.60	 Mean_loss: 0.01801669,  training time: 2.04
progress:  60%|[34m    [0m| 60/100 [02:06<01:21,  2.05s/it]progress:  61%|[34m    [0m| 61/100 [02:06<01:19,  2.04s/it]                                                          Episode 62	 reward: -5.79	 makespan: 573.40	 Mean_loss: 0.01642150,  training time: 2.04
progress:  61%|[34m    [0m| 61/100 [02:08<01:19,  2.04s/it]progress:  62%|[34m   [0m| 62/100 [02:08<01:17,  2.04s/it]                                                          Episode 63	 reward: -5.77	 makespan: 570.80	 Mean_loss: 0.01584029,  training time: 2.04
progress:  62%|[34m   [0m| 62/100 [02:10<01:17,  2.04s/it]progress:  63%|[34m   [0m| 63/100 [02:10<01:15,  2.04s/it]                                                          Episode 64	 reward: -5.50	 makespan: 544.20	 Mean_loss: 0.02348371,  training time: 2.04
progress:  63%|[34m   [0m| 63/100 [02:12<01:15,  2.04s/it]progress:  64%|[34m   [0m| 64/100 [02:12<01:13,  2.04s/it]                                                          Episode 65	 reward: -5.89	 makespan: 583.40	 Mean_loss: 0.01431512,  training time: 2.04
progress:  64%|[34m   [0m| 64/100 [02:14<01:13,  2.04s/it]progress:  65%|[34m   [0m| 65/100 [02:14<01:11,  2.04s/it]                                                          Episode 66	 reward: -6.04	 makespan: 597.80	 Mean_loss: 0.02272755,  training time: 2.04
progress:  65%|[34m   [0m| 65/100 [02:16<01:11,  2.04s/it]progress:  66%|[34m   [0m| 66/100 [02:16<01:09,  2.04s/it]                                                          Episode 67	 reward: -5.81	 makespan: 575.40	 Mean_loss: 0.01057844,  training time: 2.04
progress:  66%|[34m   [0m| 66/100 [02:18<01:09,  2.04s/it]progress:  67%|[34m   [0m| 67/100 [02:18<01:07,  2.04s/it]                                                          Episode 68	 reward: -5.87	 makespan: 581.40	 Mean_loss: 0.01755285,  training time: 2.04
progress:  67%|[34m   [0m| 67/100 [02:20<01:07,  2.04s/it]progress:  68%|[34m   [0m| 68/100 [02:20<01:05,  2.04s/it]                                                          Episode 69	 reward: -6.08	 makespan: 601.80	 Mean_loss: 0.02484090,  training time: 2.04
progress:  68%|[34m   [0m| 68/100 [02:22<01:05,  2.04s/it]progress:  69%|[34m   [0m| 69/100 [02:22<01:03,  2.04s/it]                                                          Episode 70	 reward: -5.91	 makespan: 585.00	 Mean_loss: 0.02989983,  training time: 2.04
progress:  69%|[34m   [0m| 69/100 [02:24<01:03,  2.04s/it]progress:  70%|[34m   [0m| 70/100 [02:24<01:01,  2.04s/it]                                                          Episode 71	 reward: -6.20	 makespan: 613.80	 Mean_loss: 0.02869350,  training time: 2.04
progress:  70%|[34m   [0m| 70/100 [02:27<01:01,  2.04s/it]progress:  71%|[34m   [0m| 71/100 [02:27<00:59,  2.04s/it]                                                          Episode 72	 reward: -6.20	 makespan: 614.20	 Mean_loss: 0.02314292,  training time: 2.04
progress:  71%|[34m   [0m| 71/100 [02:29<00:59,  2.04s/it]progress:  72%|[34m  [0m| 72/100 [02:29<00:57,  2.04s/it]                                                          Episode 73	 reward: -5.76	 makespan: 570.20	 Mean_loss: 0.02384269,  training time: 2.04
progress:  72%|[34m  [0m| 72/100 [02:31<00:57,  2.04s/it]progress:  73%|[34m  [0m| 73/100 [02:31<00:55,  2.04s/it]                                                          Episode 74	 reward: -5.88	 makespan: 582.20	 Mean_loss: 0.02832160,  training time: 2.04
progress:  73%|[34m  [0m| 73/100 [02:33<00:55,  2.04s/it]progress:  74%|[34m  [0m| 74/100 [02:33<00:53,  2.04s/it]                                                          Episode 75	 reward: -6.07	 makespan: 600.80	 Mean_loss: 0.01339265,  training time: 2.04
progress:  74%|[34m  [0m| 74/100 [02:35<00:53,  2.04s/it]progress:  75%|[34m  [0m| 75/100 [02:35<00:51,  2.04s/it]                                                          Episode 76	 reward: -6.09	 makespan: 603.00	 Mean_loss: 0.02858948,  training time: 2.04
progress:  75%|[34m  [0m| 75/100 [02:37<00:51,  2.04s/it]progress:  76%|[34m  [0m| 76/100 [02:37<00:49,  2.04s/it]                                                          Episode 77	 reward: -5.72	 makespan: 566.60	 Mean_loss: 0.00965124,  training time: 2.04
progress:  76%|[34m  [0m| 76/100 [02:39<00:49,  2.04s/it]progress:  77%|[34m  [0m| 77/100 [02:39<00:46,  2.04s/it]                                                          Episode 78	 reward: -5.84	 makespan: 578.40	 Mean_loss: 0.02039294,  training time: 2.04
progress:  77%|[34m  [0m| 77/100 [02:41<00:46,  2.04s/it]progress:  78%|[34m  [0m| 78/100 [02:41<00:44,  2.04s/it]                                                          Episode 79	 reward: -6.12	 makespan: 606.20	 Mean_loss: 0.01447045,  training time: 2.04
progress:  78%|[34m  [0m| 78/100 [02:43<00:44,  2.04s/it]progress:  79%|[34m  [0m| 79/100 [02:43<00:42,  2.04s/it]                                                          Episode 80	 reward: -5.88	 makespan: 582.00	 Mean_loss: 0.03739746,  training time: 2.04
progress:  79%|[34m  [0m| 79/100 [02:45<00:42,  2.04s/it]progress:  80%|[34m  [0m| 80/100 [02:45<00:40,  2.04s/it]                                                          Episode 81	 reward: -5.84	 makespan: 578.00	 Mean_loss: 0.01583967,  training time: 2.04
progress:  80%|[34m  [0m| 80/100 [02:47<00:40,  2.04s/it]progress:  81%|[34m  [0m| 81/100 [02:47<00:38,  2.04s/it]                                                          Episode 82	 reward: -5.86	 makespan: 580.20	 Mean_loss: 0.00998667,  training time: 2.04
progress:  81%|[34m  [0m| 81/100 [02:49<00:38,  2.04s/it]progress:  82%|[34m [0m| 82/100 [02:49<00:36,  2.04s/it]                                                          Episode 83	 reward: -5.99	 makespan: 593.00	 Mean_loss: 0.01608974,  training time: 2.07
progress:  82%|[34m [0m| 82/100 [02:51<00:36,  2.04s/it]progress:  83%|[34m [0m| 83/100 [02:51<00:34,  2.05s/it]                                                          Episode 84	 reward: -5.81	 makespan: 575.40	 Mean_loss: 0.01090932,  training time: 2.08
progress:  83%|[34m [0m| 83/100 [02:53<00:34,  2.05s/it]progress:  84%|[34m [0m| 84/100 [02:53<00:32,  2.06s/it]                                                          Episode 85	 reward: -5.99	 makespan: 592.80	 Mean_loss: 0.02629125,  training time: 2.08
progress:  84%|[34m [0m| 84/100 [02:55<00:32,  2.06s/it]progress:  85%|[34m [0m| 85/100 [02:55<00:30,  2.07s/it]                                                          Episode 86	 reward: -5.70	 makespan: 564.40	 Mean_loss: 0.01772177,  training time: 2.08
progress:  85%|[34m [0m| 85/100 [02:57<00:30,  2.07s/it]progress:  86%|[34m [0m| 86/100 [02:57<00:28,  2.07s/it]                                                          Episode 87	 reward: -6.19	 makespan: 612.80	 Mean_loss: 0.01500438,  training time: 2.05
progress:  86%|[34m [0m| 86/100 [02:59<00:28,  2.07s/it]progress:  87%|[34m [0m| 87/100 [02:59<00:26,  2.06s/it]                                                          Episode 88	 reward: -6.26	 makespan: 620.00	 Mean_loss: 0.02421570,  training time: 2.04
progress:  87%|[34m [0m| 87/100 [03:01<00:26,  2.06s/it]progress:  88%|[34m [0m| 88/100 [03:01<00:24,  2.06s/it]                                                          Episode 89	 reward: -5.97	 makespan: 591.20	 Mean_loss: 0.01674435,  training time: 2.05
progress:  88%|[34m [0m| 88/100 [03:03<00:24,  2.06s/it]progress:  89%|[34m [0m| 89/100 [03:03<00:22,  2.05s/it]                                                          Episode 90	 reward: -6.06	 makespan: 599.60	 Mean_loss: 0.01720138,  training time: 2.04
progress:  89%|[34m [0m| 89/100 [03:05<00:22,  2.05s/it]progress:  90%|[34m [0m| 90/100 [03:05<00:20,  2.05s/it]                                                          Episode 91	 reward: -5.96	 makespan: 589.80	 Mean_loss: 0.01592561,  training time: 2.04
progress:  90%|[34m [0m| 90/100 [03:08<00:20,  2.05s/it]progress:  91%|[34m [0m| 91/100 [03:08<00:18,  2.05s/it]                                                          Episode 92	 reward: -5.86	 makespan: 580.40	 Mean_loss: 0.01511260,  training time: 2.04
progress:  91%|[34m [0m| 91/100 [03:10<00:18,  2.05s/it]progress:  92%|[34m[0m| 92/100 [03:10<00:16,  2.05s/it]                                                          Episode 93	 reward: -5.86	 makespan: 579.80	 Mean_loss: 0.01456121,  training time: 2.04
progress:  92%|[34m[0m| 92/100 [03:12<00:16,  2.05s/it]progress:  93%|[34m[0m| 93/100 [03:12<00:14,  2.05s/it]                                                          Episode 94	 reward: -5.82	 makespan: 576.60	 Mean_loss: 0.01764073,  training time: 2.04
progress:  93%|[34m[0m| 93/100 [03:14<00:14,  2.05s/it]progress:  94%|[34m[0m| 94/100 [03:14<00:12,  2.05s/it]                                                          Episode 95	 reward: -6.16	 makespan: 610.20	 Mean_loss: 0.02524276,  training time: 2.04
progress:  94%|[34m[0m| 94/100 [03:16<00:12,  2.05s/it]progress:  95%|[34m[0m| 95/100 [03:16<00:10,  2.05s/it]                                                          Episode 96	 reward: -5.80	 makespan: 574.00	 Mean_loss: 0.02203689,  training time: 2.04
progress:  95%|[34m[0m| 95/100 [03:18<00:10,  2.05s/it]progress:  96%|[34m[0m| 96/100 [03:18<00:08,  2.05s/it]                                                          Episode 97	 reward: -5.66	 makespan: 560.60	 Mean_loss: 0.02248595,  training time: 2.04
progress:  96%|[34m[0m| 96/100 [03:20<00:08,  2.05s/it]progress:  97%|[34m[0m| 97/100 [03:20<00:06,  2.04s/it]                                                          Episode 98	 reward: -5.92	 makespan: 585.80	 Mean_loss: 0.01093327,  training time: 2.04
progress:  97%|[34m[0m| 97/100 [03:22<00:06,  2.04s/it]progress:  98%|[34m[0m| 98/100 [03:22<00:04,  2.04s/it]                                                          Episode 99	 reward: -5.61	 makespan: 555.60	 Mean_loss: 0.02311029,  training time: 2.04
progress:  98%|[34m[0m| 98/100 [03:24<00:04,  2.04s/it]progress:  99%|[34m[0m| 99/100 [03:24<00:02,  2.04s/it]                                                          Episode 100	 reward: -5.85	 makespan: 579.60	 Mean_loss: 0.01504077,  training time: 2.05
progress:  99%|[34m[0m| 99/100 [03:26<00:02,  2.04s/it]progress: 100%|[34m[0m| 100/100 [03:26<00:00,  2.05s/it]progress: 100%|[34m[0m| 100/100 [03:26<00:00,  2.06s/it]
+ IFS=,
+ read n_j n_m
+ python train/DAN_finetuning.py --logdir ./runs/exp11/transfer_maml+exp11_1000_64_3_15x10 --model_suffix exp11_maml+exp11_1000_64_3_15x10 --finetuning_model maml+exp11_1000_64_3 --max_updates 100 --n_j 15 --n_m 10 --num_envs 5 --hidden_dim_actor 64 --hidden_dim_critic 64 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x10+mix
save model name:  15x10+mix+exp11_maml+exp11_1000_64_3_15x10
./trained_network/SD2/maml+exp11_1000_64_3.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/maml+exp11_1000_64_3.pth
vali data :./data/data_train_vali/SD2/15x10+mix


progress:   0%|[34m          [0m| 0/100 [00:00<?, ?it/s]                                                 Episode 1	 reward: -7.07	 makespan: 700.40	 Mean_loss: 0.19702907,  training time: 3.37
progress:   0%|[34m          [0m| 0/100 [00:03<?, ?it/s]progress:   1%|[34m          [0m| 1/100 [00:03<05:33,  3.37s/it]                                                         Episode 2	 reward: -7.24	 makespan: 717.20	 Mean_loss: 0.14772217,  training time: 2.29
progress:   1%|[34m          [0m| 1/100 [00:05<05:33,  3.37s/it]progress:   2%|[34m         [0m| 2/100 [00:05<04:28,  2.74s/it]                                                         Episode 3	 reward: -7.72	 makespan: 763.80	 Mean_loss: 0.26995870,  training time: 2.30
progress:   2%|[34m         [0m| 2/100 [00:07<04:28,  2.74s/it]progress:   3%|[34m         [0m| 3/100 [00:07<04:06,  2.54s/it]                                                         Episode 4	 reward: -7.28	 makespan: 720.40	 Mean_loss: 0.21979141,  training time: 2.29
progress:   3%|[34m         [0m| 3/100 [00:10<04:06,  2.54s/it]progress:   4%|[34m         [0m| 4/100 [00:10<03:54,  2.44s/it]                                                         Episode 5	 reward: -7.50	 makespan: 742.40	 Mean_loss: 0.40765250,  training time: 2.29
progress:   4%|[34m         [0m| 4/100 [00:12<03:54,  2.44s/it]progress:   5%|[34m         [0m| 5/100 [00:12<03:46,  2.39s/it]                                                         Episode 6	 reward: -7.27	 makespan: 720.00	 Mean_loss: 0.24736997,  training time: 2.37
progress:   5%|[34m         [0m| 5/100 [00:14<03:46,  2.39s/it]progress:   6%|[34m         [0m| 6/100 [00:14<03:43,  2.38s/it]                                                         Episode 7	 reward: -7.07	 makespan: 699.60	 Mean_loss: 0.17809729,  training time: 2.30
progress:   6%|[34m         [0m| 6/100 [00:17<03:43,  2.38s/it]progress:   7%|[34m         [0m| 7/100 [00:17<03:38,  2.35s/it]                                                         Episode 8	 reward: -7.57	 makespan: 749.20	 Mean_loss: 0.15743348,  training time: 2.29
progress:   7%|[34m         [0m| 7/100 [00:19<03:38,  2.35s/it]progress:   8%|[34m         [0m| 8/100 [00:19<03:34,  2.34s/it]                                                         Episode 9	 reward: -7.53	 makespan: 745.20	 Mean_loss: 0.11315379,  training time: 2.29
progress:   8%|[34m         [0m| 8/100 [00:21<03:34,  2.34s/it]progress:   9%|[34m         [0m| 9/100 [00:21<03:31,  2.32s/it]                                                         Episode 10	 reward: -7.85	 makespan: 776.80	 Mean_loss: 0.19979554,  training time: 2.29
progress:   9%|[34m         [0m| 9/100 [00:24<03:31,  2.32s/it]progress:  10%|[34m         [0m| 10/100 [00:24<03:28,  2.31s/it]                                                          Episode 11	 reward: -7.45	 makespan: 737.20	 Mean_loss: 0.11305949,  training time: 2.29
progress:  10%|[34m         [0m| 10/100 [00:26<03:28,  2.31s/it]progress:  11%|[34m         [0m| 11/100 [00:26<03:25,  2.31s/it]                                                          Episode 12	 reward: -7.49	 makespan: 741.80	 Mean_loss: 0.13188756,  training time: 2.29
progress:  11%|[34m         [0m| 11/100 [00:28<03:25,  2.31s/it]progress:  12%|[34m        [0m| 12/100 [00:28<03:22,  2.30s/it]                                                          Episode 13	 reward: -7.27	 makespan: 719.40	 Mean_loss: 0.08062020,  training time: 2.29
progress:  12%|[34m        [0m| 12/100 [00:30<03:22,  2.30s/it]progress:  13%|[34m        [0m| 13/100 [00:30<03:19,  2.30s/it]                                                          Episode 14	 reward: -7.44	 makespan: 736.40	 Mean_loss: 0.09402524,  training time: 2.29
progress:  13%|[34m        [0m| 13/100 [00:33<03:19,  2.30s/it]progress:  14%|[34m        [0m| 14/100 [00:33<03:17,  2.29s/it]                                                          Episode 15	 reward: -7.03	 makespan: 695.80	 Mean_loss: 0.05831325,  training time: 2.29
progress:  14%|[34m        [0m| 14/100 [00:35<03:17,  2.29s/it]progress:  15%|[34m        [0m| 15/100 [00:35<03:14,  2.29s/it]                                                          Episode 16	 reward: -7.17	 makespan: 710.20	 Mean_loss: 0.07537517,  training time: 2.29
progress:  15%|[34m        [0m| 15/100 [00:37<03:14,  2.29s/it]progress:  16%|[34m        [0m| 16/100 [00:37<03:12,  2.29s/it]                                                          Episode 17	 reward: -6.85	 makespan: 678.40	 Mean_loss: 0.06299628,  training time: 2.29
progress:  16%|[34m        [0m| 16/100 [00:40<03:12,  2.29s/it]progress:  17%|[34m        [0m| 17/100 [00:40<03:10,  2.29s/it]                                                          Episode 18	 reward: -7.05	 makespan: 698.20	 Mean_loss: 0.05092028,  training time: 2.29
progress:  17%|[34m        [0m| 17/100 [00:42<03:10,  2.29s/it]progress:  18%|[34m        [0m| 18/100 [00:42<03:07,  2.29s/it]                                                          Episode 19	 reward: -7.01	 makespan: 694.20	 Mean_loss: 0.06759325,  training time: 2.28
progress:  18%|[34m        [0m| 18/100 [00:44<03:07,  2.29s/it]progress:  19%|[34m        [0m| 19/100 [00:44<03:05,  2.29s/it]                                                          Episode 20	 reward: -6.71	 makespan: 664.40	 Mean_loss: 0.03356441,  training time: 2.29
progress:  19%|[34m        [0m| 19/100 [00:46<03:05,  2.29s/it]progress:  20%|[34m        [0m| 20/100 [00:46<03:03,  2.29s/it]                                                          Episode 21	 reward: -6.78	 makespan: 671.20	 Mean_loss: 0.05310857,  training time: 2.28
progress:  20%|[34m        [0m| 20/100 [00:49<03:03,  2.29s/it]progress:  21%|[34m        [0m| 21/100 [00:49<03:00,  2.29s/it]                                                          Episode 22	 reward: -6.68	 makespan: 661.40	 Mean_loss: 0.04919694,  training time: 2.30
progress:  21%|[34m        [0m| 21/100 [00:51<03:00,  2.29s/it]progress:  22%|[34m       [0m| 22/100 [00:51<02:58,  2.29s/it]                                                          Episode 23	 reward: -7.01	 makespan: 694.00	 Mean_loss: 0.04720100,  training time: 2.30
progress:  22%|[34m       [0m| 22/100 [00:53<02:58,  2.29s/it]progress:  23%|[34m       [0m| 23/100 [00:53<02:56,  2.29s/it]                                                          Episode 24	 reward: -6.77	 makespan: 670.00	 Mean_loss: 0.03349562,  training time: 2.30
progress:  23%|[34m       [0m| 23/100 [00:56<02:56,  2.29s/it]progress:  24%|[34m       [0m| 24/100 [00:56<02:54,  2.30s/it]                                                          Episode 25	 reward: -6.50	 makespan: 643.20	 Mean_loss: 0.04953148,  training time: 2.30
progress:  24%|[34m       [0m| 24/100 [00:58<02:54,  2.30s/it]progress:  25%|[34m       [0m| 25/100 [00:58<02:52,  2.30s/it]                                                          Episode 26	 reward: -6.82	 makespan: 675.20	 Mean_loss: 0.08149032,  training time: 2.30
progress:  25%|[34m       [0m| 25/100 [01:00<02:52,  2.30s/it]progress:  26%|[34m       [0m| 26/100 [01:00<02:50,  2.30s/it]                                                          Episode 27	 reward: -6.67	 makespan: 660.20	 Mean_loss: 0.03949679,  training time: 2.30
progress:  26%|[34m       [0m| 26/100 [01:03<02:50,  2.30s/it]progress:  27%|[34m       [0m| 27/100 [01:03<02:47,  2.30s/it]                                                          Episode 28	 reward: -6.54	 makespan: 647.00	 Mean_loss: 0.03873440,  training time: 2.30
progress:  27%|[34m       [0m| 27/100 [01:05<02:47,  2.30s/it]progress:  28%|[34m       [0m| 28/100 [01:05<02:45,  2.30s/it]                                                          Episode 29	 reward: -6.85	 makespan: 677.80	 Mean_loss: 0.04702450,  training time: 2.29
progress:  28%|[34m       [0m| 28/100 [01:07<02:45,  2.30s/it]progress:  29%|[34m       [0m| 29/100 [01:07<02:43,  2.30s/it]                                                          Episode 30	 reward: -6.43	 makespan: 636.60	 Mean_loss: 0.08067644,  training time: 2.29
progress:  29%|[34m       [0m| 29/100 [01:09<02:43,  2.30s/it]progress:  30%|[34m       [0m| 30/100 [01:09<02:40,  2.29s/it]                                                          Episode 31	 reward: -6.49	 makespan: 642.20	 Mean_loss: 0.03474253,  training time: 2.29
progress:  30%|[34m       [0m| 30/100 [01:12<02:40,  2.29s/it]progress:  31%|[34m       [0m| 31/100 [01:12<02:38,  2.29s/it]                                                          Episode 32	 reward: -6.60	 makespan: 653.80	 Mean_loss: 0.03983119,  training time: 2.29
progress:  31%|[34m       [0m| 31/100 [01:14<02:38,  2.29s/it]progress:  32%|[34m      [0m| 32/100 [01:14<02:35,  2.29s/it]                                                          Episode 33	 reward: -6.47	 makespan: 640.60	 Mean_loss: 0.04751933,  training time: 2.29
progress:  32%|[34m      [0m| 32/100 [01:16<02:35,  2.29s/it]progress:  33%|[34m      [0m| 33/100 [01:16<02:33,  2.29s/it]                                                          Episode 34	 reward: -6.54	 makespan: 647.20	 Mean_loss: 0.02659569,  training time: 2.29
progress:  33%|[34m      [0m| 33/100 [01:19<02:33,  2.29s/it]progress:  34%|[34m      [0m| 34/100 [01:19<02:31,  2.29s/it]                                                          Episode 35	 reward: -6.89	 makespan: 682.00	 Mean_loss: 0.03962792,  training time: 2.29
progress:  34%|[34m      [0m| 34/100 [01:21<02:31,  2.29s/it]progress:  35%|[34m      [0m| 35/100 [01:21<02:28,  2.29s/it]                                                          Episode 36	 reward: -6.99	 makespan: 691.80	 Mean_loss: 0.04412086,  training time: 2.29
progress:  35%|[34m      [0m| 35/100 [01:23<02:28,  2.29s/it]progress:  36%|[34m      [0m| 36/100 [01:23<02:26,  2.29s/it]                                                          Episode 37	 reward: -6.98	 makespan: 691.40	 Mean_loss: 0.03812751,  training time: 2.29
progress:  36%|[34m      [0m| 36/100 [01:25<02:26,  2.29s/it]progress:  37%|[34m      [0m| 37/100 [01:25<02:24,  2.29s/it]                                                          Episode 38	 reward: -6.56	 makespan: 649.40	 Mean_loss: 0.03261422,  training time: 2.29
progress:  37%|[34m      [0m| 37/100 [01:28<02:24,  2.29s/it]progress:  38%|[34m      [0m| 38/100 [01:28<02:22,  2.29s/it]                                                          Episode 39	 reward: -6.82	 makespan: 675.60	 Mean_loss: 0.02617074,  training time: 2.29
progress:  38%|[34m      [0m| 38/100 [01:30<02:22,  2.29s/it]progress:  39%|[34m      [0m| 39/100 [01:30<02:19,  2.29s/it]                                                          Episode 40	 reward: -6.88	 makespan: 681.40	 Mean_loss: 0.05509562,  training time: 2.29
progress:  39%|[34m      [0m| 39/100 [01:32<02:19,  2.29s/it]progress:  40%|[34m      [0m| 40/100 [01:32<02:17,  2.29s/it]                                                          Episode 41	 reward: -6.52	 makespan: 645.60	 Mean_loss: 0.03807139,  training time: 2.29
progress:  40%|[34m      [0m| 40/100 [01:35<02:17,  2.29s/it]progress:  41%|[34m      [0m| 41/100 [01:35<02:15,  2.29s/it]                                                          Episode 42	 reward: -6.29	 makespan: 622.40	 Mean_loss: 0.05179815,  training time: 2.29
progress:  41%|[34m      [0m| 41/100 [01:37<02:15,  2.29s/it]progress:  42%|[34m     [0m| 42/100 [01:37<02:12,  2.29s/it]                                                          Episode 43	 reward: -6.49	 makespan: 642.60	 Mean_loss: 0.02888397,  training time: 2.29
progress:  42%|[34m     [0m| 42/100 [01:39<02:12,  2.29s/it]progress:  43%|[34m     [0m| 43/100 [01:39<02:10,  2.29s/it]                                                          Episode 44	 reward: -7.06	 makespan: 699.00	 Mean_loss: 0.03761972,  training time: 2.29
progress:  43%|[34m     [0m| 43/100 [01:41<02:10,  2.29s/it]progress:  44%|[34m     [0m| 44/100 [01:41<02:08,  2.29s/it]                                                          Episode 45	 reward: -6.59	 makespan: 652.80	 Mean_loss: 0.02994126,  training time: 2.29
progress:  44%|[34m     [0m| 44/100 [01:44<02:08,  2.29s/it]progress:  45%|[34m     [0m| 45/100 [01:44<02:06,  2.29s/it]                                                          Episode 46	 reward: -6.36	 makespan: 629.80	 Mean_loss: 0.01679679,  training time: 2.29
progress:  45%|[34m     [0m| 45/100 [01:46<02:06,  2.29s/it]progress:  46%|[34m     [0m| 46/100 [01:46<02:03,  2.29s/it]                                                          Episode 47	 reward: -6.52	 makespan: 645.40	 Mean_loss: 0.02873094,  training time: 2.29
progress:  46%|[34m     [0m| 46/100 [01:48<02:03,  2.29s/it]progress:  47%|[34m     [0m| 47/100 [01:48<02:01,  2.29s/it]                                                          Episode 48	 reward: -7.04	 makespan: 697.40	 Mean_loss: 0.03934470,  training time: 2.29
progress:  47%|[34m     [0m| 47/100 [01:51<02:01,  2.29s/it]progress:  48%|[34m     [0m| 48/100 [01:51<01:59,  2.29s/it]                                                          Episode 49	 reward: -6.35	 makespan: 629.00	 Mean_loss: 0.03126857,  training time: 2.29
progress:  48%|[34m     [0m| 48/100 [01:53<01:59,  2.29s/it]progress:  49%|[34m     [0m| 49/100 [01:53<01:56,  2.29s/it]                                                          Episode 50	 reward: -6.45	 makespan: 638.20	 Mean_loss: 0.03442547,  training time: 2.29
progress:  49%|[34m     [0m| 49/100 [01:55<01:56,  2.29s/it]progress:  50%|[34m     [0m| 50/100 [01:55<01:54,  2.29s/it]                                                          Episode 51	 reward: -6.99	 makespan: 691.60	 Mean_loss: 0.04259231,  training time: 2.29
progress:  50%|[34m     [0m| 50/100 [01:58<01:54,  2.29s/it]progress:  51%|[34m     [0m| 51/100 [01:58<01:52,  2.29s/it]                                                          Episode 52	 reward: -6.70	 makespan: 663.00	 Mean_loss: 0.02629479,  training time: 2.29
progress:  51%|[34m     [0m| 51/100 [02:00<01:52,  2.29s/it]progress:  52%|[34m    [0m| 52/100 [02:00<01:49,  2.29s/it]                                                          Episode 53	 reward: -6.56	 makespan: 649.80	 Mean_loss: 0.02509967,  training time: 2.29
progress:  52%|[34m    [0m| 52/100 [02:02<01:49,  2.29s/it]progress:  53%|[34m    [0m| 53/100 [02:02<01:47,  2.29s/it]                                                          Episode 54	 reward: -6.62	 makespan: 655.20	 Mean_loss: 0.03039232,  training time: 2.29
progress:  53%|[34m    [0m| 53/100 [02:04<01:47,  2.29s/it]progress:  54%|[34m    [0m| 54/100 [02:04<01:45,  2.29s/it]                                                          Episode 55	 reward: -6.83	 makespan: 676.00	 Mean_loss: 0.03405063,  training time: 2.29
progress:  54%|[34m    [0m| 54/100 [02:07<01:45,  2.29s/it]progress:  55%|[34m    [0m| 55/100 [02:07<01:43,  2.29s/it]                                                          Episode 56	 reward: -6.52	 makespan: 645.00	 Mean_loss: 0.03512399,  training time: 2.29
progress:  55%|[34m    [0m| 55/100 [02:09<01:43,  2.29s/it]progress:  56%|[34m    [0m| 56/100 [02:09<01:40,  2.29s/it]                                                          Episode 57	 reward: -6.49	 makespan: 642.40	 Mean_loss: 0.02956601,  training time: 2.29
progress:  56%|[34m    [0m| 56/100 [02:11<01:40,  2.29s/it]progress:  57%|[34m    [0m| 57/100 [02:11<01:38,  2.29s/it]                                                          Episode 58	 reward: -6.57	 makespan: 650.60	 Mean_loss: 0.02024427,  training time: 2.29
progress:  57%|[34m    [0m| 57/100 [02:14<01:38,  2.29s/it]progress:  58%|[34m    [0m| 58/100 [02:14<01:36,  2.29s/it]                                                          Episode 59	 reward: -6.55	 makespan: 648.00	 Mean_loss: 0.01873154,  training time: 2.29
progress:  58%|[34m    [0m| 58/100 [02:16<01:36,  2.29s/it]progress:  59%|[34m    [0m| 59/100 [02:16<01:34,  2.29s/it]                                                          Episode 60	 reward: -6.57	 makespan: 650.20	 Mean_loss: 0.02248273,  training time: 2.29
progress:  59%|[34m    [0m| 59/100 [02:18<01:34,  2.29s/it]progress:  60%|[34m    [0m| 60/100 [02:18<01:31,  2.29s/it]                                                          Episode 61	 reward: -6.76	 makespan: 668.80	 Mean_loss: 0.04369958,  training time: 2.29
progress:  60%|[34m    [0m| 60/100 [02:20<01:31,  2.29s/it]progress:  61%|[34m    [0m| 61/100 [02:20<01:29,  2.29s/it]                                                          Episode 62	 reward: -6.77	 makespan: 670.00	 Mean_loss: 0.02520248,  training time: 2.29
progress:  61%|[34m    [0m| 61/100 [02:23<01:29,  2.29s/it]progress:  62%|[34m   [0m| 62/100 [02:23<01:27,  2.29s/it]                                                          Episode 63	 reward: -6.33	 makespan: 627.00	 Mean_loss: 0.02154890,  training time: 2.29
progress:  62%|[34m   [0m| 62/100 [02:25<01:27,  2.29s/it]progress:  63%|[34m   [0m| 63/100 [02:25<01:24,  2.29s/it]                                                          Episode 64	 reward: -6.50	 makespan: 643.20	 Mean_loss: 0.02195498,  training time: 2.29
progress:  63%|[34m   [0m| 63/100 [02:27<01:24,  2.29s/it]progress:  64%|[34m   [0m| 64/100 [02:27<01:22,  2.29s/it]                                                          Episode 65	 reward: -6.75	 makespan: 668.40	 Mean_loss: 0.03777393,  training time: 2.29
progress:  64%|[34m   [0m| 64/100 [02:30<01:22,  2.29s/it]progress:  65%|[34m   [0m| 65/100 [02:30<01:20,  2.29s/it]                                                          Episode 66	 reward: -6.61	 makespan: 654.80	 Mean_loss: 0.01998832,  training time: 2.29
progress:  65%|[34m   [0m| 65/100 [02:32<01:20,  2.29s/it]progress:  66%|[34m   [0m| 66/100 [02:32<01:17,  2.29s/it]                                                          Episode 67	 reward: -6.49	 makespan: 642.60	 Mean_loss: 0.02041442,  training time: 2.29
progress:  66%|[34m   [0m| 66/100 [02:34<01:17,  2.29s/it]progress:  67%|[34m   [0m| 67/100 [02:34<01:15,  2.29s/it]                                                          Episode 68	 reward: -6.64	 makespan: 657.20	 Mean_loss: 0.01904503,  training time: 2.29
progress:  67%|[34m   [0m| 67/100 [02:36<01:15,  2.29s/it]progress:  68%|[34m   [0m| 68/100 [02:36<01:13,  2.29s/it]                                                          Episode 69	 reward: -6.48	 makespan: 642.00	 Mean_loss: 0.02050226,  training time: 2.29
progress:  68%|[34m   [0m| 68/100 [02:39<01:13,  2.29s/it]progress:  69%|[34m   [0m| 69/100 [02:39<01:10,  2.29s/it]                                                          Episode 70	 reward: -6.35	 makespan: 628.40	 Mean_loss: 0.02224969,  training time: 2.29
progress:  69%|[34m   [0m| 69/100 [02:41<01:10,  2.29s/it]progress:  70%|[34m   [0m| 70/100 [02:41<01:08,  2.29s/it]                                                          Episode 71	 reward: -6.37	 makespan: 630.80	 Mean_loss: 0.01247011,  training time: 2.29
progress:  70%|[34m   [0m| 70/100 [02:43<01:08,  2.29s/it]progress:  71%|[34m   [0m| 71/100 [02:43<01:06,  2.29s/it]                                                          Episode 72	 reward: -6.55	 makespan: 648.40	 Mean_loss: 0.02374716,  training time: 2.29
progress:  71%|[34m   [0m| 71/100 [02:46<01:06,  2.29s/it]progress:  72%|[34m  [0m| 72/100 [02:46<01:04,  2.29s/it]                                                          Episode 73	 reward: -6.22	 makespan: 616.20	 Mean_loss: 0.01462355,  training time: 2.29
progress:  72%|[34m  [0m| 72/100 [02:48<01:04,  2.29s/it]progress:  73%|[34m  [0m| 73/100 [02:48<01:01,  2.29s/it]                                                          Episode 74	 reward: -6.50	 makespan: 643.60	 Mean_loss: 0.02128075,  training time: 2.29
progress:  73%|[34m  [0m| 73/100 [02:50<01:01,  2.29s/it]progress:  74%|[34m  [0m| 74/100 [02:50<00:59,  2.29s/it]                                                          Episode 75	 reward: -6.70	 makespan: 663.00	 Mean_loss: 0.02032421,  training time: 2.30
progress:  74%|[34m  [0m| 74/100 [02:53<00:59,  2.29s/it]progress:  75%|[34m  [0m| 75/100 [02:53<00:57,  2.29s/it]                                                          Episode 76	 reward: -6.33	 makespan: 626.80	 Mean_loss: 0.02565591,  training time: 2.29
progress:  75%|[34m  [0m| 75/100 [02:55<00:57,  2.29s/it]progress:  76%|[34m  [0m| 76/100 [02:55<00:55,  2.29s/it]                                                          Episode 77	 reward: -6.56	 makespan: 649.80	 Mean_loss: 0.02932549,  training time: 2.29
progress:  76%|[34m  [0m| 76/100 [02:57<00:55,  2.29s/it]progress:  77%|[34m  [0m| 77/100 [02:57<00:52,  2.29s/it]                                                          Episode 78	 reward: -6.57	 makespan: 650.00	 Mean_loss: 0.01763982,  training time: 2.30
progress:  77%|[34m  [0m| 77/100 [02:59<00:52,  2.29s/it]progress:  78%|[34m  [0m| 78/100 [02:59<00:50,  2.29s/it]                                                          Episode 79	 reward: -6.21	 makespan: 614.80	 Mean_loss: 0.02783037,  training time: 2.30
progress:  78%|[34m  [0m| 78/100 [03:02<00:50,  2.29s/it]progress:  79%|[34m  [0m| 79/100 [03:02<00:48,  2.30s/it]                                                          Episode 80	 reward: -6.32	 makespan: 625.60	 Mean_loss: 0.01459742,  training time: 2.29
progress:  79%|[34m  [0m| 79/100 [03:04<00:48,  2.30s/it]progress:  80%|[34m  [0m| 80/100 [03:04<00:45,  2.30s/it]                                                          Episode 81	 reward: -6.49	 makespan: 642.60	 Mean_loss: 0.02396765,  training time: 2.30
progress:  80%|[34m  [0m| 80/100 [03:06<00:45,  2.30s/it]progress:  81%|[34m  [0m| 81/100 [03:06<00:43,  2.30s/it]                                                          Episode 82	 reward: -6.39	 makespan: 633.00	 Mean_loss: 0.01817031,  training time: 2.29
progress:  81%|[34m  [0m| 81/100 [03:09<00:43,  2.30s/it]progress:  82%|[34m [0m| 82/100 [03:09<00:41,  2.30s/it]                                                          Episode 83	 reward: -6.62	 makespan: 655.80	 Mean_loss: 0.01417150,  training time: 2.29
progress:  82%|[34m [0m| 82/100 [03:11<00:41,  2.30s/it]progress:  83%|[34m [0m| 83/100 [03:11<00:39,  2.30s/it]                                                          Episode 84	 reward: -6.42	 makespan: 635.40	 Mean_loss: 0.01452780,  training time: 2.30
progress:  83%|[34m [0m| 83/100 [03:13<00:39,  2.30s/it]progress:  84%|[34m [0m| 84/100 [03:13<00:36,  2.30s/it]                                                          Episode 85	 reward: -6.74	 makespan: 666.80	 Mean_loss: 0.03195061,  training time: 2.29
progress:  84%|[34m [0m| 84/100 [03:15<00:36,  2.30s/it]progress:  85%|[34m [0m| 85/100 [03:15<00:34,  2.30s/it]                                                          Episode 86	 reward: -6.64	 makespan: 657.40	 Mean_loss: 0.02328547,  training time: 2.29
progress:  85%|[34m [0m| 85/100 [03:18<00:34,  2.30s/it]progress:  86%|[34m [0m| 86/100 [03:18<00:32,  2.30s/it]                                                          Episode 87	 reward: -6.74	 makespan: 667.40	 Mean_loss: 0.02410666,  training time: 2.29
progress:  86%|[34m [0m| 86/100 [03:20<00:32,  2.30s/it]progress:  87%|[34m [0m| 87/100 [03:20<00:29,  2.30s/it]                                                          Episode 88	 reward: -6.50	 makespan: 643.20	 Mean_loss: 0.04219216,  training time: 2.29
progress:  87%|[34m [0m| 87/100 [03:22<00:29,  2.30s/it]progress:  88%|[34m [0m| 88/100 [03:22<00:27,  2.30s/it]                                                          Episode 89	 reward: -6.83	 makespan: 675.80	 Mean_loss: 0.03595604,  training time: 2.29
progress:  88%|[34m [0m| 88/100 [03:25<00:27,  2.30s/it]progress:  89%|[34m [0m| 89/100 [03:25<00:25,  2.30s/it]                                                          Episode 90	 reward: -6.65	 makespan: 658.60	 Mean_loss: 0.02866117,  training time: 2.30
progress:  89%|[34m [0m| 89/100 [03:27<00:25,  2.30s/it]progress:  90%|[34m [0m| 90/100 [03:27<00:22,  2.30s/it]                                                          Episode 91	 reward: -6.94	 makespan: 687.20	 Mean_loss: 0.03878694,  training time: 2.29
progress:  90%|[34m [0m| 90/100 [03:29<00:22,  2.30s/it]progress:  91%|[34m [0m| 91/100 [03:29<00:20,  2.30s/it]                                                          Episode 92	 reward: -6.85	 makespan: 678.40	 Mean_loss: 0.01244330,  training time: 2.29
progress:  91%|[34m [0m| 91/100 [03:32<00:20,  2.30s/it]progress:  92%|[34m[0m| 92/100 [03:32<00:18,  2.29s/it]                                                          Episode 93	 reward: -6.71	 makespan: 664.60	 Mean_loss: 0.02337001,  training time: 2.29
progress:  92%|[34m[0m| 92/100 [03:34<00:18,  2.29s/it]progress:  93%|[34m[0m| 93/100 [03:34<00:16,  2.29s/it]                                                          Episode 94	 reward: -6.84	 makespan: 677.60	 Mean_loss: 0.04364493,  training time: 2.29
progress:  93%|[34m[0m| 93/100 [03:36<00:16,  2.29s/it]progress:  94%|[34m[0m| 94/100 [03:36<00:13,  2.29s/it]                                                          Episode 95	 reward: -6.57	 makespan: 650.80	 Mean_loss: 0.03291605,  training time: 2.29
progress:  94%|[34m[0m| 94/100 [03:38<00:13,  2.29s/it]progress:  95%|[34m[0m| 95/100 [03:38<00:11,  2.29s/it]                                                          Episode 96	 reward: -6.75	 makespan: 668.00	 Mean_loss: 0.03126803,  training time: 2.29
progress:  95%|[34m[0m| 95/100 [03:41<00:11,  2.29s/it]progress:  96%|[34m[0m| 96/100 [03:41<00:09,  2.29s/it]                                                          Episode 97	 reward: -6.85	 makespan: 678.40	 Mean_loss: 0.02686271,  training time: 2.29
progress:  96%|[34m[0m| 96/100 [03:43<00:09,  2.29s/it]progress:  97%|[34m[0m| 97/100 [03:43<00:06,  2.29s/it]                                                          Episode 98	 reward: -6.69	 makespan: 662.80	 Mean_loss: 0.03139972,  training time: 2.29
progress:  97%|[34m[0m| 97/100 [03:45<00:06,  2.29s/it]progress:  98%|[34m[0m| 98/100 [03:45<00:04,  2.29s/it]                                                          Episode 99	 reward: -6.73	 makespan: 666.40	 Mean_loss: 0.02654587,  training time: 2.29
progress:  98%|[34m[0m| 98/100 [03:48<00:04,  2.29s/it]progress:  99%|[34m[0m| 99/100 [03:48<00:02,  2.29s/it]                                                          Episode 100	 reward: -6.73	 makespan: 666.20	 Mean_loss: 0.02131458,  training time: 2.29
progress:  99%|[34m[0m| 99/100 [03:50<00:02,  2.29s/it]progress: 100%|[34m[0m| 100/100 [03:50<00:00,  2.29s/it]progress: 100%|[34m[0m| 100/100 [03:50<00:00,  2.30s/it]
+ IFS=,
+ read n_j n_m

+ source /work/home/lxx_hzau/.bashrc
++ export CUDA_HOME=/usr/local/cuda-11.1/
++ CUDA_HOME=/usr/local/cuda-11.1/
++ export PATH=/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/.vscode-server/bin/af28b32d7e553898b2a91af498b1fb666fdebe0c/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
++ PATH=/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/.vscode-server/bin/af28b32d7e553898b2a91af498b1fb666fdebe0c/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
++ '[' -f /etc/bashrc ']'
++ . /etc/bashrc
+++ '[' '' ']'
+++ shopt -q login_shell
+++ '[' 5235 -gt 199 ']'
++++ /usr/bin/id -gn
++++ /usr/bin/id -un
+++ '[' ac6owqc808 = lxx_hzau ']'
+++ umask 022
+++ SHELL=/bin/bash
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/256term.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/256term.sh
++++ local256=truecolor
++++ '[' -n truecolor ']'
++++ case "$TERM" in
++++ export TERM
++++ '[' -n '' ']'
++++ unset local256
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/abrt-console-notification.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/abrt-console-notification.sh
++++ tty -s
++++ return 0
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/bash_completion.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/bash_completion.sh
++++ '[' -z '4.2.46(2)-release' -o -z '' -o -n '' ']'
++++ return
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/check.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/check.sh
++++ export CHECK_HOME=/opt/hpc/setfreq/
++++ CHECK_HOME=/opt/hpc/setfreq/
++++ export PATH=/opt/hpc/setfreq/:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/.vscode-server/bin/af28b32d7e553898b2a91af498b1fb666fdebe0c/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
++++ PATH=/opt/hpc/setfreq/:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/.vscode-server/bin/af28b32d7e553898b2a91af498b1fb666fdebe0c/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/clusconf-env.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/clusconf-env.sh
++++ export CLUSCONF_HOME=/opt/clusconf
++++ CLUSCONF_HOME=/opt/clusconf
++++ export PATH=/opt/clusconf/bin:/opt/hpc/setfreq/:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/.vscode-server/bin/af28b32d7e553898b2a91af498b1fb666fdebe0c/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
++++ PATH=/opt/clusconf/bin:/opt/hpc/setfreq/:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/.vscode-server/bin/af28b32d7e553898b2a91af498b1fb666fdebe0c/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
++++ export PATH=/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq/:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/.vscode-server/bin/af28b32d7e553898b2a91af498b1fb666fdebe0c/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
++++ PATH=/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq/:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/.vscode-server/bin/af28b32d7e553898b2a91af498b1fb666fdebe0c/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
++++ export NFSCONF=/opt/clusconf/etc/nfs.cfg
++++ NFSCONF=/opt/clusconf/etc/nfs.cfg
++++ export IPMICONF=/opt/clusconf/etc/ipmi.cfg
++++ IPMICONF=/opt/clusconf/etc/ipmi.cfg
++++ export AUTOCLUSCONF=/opt/clusconf/etc/autoconf.cfg
++++ AUTOCLUSCONF=/opt/clusconf/etc/autoconf.cfg
++++ export STARTWAITTIME=300
++++ STARTWAITTIME=300
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/colorgrep.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/colorgrep.sh
++++ /usr/libexec/grepconf.sh -c
++++ alias 'grep=grep --color=auto'
++++ alias 'egrep=egrep --color=auto'
++++ alias 'fgrep=fgrep --color=auto'
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/colorls.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/colorls.sh
++++ '[' '!' -t 0 ']'
++++ return
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/dawning.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/dawning.sh
++++ export GRIDVIEW_HOME=/opt/gridview
++++ GRIDVIEW_HOME=/opt/gridview
++++ export MUNGE_HOME=/opt/gridview/munge
++++ MUNGE_HOME=/opt/gridview/munge
++++ export PATH=/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq/:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/.vscode-server/bin/af28b32d7e553898b2a91af498b1fb666fdebe0c/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
++++ PATH=/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq/:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/.vscode-server/bin/af28b32d7e553898b2a91af498b1fb666fdebe0c/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
++++ export LD_LIBRARY_PATH=/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/sharp/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/lib:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:
++++ LD_LIBRARY_PATH=/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/sharp/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/lib:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:
++++ export MANPATH=/opt/gridview/munge/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/share/man:/opt/rh/devtoolset-7/root/usr/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:
++++ MANPATH=/opt/gridview/munge/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/share/man:/opt/rh/devtoolset-7/root/usr/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:
++++ export C_INCLUDE_PATH=/opt/gridview/munge/include:/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/rh/devtoolset-7/root/usr/include/c++/7:/opt/rh/devtoolset-7/root/usr/include/c++/7/x86_64-redhat-linux:/opt/gridview/slurm/include:/opt/gridview/munge/include:
++++ C_INCLUDE_PATH=/opt/gridview/munge/include:/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/rh/devtoolset-7/root/usr/include/c++/7:/opt/rh/devtoolset-7/root/usr/include/c++/7/x86_64-redhat-linux:/opt/gridview/slurm/include:/opt/gridview/munge/include:
++++ export SLURM_HOME=/opt/gridview/slurm
++++ SLURM_HOME=/opt/gridview/slurm
++++ export PATH=/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq/:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/.vscode-server/bin/af28b32d7e553898b2a91af498b1fb666fdebe0c/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
++++ PATH=/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq/:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/.vscode-server/bin/af28b32d7e553898b2a91af498b1fb666fdebe0c/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
++++ export LD_LIBRARY_PATH=/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/sharp/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/lib:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:
++++ LD_LIBRARY_PATH=/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/sharp/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/lib:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:
++++ export MANPATH=/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/share/man:/opt/rh/devtoolset-7/root/usr/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:
++++ MANPATH=/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/share/man:/opt/rh/devtoolset-7/root/usr/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:
++++ export C_INCLUDE_PATH=/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/rh/devtoolset-7/root/usr/include/c++/7:/opt/rh/devtoolset-7/root/usr/include/c++/7/x86_64-redhat-linux:/opt/gridview/slurm/include:/opt/gridview/munge/include:
++++ C_INCLUDE_PATH=/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/rh/devtoolset-7/root/usr/include/c++/7:/opt/rh/devtoolset-7/root/usr/include/c++/7/x86_64-redhat-linux:/opt/gridview/slurm/include:/opt/gridview/munge/include:
++++ export SLURM_PMIX_DIRECT_CONN=true
++++ SLURM_PMIX_DIRECT_CONN=true
++++ export SLURM_PMIX_DIRECT_CONN_UCX=true
++++ SLURM_PMIX_DIRECT_CONN_UCX=true
++++ export SLURM_PMIX_DIRECT_CONN_EARLY=true
++++ SLURM_PMIX_DIRECT_CONN_EARLY=true
++++ export SLURM_PMIX_TIMEOUT=3000
++++ SLURM_PMIX_TIMEOUT=3000
++++ '[' lxx_hzau '!=' root ']'
++++ export SQUEUE_USERS=lxx_hzau
++++ SQUEUE_USERS=lxx_hzau
++++ export SCONTROL_JOB_USERS=lxx_hzau
++++ SCONTROL_JOB_USERS=lxx_hzau
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/flatpak.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/flatpak.sh
++++ '[' /exports/share:/var/lib/flatpak/exports/share:/usr/local/share:/usr/share = /work/home/lxx_hzau/.local/share/flatpak/exports/share:/var/lib/flatpak/exports/share:/usr/local/share:/usr/share ']'
++++ export XDG_DATA_DIRS
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/kde.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/kde.sh
++++ '[' -z '' ']'
++++ KDEDIRS=/usr
++++ export KDEDIRS
++++ '[' -z '' ']'
++++ grep --color=auto -qs '^PRELINKING=yes' /etc/sysconfig/prelink
++++ for libdir in /usr/lib64 /usr/lib
++++ '[' -n '' ']'
++++ QT_PLUGIN_PATH=/usr/lib64/kde4/plugins
++++ export QT_PLUGIN_PATH
++++ for libdir in /usr/lib64 /usr/lib
++++ '[' -n /usr/lib64/kde4/plugins ']'
++++ echo /usr/lib64/kde4/plugins
++++ /bin/grep -q /usr/lib/kde4/plugins
++++ QT_PLUGIN_PATH=/usr/lib64/kde4/plugins:/usr/lib/kde4/plugins
++++ export QT_PLUGIN_PATH
++++ '[' '!' -d /work/home/lxx_hzau/.local/share ']'
++++ unset libdir
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/lang.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/lang.sh
++++ sourced=0
++++ '[' -n en_US.UTF-8 ']'
++++ saved_lang=en_US.UTF-8
++++ '[' -f /work/home/lxx_hzau/.i18n ']'
++++ LANG=en_US.UTF-8
++++ unset saved_lang
++++ '[' 0 = 1 ']'
++++ unset sourced
++++ unset langfile
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/less.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/less.sh
++++ '[' -x /usr/bin/lesspipe.sh ']'
++++ export 'LESSOPEN=||/usr/bin/lesspipe.sh %s'
++++ LESSOPEN='||/usr/bin/lesspipe.sh %s'
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/modules.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/modules.sh
++++++ /bin/ps -p 13939 -ocomm=
+++++ /bin/basename slurm_script
++++ shell=slurm_script
++++ '[' -f /usr/share/Modules/init/slurm_script ']'
++++ . /usr/share/Modules/init/sh
+++++ MODULESHOME=/usr/share/Modules
+++++ export MODULESHOME
+++++ '[' compiler/devtoolset/7.3.1:mpi/hpcx/gcc-7.3.1 = '' ']'
+++++ '[' /public/software/modules/base:/public/software/modules/apps = '' ']'
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/mpi-selector.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/mpi-selector.sh
++++ mpi_selector_dir=/var/mpi-selector/data
++++ mpi_selector_homefile=/work/home/lxx_hzau/.mpi-selector
++++ mpi_selector_sysfile=/etc/sysconfig/mpi-selector
++++ mpi_selection=
++++ test -f /work/home/lxx_hzau/.mpi-selector
++++ test -f /etc/sysconfig/mpi-selector
++++ test '' '!=' '' -a -f /var/mpi-selector/data/.sh
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/PackageKit.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/PackageKit.sh
++++ [[ -n '' ]]
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/perl-homedir.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/perl-homedir.sh
++++ PERL_HOMEDIR=1
++++ '[' -f /etc/sysconfig/perl-homedir ']'
++++ '[' -f /work/home/lxx_hzau/.perl-homedir ']'
++++ alias 'perlll=eval `perl -Mlocal::lib`'
++++ '[' x1 = x1 ']'
+++++ perl -Mlocal::lib
++++ eval export 'PERL_LOCAL_LIB_ROOT="$PERL_LOCAL_LIB_ROOT:/work/home/lxx_hzau/perl5";' export 'PERL_MB_OPT="--install_base' '/work/home/lxx_hzau/perl5";' export 'PERL_MM_OPT="INSTALL_BASE=/work/home/lxx_hzau/perl5";' export 'PERL5LIB="/work/home/lxx_hzau/perl5/lib/perl5:$PERL5LIB";' export 'PATH="/work/home/lxx_hzau/perl5/bin:$PATH";'
+++++ export PERL_LOCAL_LIB_ROOT=:/work/home/lxx_hzau/perl5
+++++ PERL_LOCAL_LIB_ROOT=:/work/home/lxx_hzau/perl5
+++++ export 'PERL_MB_OPT=--install_base /work/home/lxx_hzau/perl5'
+++++ PERL_MB_OPT='--install_base /work/home/lxx_hzau/perl5'
+++++ export PERL_MM_OPT=INSTALL_BASE=/work/home/lxx_hzau/perl5
+++++ PERL_MM_OPT=INSTALL_BASE=/work/home/lxx_hzau/perl5
+++++ export PERL5LIB=/work/home/lxx_hzau/perl5/lib/perl5:/opt/rh/devtoolset-7/root/usr/lib64/perl5/vendor_perl:/opt/rh/devtoolset-7/root/usr/share/perl5/vendor_perl
+++++ PERL5LIB=/work/home/lxx_hzau/perl5/lib/perl5:/opt/rh/devtoolset-7/root/usr/lib64/perl5/vendor_perl:/opt/rh/devtoolset-7/root/usr/share/perl5/vendor_perl
+++++ export PATH=/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq/:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/.vscode-server/bin/af28b32d7e553898b2a91af498b1fb666fdebe0c/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
+++++ PATH=/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq/:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/.vscode-server/bin/af28b32d7e553898b2a91af498b1fb666fdebe0c/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/qt-graphicssystem.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/qt-graphicssystem.sh
++++ '[' -z 1 -a -z '' ']'
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/qt.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/qt.sh
++++ '[' -z /usr/lib64/qt-3.3 ']'
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/rocm-gcc-mpi-default.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/rocm-gcc-mpi-default.sh
++++ [[ lxx_hzau =~ root|xhadmin ]]
++++ module use /public/software/modules/apps
+++++ /usr/bin/modulecmd sh use /public/software/modules/apps
++++ eval
++++ module use /public/software/modules/base
+++++ /usr/bin/modulecmd sh use /public/software/modules/base
++++ eval
++++ module unuse /opt/hpc/software/modules
+++++ /usr/bin/modulecmd sh unuse /opt/hpc/software/modules
++++ eval
++++ module load compiler/devtoolset/7.3.1
+++++ /usr/bin/modulecmd sh load compiler/devtoolset/7.3.1
++++ eval
++++ module load mpi/hpcx/gcc-7.3.1
+++++ /usr/bin/modulecmd sh load mpi/hpcx/gcc-7.3.1
++++ eval
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/vim.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/vim.sh
++++ '[' -n '4.2.46(2)-release' -o -n '' -o -n '' ']'
++++ '[' -x /usr/bin/id ']'
+++++ /usr/bin/id -u
++++ ID=5235
++++ '[' -n 5235 -a 5235 -le 200 ']'
++++ alias vi
++++ alias vi=vim
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/vte.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/vte.sh
++++ '[' -n '4.2.46(2)-release' -o -n '' ']'
++++ [[ hxB == *i* ]]
++++ return 0
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/which2.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/which2.sh
++++ alias 'which=alias | /usr/bin/which --tty-only --read-alias --show-dot --show-tilde'
+++ unset i
+++ unset -f pathmunge
+++ /work/home/lxx_hzau/miniconda3/bin/conda shell.bash hook
++ __conda_setup='export CONDA_EXE='\''/work/home/lxx_hzau/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/work/home/lxx_hzau/miniconda3/bin/python'\''

# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
__conda_exe() (
    "$CONDA_EXE" $_CE_M $_CE_CONDA "$@"
)

__conda_hashr() {
    if [ -n "${ZSH_VERSION:+x}" ]; then
        \rehash
    elif [ -n "${POSH_VERSION:+x}" ]; then
        :  # pass
    else
        \hash -r
    fi
}

__conda_activate() {
    if [ -n "${CONDA_PS1_BACKUP:+x}" ]; then
        # Handle transition from shell activated with conda <= 4.3 to a subsequent activation
        # after conda updated to >= 4.4. See issue #6173.
        PS1="$CONDA_PS1_BACKUP"
        \unset CONDA_PS1_BACKUP
    fi
    \local ask_conda
    ask_conda="$(PS1="${PS1:-}" __conda_exe shell.posix "$@")" || \return
    \eval "$ask_conda"
    __conda_hashr
}

__conda_reactivate() {
    \local ask_conda
    ask_conda="$(PS1="${PS1:-}" __conda_exe shell.posix reactivate)" || \return
    \eval "$ask_conda"
    __conda_hashr
}

conda() {
    \local cmd="${1-__missing__}"
    case "$cmd" in
        activate|deactivate)
            __conda_activate "$@"
            ;;
        install|update|upgrade|remove|uninstall)
            __conda_exe "$@" || \return
            __conda_reactivate
            ;;
        *)
            __conda_exe "$@"
            ;;
    esac
}

if [ -z "${CONDA_SHLVL+x}" ]; then
    \export CONDA_SHLVL=0
    # In dev-mode CONDA_EXE is python.exe and on Windows
    # it is in a different relative location to condabin.
    if [ -n "${_CE_CONDA:+x}" ] && [ -n "${WINDIR+x}" ]; then
        PATH="$(\dirname "$CONDA_EXE")/condabin${PATH:+":${PATH}"}"
    else
        PATH="$(\dirname "$(\dirname "$CONDA_EXE")")/condabin${PATH:+":${PATH}"}"
    fi
    \export PATH

    # We'\''re not allowing PS1 to be unbound. It must at least be set.
    # However, we'\''re not exporting it, which can cause problems when starting a second shell
    # via a first shell (i.e. starting zsh from bash).
    if [ -z "${PS1+x}" ]; then
        PS1=
    fi
fi

conda activate base'
++ '[' 0 -eq 0 ']'
++ eval 'export CONDA_EXE='\''/work/home/lxx_hzau/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/work/home/lxx_hzau/miniconda3/bin/python'\''

# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
__conda_exe() (
    "$CONDA_EXE" $_CE_M $_CE_CONDA "$@"
)

__conda_hashr() {
    if [ -n "${ZSH_VERSION:+x}" ]; then
        \rehash
    elif [ -n "${POSH_VERSION:+x}" ]; then
        :  # pass
    else
        \hash -r
    fi
}

__conda_activate() {
    if [ -n "${CONDA_PS1_BACKUP:+x}" ]; then
        # Handle transition from shell activated with conda <= 4.3 to a subsequent activation
        # after conda updated to >= 4.4. See issue #6173.
        PS1="$CONDA_PS1_BACKUP"
        \unset CONDA_PS1_BACKUP
    fi
    \local ask_conda
    ask_conda="$(PS1="${PS1:-}" __conda_exe shell.posix "$@")" || \return
    \eval "$ask_conda"
    __conda_hashr
}

__conda_reactivate() {
    \local ask_conda
    ask_conda="$(PS1="${PS1:-}" __conda_exe shell.posix reactivate)" || \return
    \eval "$ask_conda"
    __conda_hashr
}

conda() {
    \local cmd="${1-__missing__}"
    case "$cmd" in
        activate|deactivate)
            __conda_activate "$@"
            ;;
        install|update|upgrade|remove|uninstall)
            __conda_exe "$@" || \return
            __conda_reactivate
            ;;
        *)
            __conda_exe "$@"
            ;;
    esac
}

if [ -z "${CONDA_SHLVL+x}" ]; then
    \export CONDA_SHLVL=0
    # In dev-mode CONDA_EXE is python.exe and on Windows
    # it is in a different relative location to condabin.
    if [ -n "${_CE_CONDA:+x}" ] && [ -n "${WINDIR+x}" ]; then
        PATH="$(\dirname "$CONDA_EXE")/condabin${PATH:+":${PATH}"}"
    else
        PATH="$(\dirname "$(\dirname "$CONDA_EXE")")/condabin${PATH:+":${PATH}"}"
    fi
    \export PATH

    # We'\''re not allowing PS1 to be unbound. It must at least be set.
    # However, we'\''re not exporting it, which can cause problems when starting a second shell
    # via a first shell (i.e. starting zsh from bash).
    if [ -z "${PS1+x}" ]; then
        PS1=
    fi
fi

conda activate base'
+++ export CONDA_EXE=/work/home/lxx_hzau/miniconda3/bin/conda
+++ CONDA_EXE=/work/home/lxx_hzau/miniconda3/bin/conda
+++ export _CE_M=
+++ _CE_M=
+++ export _CE_CONDA=
+++ _CE_CONDA=
+++ export CONDA_PYTHON_EXE=/work/home/lxx_hzau/miniconda3/bin/python
+++ CONDA_PYTHON_EXE=/work/home/lxx_hzau/miniconda3/bin/python
+++ '[' -z x ']'
+++ conda activate base
+++ local cmd=activate
+++ case "$cmd" in
+++ __conda_activate activate base
+++ '[' -n '' ']'
+++ local ask_conda
++++ PS1=
++++ __conda_exe shell.posix activate base
++++ /work/home/lxx_hzau/miniconda3/bin/conda shell.posix activate base
+++ ask_conda='PS1='\''(base) '\''
export PATH='\''/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/.vscode-server/bin/af28b32d7e553898b2a91af498b1fb666fdebe0c/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin'\''
export CONDA_SHLVL='\''1'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export PYTHONPATH='\''/work/home/lxx_hzau/project/FJSP-DRL-main'\'''
+++ eval 'PS1='\''(base) '\''
export PATH='\''/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/.vscode-server/bin/af28b32d7e553898b2a91af498b1fb666fdebe0c/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin'\''
export CONDA_SHLVL='\''1'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export PYTHONPATH='\''/work/home/lxx_hzau/project/FJSP-DRL-main'\'''
++++ PS1='(base) '
++++ export PATH=/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/.vscode-server/bin/af28b32d7e553898b2a91af498b1fb666fdebe0c/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin
++++ PATH=/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/.vscode-server/bin/af28b32d7e553898b2a91af498b1fb666fdebe0c/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin
++++ export CONDA_SHLVL=1
++++ CONDA_SHLVL=1
++++ export 'CONDA_PROMPT_MODIFIER=(base) '
++++ CONDA_PROMPT_MODIFIER='(base) '
++++ export PYTHONPATH=/work/home/lxx_hzau/project/FJSP-DRL-main
++++ PYTHONPATH=/work/home/lxx_hzau/project/FJSP-DRL-main
+++ __conda_hashr
+++ '[' -n '' ']'
+++ '[' -n '' ']'
+++ hash -r
++ unset __conda_setup
++ export PATH=/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/.vscode-server/bin/af28b32d7e553898b2a91af498b1fb666fdebe0c/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin
++ PATH=/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/.vscode-server/bin/af28b32d7e553898b2a91af498b1fb666fdebe0c/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin
+ module load nvidia/cuda/11.6
++ /usr/bin/modulecmd sh load nvidia/cuda/11.6
+ eval CUDA_HOME=/public/software/compiler/nvidia/cuda/11.6.0:/usr/local/cuda-11.1/ ';export' 'CUDA_HOME;CUDA_ROOT=/public/software/compiler/nvidia/cuda/11.6.0' ';export' 'CUDA_ROOT;INCLUDE=/public/software/compiler/nvidia/cuda/11.6.0/include:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/include:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/include:/opt/hpc/software/mpi/hpcx/v2.11.0/sharp/include:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/include' ';export' 'INCLUDE;LD_LIBRARY_PATH=/public/software/compiler/nvidia/cuda/11.6.0/lib64:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/sharp/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/lib:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:' ';export' 'LD_LIBRARY_PATH;LOADEDMODULES=compiler/devtoolset/7.3.1:mpi/hpcx/gcc-7.3.1:nvidia/cuda/11.6' ';export' 'LOADEDMODULES;PATH=/public/software/compiler/nvidia/cuda/11.6.0/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/.vscode-server/bin/af28b32d7e553898b2a91af498b1fb666fdebe0c/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin' ';export' 'PATH;_LMFILES_=/public/software/modules/base/compiler/devtoolset/7.3.1:/public/software/modules/base/mpi/hpcx/gcc-7.3.1:/public/software/modules/base/nvidia/cuda/11.6' ';export' '_LMFILES_;'
++ CUDA_HOME=/public/software/compiler/nvidia/cuda/11.6.0:/usr/local/cuda-11.1/
++ export CUDA_HOME
++ CUDA_ROOT=/public/software/compiler/nvidia/cuda/11.6.0
++ export CUDA_ROOT
++ INCLUDE=/public/software/compiler/nvidia/cuda/11.6.0/include:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/include:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/include:/opt/hpc/software/mpi/hpcx/v2.11.0/sharp/include:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/include
++ export INCLUDE
++ LD_LIBRARY_PATH=/public/software/compiler/nvidia/cuda/11.6.0/lib64:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/sharp/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/lib:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:
++ export LD_LIBRARY_PATH
++ LOADEDMODULES=compiler/devtoolset/7.3.1:mpi/hpcx/gcc-7.3.1:nvidia/cuda/11.6
++ export LOADEDMODULES
++ PATH=/public/software/compiler/nvidia/cuda/11.6.0/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/.vscode-server/bin/af28b32d7e553898b2a91af498b1fb666fdebe0c/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin
++ export PATH
++ _LMFILES_=/public/software/modules/base/compiler/devtoolset/7.3.1:/public/software/modules/base/mpi/hpcx/gcc-7.3.1:/public/software/modules/base/nvidia/cuda/11.6
++ export _LMFILES_
+ conda activate RL-torch
+ local cmd=activate
+ case "$cmd" in
+ __conda_activate activate RL-torch
+ '[' -n '' ']'
+ local ask_conda
++ PS1='(base) '
++ __conda_exe shell.posix activate RL-torch
++ /work/home/lxx_hzau/miniconda3/bin/conda shell.posix activate RL-torch
+ ask_conda='PS1='\''(RL-torch) '\''
export PATH='\''/public/software/compiler/nvidia/cuda/11.6.0/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/.vscode-server/bin/af28b32d7e553898b2a91af498b1fb666fdebe0c/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/envs/RL-torch/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin'\''
export CONDA_PREFIX='\''/work/home/lxx_hzau/miniconda3/envs/RL-torch'\''
export CONDA_SHLVL='\''2'\''
export CONDA_DEFAULT_ENV='\''RL-torch'\''
export CONDA_PROMPT_MODIFIER='\''(RL-torch) '\''
export PYTHONPATH='\''/work/home/lxx_hzau/project/FJSP-DRL-main'\''
export CONDA_PREFIX_1='\''/work/home/lxx_hzau/miniconda3'\''
export CONDA_EXE='\''/work/home/lxx_hzau/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/work/home/lxx_hzau/miniconda3/bin/python'\'''
+ eval 'PS1='\''(RL-torch) '\''
export PATH='\''/public/software/compiler/nvidia/cuda/11.6.0/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/.vscode-server/bin/af28b32d7e553898b2a91af498b1fb666fdebe0c/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/envs/RL-torch/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin'\''
export CONDA_PREFIX='\''/work/home/lxx_hzau/miniconda3/envs/RL-torch'\''
export CONDA_SHLVL='\''2'\''
export CONDA_DEFAULT_ENV='\''RL-torch'\''
export CONDA_PROMPT_MODIFIER='\''(RL-torch) '\''
export PYTHONPATH='\''/work/home/lxx_hzau/project/FJSP-DRL-main'\''
export CONDA_PREFIX_1='\''/work/home/lxx_hzau/miniconda3'\''
export CONDA_EXE='\''/work/home/lxx_hzau/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/work/home/lxx_hzau/miniconda3/bin/python'\'''
++ PS1='(RL-torch) '
++ export PATH=/public/software/compiler/nvidia/cuda/11.6.0/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/.vscode-server/bin/af28b32d7e553898b2a91af498b1fb666fdebe0c/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/envs/RL-torch/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin
++ PATH=/public/software/compiler/nvidia/cuda/11.6.0/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/.vscode-server/bin/af28b32d7e553898b2a91af498b1fb666fdebe0c/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/envs/RL-torch/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin
++ export CONDA_PREFIX=/work/home/lxx_hzau/miniconda3/envs/RL-torch
++ CONDA_PREFIX=/work/home/lxx_hzau/miniconda3/envs/RL-torch
++ export CONDA_SHLVL=2
++ CONDA_SHLVL=2
++ export CONDA_DEFAULT_ENV=RL-torch
++ CONDA_DEFAULT_ENV=RL-torch
++ export 'CONDA_PROMPT_MODIFIER=(RL-torch) '
++ CONDA_PROMPT_MODIFIER='(RL-torch) '
++ export PYTHONPATH=/work/home/lxx_hzau/project/FJSP-DRL-main
++ PYTHONPATH=/work/home/lxx_hzau/project/FJSP-DRL-main
++ export CONDA_PREFIX_1=/work/home/lxx_hzau/miniconda3
++ CONDA_PREFIX_1=/work/home/lxx_hzau/miniconda3
++ export CONDA_EXE=/work/home/lxx_hzau/miniconda3/bin/conda
++ CONDA_EXE=/work/home/lxx_hzau/miniconda3/bin/conda
++ export _CE_M=
++ _CE_M=
++ export _CE_CONDA=
++ _CE_CONDA=
++ export CONDA_PYTHON_EXE=/work/home/lxx_hzau/miniconda3/bin/python
++ CONDA_PYTHON_EXE=/work/home/lxx_hzau/miniconda3/bin/python
+ __conda_hashr
+ '[' -n '' ']'
+ '[' -n '' ']'
+ hash -r
+ t=train
+ ADAPT_NUMS=5
+ TEST_DIR=./test_script
+ exp=exp14
+ echo exp14
exp14
+ echo MAML
MAML
+ logdir_maml=./runs/exp11_maml
+ logdir=./runs/exp14
+ hidden_dim_actor=64
+ hidden_dim_critic=64
+ num_mlp_layers_actor=3
+ num_mlp_layers_critic=3
+ num_envs=4
+ meta_iterations=1000
+ max_updates_maml=1000
+ model_suffix=exp14_1000_64_3
+ n_j_options='15 15 15 15'
+ n_m_options='5  7  9  10'
+ num_tasks=4
+ max_updates_finetune=100
+ lr=0.003
+ data='15,5 15,7 15,9 15,10'
+ echo exp2 maml15x5 SD2
exp2 maml15x5 SD2
+ python train/multi_task_maml_exp14.py --logdir ./runs/exp11_maml --model_suffix exp14_1000_64_3 --maml_model True --meta_iterations 1000 --num_tasks 4 --max_updates 1000 --num_envs 4 --hidden_dim_actor 64 --hidden_dim_critic 64 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --n_j_options 15 15 15 15 --n_m_options 5 7 9 10
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/10x5+mix
save model name:  maml+exp14_1000_64_3
self.n_js [15, 15, 15, 15]
[5, 7, 9, 10]
[(15, 5), (15, 7), (15, 9), (15, 10)]
[827.25, 908.5, 856.75, 923.0]
Episode 1	 reward: -9.10	 Mean_loss: 1.25358188,  training time: 16.16
[862.0, 883.75, 1000.75, 930.0]
Episode 2	 reward: -9.45	 Mean_loss: 1.31251490,  training time: 13.55
[832.5, 860.25, 869.5, 905.5]
Episode 3	 reward: -10.09	 Mean_loss: 1.17114794,  training time: 13.54
[791.0, 887.5, 896.25, 957.75]
Episode 4	 reward: -9.64	 Mean_loss: 1.36482489,  training time: 13.52
[872.25, 910.25, 893.75, 933.5]
Episode 5	 reward: -9.49	 Mean_loss: 1.23727334,  training time: 13.59
[862.5, 872.0, 848.0, 926.25]
Episode 6	 reward: -9.53	 Mean_loss: 1.16478384,  training time: 13.35
[850.5, 915.25, 891.25, 935.5]
Episode 7	 reward: -9.40	 Mean_loss: 1.18441391,  training time: 13.31
[848.0, 797.0, 906.0, 871.0]
Episode 8	 reward: -9.24	 Mean_loss: 0.92027020,  training time: 13.31
[790.5, 896.75, 895.75, 882.75]
Episode 9	 reward: -10.15	 Mean_loss: 1.01195502,  training time: 13.32
[867.5, 901.25, 868.25, 903.5]
Episode 10	 reward: -9.53	 Mean_loss: 1.05341578,  training time: 13.31
[874.75, 849.0, 876.25, 879.75]
Episode 11	 reward: -9.37	 Mean_loss: 0.98967081,  training time: 13.31
[815.5, 966.25, 967.5, 902.75]
Episode 12	 reward: -9.44	 Mean_loss: 1.05329967,  training time: 13.33
[789.0, 842.5, 862.25, 993.75]
Episode 13	 reward: -9.26	 Mean_loss: 1.47917569,  training time: 13.34
[812.75, 867.75, 868.5, 928.75]
Episode 14	 reward: -9.34	 Mean_loss: 1.13633680,  training time: 13.35
[778.5, 879.75, 848.5, 928.5]
Episode 15	 reward: -8.96	 Mean_loss: 1.28557193,  training time: 13.35
[847.75, 847.0, 922.5, 976.25]
Episode 16	 reward: -9.44	 Mean_loss: 1.40804839,  training time: 13.35
[831.0, 806.25, 906.0, 911.75]
Episode 17	 reward: -9.85	 Mean_loss: 1.07667255,  training time: 13.35
[844.5, 850.25, 915.0, 879.25]
Episode 18	 reward: -9.86	 Mean_loss: 0.95579135,  training time: 13.38
[854.5, 836.75, 947.25, 954.0]
Episode 19	 reward: -9.66	 Mean_loss: 1.27152777,  training time: 13.32
[846.25, 858.0, 879.75, 919.25]
Episode 20	 reward: -9.71	 Mean_loss: 1.08726037,  training time: 13.35
[(15, 5), (15, 7), (15, 9), (15, 10)]
[850.25, 946.25, 888.25, 903.5]
Episode 21	 reward: -9.03	 Mean_loss: 1.02873445,  training time: 13.49
[887.25, 928.0, 945.75, 912.75]
Episode 22	 reward: -9.84	 Mean_loss: 1.12799597,  training time: 13.39
[883.0, 1000.25, 863.0, 1011.5]
Episode 23	 reward: -9.49	 Mean_loss: 1.65669656,  training time: 13.29
[943.5, 861.0, 873.5, 922.75]
Episode 24	 reward: -9.19	 Mean_loss: 1.21764016,  training time: 13.31
[914.5, 963.0, 934.75, 955.5]
Episode 25	 reward: -9.70	 Mean_loss: 1.33763683,  training time: 13.33
[808.5, 935.0, 908.5, 1015.25]
Episode 26	 reward: -9.72	 Mean_loss: 1.75310791,  training time: 13.32
[837.25, 906.25, 876.5, 918.5]
Episode 27	 reward: -9.71	 Mean_loss: 1.11888230,  training time: 13.32
[826.75, 873.0, 901.25, 916.5]
Episode 28	 reward: -9.34	 Mean_loss: 1.11092389,  training time: 13.32
[822.5, 964.0, 938.0, 947.25]
Episode 29	 reward: -9.67	 Mean_loss: 1.26102626,  training time: 13.31
[850.25, 897.0, 858.5, 958.0]
Episode 30	 reward: -8.89	 Mean_loss: 1.29052579,  training time: 13.31
[844.5, 896.75, 884.25, 947.25]
Episode 31	 reward: -10.01	 Mean_loss: 1.20435345,  training time: 13.30
[888.0, 859.25, 900.5, 910.5]
Episode 32	 reward: -9.99	 Mean_loss: 1.07769001,  training time: 13.34
[867.75, 913.0, 949.5, 887.5]
Episode 33	 reward: -9.32	 Mean_loss: 1.01098132,  training time: 13.34
[867.25, 935.75, 935.5, 951.75]
Episode 34	 reward: -9.05	 Mean_loss: 1.23331571,  training time: 13.34
[852.0, 986.75, 886.75, 903.0]
Episode 35	 reward: -9.73	 Mean_loss: 1.00918233,  training time: 13.32
[886.25, 910.0, 864.0, 933.75]
Episode 36	 reward: -9.45	 Mean_loss: 1.09724891,  training time: 13.32
[842.25, 949.5, 947.5, 933.75]
Episode 37	 reward: -9.64	 Mean_loss: 1.15207779,  training time: 13.30
[814.5, 869.25, 888.25, 903.0]
Episode 38	 reward: -9.41	 Mean_loss: 1.04494703,  training time: 13.35
[880.0, 906.25, 942.25, 883.5]
Episode 39	 reward: -9.29	 Mean_loss: 0.94213241,  training time: 13.41
[832.25, 954.75, 886.0, 946.25]
Episode 40	 reward: -10.36	 Mean_loss: 1.24974060,  training time: 13.19
[(15, 5), (15, 7), (15, 9), (15, 10)]
[885.5, 858.25, 861.5, 935.0]
Episode 41	 reward: -10.18	 Mean_loss: 1.06313789,  training time: 13.42
[890.0, 861.75, 891.0, 964.0]
Episode 42	 reward: -9.71	 Mean_loss: 1.15362883,  training time: 13.19
[843.0, 894.75, 933.0, 917.25]
Episode 43	 reward: -9.51	 Mean_loss: 1.02655196,  training time: 13.31
[886.25, 881.25, 938.0, 923.25]
Episode 44	 reward: -9.79	 Mean_loss: 1.00650537,  training time: 13.19
[816.25, 912.75, 907.5, 916.25]
Episode 45	 reward: -9.85	 Mean_loss: 0.99828190,  training time: 13.21
[859.5, 892.0, 909.0, 902.0]
Episode 46	 reward: -9.54	 Mean_loss: 0.90947056,  training time: 13.22
[824.75, 830.75, 872.75, 952.5]
Episode 47	 reward: -9.85	 Mean_loss: 1.10632324,  training time: 13.28
[809.5, 956.5, 862.75, 941.0]
Episode 48	 reward: -9.39	 Mean_loss: 1.01620972,  training time: 13.23
[851.5, 795.75, 880.5, 862.0]
Episode 49	 reward: -9.18	 Mean_loss: 0.75980645,  training time: 13.24
[874.25, 874.25, 898.75, 942.5]
Episode 50	 reward: -9.66	 Mean_loss: 1.07288051,  training time: 13.21
[877.5, 879.75, 904.75, 886.0]
Episode 51	 reward: -9.34	 Mean_loss: 0.82257742,  training time: 13.22
[799.5, 943.25, 886.25, 870.5]
Episode 52	 reward: -9.34	 Mean_loss: 0.77469516,  training time: 13.22
[895.0, 867.25, 888.75, 910.5]
Episode 53	 reward: -9.16	 Mean_loss: 0.85270864,  training time: 13.18
[789.0, 873.0, 856.5, 874.5]
Episode 54	 reward: -9.94	 Mean_loss: 0.80154788,  training time: 13.26
[870.75, 834.5, 913.5, 994.25]
Episode 55	 reward: -9.71	 Mean_loss: 1.34101701,  training time: 13.32
[845.25, 923.75, 863.5, 872.75]
Episode 56	 reward: -9.34	 Mean_loss: 0.77492177,  training time: 13.35
[842.25, 938.25, 869.0, 884.75]
Episode 57	 reward: -9.73	 Mean_loss: 0.76502776,  training time: 13.34
[815.5, 845.0, 903.0, 897.0]
Episode 58	 reward: -9.35	 Mean_loss: 0.79510611,  training time: 13.31
[841.75, 894.75, 870.5, 990.0]
Episode 59	 reward: -9.80	 Mean_loss: 1.18050671,  training time: 13.35
[848.0, 943.25, 921.25, 913.0]
Episode 60	 reward: -9.79	 Mean_loss: 0.80389118,  training time: 13.33
[(15, 5), (15, 7), (15, 9), (15, 10)]
[962.5, 835.0, 858.25, 905.0]
Episode 61	 reward: -9.08	 Mean_loss: 0.90534365,  training time: 13.38
[841.75, 916.5, 838.75, 973.75]
Episode 62	 reward: -10.04	 Mean_loss: 1.09041488,  training time: 13.32
[880.5, 841.25, 878.75, 916.75]
Episode 63	 reward: -9.08	 Mean_loss: 0.79641199,  training time: 13.32
[920.5, 851.25, 843.25, 916.5]
Episode 64	 reward: -10.20	 Mean_loss: 0.81879163,  training time: 13.43
[843.5, 852.5, 887.25, 908.0]
Episode 65	 reward: -9.93	 Mean_loss: 0.76354605,  training time: 13.33
[944.25, 856.5, 993.0, 900.75]
Episode 66	 reward: -9.68	 Mean_loss: 0.76501811,  training time: 13.34
[858.5, 847.75, 867.0, 875.0]
Episode 67	 reward: -9.56	 Mean_loss: 0.68679905,  training time: 13.33
[847.75, 905.5, 945.75, 938.25]
Episode 68	 reward: -9.59	 Mean_loss: 0.86980349,  training time: 13.33
[837.75, 840.5, 947.0, 968.25]
Episode 69	 reward: -9.49	 Mean_loss: 1.08411562,  training time: 13.30
[885.0, 864.25, 870.5, 965.25]
Episode 70	 reward: -9.78	 Mean_loss: 1.04463291,  training time: 13.35
[855.5, 858.25, 896.0, 956.0]
Episode 71	 reward: -9.68	 Mean_loss: 0.92151022,  training time: 13.52
[847.0, 849.5, 869.25, 930.5]
Episode 72	 reward: -9.73	 Mean_loss: 0.80315036,  training time: 13.49
[826.75, 836.0, 821.0, 986.5]
Episode 73	 reward: -9.74	 Mean_loss: 0.97350812,  training time: 13.51
[843.25, 925.25, 888.5, 953.5]
Episode 74	 reward: -9.48	 Mean_loss: 0.88706237,  training time: 13.51
[849.5, 932.0, 880.0, 957.75]
Episode 75	 reward: -9.75	 Mean_loss: 0.87087262,  training time: 13.45
[845.75, 800.0, 876.25, 886.75]
Episode 76	 reward: -9.49	 Mean_loss: 0.65132934,  training time: 13.28
[864.0, 900.75, 871.0, 919.0]
Episode 77	 reward: -9.79	 Mean_loss: 0.68971926,  training time: 13.28
[915.25, 904.75, 858.0, 987.25]
Episode 78	 reward: -10.04	 Mean_loss: 0.97768366,  training time: 13.33
[844.5, 867.25, 844.75, 934.0]
Episode 79	 reward: -9.77	 Mean_loss: 0.78973603,  training time: 13.27
[870.5, 834.25, 888.0, 971.25]
Episode 80	 reward: -9.18	 Mean_loss: 0.80554944,  training time: 13.52
[(15, 5), (15, 7), (15, 9), (15, 10)]
[874.75, 847.0, 853.25, 900.0]
Episode 81	 reward: -9.55	 Mean_loss: 0.51131451,  training time: 13.56
[834.0, 783.25, 898.75, 918.0]
Episode 82	 reward: -8.94	 Mean_loss: 0.54987508,  training time: 13.53
[829.5, 837.75, 901.75, 986.5]
Episode 83	 reward: -9.10	 Mean_loss: 0.82584506,  training time: 13.53
[863.0, 848.5, 856.5, 956.5]
Episode 84	 reward: -9.13	 Mean_loss: 0.65768427,  training time: 13.49
[893.0, 812.5, 914.5, 885.0]
Episode 85	 reward: -10.04	 Mean_loss: 0.46162471,  training time: 13.62
[798.0, 842.5, 833.25, 835.0]
Episode 86	 reward: -10.14	 Mean_loss: 0.32476890,  training time: 13.53
[858.5, 808.25, 883.75, 891.75]
Episode 87	 reward: -9.07	 Mean_loss: 0.39945880,  training time: 13.50
[767.25, 839.5, 871.5, 973.0]
Episode 88	 reward: -9.34	 Mean_loss: 0.70343918,  training time: 13.51
[863.25, 829.25, 859.0, 849.75]
Episode 89	 reward: -8.96	 Mean_loss: 0.33713394,  training time: 13.50
[791.0, 765.5, 861.75, 903.5]
Episode 90	 reward: -10.12	 Mean_loss: 0.42351773,  training time: 13.51
[847.25, 819.25, 853.5, 920.25]
Episode 91	 reward: -9.77	 Mean_loss: 0.46390602,  training time: 13.51
[807.75, 883.0, 880.0, 877.0]
Episode 92	 reward: -8.97	 Mean_loss: 0.36899400,  training time: 13.53
[785.75, 794.0, 842.25, 930.75]
Episode 93	 reward: -9.79	 Mean_loss: 0.42207611,  training time: 13.52
[783.75, 775.75, 867.5, 1010.25]
Episode 94	 reward: -9.94	 Mean_loss: 0.76812041,  training time: 13.54
[844.0, 853.75, 849.75, 943.0]
Episode 95	 reward: -9.27	 Mean_loss: 0.51957518,  training time: 13.55
[804.75, 792.75, 835.0, 951.75]
Episode 96	 reward: -9.32	 Mean_loss: 0.50441068,  training time: 13.53
[800.75, 823.75, 919.25, 916.5]
Episode 97	 reward: -9.26	 Mean_loss: 0.40153623,  training time: 13.15
[831.75, 905.75, 885.75, 985.0]
Episode 98	 reward: -9.55	 Mean_loss: 0.60105258,  training time: 13.12
[799.0, 804.5, 885.5, 896.75]
Episode 99	 reward: -8.91	 Mean_loss: 0.35545772,  training time: 13.13
[832.0, 842.75, 851.75, 878.25]
Episode 100	 reward: -9.83	 Mean_loss: 0.34587771,  training time: 13.14
[(15, 5), (15, 7), (15, 9), (15, 10)]
[799.5, 845.25, 857.0, 875.75]
Episode 101	 reward: -9.57	 Mean_loss: 0.35976025,  training time: 13.19
[866.75, 874.25, 904.0, 899.25]
Episode 102	 reward: -10.10	 Mean_loss: 0.38786814,  training time: 13.13
[742.75, 843.25, 931.25, 865.5]
Episode 103	 reward: -9.49	 Mean_loss: 0.28502172,  training time: 13.24
[783.0, 855.75, 896.0, 908.5]
Episode 104	 reward: -9.28	 Mean_loss: 0.36077517,  training time: 13.35
[828.25, 809.0, 933.0, 889.0]
Episode 105	 reward: -9.08	 Mean_loss: 0.38929987,  training time: 13.25
[837.0, 860.25, 827.5, 935.5]
Episode 106	 reward: -9.38	 Mean_loss: 0.39209867,  training time: 13.34
[832.25, 869.5, 918.25, 837.5]
Episode 107	 reward: -9.15	 Mean_loss: 0.25738996,  training time: 13.35
[798.25, 832.5, 902.0, 858.25]
Episode 108	 reward: -9.32	 Mean_loss: 0.22991814,  training time: 13.20
[817.5, 812.75, 839.5, 877.5]
Episode 109	 reward: -9.13	 Mean_loss: 0.27799717,  training time: 13.19
[780.25, 814.75, 846.0, 889.75]
Episode 110	 reward: -9.73	 Mean_loss: 0.27033871,  training time: 13.11
[826.0, 826.25, 926.75, 841.75]
Episode 111	 reward: -9.19	 Mean_loss: 0.21807915,  training time: 13.13
[800.75, 829.25, 893.5, 890.25]
Episode 112	 reward: -9.36	 Mean_loss: 0.25878027,  training time: 13.13
[817.75, 900.75, 875.5, 905.0]
Episode 113	 reward: -9.32	 Mean_loss: 0.25949878,  training time: 13.10
[780.5, 760.75, 891.0, 937.75]
Episode 114	 reward: -9.53	 Mean_loss: 0.40425754,  training time: 13.14
[822.5, 826.5, 831.25, 839.0]
Episode 115	 reward: -9.03	 Mean_loss: 0.17437264,  training time: 13.15
[783.25, 890.25, 892.0, 838.25]
Episode 116	 reward: -9.46	 Mean_loss: 0.12563850,  training time: 13.16
[791.75, 852.0, 938.25, 888.25]
Episode 117	 reward: -9.65	 Mean_loss: 0.17463107,  training time: 13.24
[792.5, 816.0, 927.0, 877.75]
Episode 118	 reward: -9.64	 Mean_loss: 0.16914003,  training time: 13.24
[786.75, 894.0, 844.0, 937.0]
Episode 119	 reward: -9.01	 Mean_loss: 0.22834972,  training time: 13.25
[827.75, 814.75, 885.25, 923.75]
Episode 120	 reward: -9.11	 Mean_loss: 0.23127799,  training time: 13.24
[(15, 5), (15, 7), (15, 9), (15, 10)]
[807.25, 898.0, 869.5, 806.0]
Episode 121	 reward: -8.96	 Mean_loss: 0.12367915,  training time: 13.28
[834.25, 912.5, 871.5, 844.75]
Episode 122	 reward: -9.26	 Mean_loss: 0.13580205,  training time: 13.57
[755.25, 868.5, 880.75, 840.25]
Episode 123	 reward: -9.43	 Mean_loss: 0.14974964,  training time: 13.47
[759.0, 804.75, 877.25, 896.75]
Episode 124	 reward: -9.49	 Mean_loss: 0.18098986,  training time: 13.51
[802.75, 834.75, 872.0, 887.75]
Episode 125	 reward: -8.39	 Mean_loss: 0.21089920,  training time: 13.48
[797.0, 817.5, 815.25, 897.75]
Episode 126	 reward: -8.57	 Mean_loss: 0.23790121,  training time: 13.35
[806.25, 825.25, 845.5, 792.0]
Episode 127	 reward: -8.85	 Mean_loss: 0.09377947,  training time: 13.32
[754.25, 852.5, 860.0, 846.0]
Episode 128	 reward: -9.79	 Mean_loss: 0.15011890,  training time: 13.23
[790.0, 820.25, 908.75, 853.75]
Episode 129	 reward: -9.20	 Mean_loss: 0.12017021,  training time: 13.26
[767.25, 829.25, 848.5, 855.0]
Episode 130	 reward: -8.45	 Mean_loss: 0.13048682,  training time: 13.33
[764.75, 848.5, 920.5, 833.75]
Episode 131	 reward: -8.74	 Mean_loss: 0.11452557,  training time: 13.25
[753.75, 832.25, 862.75, 840.5]
Episode 132	 reward: -8.43	 Mean_loss: 0.10689476,  training time: 13.32
[773.0, 814.0, 805.0, 804.75]
Episode 133	 reward: -8.92	 Mean_loss: 0.11302039,  training time: 13.28
[805.5, 824.75, 880.25, 830.75]
Episode 134	 reward: -8.74	 Mean_loss: 0.10954984,  training time: 13.25
[805.0, 881.25, 858.0, 883.5]
Episode 135	 reward: -8.52	 Mean_loss: 0.18742231,  training time: 13.22
[750.25, 855.75, 885.5, 798.0]
Episode 136	 reward: -8.89	 Mean_loss: 0.11571629,  training time: 13.25
[788.5, 812.5, 838.5, 868.5]
Episode 137	 reward: -9.04	 Mean_loss: 0.14761342,  training time: 13.27
[783.25, 841.0, 895.25, 881.5]
Episode 138	 reward: -9.21	 Mean_loss: 0.13293478,  training time: 13.22
[781.0, 882.0, 815.25, 862.25]
Episode 139	 reward: -8.78	 Mean_loss: 0.13724530,  training time: 13.57
[797.0, 808.0, 862.75, 832.5]
Episode 140	 reward: -9.19	 Mean_loss: 0.12767316,  training time: 13.27
[(15, 5), (15, 7), (15, 9), (15, 10)]
[834.25, 839.0, 912.5, 848.25]
Episode 141	 reward: -9.38	 Mean_loss: 0.11204913,  training time: 13.28
[780.25, 829.25, 900.25, 951.0]
Episode 142	 reward: -8.95	 Mean_loss: 0.18154255,  training time: 13.23
[780.0, 880.5, 829.75, 891.75]
Episode 143	 reward: -9.42	 Mean_loss: 0.14592594,  training time: 13.23
[780.0, 850.25, 863.0, 888.0]
Episode 144	 reward: -9.34	 Mean_loss: 0.10167271,  training time: 13.25
[812.0, 814.75, 896.0, 876.0]
Episode 145	 reward: -9.09	 Mean_loss: 0.12975416,  training time: 13.87
[801.5, 828.5, 822.0, 883.75]
Episode 146	 reward: -8.93	 Mean_loss: 0.09217340,  training time: 13.34
[767.75, 819.0, 845.25, 902.0]
Episode 147	 reward: -9.01	 Mean_loss: 0.20887473,  training time: 13.27
[803.0, 800.75, 959.25, 954.0]
Episode 148	 reward: -8.69	 Mean_loss: 0.10956875,  training time: 13.38
[748.25, 791.25, 871.0, 992.25]
Episode 149	 reward: -9.57	 Mean_loss: 0.22452983,  training time: 13.46
[781.0, 792.5, 850.0, 877.0]
Episode 150	 reward: -9.27	 Mean_loss: 0.15174679,  training time: 13.49
[806.75, 851.0, 883.0, 950.75]
Episode 151	 reward: -9.25	 Mean_loss: 0.17008348,  training time: 13.46
[778.25, 885.5, 844.5, 940.5]
Episode 152	 reward: -9.04	 Mean_loss: 0.15152597,  training time: 13.30
[840.0, 839.5, 940.75, 889.25]
Episode 153	 reward: -9.27	 Mean_loss: 0.12183831,  training time: 13.27
[789.0, 799.0, 837.0, 839.5]
Episode 154	 reward: -9.53	 Mean_loss: 0.13718982,  training time: 13.26
[781.75, 870.25, 819.75, 897.25]
Episode 155	 reward: -9.40	 Mean_loss: 0.14088976,  training time: 13.39
[782.75, 822.5, 871.25, 892.25]
Episode 156	 reward: -8.96	 Mean_loss: 0.09150821,  training time: 13.45
[755.5, 799.5, 868.25, 859.0]
Episode 157	 reward: -9.33	 Mean_loss: 0.15536462,  training time: 13.41
[756.0, 809.75, 881.5, 930.75]
Episode 158	 reward: -8.64	 Mean_loss: 0.09961443,  training time: 13.48
[794.25, 819.5, 854.5, 889.75]
Episode 159	 reward: -9.70	 Mean_loss: 0.13127653,  training time: 13.36
[806.0, 803.75, 822.75, 874.75]
Episode 160	 reward: -9.81	 Mean_loss: 0.11409569,  training time: 13.49
[(15, 5), (15, 7), (15, 9), (15, 10)]
[770.75, 923.5, 845.0, 872.5]
Episode 161	 reward: -9.24	 Mean_loss: 0.12502134,  training time: 13.40
[814.75, 823.0, 798.25, 826.0]
Episode 162	 reward: -9.09	 Mean_loss: 0.11676511,  training time: 13.43
[743.75, 837.0, 823.0, 901.25]
Episode 163	 reward: -9.38	 Mean_loss: 0.10673625,  training time: 13.47
[819.0, 866.25, 817.0, 880.75]
Episode 164	 reward: -9.74	 Mean_loss: 0.12375667,  training time: 13.45
[822.0, 918.0, 845.75, 894.0]
Episode 165	 reward: -9.20	 Mean_loss: 0.21631560,  training time: 13.47
[781.75, 843.75, 825.0, 867.75]
Episode 166	 reward: -9.04	 Mean_loss: 0.10464364,  training time: 13.47
[813.25, 867.0, 841.5, 892.0]
Episode 167	 reward: -9.49	 Mean_loss: 0.06320661,  training time: 13.49
[772.0, 852.0, 855.5, 800.75]
Episode 168	 reward: -9.44	 Mean_loss: 0.21827935,  training time: 13.44
[853.75, 856.75, 828.25, 847.25]
Episode 169	 reward: -9.11	 Mean_loss: 0.14594610,  training time: 13.53
[769.75, 899.0, 825.0, 775.25]
Episode 170	 reward: -9.44	 Mean_loss: 0.21145378,  training time: 13.45
[752.5, 788.75, 827.25, 900.25]
Episode 171	 reward: -9.35	 Mean_loss: 0.10620602,  training time: 13.51
[780.5, 935.0, 831.25, 934.75]
Episode 172	 reward: -8.84	 Mean_loss: 0.10755929,  training time: 13.45
[779.5, 828.75, 853.5, 891.75]
Episode 173	 reward: -9.20	 Mean_loss: 0.12709993,  training time: 13.49
[812.5, 903.0, 866.25, 852.5]
Episode 174	 reward: -8.87	 Mean_loss: 0.13442831,  training time: 13.46
[757.75, 873.5, 810.75, 844.0]
Episode 175	 reward: -9.58	 Mean_loss: 0.17993248,  training time: 13.40
[786.75, 891.0, 819.25, 964.75]
Episode 176	 reward: -8.81	 Mean_loss: 0.16683531,  training time: 13.21
[746.5, 808.0, 960.5, 819.25]
Episode 177	 reward: -9.94	 Mean_loss: 0.17597874,  training time: 13.21
[792.5, 901.0, 847.0, 870.0]
Episode 178	 reward: -9.20	 Mean_loss: 0.13958046,  training time: 13.20
[723.5, 803.75, 833.5, 871.75]
Episode 179	 reward: -9.90	 Mean_loss: 0.16963387,  training time: 13.14
[809.75, 858.0, 783.5, 842.75]
Episode 180	 reward: -9.63	 Mean_loss: 0.14421612,  training time: 13.12
[(15, 5), (15, 7), (15, 9), (15, 10)]
[864.0, 846.5, 821.75, 857.25]
Episode 181	 reward: -9.86	 Mean_loss: 0.15497062,  training time: 13.08
[829.25, 774.5, 856.0, 856.0]
Episode 182	 reward: -8.95	 Mean_loss: 0.15719499,  training time: 13.07
[825.0, 790.25, 901.25, 875.25]
Episode 183	 reward: -9.21	 Mean_loss: 0.15049179,  training time: 13.02
[821.0, 863.5, 816.75, 885.25]
Episode 184	 reward: -9.35	 Mean_loss: 0.10342580,  training time: 13.24
[800.25, 765.5, 816.75, 879.5]
Episode 185	 reward: -9.21	 Mean_loss: 0.15037078,  training time: 13.35
[850.25, 789.5, 894.75, 862.0]
Episode 186	 reward: -9.32	 Mean_loss: 0.17377332,  training time: 13.39
[787.75, 830.75, 924.75, 901.25]
Episode 187	 reward: -9.30	 Mean_loss: 0.08780111,  training time: 13.38
[830.5, 815.75, 822.75, 841.25]
Episode 188	 reward: -9.34	 Mean_loss: 0.16935293,  training time: 13.37
[800.5, 759.25, 850.25, 898.25]
Episode 189	 reward: -9.44	 Mean_loss: 0.12298172,  training time: 13.39
[856.25, 805.5, 842.25, 893.0]
Episode 190	 reward: -9.44	 Mean_loss: 0.11507487,  training time: 13.43
[859.25, 786.75, 867.0, 877.25]
Episode 191	 reward: -9.31	 Mean_loss: 0.13819431,  training time: 13.37
[881.0, 819.25, 839.5, 829.25]
Episode 192	 reward: -8.96	 Mean_loss: 0.19260396,  training time: 13.38
[793.0, 783.75, 856.5, 904.25]
Episode 193	 reward: -9.12	 Mean_loss: 0.12678491,  training time: 13.21
[846.0, 838.25, 932.0, 914.75]
Episode 194	 reward: -8.82	 Mean_loss: 0.08156098,  training time: 13.18
[832.0, 843.75, 849.25, 849.0]
Episode 195	 reward: -9.55	 Mean_loss: 0.17164119,  training time: 13.19
[765.25, 816.75, 858.5, 890.0]
Episode 196	 reward: -9.44	 Mean_loss: 0.10744622,  training time: 13.17
[813.75, 805.5, 888.75, 818.75]
Episode 197	 reward: -8.72	 Mean_loss: 0.17857802,  training time: 13.16
[874.0, 824.25, 850.25, 810.0]
Episode 198	 reward: -9.11	 Mean_loss: 0.19739561,  training time: 13.41
[797.75, 847.25, 914.25, 836.75]
Episode 199	 reward: -9.11	 Mean_loss: 0.17858748,  training time: 13.39
[798.5, 792.75, 862.25, 873.5]
Episode 200	 reward: -9.69	 Mean_loss: 0.15627663,  training time: 13.40
[(15, 5), (15, 7), (15, 9), (15, 10)]
[773.0, 796.5, 831.0, 788.75]
Episode 201	 reward: -9.25	 Mean_loss: 0.18800353,  training time: 13.47
[858.25, 811.75, 790.5, 877.75]
Episode 202	 reward: -10.07	 Mean_loss: 0.14582931,  training time: 13.41
[850.75, 839.5, 907.0, 871.75]
Episode 203	 reward: -9.55	 Mean_loss: 0.15267597,  training time: 13.38
[810.5, 823.75, 890.25, 879.5]
Episode 204	 reward: -10.01	 Mean_loss: 0.15546866,  training time: 13.26
[826.75, 886.5, 838.0, 888.25]
Episode 205	 reward: -9.95	 Mean_loss: 0.17770138,  training time: 13.44
[823.25, 785.5, 880.5, 859.75]
Episode 206	 reward: -9.32	 Mean_loss: 0.18194064,  training time: 13.49
[809.5, 851.0, 825.25, 873.5]
Episode 207	 reward: -9.35	 Mean_loss: 0.19995862,  training time: 13.36
[855.25, 866.75, 865.75, 856.0]
Episode 208	 reward: -9.03	 Mean_loss: 0.12095782,  training time: 13.33
[746.75, 827.25, 807.75, 852.75]
Episode 209	 reward: -9.71	 Mean_loss: 0.21806066,  training time: 13.17
[797.0, 882.5, 865.75, 847.0]
Episode 210	 reward: -8.86	 Mean_loss: 0.10174198,  training time: 13.16
[758.25, 940.75, 856.5, 932.0]
Episode 211	 reward: -10.28	 Mean_loss: 0.20394890,  training time: 13.27
[783.25, 845.0, 880.25, 832.5]
Episode 212	 reward: -8.50	 Mean_loss: 0.15074018,  training time: 13.17
[784.0, 818.5, 768.75, 899.5]
Episode 213	 reward: -9.37	 Mean_loss: 0.22673278,  training time: 13.16
[810.75, 871.0, 832.5, 910.25]
Episode 214	 reward: -9.47	 Mean_loss: 0.11908539,  training time: 13.18
[808.75, 865.75, 850.75, 864.25]
Episode 215	 reward: -9.92	 Mean_loss: 0.19916153,  training time: 13.59
[798.25, 919.25, 849.25, 885.5]
Episode 216	 reward: -9.84	 Mean_loss: 0.14423992,  training time: 13.40
[726.75, 823.5, 815.25, 849.75]
Episode 217	 reward: -9.27	 Mean_loss: 0.15944046,  training time: 13.39
[837.25, 831.5, 865.25, 870.75]
Episode 218	 reward: -9.68	 Mean_loss: 0.15739354,  training time: 13.40
[795.75, 809.75, 876.75, 869.75]
Episode 219	 reward: -9.98	 Mean_loss: 0.11255665,  training time: 13.41
[775.75, 849.75, 889.5, 893.25]
Episode 220	 reward: -9.52	 Mean_loss: 0.16380928,  training time: 13.42
[(15, 5), (15, 7), (15, 9), (15, 10)]
[816.75, 771.75, 787.75, 877.25]
Episode 221	 reward: -9.41	 Mean_loss: 0.16344786,  training time: 13.46
[782.0, 806.25, 864.25, 921.75]
Episode 222	 reward: -10.42	 Mean_loss: 0.12701803,  training time: 13.42
[782.25, 820.75, 834.25, 899.75]
Episode 223	 reward: -10.17	 Mean_loss: 0.11702202,  training time: 13.43
[844.25, 782.25, 890.0, 812.25]
Episode 224	 reward: -9.55	 Mean_loss: 0.19814767,  training time: 13.42
[746.75, 824.0, 917.5, 898.5]
Episode 225	 reward: -9.31	 Mean_loss: 0.11777586,  training time: 13.39
[791.0, 826.0, 878.5, 900.0]
Episode 226	 reward: -9.66	 Mean_loss: 0.18678924,  training time: 13.41
[781.25, 774.0, 822.75, 849.0]
Episode 227	 reward: -10.04	 Mean_loss: 0.14496349,  training time: 13.28
[777.75, 818.75, 872.25, 986.25]
Episode 228	 reward: -10.06	 Mean_loss: 0.15209769,  training time: 13.11
[787.5, 821.75, 885.0, 817.25]
Episode 229	 reward: -9.68	 Mean_loss: 0.15470518,  training time: 13.12
[777.75, 845.25, 881.75, 890.75]
Episode 230	 reward: -9.40	 Mean_loss: 0.17158191,  training time: 13.09
[773.5, 865.25, 887.5, 874.5]
Episode 231	 reward: -9.30	 Mean_loss: 0.16562997,  training time: 13.11
[772.5, 818.5, 862.25, 852.75]
Episode 232	 reward: -9.79	 Mean_loss: 0.15440623,  training time: 13.28
[808.25, 804.25, 834.5, 847.25]
Episode 233	 reward: -9.87	 Mean_loss: 0.16543826,  training time: 13.50
[808.75, 801.25, 872.75, 912.25]
Episode 234	 reward: -9.27	 Mean_loss: 0.12337112,  training time: 13.23
[800.5, 770.25, 874.5, 922.75]
Episode 235	 reward: -8.64	 Mean_loss: 0.15168956,  training time: 13.16
[782.75, 907.25, 866.0, 877.75]
Episode 236	 reward: -9.54	 Mean_loss: 0.13233989,  training time: 13.21
[773.75, 821.75, 832.5, 833.25]
Episode 237	 reward: -10.15	 Mean_loss: 0.18802841,  training time: 12.99
[821.75, 797.0, 875.0, 833.0]
Episode 238	 reward: -9.48	 Mean_loss: 0.16956797,  training time: 13.14
[826.25, 762.5, 855.5, 855.0]
Episode 239	 reward: -9.74	 Mean_loss: 0.16240324,  training time: 13.18
[764.75, 828.25, 879.5, 870.5]
Episode 240	 reward: -9.70	 Mean_loss: 0.11561857,  training time: 13.32
[(15, 5), (15, 7), (15, 9), (15, 10)]
[883.75, 810.0, 844.75, 851.25]
Episode 241	 reward: -9.45	 Mean_loss: 0.16321015,  training time: 13.07
[774.0, 798.25, 822.75, 902.5]
Episode 242	 reward: -9.65	 Mean_loss: 0.09893936,  training time: 13.04
[753.25, 823.0, 819.5, 854.5]
Episode 243	 reward: -9.00	 Mean_loss: 0.16607413,  training time: 13.04
[737.25, 829.75, 822.5, 803.5]
Episode 244	 reward: -9.99	 Mean_loss: 0.20727244,  training time: 13.01
[776.75, 836.0, 787.5, 925.75]
Episode 245	 reward: -8.93	 Mean_loss: 0.13089602,  training time: 13.03
[786.75, 846.75, 821.75, 897.5]
Episode 246	 reward: -9.15	 Mean_loss: 0.16168506,  training time: 13.00
[800.0, 832.25, 827.75, 858.5]
Episode 247	 reward: -9.36	 Mean_loss: 0.15287533,  training time: 13.01
[745.25, 802.0, 855.0, 878.5]
Episode 248	 reward: -9.41	 Mean_loss: 0.13230321,  training time: 13.02
[751.25, 839.0, 826.25, 825.5]
Episode 249	 reward: -8.85	 Mean_loss: 0.17215964,  training time: 13.13
[825.0, 793.5, 782.75, 840.75]
Episode 250	 reward: -9.21	 Mean_loss: 0.14225672,  training time: 13.44
[831.75, 788.25, 844.25, 904.75]
Episode 251	 reward: -9.59	 Mean_loss: 0.09031891,  training time: 13.12
[779.25, 741.25, 845.75, 875.0]
Episode 252	 reward: -9.51	 Mean_loss: 0.16661315,  training time: 13.12
[777.75, 824.75, 861.75, 845.25]
Episode 253	 reward: -9.81	 Mean_loss: 0.16746394,  training time: 13.18
[808.25, 822.5, 789.0, 909.5]
Episode 254	 reward: -8.98	 Mean_loss: 0.14608774,  training time: 13.33
[802.25, 854.5, 822.25, 851.0]
Episode 255	 reward: -9.24	 Mean_loss: 0.16660328,  training time: 13.16
[819.0, 786.0, 762.25, 839.5]
Episode 256	 reward: -9.05	 Mean_loss: 0.14546174,  training time: 13.16
[756.75, 850.5, 879.5, 843.5]
Episode 257	 reward: -9.09	 Mean_loss: 0.15270410,  training time: 13.18
[814.25, 800.75, 819.0, 854.5]
Episode 258	 reward: -9.55	 Mean_loss: 0.14486074,  training time: 13.17
[780.75, 795.5, 793.25, 798.75]
Episode 259	 reward: -9.35	 Mean_loss: 0.22794043,  training time: 13.15
[763.25, 768.75, 769.25, 894.5]
Episode 260	 reward: -9.68	 Mean_loss: 0.11722761,  training time: 13.15
[(15, 5), (15, 7), (15, 9), (15, 10)]
[848.75, 839.5, 867.25, 787.5]
Episode 261	 reward: -8.96	 Mean_loss: 0.20749225,  training time: 13.18
[809.25, 818.0, 886.25, 864.75]
Episode 262	 reward: -9.17	 Mean_loss: 0.08815748,  training time: 13.17
[832.5, 828.75, 851.25, 823.0]
Episode 263	 reward: -8.61	 Mean_loss: 0.16658767,  training time: 13.37
[815.25, 819.25, 880.75, 807.25]
Episode 264	 reward: -8.96	 Mean_loss: 0.12477740,  training time: 13.41
[812.5, 873.25, 897.0, 849.0]
Episode 265	 reward: -8.77	 Mean_loss: 0.27816835,  training time: 13.38
[882.25, 833.25, 900.5, 828.25]
Episode 266	 reward: -8.90	 Mean_loss: 0.16842492,  training time: 13.39
[844.25, 819.0, 928.25, 864.25]
Episode 267	 reward: -9.48	 Mean_loss: 0.17081890,  training time: 13.38
[866.25, 821.0, 891.75, 867.75]
Episode 268	 reward: -9.00	 Mean_loss: 0.16867010,  training time: 13.41
[778.75, 777.25, 857.75, 847.0]
Episode 269	 reward: -9.68	 Mean_loss: 0.07282650,  training time: 13.42
[814.5, 822.75, 876.75, 842.25]
Episode 270	 reward: -8.74	 Mean_loss: 0.15209265,  training time: 13.38
[856.25, 837.0, 917.0, 822.5]
Episode 271	 reward: -9.63	 Mean_loss: 0.15682954,  training time: 13.30
[822.75, 822.0, 875.25, 845.75]
Episode 272	 reward: -8.87	 Mean_loss: 0.11387612,  training time: 13.17
[797.75, 790.75, 817.25, 796.0]
Episode 273	 reward: -9.24	 Mean_loss: 0.19547418,  training time: 13.15
[810.0, 855.0, 881.75, 823.0]
Episode 274	 reward: -8.80	 Mean_loss: 0.16254225,  training time: 13.19
[788.5, 847.25, 879.5, 812.5]
Episode 275	 reward: -9.02	 Mean_loss: 0.20288230,  training time: 13.14
[815.25, 753.5, 882.0, 881.5]
Episode 276	 reward: -9.26	 Mean_loss: 0.08677485,  training time: 13.16
[790.75, 758.5, 981.75, 792.5]
Episode 277	 reward: -9.10	 Mean_loss: 0.18605739,  training time: 13.12
[871.75, 820.5, 841.25, 823.5]
Episode 278	 reward: -9.26	 Mean_loss: 0.16530575,  training time: 13.15
[814.0, 836.5, 815.75, 816.25]
Episode 279	 reward: -9.60	 Mean_loss: 0.13534965,  training time: 13.44
[791.0, 877.75, 901.5, 819.25]
Episode 280	 reward: -9.59	 Mean_loss: 0.18022326,  training time: 13.40
[(15, 5), (15, 7), (15, 9), (15, 10)]
[784.75, 844.25, 875.25, 870.75]
Episode 281	 reward: -9.23	 Mean_loss: 0.14050195,  training time: 13.44
[732.5, 809.0, 871.75, 931.75]
Episode 282	 reward: -9.24	 Mean_loss: 0.15973803,  training time: 13.40
[711.0, 902.0, 837.25, 885.75]
Episode 283	 reward: -9.03	 Mean_loss: 0.14397241,  training time: 13.40
[737.25, 806.0, 815.25, 805.75]
Episode 284	 reward: -9.53	 Mean_loss: 0.18220508,  training time: 13.26
[749.5, 795.5, 804.5, 803.0]
Episode 285	 reward: -9.79	 Mean_loss: 0.21759044,  training time: 13.19
[729.25, 820.0, 822.75, 847.0]
Episode 286	 reward: -9.20	 Mean_loss: 0.13375019,  training time: 13.26
[745.25, 873.5, 824.75, 828.0]
Episode 287	 reward: -9.72	 Mean_loss: 0.13391420,  training time: 13.16
[742.0, 834.25, 806.5, 811.25]
Episode 288	 reward: -9.22	 Mean_loss: 0.18308339,  training time: 13.12
[709.25, 823.25, 882.75, 824.75]
Episode 289	 reward: -9.97	 Mean_loss: 0.20982404,  training time: 13.15
[770.0, 804.75, 888.25, 792.75]
Episode 290	 reward: -9.46	 Mean_loss: 0.18191899,  training time: 13.15
[743.75, 835.0, 789.25, 829.5]
Episode 291	 reward: -9.57	 Mean_loss: 0.12632526,  training time: 13.11
[733.25, 879.0, 817.5, 809.5]
Episode 292	 reward: -9.41	 Mean_loss: 0.18780659,  training time: 13.10
[730.0, 902.75, 948.25, 801.25]
Episode 293	 reward: -8.91	 Mean_loss: 0.16328734,  training time: 13.16
[725.0, 878.75, 888.5, 829.75]
Episode 294	 reward: -9.57	 Mean_loss: 0.14529607,  training time: 13.03
[743.25, 809.5, 863.0, 815.5]
Episode 295	 reward: -9.22	 Mean_loss: 0.15868819,  training time: 13.12
[786.75, 865.75, 858.25, 835.0]
Episode 296	 reward: -9.90	 Mean_loss: 0.13006164,  training time: 13.08
[772.25, 801.25, 812.0, 862.75]
Episode 297	 reward: -9.22	 Mean_loss: 0.10095914,  training time: 13.07
[739.75, 836.75, 862.0, 891.0]
Episode 298	 reward: -9.02	 Mean_loss: 0.07061827,  training time: 13.04
[711.0, 812.75, 856.0, 808.25]
Episode 299	 reward: -9.04	 Mean_loss: 0.15105799,  training time: 13.04
[691.25, 795.75, 863.5, 795.25]
Episode 300	 reward: -9.14	 Mean_loss: 0.16714071,  training time: 13.05
[(15, 5), (15, 7), (15, 9), (15, 10)]
[747.75, 775.0, 880.5, 743.25]
Episode 301	 reward: -9.09	 Mean_loss: 0.17226265,  training time: 13.22
[844.25, 791.25, 854.25, 772.75]
Episode 302	 reward: -8.39	 Mean_loss: 0.14857157,  training time: 13.27
[857.5, 742.5, 772.25, 772.5]
Episode 303	 reward: -9.10	 Mean_loss: 0.14179036,  training time: 13.19
[773.5, 749.25, 796.75, 788.75]
Episode 304	 reward: -9.07	 Mean_loss: 0.14032738,  training time: 13.18
[802.0, 773.25, 723.25, 767.25]
Episode 305	 reward: -9.08	 Mean_loss: 0.14936092,  training time: 13.19
[795.75, 810.25, 872.25, 797.25]
Episode 306	 reward: -9.51	 Mean_loss: 0.12600060,  training time: 13.18
[782.5, 781.0, 866.25, 798.25]
Episode 307	 reward: -9.13	 Mean_loss: 0.09735484,  training time: 13.16
[831.0, 692.0, 850.5, 757.5]
Episode 308	 reward: -9.12	 Mean_loss: 0.13990863,  training time: 13.16
[829.75, 796.75, 815.0, 806.75]
Episode 309	 reward: -8.65	 Mean_loss: 0.11456817,  training time: 13.19
[829.25, 804.75, 793.5, 827.5]
Episode 310	 reward: -8.79	 Mean_loss: 0.22726934,  training time: 13.18
[785.25, 731.75, 802.25, 817.5]
Episode 311	 reward: -8.48	 Mean_loss: 0.12478690,  training time: 13.16
[764.75, 772.0, 836.5, 810.25]
Episode 312	 reward: -8.80	 Mean_loss: 0.11771346,  training time: 13.19
[780.25, 795.75, 820.25, 801.75]
Episode 313	 reward: -8.84	 Mean_loss: 0.10211965,  training time: 13.17
[780.0, 830.5, 743.0, 793.25]
Episode 314	 reward: -9.00	 Mean_loss: 0.08740437,  training time: 13.15
[833.5, 813.0, 885.75, 798.0]
Episode 315	 reward: -9.29	 Mean_loss: 0.09696488,  training time: 13.18
[789.0, 776.75, 829.25, 754.0]
Episode 316	 reward: -9.51	 Mean_loss: 0.10718075,  training time: 13.33
[768.5, 738.5, 851.25, 851.5]
Episode 317	 reward: -9.06	 Mean_loss: 0.06184056,  training time: 13.21
[799.25, 771.0, 766.5, 741.75]
Episode 318	 reward: -9.36	 Mean_loss: 0.15538621,  training time: 13.04
[825.5, 777.5, 900.75, 758.0]
Episode 319	 reward: -9.06	 Mean_loss: 0.16128811,  training time: 13.07
[787.0, 757.0, 781.25, 806.75]
Episode 320	 reward: -9.18	 Mean_loss: 0.11388791,  training time: 13.09
[(15, 5), (15, 7), (15, 9), (15, 10)]
[747.25, 743.0, 794.0, 891.75]
Episode 321	 reward: -9.41	 Mean_loss: 0.07480863,  training time: 13.10
[742.25, 811.5, 786.0, 816.25]
Episode 322	 reward: -9.44	 Mean_loss: 0.15540305,  training time: 13.04
[791.75, 754.75, 869.25, 809.25]
Episode 323	 reward: -9.01	 Mean_loss: 0.19706582,  training time: 13.00
[785.5, 745.5, 852.25, 855.5]
Episode 324	 reward: -9.42	 Mean_loss: 0.19460738,  training time: 13.05
[770.0, 731.5, 876.75, 867.5]
Episode 325	 reward: -9.10	 Mean_loss: 0.13306098,  training time: 13.07
[725.5, 756.75, 822.0, 847.5]
Episode 326	 reward: -9.18	 Mean_loss: 0.10329070,  training time: 13.06
[710.5, 830.0, 816.0, 834.0]
Episode 327	 reward: -9.05	 Mean_loss: 0.10367328,  training time: 13.12
[751.25, 800.5, 916.75, 809.0]
Episode 328	 reward: -9.06	 Mean_loss: 0.15408087,  training time: 13.15
[738.0, 788.75, 828.75, 862.75]
Episode 329	 reward: -8.85	 Mean_loss: 0.08934560,  training time: 13.25
[724.75, 818.0, 801.75, 870.0]
Episode 330	 reward: -10.18	 Mean_loss: 0.09723887,  training time: 13.38
[742.75, 796.5, 831.25, 839.75]
Episode 331	 reward: -9.35	 Mean_loss: 0.10615942,  training time: 13.36
[762.5, 799.25, 936.5, 909.5]
Episode 332	 reward: -9.68	 Mean_loss: 0.14245732,  training time: 13.38
[704.75, 791.25, 808.5, 817.75]
Episode 333	 reward: -8.90	 Mean_loss: 0.18357605,  training time: 13.39
[728.25, 783.75, 844.0, 858.0]
Episode 334	 reward: -9.27	 Mean_loss: 0.12282454,  training time: 13.49
[763.25, 783.5, 818.5, 870.75]
Episode 335	 reward: -9.41	 Mean_loss: 0.09177534,  training time: 13.18
[727.75, 795.0, 896.75, 876.5]
Episode 336	 reward: -9.81	 Mean_loss: 0.08887412,  training time: 13.40
[714.75, 839.25, 751.0, 858.5]
Episode 337	 reward: -9.56	 Mean_loss: 0.13554393,  training time: 13.53
[742.75, 795.5, 826.25, 833.5]
Episode 338	 reward: -9.11	 Mean_loss: 0.09543026,  training time: 13.39
[750.25, 763.0, 798.75, 827.5]
Episode 339	 reward: -8.88	 Mean_loss: 0.19243504,  training time: 13.37
[713.0, 741.5, 754.0, 861.5]
Episode 340	 reward: -9.27	 Mean_loss: 0.08852776,  training time: 13.34
[(15, 5), (15, 7), (15, 9), (15, 10)]
[796.5, 824.75, 782.75, 865.0]
Episode 341	 reward: -9.49	 Mean_loss: 0.10194652,  training time: 13.13
[813.25, 872.25, 862.25, 877.25]
Episode 342	 reward: -9.55	 Mean_loss: 0.11630346,  training time: 13.06
[787.5, 796.0, 845.0, 858.5]
Episode 343	 reward: -10.74	 Mean_loss: 0.14869523,  training time: 13.11
[819.25, 838.25, 836.5, 878.0]
Episode 344	 reward: -9.72	 Mean_loss: 0.06228994,  training time: 13.21
[750.75, 788.0, 761.5, 871.0]
Episode 345	 reward: -9.91	 Mean_loss: 0.11318994,  training time: 13.16
[709.75, 829.0, 852.25, 906.25]
Episode 346	 reward: -10.03	 Mean_loss: 0.10879657,  training time: 13.19
[736.5, 852.0, 828.5, 850.5]
Episode 347	 reward: -9.63	 Mean_loss: 0.11171091,  training time: 13.03
[802.0, 834.75, 825.75, 891.0]
Episode 348	 reward: -10.33	 Mean_loss: 0.08554827,  training time: 13.23
[795.75, 839.25, 823.75, 868.5]
Episode 349	 reward: -9.72	 Mean_loss: 0.13323887,  training time: 13.21
[751.0, 796.75, 812.0, 902.25]
Episode 350	 reward: -10.07	 Mean_loss: 0.11292135,  training time: 13.39
[830.75, 839.5, 860.0, 876.5]
Episode 351	 reward: -10.09	 Mean_loss: 0.12269941,  training time: 13.15
[764.5, 809.25, 857.25, 857.5]
Episode 352	 reward: -9.57	 Mean_loss: 0.14368509,  training time: 13.48
[813.0, 830.75, 816.5, 833.0]
Episode 353	 reward: -9.29	 Mean_loss: 0.11089641,  training time: 13.18
[721.0, 802.0, 842.0, 873.25]
Episode 354	 reward: -9.44	 Mean_loss: 0.21377671,  training time: 13.01
[775.25, 781.0, 861.0, 836.75]
Episode 355	 reward: -9.54	 Mean_loss: 0.10955302,  training time: 13.00
[743.75, 792.25, 871.5, 838.5]
Episode 356	 reward: -9.91	 Mean_loss: 0.10420193,  training time: 13.03
[734.25, 843.5, 842.25, 818.25]
Episode 357	 reward: -9.43	 Mean_loss: 0.14681712,  training time: 13.21
[753.25, 841.5, 861.0, 844.5]
Episode 358	 reward: -9.54	 Mean_loss: 0.13342851,  training time: 13.22
[786.25, 878.5, 802.5, 812.75]
Episode 359	 reward: -9.06	 Mean_loss: 0.17131440,  training time: 13.16
[738.0, 869.0, 769.25, 933.0]
Episode 360	 reward: -9.67	 Mean_loss: 0.18641706,  training time: 13.27
[(15, 5), (15, 7), (15, 9), (15, 10)]
[726.25, 814.75, 797.25, 896.25]
Episode 361	 reward: -9.41	 Mean_loss: 0.12789336,  training time: 13.25
[692.75, 718.0, 829.5, 884.25]
Episode 362	 reward: -9.63	 Mean_loss: 0.07783128,  training time: 13.20
[766.75, 766.5, 807.5, 854.75]
Episode 363	 reward: -10.24	 Mean_loss: 0.09243368,  training time: 13.16
[702.75, 736.25, 762.25, 891.0]
Episode 364	 reward: -9.16	 Mean_loss: 0.09283183,  training time: 13.15
[687.0, 829.5, 771.25, 927.75]
Episode 365	 reward: -9.59	 Mean_loss: 0.14066409,  training time: 13.33
[756.5, 813.0, 834.0, 909.75]
Episode 366	 reward: -8.97	 Mean_loss: 0.04793652,  training time: 13.20
[760.0, 769.0, 776.25, 872.75]
Episode 367	 reward: -9.51	 Mean_loss: 0.09930674,  training time: 13.17
[770.75, 778.5, 813.25, 935.25]
Episode 368	 reward: -9.49	 Mean_loss: 0.10256434,  training time: 13.08
[734.25, 756.25, 822.0, 855.0]
Episode 369	 reward: -9.89	 Mean_loss: 0.11692958,  training time: 13.10
[700.75, 774.75, 766.75, 833.5]
Episode 370	 reward: -9.39	 Mean_loss: 0.12703355,  training time: 13.42
[775.5, 763.75, 828.75, 829.75]
Episode 371	 reward: -10.07	 Mean_loss: 0.09302642,  training time: 13.20
[715.0, 762.0, 811.75, 858.0]
Episode 372	 reward: -9.36	 Mean_loss: 0.10968424,  training time: 13.15
[835.5, 832.5, 787.25, 877.75]
Episode 373	 reward: -9.18	 Mean_loss: 0.08142111,  training time: 13.28
[765.0, 763.75, 802.25, 876.5]
Episode 374	 reward: -9.81	 Mean_loss: 0.08592319,  training time: 13.25
[743.75, 775.5, 831.5, 925.5]
Episode 375	 reward: -9.57	 Mean_loss: 0.06974843,  training time: 13.16
[772.0, 803.75, 775.0, 841.5]
Episode 376	 reward: -10.33	 Mean_loss: 0.10327188,  training time: 13.21
[757.5, 773.75, 809.25, 819.25]
Episode 377	 reward: -10.06	 Mean_loss: 0.07766387,  training time: 13.10
[754.25, 742.5, 778.0, 892.5]
Episode 378	 reward: -9.32	 Mean_loss: 0.13919149,  training time: 13.09
[734.75, 746.5, 809.5, 881.0]
Episode 379	 reward: -10.63	 Mean_loss: 0.09713415,  training time: 13.17
[724.5, 821.0, 798.5, 800.75]
Episode 380	 reward: -9.51	 Mean_loss: 0.12734801,  training time: 13.09
[(15, 5), (15, 7), (15, 9), (15, 10)]
[771.0, 783.5, 801.5, 807.75]
Episode 381	 reward: -10.11	 Mean_loss: 0.11554449,  training time: 13.14
[785.25, 803.0, 792.5, 856.75]
Episode 382	 reward: -9.72	 Mean_loss: 0.07331440,  training time: 13.18
[712.75, 884.25, 836.5, 906.0]
Episode 383	 reward: -9.33	 Mean_loss: 0.12128583,  training time: 13.06
[735.0, 789.75, 789.5, 830.25]
Episode 384	 reward: -9.17	 Mean_loss: 0.05841648,  training time: 13.07
[739.75, 797.75, 847.0, 823.0]
Episode 385	 reward: -9.32	 Mean_loss: 0.08486898,  training time: 13.06
[717.25, 840.25, 761.25, 834.25]
Episode 386	 reward: -9.83	 Mean_loss: 0.10401313,  training time: 13.09
[729.5, 761.5, 795.25, 863.75]
Episode 387	 reward: -9.86	 Mean_loss: 0.05829451,  training time: 13.06
[776.75, 804.75, 746.25, 833.25]
Episode 388	 reward: -9.83	 Mean_loss: 0.11496787,  training time: 13.04
[747.5, 832.0, 774.0, 797.5]
Episode 389	 reward: -9.14	 Mean_loss: 0.12950192,  training time: 13.36
[766.25, 784.0, 807.25, 802.5]
Episode 390	 reward: -9.63	 Mean_loss: 0.09001917,  training time: 13.34
[718.0, 774.5, 863.25, 818.5]
Episode 391	 reward: -9.56	 Mean_loss: 0.07715486,  training time: 13.40
[773.0, 845.25, 790.75, 820.5]
Episode 392	 reward: -9.54	 Mean_loss: 0.13556260,  training time: 13.37
[767.75, 766.75, 774.5, 867.0]
Episode 393	 reward: -9.64	 Mean_loss: 0.06591856,  training time: 13.41
[790.75, 788.25, 766.5, 812.75]
Episode 394	 reward: -9.60	 Mean_loss: 0.18750064,  training time: 13.42
[740.75, 883.5, 733.25, 885.5]
Episode 395	 reward: -8.58	 Mean_loss: 0.10185251,  training time: 13.42
[748.5, 782.5, 794.75, 871.0]
Episode 396	 reward: -9.09	 Mean_loss: 0.06629756,  training time: 13.36
[746.25, 786.0, 815.25, 849.75]
Episode 397	 reward: -9.90	 Mean_loss: 0.10764748,  training time: 13.31
[777.75, 826.25, 839.25, 852.25]
Episode 398	 reward: -9.13	 Mean_loss: 0.11752421,  training time: 13.19
[749.5, 745.0, 785.75, 820.5]
Episode 399	 reward: -9.09	 Mean_loss: 0.10371372,  training time: 13.03
[711.75, 848.5, 779.75, 872.25]
Episode 400	 reward: -8.39	 Mean_loss: 0.11527831,  training time: 13.15
[(15, 5), (15, 7), (15, 9), (15, 10)]
[746.5, 746.75, 790.5, 869.0]
Episode 401	 reward: -9.19	 Mean_loss: 0.09429951,  training time: 13.12
[756.0, 767.0, 781.5, 806.5]
Episode 402	 reward: -9.70	 Mean_loss: 0.14664371,  training time: 13.05
[701.25, 750.75, 761.25, 880.75]
Episode 403	 reward: -9.66	 Mean_loss: 0.17717130,  training time: 13.01
[751.0, 769.25, 769.75, 916.25]
Episode 404	 reward: -9.91	 Mean_loss: 0.29148212,  training time: 13.05
[781.25, 746.0, 807.75, 802.5]
Episode 405	 reward: -9.01	 Mean_loss: 0.14170974,  training time: 13.07
[716.75, 808.75, 754.75, 816.75]
Episode 406	 reward: -9.46	 Mean_loss: 0.11700433,  training time: 13.06
[745.75, 769.5, 823.75, 781.0]
Episode 407	 reward: -9.60	 Mean_loss: 0.14327066,  training time: 13.02
[714.25, 739.5, 845.5, 861.5]
Episode 408	 reward: -9.36	 Mean_loss: 0.09891769,  training time: 13.02
[770.5, 742.0, 795.25, 869.75]
Episode 409	 reward: -10.14	 Mean_loss: 0.14321791,  training time: 13.02
[774.0, 772.75, 713.5, 865.0]
Episode 410	 reward: -9.46	 Mean_loss: 0.07588844,  training time: 13.03
[767.75, 778.25, 777.5, 825.5]
Episode 411	 reward: -8.93	 Mean_loss: 0.11524870,  training time: 13.07
[675.25, 739.5, 763.0, 775.5]
Episode 412	 reward: -9.64	 Mean_loss: 0.17478348,  training time: 13.04
[759.25, 774.0, 802.75, 807.5]
Episode 413	 reward: -9.53	 Mean_loss: 0.10971756,  training time: 13.07
[829.25, 717.75, 843.25, 822.75]
Episode 414	 reward: -8.99	 Mean_loss: 0.09491166,  training time: 13.10
[784.0, 764.25, 773.75, 852.0]
Episode 415	 reward: -9.36	 Mean_loss: 0.09956162,  training time: 13.04
[711.75, 797.5, 790.0, 812.75]
Episode 416	 reward: -9.56	 Mean_loss: 0.10259437,  training time: 13.04
[729.75, 724.0, 812.25, 819.0]
Episode 417	 reward: -9.73	 Mean_loss: 0.10491597,  training time: 13.08
[782.5, 767.0, 790.25, 856.0]
Episode 418	 reward: -9.09	 Mean_loss: 0.06780427,  training time: 13.05
[758.25, 716.75, 777.25, 831.25]
Episode 419	 reward: -9.54	 Mean_loss: 0.11402512,  training time: 13.35
[671.0, 757.0, 730.75, 821.0]
Episode 420	 reward: -9.50	 Mean_loss: 0.12674513,  training time: 13.40
[(15, 5), (15, 7), (15, 9), (15, 10)]
[752.25, 785.0, 798.0, 757.5]
Episode 421	 reward: -8.96	 Mean_loss: 0.11480201,  training time: 13.53
[697.25, 822.75, 771.75, 795.25]
Episode 422	 reward: -8.96	 Mean_loss: 0.08241815,  training time: 13.35
[727.5, 773.0, 828.25, 753.5]
Episode 423	 reward: -9.37	 Mean_loss: 0.10583091,  training time: 13.36
[698.75, 798.5, 832.75, 775.0]
Episode 424	 reward: -9.10	 Mean_loss: 0.08274551,  training time: 13.39
[720.75, 790.5, 831.75, 770.75]
Episode 425	 reward: -8.79	 Mean_loss: 0.11545766,  training time: 13.33
[730.0, 792.0, 816.75, 780.25]
Episode 426	 reward: -9.16	 Mean_loss: 0.08922776,  training time: 13.14
[754.5, 752.0, 782.0, 737.0]
Episode 427	 reward: -8.85	 Mean_loss: 0.10916866,  training time: 13.18
[759.75, 800.25, 780.25, 719.5]
Episode 428	 reward: -8.60	 Mean_loss: 0.12424333,  training time: 13.17
[773.25, 759.0, 844.0, 786.5]
Episode 429	 reward: -8.88	 Mean_loss: 0.09468395,  training time: 13.20
[717.0, 823.25, 703.5, 760.75]
Episode 430	 reward: -9.63	 Mean_loss: 0.09859445,  training time: 13.17
[704.75, 827.5, 789.5, 775.0]
Episode 431	 reward: -8.78	 Mean_loss: 0.09040121,  training time: 13.38
[777.0, 745.5, 800.5, 818.25]
Episode 432	 reward: -8.75	 Mean_loss: 0.06202816,  training time: 13.18
[715.0, 737.5, 791.75, 768.25]
Episode 433	 reward: -8.79	 Mean_loss: 0.06501346,  training time: 13.19
[696.25, 757.5, 838.75, 780.75]
Episode 434	 reward: -9.29	 Mean_loss: 0.07453514,  training time: 13.17
[720.75, 771.0, 800.0, 747.75]
Episode 435	 reward: -8.59	 Mean_loss: 0.10410983,  training time: 13.42
[720.0, 800.25, 813.0, 836.0]
Episode 436	 reward: -8.98	 Mean_loss: 0.09889349,  training time: 13.37
[695.75, 762.5, 833.0, 713.25]
Episode 437	 reward: -9.45	 Mean_loss: 0.15967070,  training time: 13.36
[804.25, 794.25, 815.0, 801.0]
Episode 438	 reward: -9.27	 Mean_loss: 0.06646838,  training time: 13.35
[680.0, 752.75, 804.0, 767.0]
Episode 439	 reward: -9.70	 Mean_loss: 0.07099293,  training time: 13.41
[700.0, 717.75, 816.75, 739.75]
Episode 440	 reward: -9.47	 Mean_loss: 0.12156711,  training time: 13.18
[(15, 5), (15, 7), (15, 9), (15, 10)]
[789.75, 811.25, 796.0, 816.5]
Episode 441	 reward: -9.56	 Mean_loss: 0.08096569,  training time: 13.29
[733.75, 794.0, 770.75, 757.75]
Episode 442	 reward: -9.00	 Mean_loss: 0.08816500,  training time: 13.42
[717.75, 764.5, 802.25, 746.25]
Episode 443	 reward: -8.74	 Mean_loss: 0.13365601,  training time: 13.27
[713.75, 785.5, 782.75, 822.0]
Episode 444	 reward: -9.15	 Mean_loss: 0.04949214,  training time: 13.40
[770.5, 774.25, 799.25, 774.75]
Episode 445	 reward: -8.85	 Mean_loss: 0.07098562,  training time: 13.38
[719.5, 753.25, 858.5, 770.5]
Episode 446	 reward: -8.33	 Mean_loss: 0.08695567,  training time: 13.36
[658.25, 786.5, 752.5, 789.5]
Episode 447	 reward: -9.53	 Mean_loss: 0.13673286,  training time: 13.39
[739.0, 784.5, 779.5, 751.25]
Episode 448	 reward: -10.20	 Mean_loss: 0.12061412,  training time: 13.40
[713.5, 806.75, 779.5, 829.25]
Episode 449	 reward: -9.43	 Mean_loss: 0.12786037,  training time: 13.39
[683.5, 822.75, 776.5, 813.75]
Episode 450	 reward: -8.76	 Mean_loss: 0.08870720,  training time: 13.39
[721.25, 822.25, 794.5, 806.0]
Episode 451	 reward: -9.35	 Mean_loss: 0.11533602,  training time: 13.18
[706.0, 805.75, 772.0, 832.0]
Episode 452	 reward: -9.39	 Mean_loss: 0.10985199,  training time: 13.18
[715.0, 760.25, 820.5, 760.0]
Episode 453	 reward: -10.26	 Mean_loss: 0.13344832,  training time: 13.20
[699.5, 812.5, 798.25, 856.75]
Episode 454	 reward: -9.77	 Mean_loss: 0.11118958,  training time: 13.18
[706.75, 766.75, 794.5, 757.5]
Episode 455	 reward: -9.25	 Mean_loss: 0.11026254,  training time: 13.18
[676.0, 771.75, 804.75, 795.75]
Episode 456	 reward: -9.13	 Mean_loss: 0.08732401,  training time: 13.18
[714.5, 772.75, 782.0, 779.75]
Episode 457	 reward: -9.58	 Mean_loss: 0.09563454,  training time: 13.18
[736.0, 788.25, 775.0, 728.25]
Episode 458	 reward: -9.15	 Mean_loss: 0.14492537,  training time: 13.32
[710.25, 791.5, 879.5, 788.25]
Episode 459	 reward: -9.20	 Mean_loss: 0.13681613,  training time: 13.17
[733.25, 726.75, 785.0, 760.75]
Episode 460	 reward: -8.88	 Mean_loss: 0.11713339,  training time: 13.20
[(15, 5), (15, 7), (15, 9), (15, 10)]
[787.75, 764.0, 787.0, 797.25]
Episode 461	 reward: -8.33	 Mean_loss: 0.06540629,  training time: 13.32
[746.0, 757.0, 817.75, 747.5]
Episode 462	 reward: -9.56	 Mean_loss: 0.12610021,  training time: 13.17
[746.25, 727.25, 825.75, 760.75]
Episode 463	 reward: -8.54	 Mean_loss: 0.13348936,  training time: 13.24
[696.25, 749.5, 776.75, 861.5]
Episode 464	 reward: -8.77	 Mean_loss: 0.12276879,  training time: 13.14
[735.75, 794.25, 787.25, 797.5]
Episode 465	 reward: -9.41	 Mean_loss: 0.08997721,  training time: 13.14
[721.25, 773.0, 816.25, 834.5]
Episode 466	 reward: -8.56	 Mean_loss: 0.10794409,  training time: 13.13
[740.0, 777.75, 771.0, 795.5]
Episode 467	 reward: -8.83	 Mean_loss: 0.14017694,  training time: 13.12
[755.5, 810.25, 824.75, 860.25]
Episode 468	 reward: -9.31	 Mean_loss: 0.12771176,  training time: 13.28
[731.5, 823.5, 785.5, 761.5]
Episode 469	 reward: -9.10	 Mean_loss: 0.08945706,  training time: 13.09
[721.5, 776.25, 783.75, 800.5]
Episode 470	 reward: -9.25	 Mean_loss: 0.10791006,  training time: 13.04
[742.75, 753.0, 752.0, 762.5]
Episode 471	 reward: -8.47	 Mean_loss: 0.13645284,  training time: 13.03
[737.0, 748.25, 776.0, 734.0]
Episode 472	 reward: -9.38	 Mean_loss: 0.15100518,  training time: 13.02
[731.75, 756.25, 770.5, 744.5]
Episode 473	 reward: -8.99	 Mean_loss: 0.12527932,  training time: 13.02
[731.75, 737.75, 806.75, 748.75]
Episode 474	 reward: -8.84	 Mean_loss: 0.12274577,  training time: 13.03
[762.25, 740.5, 832.5, 775.25]
Episode 475	 reward: -9.15	 Mean_loss: 0.14127509,  training time: 13.04
[764.25, 768.5, 787.25, 719.0]
Episode 476	 reward: -9.01	 Mean_loss: 0.14669712,  training time: 13.05
[738.25, 743.0, 809.5, 769.25]
Episode 477	 reward: -8.52	 Mean_loss: 0.14678618,  training time: 13.04
[771.0, 816.0, 817.75, 773.25]
Episode 478	 reward: -9.11	 Mean_loss: 0.10170076,  training time: 13.03
[723.0, 748.75, 786.0, 718.0]
Episode 479	 reward: -9.48	 Mean_loss: 0.14488906,  training time: 13.12
[731.5, 739.25, 750.5, 767.0]
Episode 480	 reward: -9.35	 Mean_loss: 0.10116328,  training time: 13.14
[(15, 5), (15, 7), (15, 9), (15, 10)]
[796.75, 712.0, 739.5, 764.5]
Episode 481	 reward: -8.76	 Mean_loss: 0.16122752,  training time: 13.14
[776.0, 681.5, 758.5, 822.5]
Episode 482	 reward: -8.86	 Mean_loss: 0.09257868,  training time: 13.34
[749.0, 708.5, 788.75, 739.0]
Episode 483	 reward: -9.39	 Mean_loss: 0.18360367,  training time: 13.40
[774.0, 710.0, 768.0, 806.5]
Episode 484	 reward: -9.41	 Mean_loss: 0.09109266,  training time: 13.45
[806.0, 683.0, 779.5, 756.75]
Episode 485	 reward: -9.24	 Mean_loss: 0.10969311,  training time: 13.50
[759.75, 707.75, 803.0, 808.25]
Episode 486	 reward: -9.29	 Mean_loss: 0.11110108,  training time: 13.14
[786.75, 669.0, 775.5, 823.0]
Episode 487	 reward: -8.99	 Mean_loss: 0.12879060,  training time: 13.15
[735.75, 730.75, 741.75, 824.0]
Episode 488	 reward: -9.20	 Mean_loss: 0.14698736,  training time: 13.12
[777.5, 735.0, 801.5, 832.0]
Episode 489	 reward: -9.63	 Mean_loss: 0.09802704,  training time: 13.14
[825.25, 747.25, 795.0, 820.5]
Episode 490	 reward: -9.22	 Mean_loss: 0.07766616,  training time: 13.13
[711.25, 690.75, 741.25, 808.25]
Episode 491	 reward: -9.35	 Mean_loss: 0.15600403,  training time: 13.17
[719.0, 714.0, 821.0, 786.75]
Episode 492	 reward: -8.97	 Mean_loss: 0.14179936,  training time: 13.17
[774.25, 704.75, 824.0, 823.25]
Episode 493	 reward: -9.09	 Mean_loss: 0.17241542,  training time: 13.17
[741.5, 706.0, 774.75, 802.75]
Episode 494	 reward: -9.56	 Mean_loss: 0.12146787,  training time: 13.15
[736.0, 700.75, 741.25, 863.75]
Episode 495	 reward: -8.98	 Mean_loss: 0.14997342,  training time: 13.11
[777.25, 675.0, 781.75, 824.5]
Episode 496	 reward: -8.78	 Mean_loss: 0.11474769,  training time: 13.12
[866.0, 644.25, 793.75, 769.25]
Episode 497	 reward: -8.73	 Mean_loss: 0.09016750,  training time: 13.17
[765.0, 687.75, 832.25, 805.25]
Episode 498	 reward: -9.17	 Mean_loss: 0.09979776,  training time: 13.19
[735.5, 683.0, 831.25, 772.5]
Episode 499	 reward: -9.74	 Mean_loss: 0.12701598,  training time: 13.04
[745.25, 661.5, 828.5, 785.5]
Episode 500	 reward: -9.82	 Mean_loss: 0.09661514,  training time: 13.22
[(15, 5), (15, 7), (15, 9), (15, 10)]
[716.0, 741.0, 796.5, 798.75]
Episode 501	 reward: -9.87	 Mean_loss: 0.08992887,  training time: 13.19
[682.25, 752.25, 840.75, 782.25]
Episode 502	 reward: -10.16	 Mean_loss: 0.11534462,  training time: 13.12
[752.5, 726.5, 799.0, 794.5]
Episode 503	 reward: -9.10	 Mean_loss: 0.09986433,  training time: 13.10
[743.25, 713.75, 752.5, 870.5]
Episode 504	 reward: -9.12	 Mean_loss: 0.17826544,  training time: 13.11
[741.0, 730.25, 763.0, 778.75]
Episode 505	 reward: -8.84	 Mean_loss: 0.12741630,  training time: 13.21
[709.75, 732.5, 775.25, 819.75]
Episode 506	 reward: -9.25	 Mean_loss: 0.08983909,  training time: 13.12
[750.25, 759.5, 776.75, 747.25]
Episode 507	 reward: -9.24	 Mean_loss: 0.11651055,  training time: 13.14
[693.25, 732.0, 804.25, 778.25]
Episode 508	 reward: -8.94	 Mean_loss: 0.08303346,  training time: 13.16
[739.75, 753.0, 842.25, 815.25]
Episode 509	 reward: -9.55	 Mean_loss: 0.10106120,  training time: 13.10
[683.25, 710.0, 771.25, 832.0]
Episode 510	 reward: -9.28	 Mean_loss: 0.15858246,  training time: 13.12
[740.0, 773.75, 794.5, 749.0]
Episode 511	 reward: -9.58	 Mean_loss: 0.10360920,  training time: 13.16
[702.75, 762.25, 788.5, 767.75]
Episode 512	 reward: -9.38	 Mean_loss: 0.08791406,  training time: 13.13
[743.0, 708.5, 816.75, 833.0]
Episode 513	 reward: -9.64	 Mean_loss: 0.06645776,  training time: 13.34
[694.75, 769.5, 762.25, 737.0]
Episode 514	 reward: -9.31	 Mean_loss: 0.10753496,  training time: 13.39
[740.0, 684.25, 768.75, 782.5]
Episode 515	 reward: -9.45	 Mean_loss: 0.06638891,  training time: 13.39
[731.25, 740.25, 780.25, 847.75]
Episode 516	 reward: -9.65	 Mean_loss: 0.10687407,  training time: 13.33
[726.0, 731.5, 774.5, 858.0]
Episode 517	 reward: -10.27	 Mean_loss: 0.12128922,  training time: 13.39
[716.75, 739.25, 825.75, 765.5]
Episode 518	 reward: -9.23	 Mean_loss: 0.09137888,  training time: 13.37
[668.5, 811.75, 744.5, 836.75]
Episode 519	 reward: -9.60	 Mean_loss: 0.10947844,  training time: 13.19
[689.25, 753.0, 767.25, 826.25]
Episode 520	 reward: -10.14	 Mean_loss: 0.11514611,  training time: 13.07
[(15, 5), (15, 7), (15, 9), (15, 10)]
[752.25, 706.5, 834.5, 780.0]
Episode 521	 reward: -9.95	 Mean_loss: 0.09487312,  training time: 13.21
[731.75, 718.75, 734.25, 843.75]
Episode 522	 reward: -9.25	 Mean_loss: 0.07471629,  training time: 13.05
[734.75, 770.75, 798.25, 799.25]
Episode 523	 reward: -9.13	 Mean_loss: 0.10783175,  training time: 13.06
[760.25, 694.5, 740.5, 754.0]
Episode 524	 reward: -9.71	 Mean_loss: 0.11172462,  training time: 13.08
[739.0, 747.25, 716.75, 769.5]
Episode 525	 reward: -9.28	 Mean_loss: 0.08118856,  training time: 13.07
[739.75, 691.75, 762.5, 784.5]
Episode 526	 reward: -9.25	 Mean_loss: 0.07994902,  training time: 13.14
[728.5, 703.25, 806.5, 840.25]
Episode 527	 reward: -9.74	 Mean_loss: 0.10126504,  training time: 13.07
[750.0, 721.5, 794.0, 820.75]
Episode 528	 reward: -9.22	 Mean_loss: 0.08917982,  training time: 13.05
[729.0, 686.75, 817.75, 769.0]
Episode 529	 reward: -9.58	 Mean_loss: 0.07139669,  training time: 13.04
[743.75, 686.25, 767.0, 779.5]
Episode 530	 reward: -9.44	 Mean_loss: 0.08261362,  training time: 13.06
[737.5, 711.0, 775.5, 784.75]
Episode 531	 reward: -9.86	 Mean_loss: 0.10305102,  training time: 13.10
[794.75, 716.0, 786.5, 772.75]
Episode 532	 reward: -9.94	 Mean_loss: 0.09063505,  training time: 13.08
[736.25, 744.0, 746.25, 810.0]
Episode 533	 reward: -10.07	 Mean_loss: 0.07316498,  training time: 13.05
[721.75, 706.5, 744.0, 816.5]
Episode 534	 reward: -9.90	 Mean_loss: 0.13051039,  training time: 13.08
[742.0, 705.0, 783.0, 809.75]
Episode 535	 reward: -9.24	 Mean_loss: 0.06394663,  training time: 13.08
[760.5, 719.0, 742.0, 845.75]
Episode 536	 reward: -9.88	 Mean_loss: 0.09943902,  training time: 13.07
[731.5, 696.75, 792.5, 762.25]
Episode 537	 reward: -9.51	 Mean_loss: 0.07193329,  training time: 13.07
[744.75, 720.75, 769.75, 741.25]
Episode 538	 reward: -9.87	 Mean_loss: 0.06922462,  training time: 13.21
[749.75, 762.25, 767.25, 854.25]
Episode 539	 reward: -9.31	 Mean_loss: 0.10157672,  training time: 13.30
[741.25, 747.75, 877.5, 813.5]
Episode 540	 reward: -9.59	 Mean_loss: 0.07043429,  training time: 13.23
[(15, 5), (15, 7), (15, 9), (15, 10)]
[717.75, 812.5, 789.25, 813.25]
Episode 541	 reward: -9.42	 Mean_loss: 0.09479249,  training time: 13.13
[769.25, 739.5, 805.25, 804.5]
Episode 542	 reward: -9.22	 Mean_loss: 0.10162222,  training time: 13.17
[689.0, 780.75, 782.0, 813.25]
Episode 543	 reward: -9.68	 Mean_loss: 0.07329271,  training time: 13.23
[715.25, 771.5, 791.25, 757.5]
Episode 544	 reward: -8.98	 Mean_loss: 0.18859324,  training time: 13.01
[778.0, 777.5, 801.75, 815.75]
Episode 545	 reward: -9.44	 Mean_loss: 0.08607440,  training time: 13.01
[707.75, 691.0, 773.0, 821.75]
Episode 546	 reward: -9.65	 Mean_loss: 0.09554655,  training time: 13.09
[684.5, 745.5, 766.75, 802.5]
Episode 547	 reward: -9.94	 Mean_loss: 0.10957202,  training time: 13.13
[762.25, 730.75, 794.5, 818.5]
Episode 548	 reward: -8.88	 Mean_loss: 0.14894934,  training time: 12.98
[660.75, 744.5, 841.25, 802.0]
Episode 549	 reward: -9.10	 Mean_loss: 0.09582160,  training time: 12.99
[677.5, 738.5, 824.75, 804.0]
Episode 550	 reward: -9.17	 Mean_loss: 0.09975251,  training time: 13.00
[747.75, 755.5, 759.0, 807.5]
Episode 551	 reward: -9.74	 Mean_loss: 0.09171975,  training time: 13.22
[737.5, 770.25, 777.75, 789.5]
Episode 552	 reward: -9.42	 Mean_loss: 0.09517076,  training time: 13.05
[691.5, 734.75, 771.5, 806.5]
Episode 553	 reward: -9.06	 Mean_loss: 0.13455597,  training time: 13.21
[728.5, 739.25, 785.75, 844.75]
Episode 554	 reward: -9.65	 Mean_loss: 0.12399857,  training time: 13.07
[720.25, 705.75, 800.5, 759.75]
Episode 555	 reward: -9.73	 Mean_loss: 0.12723511,  training time: 13.17
[748.5, 696.25, 802.5, 800.25]
Episode 556	 reward: -9.38	 Mean_loss: 0.09141817,  training time: 13.05
[726.25, 696.0, 780.75, 806.0]
Episode 557	 reward: -9.28	 Mean_loss: 0.09260595,  training time: 13.03
[755.75, 748.5, 825.25, 794.0]
Episode 558	 reward: -9.14	 Mean_loss: 0.08728595,  training time: 13.03
[734.75, 740.75, 816.0, 778.75]
Episode 559	 reward: -9.08	 Mean_loss: 0.11274058,  training time: 13.03
[672.25, 797.5, 783.75, 806.75]
Episode 560	 reward: -9.47	 Mean_loss: 0.13293622,  training time: 13.02
[(15, 5), (15, 7), (15, 9), (15, 10)]
[699.5, 727.5, 863.25, 786.0]
Episode 561	 reward: -10.14	 Mean_loss: 0.07252957,  training time: 13.08
[734.0, 773.25, 824.5, 794.25]
Episode 562	 reward: -9.06	 Mean_loss: 0.06652929,  training time: 13.01
[744.75, 730.0, 829.0, 819.5]
Episode 563	 reward: -9.38	 Mean_loss: 0.05203262,  training time: 13.06
[778.0, 756.0, 776.75, 784.0]
Episode 564	 reward: -9.04	 Mean_loss: 0.05708656,  training time: 13.04
[748.0, 738.0, 729.75, 788.75]
Episode 565	 reward: -9.95	 Mean_loss: 0.07116703,  training time: 13.04
[760.0, 757.5, 748.25, 850.75]
Episode 566	 reward: -9.00	 Mean_loss: 0.11350169,  training time: 13.07
[740.5, 776.5, 814.75, 817.5]
Episode 567	 reward: -10.02	 Mean_loss: 0.07737752,  training time: 13.05
[736.5, 721.5, 799.5, 812.25]
Episode 568	 reward: -9.63	 Mean_loss: 0.08530265,  training time: 13.13
[741.25, 682.25, 764.75, 760.75]
Episode 569	 reward: -8.94	 Mean_loss: 0.07158116,  training time: 13.07
[714.25, 776.0, 785.5, 764.75]
Episode 570	 reward: -10.00	 Mean_loss: 0.06831262,  training time: 13.07
[736.25, 747.75, 792.75, 755.0]
Episode 571	 reward: -9.44	 Mean_loss: 0.08739314,  training time: 13.04
[778.25, 756.75, 777.25, 823.5]
Episode 572	 reward: -9.59	 Mean_loss: 0.07207149,  training time: 13.06
[740.25, 710.5, 750.25, 752.0]
Episode 573	 reward: -9.72	 Mean_loss: 0.09813505,  training time: 13.04
[753.75, 707.25, 796.75, 722.75]
Episode 574	 reward: -9.21	 Mean_loss: 0.12321947,  training time: 13.05
[724.75, 781.75, 784.75, 802.5]
Episode 575	 reward: -9.60	 Mean_loss: 0.04074758,  training time: 13.05
[761.5, 722.0, 793.25, 785.25]
Episode 576	 reward: -9.97	 Mean_loss: 0.07461181,  training time: 13.06
[775.75, 709.0, 796.75, 799.0]
Episode 577	 reward: -9.64	 Mean_loss: 0.06809428,  training time: 13.07
[745.5, 748.75, 756.0, 809.5]
Episode 578	 reward: -9.49	 Mean_loss: 0.05736129,  training time: 13.10
[775.75, 723.25, 758.75, 734.0]
Episode 579	 reward: -9.49	 Mean_loss: 0.09880386,  training time: 13.20
[761.5, 736.0, 762.0, 749.25]
Episode 580	 reward: -10.15	 Mean_loss: 0.10634802,  training time: 13.16
[(15, 5), (15, 7), (15, 9), (15, 10)]
[734.5, 798.25, 804.75, 753.25]
Episode 581	 reward: -8.64	 Mean_loss: 0.07877219,  training time: 13.22
[793.0, 700.75, 798.75, 801.5]
Episode 582	 reward: -8.91	 Mean_loss: 0.08913769,  training time: 13.15
[761.0, 696.0, 803.75, 802.25]
Episode 583	 reward: -8.84	 Mean_loss: 0.11284391,  training time: 13.26
[716.25, 697.5, 749.0, 725.25]
Episode 584	 reward: -9.51	 Mean_loss: 0.08995639,  training time: 13.14
[744.25, 695.25, 743.0, 763.25]
Episode 585	 reward: -9.45	 Mean_loss: 0.07065368,  training time: 13.12
[731.75, 773.5, 777.0, 671.75]
Episode 586	 reward: -8.79	 Mean_loss: 0.17650147,  training time: 13.15
[782.75, 758.0, 764.0, 748.75]
Episode 587	 reward: -9.79	 Mean_loss: 0.06597917,  training time: 13.14
[756.75, 711.75, 750.25, 748.25]
Episode 588	 reward: -9.53	 Mean_loss: 0.13266286,  training time: 13.12
[803.75, 745.5, 831.5, 768.75]
Episode 589	 reward: -9.57	 Mean_loss: 0.05924828,  training time: 13.17
[771.0, 732.5, 802.75, 767.0]
Episode 590	 reward: -8.41	 Mean_loss: 0.10207389,  training time: 13.11
[739.75, 761.5, 789.5, 756.25]
Episode 591	 reward: -9.00	 Mean_loss: 0.06966880,  training time: 13.12
[710.25, 747.0, 746.5, 782.25]
Episode 592	 reward: -8.90	 Mean_loss: 0.10867760,  training time: 13.10
[795.0, 717.25, 747.0, 813.75]
Episode 593	 reward: -9.05	 Mean_loss: 0.12577955,  training time: 13.10
[755.25, 739.25, 805.25, 802.25]
Episode 594	 reward: -8.82	 Mean_loss: 0.08571824,  training time: 13.13
[724.0, 758.5, 812.25, 728.5]
Episode 595	 reward: -8.98	 Mean_loss: 0.11153365,  training time: 13.12
[720.0, 741.0, 757.25, 777.5]
Episode 596	 reward: -9.22	 Mean_loss: 0.09319215,  training time: 13.15
[784.0, 739.25, 807.25, 781.25]
Episode 597	 reward: -9.12	 Mean_loss: 0.07945411,  training time: 13.14
[734.0, 726.25, 775.25, 789.0]
Episode 598	 reward: -8.60	 Mean_loss: 0.10075594,  training time: 13.13
[725.0, 754.25, 810.0, 743.75]
Episode 599	 reward: -9.59	 Mean_loss: 0.06415656,  training time: 13.16
[815.75, 732.25, 809.25, 806.75]
Episode 600	 reward: -9.90	 Mean_loss: 0.09492877,  training time: 13.09
[(15, 5), (15, 7), (15, 9), (15, 10)]
[734.5, 768.25, 729.5, 771.0]
Episode 601	 reward: -9.44	 Mean_loss: 0.07091657,  training time: 13.16
[768.0, 732.5, 773.25, 834.0]
Episode 602	 reward: -9.56	 Mean_loss: 0.15044563,  training time: 13.17
[701.0, 737.75, 761.75, 798.5]
Episode 603	 reward: -8.93	 Mean_loss: 0.10292846,  training time: 13.14
[695.75, 733.75, 801.25, 780.75]
Episode 604	 reward: -9.69	 Mean_loss: 0.08255026,  training time: 13.08
[743.25, 712.0, 775.5, 813.25]
Episode 605	 reward: -10.04	 Mean_loss: 0.07788184,  training time: 13.13
[744.75, 775.0, 747.75, 747.25]
Episode 606	 reward: -8.94	 Mean_loss: 0.12798066,  training time: 13.12
[761.25, 746.25, 765.75, 790.5]
Episode 607	 reward: -10.13	 Mean_loss: 0.08474103,  training time: 13.12
[742.5, 732.25, 733.25, 821.0]
Episode 608	 reward: -8.85	 Mean_loss: 0.07415631,  training time: 13.10
[726.0, 790.5, 783.25, 793.5]
Episode 609	 reward: -9.61	 Mean_loss: 0.09352645,  training time: 13.40
[715.25, 705.5, 742.75, 822.75]
Episode 610	 reward: -9.86	 Mean_loss: 0.09644679,  training time: 13.48
[746.75, 736.5, 796.25, 755.5]
Episode 611	 reward: -9.14	 Mean_loss: 0.09540957,  training time: 13.39
[731.5, 738.25, 758.25, 803.0]
Episode 612	 reward: -10.24	 Mean_loss: 0.14926744,  training time: 13.38
[737.5, 732.5, 759.75, 778.25]
Episode 613	 reward: -9.54	 Mean_loss: 0.13570526,  training time: 13.36
[745.25, 760.5, 794.5, 800.0]
Episode 614	 reward: -9.80	 Mean_loss: 0.10171804,  training time: 13.37
[745.5, 753.5, 814.5, 814.75]
Episode 615	 reward: -9.67	 Mean_loss: 0.09896953,  training time: 13.12
[667.25, 735.5, 846.5, 770.25]
Episode 616	 reward: -9.91	 Mean_loss: 0.10105241,  training time: 13.14
[720.5, 792.5, 742.5, 801.5]
Episode 617	 reward: -9.38	 Mean_loss: 0.11925121,  training time: 13.17
[721.5, 803.25, 792.0, 793.5]
Episode 618	 reward: -9.31	 Mean_loss: 0.10667090,  training time: 13.19
[730.5, 732.5, 794.0, 790.75]
Episode 619	 reward: -9.09	 Mean_loss: 0.08657829,  training time: 13.18
[687.25, 737.75, 815.75, 836.25]
Episode 620	 reward: -9.77	 Mean_loss: 0.10961948,  training time: 13.19
[(15, 5), (15, 7), (15, 9), (15, 10)]
[640.25, 711.75, 790.25, 728.0]
Episode 621	 reward: -9.33	 Mean_loss: 0.07692609,  training time: 13.23
[680.0, 721.75, 825.5, 800.75]
Episode 622	 reward: -9.29	 Mean_loss: 0.07395969,  training time: 13.19
[692.25, 757.25, 808.25, 778.0]
Episode 623	 reward: -8.94	 Mean_loss: 0.05171438,  training time: 13.17
[669.5, 742.0, 799.5, 799.5]
Episode 624	 reward: -9.30	 Mean_loss: 0.06046319,  training time: 13.16
[684.0, 765.5, 740.75, 792.0]
Episode 625	 reward: -9.22	 Mean_loss: 0.10835829,  training time: 13.46
[697.75, 706.75, 818.25, 736.75]
Episode 626	 reward: -9.75	 Mean_loss: 0.11391424,  training time: 13.37
[671.75, 722.0, 777.75, 742.0]
Episode 627	 reward: -9.31	 Mean_loss: 0.06880794,  training time: 13.50
[707.5, 686.0, 744.0, 768.0]
Episode 628	 reward: -9.63	 Mean_loss: 0.10016883,  training time: 13.13
[664.0, 793.75, 729.75, 747.25]
Episode 629	 reward: -9.50	 Mean_loss: 0.05992560,  training time: 13.33
[675.5, 743.0, 711.5, 814.25]
Episode 630	 reward: -10.02	 Mean_loss: 0.07643682,  training time: 13.40
[677.0, 662.0, 825.25, 782.0]
Episode 631	 reward: -9.30	 Mean_loss: 0.09844859,  training time: 13.44
[714.75, 757.5, 753.75, 720.75]
Episode 632	 reward: -9.13	 Mean_loss: 0.12489327,  training time: 13.37
[680.25, 725.0, 760.25, 761.25]
Episode 633	 reward: -9.15	 Mean_loss: 0.08233561,  training time: 13.38
[682.75, 771.5, 793.5, 820.0]
Episode 634	 reward: -8.98	 Mean_loss: 0.13424955,  training time: 13.36
[662.75, 743.5, 751.25, 732.75]
Episode 635	 reward: -9.76	 Mean_loss: 0.09311876,  training time: 13.41
[713.5, 691.75, 731.75, 761.5]
Episode 636	 reward: -9.39	 Mean_loss: 0.08211578,  training time: 13.39
[651.25, 745.0, 769.75, 745.25]
Episode 637	 reward: -8.91	 Mean_loss: 0.09673089,  training time: 13.38
[661.25, 727.5, 786.75, 788.5]
Episode 638	 reward: -9.57	 Mean_loss: 0.08479563,  training time: 13.39
[688.5, 723.5, 733.25, 798.0]
Episode 639	 reward: -9.63	 Mean_loss: 0.10595253,  training time: 13.40
[747.75, 768.5, 757.25, 798.25]
Episode 640	 reward: -8.97	 Mean_loss: 0.08582035,  training time: 13.41
[(15, 5), (15, 7), (15, 9), (15, 10)]
[746.25, 686.75, 778.0, 836.25]
Episode 641	 reward: -9.67	 Mean_loss: 0.08824296,  training time: 13.29
[740.5, 678.0, 770.0, 810.75]
Episode 642	 reward: -9.82	 Mean_loss: 0.09259228,  training time: 13.07
[682.5, 721.75, 803.75, 806.25]
Episode 643	 reward: -10.43	 Mean_loss: 0.09613084,  training time: 13.32
[718.25, 680.25, 745.5, 805.75]
Episode 644	 reward: -8.79	 Mean_loss: 0.15372074,  training time: 13.36
[713.5, 728.0, 746.25, 851.5]
Episode 645	 reward: -9.31	 Mean_loss: 0.17268126,  training time: 13.39
[773.25, 732.75, 756.75, 767.25]
Episode 646	 reward: -9.45	 Mean_loss: 0.10058565,  training time: 13.36
[715.25, 748.0, 690.5, 797.0]
Episode 647	 reward: -9.62	 Mean_loss: 0.07102446,  training time: 13.40
[763.5, 695.5, 789.0, 739.5]
Episode 648	 reward: -9.81	 Mean_loss: 0.10941025,  training time: 13.37
[704.0, 698.0, 741.25, 807.5]
Episode 649	 reward: -9.38	 Mean_loss: 0.08297011,  training time: 13.37
[713.25, 650.0, 820.25, 760.0]
Episode 650	 reward: -9.47	 Mean_loss: 0.09392477,  training time: 13.37
[753.75, 678.25, 825.5, 813.75]
Episode 651	 reward: -9.09	 Mean_loss: 0.14487343,  training time: 13.38
[738.25, 699.5, 792.75, 790.0]
Episode 652	 reward: -8.81	 Mean_loss: 0.08861290,  training time: 13.44
[763.75, 734.75, 740.5, 761.0]
Episode 653	 reward: -9.64	 Mean_loss: 0.11569023,  training time: 13.39
[741.75, 776.75, 714.25, 806.25]
Episode 654	 reward: -9.56	 Mean_loss: 0.06802900,  training time: 13.39
[772.25, 672.5, 764.75, 837.25]
Episode 655	 reward: -9.34	 Mean_loss: 0.10993253,  training time: 13.13
[721.75, 709.0, 758.0, 781.5]
Episode 656	 reward: -9.41	 Mean_loss: 0.09260217,  training time: 13.03
[799.5, 718.25, 792.75, 769.5]
Episode 657	 reward: -9.02	 Mean_loss: 0.10833845,  training time: 13.09
[730.5, 695.25, 722.5, 755.5]
Episode 658	 reward: -9.37	 Mean_loss: 0.11486631,  training time: 13.06
[774.5, 721.5, 753.0, 739.5]
Episode 659	 reward: -9.82	 Mean_loss: 0.10716671,  training time: 13.05
[772.5, 667.5, 808.5, 841.5]
Episode 660	 reward: -9.34	 Mean_loss: 0.10416265,  training time: 13.05
[(15, 5), (15, 7), (15, 9), (15, 10)]
[709.75, 762.0, 711.0, 775.75]
Episode 661	 reward: -9.35	 Mean_loss: 0.10511056,  training time: 13.29
[694.5, 758.0, 698.0, 768.0]
Episode 662	 reward: -9.45	 Mean_loss: 0.15318044,  training time: 13.24
[688.25, 789.0, 737.25, 795.0]
Episode 663	 reward: -9.15	 Mean_loss: 0.08304469,  training time: 13.16
[678.25, 787.5, 703.25, 798.75]
Episode 664	 reward: -8.34	 Mean_loss: 0.12743449,  training time: 13.16
[707.5, 747.75, 694.0, 763.75]
Episode 665	 reward: -9.77	 Mean_loss: 0.08786707,  training time: 13.15
[714.5, 760.75, 712.25, 753.75]
Episode 666	 reward: -9.09	 Mean_loss: 0.07705251,  training time: 13.15
[686.5, 735.75, 700.75, 712.0]
Episode 667	 reward: -9.48	 Mean_loss: 0.12253559,  training time: 13.13
[639.75, 835.5, 741.5, 773.0]
Episode 668	 reward: -9.67	 Mean_loss: 0.10740570,  training time: 13.12
[644.0, 770.25, 723.75, 762.25]
Episode 669	 reward: -9.28	 Mean_loss: 0.08941098,  training time: 13.40
[705.75, 826.75, 759.25, 767.0]
Episode 670	 reward: -9.14	 Mean_loss: 0.12535532,  training time: 13.40
[705.5, 740.0, 668.75, 782.0]
Episode 671	 reward: -9.22	 Mean_loss: 0.07550126,  training time: 13.20
[668.75, 770.25, 662.75, 815.5]
Episode 672	 reward: -9.66	 Mean_loss: 0.10431946,  training time: 13.18
[696.0, 752.0, 693.0, 766.0]
Episode 673	 reward: -9.41	 Mean_loss: 0.11135422,  training time: 13.26
[641.25, 761.5, 712.0, 769.5]
Episode 674	 reward: -10.20	 Mean_loss: 0.14133720,  training time: 13.19
[659.25, 820.25, 708.0, 790.5]
Episode 675	 reward: -9.46	 Mean_loss: 0.15829819,  training time: 13.16
[682.5, 775.75, 714.5, 765.25]
Episode 676	 reward: -9.39	 Mean_loss: 0.10054851,  training time: 13.17
[678.75, 755.75, 727.0, 774.75]
Episode 677	 reward: -9.57	 Mean_loss: 0.10351471,  training time: 13.19
[667.0, 779.5, 688.25, 752.75]
Episode 678	 reward: -9.26	 Mean_loss: 0.11643523,  training time: 13.16
[689.5, 753.75, 677.25, 797.75]
Episode 679	 reward: -9.05	 Mean_loss: 0.07324955,  training time: 13.20
[723.25, 808.0, 702.75, 765.75]
Episode 680	 reward: -9.97	 Mean_loss: 0.09814435,  training time: 13.21
[(15, 5), (15, 7), (15, 9), (15, 10)]
[708.25, 803.25, 798.75, 781.75]
Episode 681	 reward: -8.84	 Mean_loss: 0.06197698,  training time: 13.47
[715.25, 696.75, 814.75, 771.25]
Episode 682	 reward: -9.21	 Mean_loss: 0.07175007,  training time: 13.41
[710.5, 774.5, 765.25, 811.0]
Episode 683	 reward: -9.62	 Mean_loss: 0.09157191,  training time: 13.39
[703.75, 751.75, 779.75, 802.25]
Episode 684	 reward: -9.21	 Mean_loss: 0.10745677,  training time: 13.41
[701.75, 766.25, 735.25, 747.25]
Episode 685	 reward: -9.58	 Mean_loss: 0.06394708,  training time: 13.39
[703.75, 727.25, 815.25, 809.0]
Episode 686	 reward: -9.56	 Mean_loss: 0.05217669,  training time: 13.40
[705.5, 749.5, 808.5, 771.25]
Episode 687	 reward: -9.31	 Mean_loss: 0.07474867,  training time: 13.37
[690.75, 745.75, 806.25, 751.25]
Episode 688	 reward: -9.26	 Mean_loss: 0.09689690,  training time: 13.40
[681.25, 727.25, 768.5, 762.5]
Episode 689	 reward: -9.33	 Mean_loss: 0.10478111,  training time: 13.39
[721.0, 763.75, 700.75, 789.25]
Episode 690	 reward: -9.02	 Mean_loss: 0.09884895,  training time: 13.38
[727.0, 740.0, 749.0, 791.25]
Episode 691	 reward: -9.23	 Mean_loss: 0.06722897,  training time: 13.39
[665.5, 768.25, 807.5, 741.75]
Episode 692	 reward: -10.28	 Mean_loss: 0.07920277,  training time: 13.38
[671.25, 758.0, 785.0, 765.0]
Episode 693	 reward: -8.84	 Mean_loss: 0.10103570,  training time: 13.38
[680.5, 777.5, 776.5, 732.25]
Episode 694	 reward: -10.04	 Mean_loss: 0.10923211,  training time: 13.47
[685.0, 718.75, 779.25, 731.25]
Episode 695	 reward: -9.32	 Mean_loss: 0.07053938,  training time: 13.26
[668.0, 785.25, 775.5, 755.25]
Episode 696	 reward: -9.46	 Mean_loss: 0.09370552,  training time: 13.13
[686.75, 757.25, 772.5, 835.75]
Episode 697	 reward: -9.20	 Mean_loss: 0.09597449,  training time: 13.15
[687.75, 755.5, 823.75, 743.75]
Episode 698	 reward: -9.14	 Mean_loss: 0.07258233,  training time: 13.14
[754.0, 723.25, 799.0, 789.75]
Episode 699	 reward: -8.28	 Mean_loss: 0.07827179,  training time: 13.12
[699.75, 803.75, 756.75, 753.75]
Episode 700	 reward: -8.51	 Mean_loss: 0.08239225,  training time: 13.13
[(15, 5), (15, 7), (15, 9), (15, 10)]
[728.0, 668.5, 746.25, 791.75]
Episode 701	 reward: -9.18	 Mean_loss: 0.06464489,  training time: 13.17
[726.75, 678.75, 745.5, 748.0]
Episode 702	 reward: -9.08	 Mean_loss: 0.11495207,  training time: 13.15
[773.5, 719.5, 741.75, 767.5]
Episode 703	 reward: -8.98	 Mean_loss: 0.08965341,  training time: 13.12
[721.0, 709.25, 744.0, 749.5]
Episode 704	 reward: -9.78	 Mean_loss: 0.06218357,  training time: 13.13
[739.75, 682.0, 785.75, 706.25]
Episode 705	 reward: -9.03	 Mean_loss: 0.09091109,  training time: 13.10
[760.25, 684.0, 739.0, 720.5]
Episode 706	 reward: -9.45	 Mean_loss: 0.08259223,  training time: 13.15
[726.5, 757.0, 749.75, 697.25]
Episode 707	 reward: -9.45	 Mean_loss: 0.11297820,  training time: 13.04
[749.75, 706.75, 754.5, 746.5]
Episode 708	 reward: -9.16	 Mean_loss: 0.07549942,  training time: 13.05
[697.5, 700.5, 756.25, 730.0]
Episode 709	 reward: -9.44	 Mean_loss: 0.07389752,  training time: 13.06
[737.75, 663.5, 701.25, 702.0]
Episode 710	 reward: -9.21	 Mean_loss: 0.09149286,  training time: 13.04
[700.25, 712.5, 761.5, 725.5]
Episode 711	 reward: -8.58	 Mean_loss: 0.07726408,  training time: 13.04
[711.25, 691.0, 758.5, 760.0]
Episode 712	 reward: -9.86	 Mean_loss: 0.05529797,  training time: 13.09
[750.5, 696.5, 803.5, 743.75]
Episode 713	 reward: -8.67	 Mean_loss: 0.07430577,  training time: 13.03
[723.5, 727.25, 715.25, 773.75]
Episode 714	 reward: -9.01	 Mean_loss: 0.07783344,  training time: 13.06
[706.5, 669.25, 787.5, 736.75]
Episode 715	 reward: -9.93	 Mean_loss: 0.07924166,  training time: 13.40
[742.0, 650.75, 788.75, 835.0]
Episode 716	 reward: -9.33	 Mean_loss: 0.08756080,  training time: 13.13
[707.25, 728.5, 757.0, 760.5]
Episode 717	 reward: -9.52	 Mean_loss: 0.09065263,  training time: 13.14
[760.25, 731.25, 759.75, 801.0]
Episode 718	 reward: -9.44	 Mean_loss: 0.08665644,  training time: 13.12
[788.0, 697.5, 724.25, 712.0]
Episode 719	 reward: -9.13	 Mean_loss: 0.06931634,  training time: 13.14
[735.5, 707.5, 748.75, 796.75]
Episode 720	 reward: -9.05	 Mean_loss: 0.09202828,  training time: 13.18
[(15, 5), (15, 7), (15, 9), (15, 10)]
[713.25, 712.0, 723.0, 801.75]
Episode 721	 reward: -8.96	 Mean_loss: 0.05398833,  training time: 13.21
[726.0, 750.5, 727.5, 752.25]
Episode 722	 reward: -9.61	 Mean_loss: 0.04322561,  training time: 13.15
[713.25, 708.0, 764.25, 736.75]
Episode 723	 reward: -9.72	 Mean_loss: 0.04719509,  training time: 13.14
[745.5, 691.0, 748.0, 710.5]
Episode 724	 reward: -9.96	 Mean_loss: 0.08499064,  training time: 13.15
[760.5, 701.5, 742.25, 748.0]
Episode 725	 reward: -8.73	 Mean_loss: 0.03818459,  training time: 13.22
[757.75, 682.5, 731.0, 737.5]
Episode 726	 reward: -9.71	 Mean_loss: 0.05752065,  training time: 13.22
[734.25, 723.5, 727.5, 744.5]
Episode 727	 reward: -9.43	 Mean_loss: 0.05899314,  training time: 13.24
[735.5, 693.5, 767.75, 784.75]
Episode 728	 reward: -9.08	 Mean_loss: 0.06845110,  training time: 13.19
[766.5, 713.5, 780.75, 799.25]
Episode 729	 reward: -9.20	 Mean_loss: 0.09642325,  training time: 13.17
[735.5, 699.5, 747.25, 789.75]
Episode 730	 reward: -9.33	 Mean_loss: 0.04741092,  training time: 13.22
[714.25, 683.75, 713.75, 791.0]
Episode 731	 reward: -9.37	 Mean_loss: 0.05222944,  training time: 13.46
[707.0, 695.25, 716.5, 811.0]
Episode 732	 reward: -9.64	 Mean_loss: 0.06838980,  training time: 13.39
[735.25, 727.75, 776.75, 767.25]
Episode 733	 reward: -9.13	 Mean_loss: 0.05589754,  training time: 13.36
[764.25, 687.0, 744.75, 729.75]
Episode 734	 reward: -9.59	 Mean_loss: 0.04895249,  training time: 13.08
[728.0, 743.5, 763.5, 760.0]
Episode 735	 reward: -9.96	 Mean_loss: 0.05834014,  training time: 13.07
[689.25, 719.5, 731.0, 723.5]
Episode 736	 reward: -9.00	 Mean_loss: 0.06227453,  training time: 13.16
[694.0, 695.75, 755.5, 784.25]
Episode 737	 reward: -9.47	 Mean_loss: 0.05078265,  training time: 13.08
[683.0, 690.5, 748.0, 874.0]
Episode 738	 reward: -9.14	 Mean_loss: 0.14095090,  training time: 13.25
[731.5, 733.5, 738.0, 794.5]
Episode 739	 reward: -10.68	 Mean_loss: 0.07983442,  training time: 13.36
[713.75, 723.0, 708.75, 787.5]
Episode 740	 reward: -8.87	 Mean_loss: 0.05444454,  training time: 13.17
[(15, 5), (15, 7), (15, 9), (15, 10)]
[740.0, 675.25, 815.0, 792.5]
Episode 741	 reward: -9.08	 Mean_loss: 0.07356136,  training time: 13.38
[733.5, 717.25, 741.25, 749.5]
Episode 742	 reward: -9.22	 Mean_loss: 0.10205654,  training time: 13.38
[737.0, 695.0, 786.5, 796.75]
Episode 743	 reward: -9.54	 Mean_loss: 0.14183000,  training time: 13.38
[763.75, 722.25, 742.5, 727.0]
Episode 744	 reward: -8.77	 Mean_loss: 0.08783041,  training time: 13.37
[736.5, 732.0, 734.25, 785.25]
Episode 745	 reward: -9.35	 Mean_loss: 0.07178028,  training time: 13.37
[761.75, 705.75, 719.5, 782.75]
Episode 746	 reward: -9.35	 Mean_loss: 0.07402579,  training time: 13.39
[722.25, 725.75, 674.5, 786.25]
Episode 747	 reward: -9.23	 Mean_loss: 0.06567485,  training time: 13.39
[766.0, 694.25, 760.5, 726.5]
Episode 748	 reward: -9.78	 Mean_loss: 0.15596189,  training time: 13.39
[706.0, 683.5, 776.75, 775.75]
Episode 749	 reward: -9.94	 Mean_loss: 0.08101445,  training time: 13.37
[730.0, 694.25, 736.25, 733.25]
Episode 750	 reward: -9.61	 Mean_loss: 0.12255567,  training time: 13.41
[747.0, 694.5, 808.5, 765.25]
Episode 751	 reward: -8.89	 Mean_loss: 0.10381649,  training time: 13.39
[719.25, 686.25, 750.0, 743.5]
Episode 752	 reward: -9.04	 Mean_loss: 0.10232943,  training time: 13.41
[747.25, 715.25, 749.5, 768.25]
Episode 753	 reward: -9.29	 Mean_loss: 0.12727018,  training time: 13.40
[710.75, 685.5, 730.75, 778.25]
Episode 754	 reward: -9.20	 Mean_loss: 0.10096270,  training time: 13.38
[724.0, 668.75, 760.25, 760.0]
Episode 755	 reward: -8.94	 Mean_loss: 0.09313922,  training time: 13.34
[727.75, 692.25, 728.75, 791.0]
Episode 756	 reward: -9.28	 Mean_loss: 0.06909811,  training time: 13.04
[743.5, 661.0, 718.25, 774.5]
Episode 757	 reward: -9.48	 Mean_loss: 0.09703269,  training time: 13.17
[751.0, 693.0, 752.75, 741.75]
Episode 758	 reward: -9.76	 Mean_loss: 0.10369194,  training time: 13.07
[727.0, 717.0, 777.25, 773.25]
Episode 759	 reward: -9.42	 Mean_loss: 0.11640723,  training time: 13.15
[723.75, 668.25, 693.75, 747.0]
Episode 760	 reward: -9.01	 Mean_loss: 0.10169959,  training time: 13.17
[(15, 5), (15, 7), (15, 9), (15, 10)]
[695.5, 693.75, 757.75, 721.5]
Episode 761	 reward: -8.66	 Mean_loss: 0.08181894,  training time: 13.22
[718.75, 697.5, 734.75, 756.5]
Episode 762	 reward: -9.52	 Mean_loss: 0.06907966,  training time: 13.17
[698.75, 708.25, 714.5, 790.25]
Episode 763	 reward: -9.29	 Mean_loss: 0.07728529,  training time: 13.17
[659.0, 706.25, 726.75, 787.0]
Episode 764	 reward: -9.22	 Mean_loss: 0.06176562,  training time: 13.15
[685.5, 686.5, 712.0, 778.5]
Episode 765	 reward: -9.03	 Mean_loss: 0.07413049,  training time: 13.16
[781.5, 691.25, 782.75, 762.25]
Episode 766	 reward: -9.15	 Mean_loss: 0.10797896,  training time: 13.20
[705.75, 677.5, 753.5, 794.0]
Episode 767	 reward: -9.03	 Mean_loss: 0.07406420,  training time: 13.25
[712.75, 685.75, 734.0, 740.5]
Episode 768	 reward: -9.36	 Mean_loss: 0.08338612,  training time: 13.18
[729.75, 717.5, 797.0, 753.0]
Episode 769	 reward: -8.77	 Mean_loss: 0.04510355,  training time: 13.16
[759.75, 711.75, 728.0, 796.0]
Episode 770	 reward: -9.06	 Mean_loss: 0.10687434,  training time: 13.23
[717.25, 693.0, 729.75, 758.0]
Episode 771	 reward: -8.83	 Mean_loss: 0.11182281,  training time: 13.21
[698.75, 686.0, 750.0, 741.0]
Episode 772	 reward: -9.16	 Mean_loss: 0.07022549,  training time: 13.18
[699.5, 680.25, 727.75, 749.0]
Episode 773	 reward: -8.87	 Mean_loss: 0.12312680,  training time: 13.17
[726.0, 699.25, 746.25, 728.25]
Episode 774	 reward: -9.02	 Mean_loss: 0.07808414,  training time: 13.21
[703.0, 727.25, 745.0, 743.75]
Episode 775	 reward: -9.46	 Mean_loss: 0.07211047,  training time: 13.18
[689.25, 700.0, 797.25, 715.75]
Episode 776	 reward: -10.52	 Mean_loss: 0.10388248,  training time: 13.19
[695.75, 645.5, 745.25, 752.25]
Episode 777	 reward: -8.71	 Mean_loss: 0.07872648,  training time: 13.18
[721.25, 667.25, 743.25, 751.75]
Episode 778	 reward: -9.73	 Mean_loss: 0.07481814,  training time: 13.24
[688.25, 705.5, 718.0, 768.0]
Episode 779	 reward: -8.87	 Mean_loss: 0.08078645,  training time: 13.19
[711.0, 657.75, 754.25, 803.0]
Episode 780	 reward: -9.20	 Mean_loss: 0.15191369,  training time: 13.31
[(15, 5), (15, 7), (15, 9), (15, 10)]
[733.0, 689.0, 724.75, 815.0]
Episode 781	 reward: -9.59	 Mean_loss: 0.07091726,  training time: 13.34
[771.5, 710.25, 746.75, 865.5]
Episode 782	 reward: -9.60	 Mean_loss: 0.10357824,  training time: 13.32
[735.5, 770.5, 704.5, 850.5]
Episode 783	 reward: -9.74	 Mean_loss: 0.09523545,  training time: 13.20
[726.25, 659.25, 750.25, 827.0]
Episode 784	 reward: -9.96	 Mean_loss: 0.11508206,  training time: 13.20
[722.0, 674.75, 788.75, 812.5]
Episode 785	 reward: -10.13	 Mean_loss: 0.06926410,  training time: 13.18
[727.25, 688.75, 765.5, 865.0]
Episode 786	 reward: -9.84	 Mean_loss: 0.12920335,  training time: 13.19
[765.5, 669.75, 783.25, 833.25]
Episode 787	 reward: -10.06	 Mean_loss: 0.09258687,  training time: 13.19
[729.25, 712.75, 795.75, 870.75]
Episode 788	 reward: -9.78	 Mean_loss: 0.12995347,  training time: 13.16
[746.25, 735.5, 718.75, 808.75]
Episode 789	 reward: -10.38	 Mean_loss: 0.09332103,  training time: 13.24
[715.5, 779.25, 792.5, 890.25]
Episode 790	 reward: -9.88	 Mean_loss: 0.17686817,  training time: 13.20
[775.5, 682.0, 780.75, 802.5]
Episode 791	 reward: -10.19	 Mean_loss: 0.08872613,  training time: 13.20
[722.75, 730.0, 751.0, 825.25]
Episode 792	 reward: -9.87	 Mean_loss: 0.09140902,  training time: 13.20
[746.25, 708.0, 800.5, 853.5]
Episode 793	 reward: -10.07	 Mean_loss: 0.14213580,  training time: 13.36
[758.25, 702.5, 749.0, 817.0]
Episode 794	 reward: -9.87	 Mean_loss: 0.09999739,  training time: 13.19
[749.25, 706.0, 752.5, 815.0]
Episode 795	 reward: -9.92	 Mean_loss: 0.11387436,  training time: 13.07
[775.75, 640.5, 784.5, 831.0]
Episode 796	 reward: -9.74	 Mean_loss: 0.06270623,  training time: 13.04
[703.75, 735.75, 735.0, 850.0]
Episode 797	 reward: -9.40	 Mean_loss: 0.08497934,  training time: 13.09
[723.5, 726.75, 788.5, 811.0]
Episode 798	 reward: -9.50	 Mean_loss: 0.07368009,  training time: 13.08
[666.5, 739.0, 726.0, 784.5]
Episode 799	 reward: -9.00	 Mean_loss: 0.11475027,  training time: 13.12
[748.0, 785.75, 773.0, 837.0]
Episode 800	 reward: -9.83	 Mean_loss: 0.07897326,  training time: 13.06
[(15, 5), (15, 7), (15, 9), (15, 10)]
[770.75, 673.5, 714.0, 832.5]
Episode 801	 reward: -9.32	 Mean_loss: 0.13873355,  training time: 13.13
[749.0, 687.0, 733.75, 768.75]
Episode 802	 reward: -10.08	 Mean_loss: 0.06527138,  training time: 13.08
[707.75, 674.0, 771.75, 761.5]
Episode 803	 reward: -9.70	 Mean_loss: 0.12081126,  training time: 13.06
[682.25, 744.5, 748.75, 789.25]
Episode 804	 reward: -9.68	 Mean_loss: 0.08086095,  training time: 13.22
[673.75, 687.25, 726.0, 783.5]
Episode 805	 reward: -8.71	 Mean_loss: 0.08404796,  training time: 13.31
[723.25, 684.0, 729.5, 772.25]
Episode 806	 reward: -9.50	 Mean_loss: 0.07966185,  training time: 13.18
[727.25, 663.0, 734.0, 808.75]
Episode 807	 reward: -9.69	 Mean_loss: 0.06435256,  training time: 13.19
[699.0, 703.0, 718.0, 802.5]
Episode 808	 reward: -8.89	 Mean_loss: 0.07341865,  training time: 13.16
[751.25, 723.0, 767.75, 750.25]
Episode 809	 reward: -9.08	 Mean_loss: 0.09835831,  training time: 13.17
[675.0, 716.75, 741.25, 765.25]
Episode 810	 reward: -9.32	 Mean_loss: 0.06904879,  training time: 13.27
[730.5, 736.5, 735.25, 796.75]
Episode 811	 reward: -9.35	 Mean_loss: 0.10589366,  training time: 13.39
[720.75, 695.25, 757.75, 732.25]
Episode 812	 reward: -9.93	 Mean_loss: 0.07398505,  training time: 13.37
[745.0, 693.25, 738.25, 795.5]
Episode 813	 reward: -9.93	 Mean_loss: 0.08092088,  training time: 13.34
[752.5, 703.75, 721.5, 786.0]
Episode 814	 reward: -9.67	 Mean_loss: 0.09234624,  training time: 13.36
[725.5, 716.0, 728.0, 804.5]
Episode 815	 reward: -9.22	 Mean_loss: 0.07158918,  training time: 13.38
[706.75, 676.25, 734.25, 794.75]
Episode 816	 reward: -9.19	 Mean_loss: 0.08986325,  training time: 13.36
[717.25, 679.25, 724.5, 760.75]
Episode 817	 reward: -9.18	 Mean_loss: 0.07429319,  training time: 13.37
[694.75, 707.5, 695.25, 836.5]
Episode 818	 reward: -9.37	 Mean_loss: 0.04799344,  training time: 13.39
[705.75, 717.5, 768.75, 785.5]
Episode 819	 reward: -9.54	 Mean_loss: 0.09013326,  training time: 13.37
[730.25, 692.25, 735.75, 772.25]
Episode 820	 reward: -9.51	 Mean_loss: 0.07356754,  training time: 13.51
[(15, 5), (15, 7), (15, 9), (15, 10)]
[714.0, 726.0, 794.25, 788.75]
Episode 821	 reward: -8.97	 Mean_loss: 0.07315821,  training time: 13.28
[708.0, 747.0, 757.75, 744.0]
Episode 822	 reward: -9.34	 Mean_loss: 0.06250632,  training time: 13.19
[743.75, 774.75, 788.25, 738.25]
Episode 823	 reward: -9.43	 Mean_loss: 0.07479463,  training time: 13.15
[703.0, 766.5, 759.0, 761.75]
Episode 824	 reward: -9.12	 Mean_loss: 0.07093067,  training time: 13.10
[666.75, 785.25, 741.25, 790.75]
Episode 825	 reward: -9.24	 Mean_loss: 0.12722962,  training time: 13.03
[732.75, 767.75, 694.75, 737.5]
Episode 826	 reward: -9.65	 Mean_loss: 0.07467575,  training time: 13.05
[726.75, 756.5, 820.0, 770.25]
Episode 827	 reward: -9.09	 Mean_loss: 0.06903803,  training time: 13.03
[707.75, 764.25, 731.25, 776.0]
Episode 828	 reward: -9.49	 Mean_loss: 0.08342972,  training time: 13.70
[706.5, 724.5, 786.25, 719.75]
Episode 829	 reward: -9.07	 Mean_loss: 0.09761281,  training time: 13.39
[669.5, 801.5, 729.25, 745.0]
Episode 830	 reward: -9.33	 Mean_loss: 0.06636574,  training time: 13.37
[711.25, 743.0, 743.0, 759.25]
Episode 831	 reward: -9.51	 Mean_loss: 0.09966921,  training time: 13.39
[789.75, 777.5, 779.0, 748.0]
Episode 832	 reward: -9.82	 Mean_loss: 0.07881801,  training time: 13.37
[666.0, 798.0, 743.75, 742.0]
Episode 833	 reward: -8.78	 Mean_loss: 0.05818846,  training time: 13.38
[717.75, 789.0, 755.5, 744.25]
Episode 834	 reward: -8.55	 Mean_loss: 0.06450964,  training time: 13.35
[711.5, 772.25, 711.25, 761.5]
Episode 835	 reward: -8.43	 Mean_loss: 0.06590594,  training time: 13.17
[731.25, 736.5, 743.75, 826.25]
Episode 836	 reward: -8.73	 Mean_loss: 0.08732141,  training time: 13.18
[697.25, 762.0, 794.0, 760.25]
Episode 837	 reward: -9.42	 Mean_loss: 0.06943619,  training time: 13.24
[689.0, 775.0, 760.5, 767.75]
Episode 838	 reward: -9.17	 Mean_loss: 0.06308126,  training time: 13.15
[674.5, 766.0, 782.25, 767.5]
Episode 839	 reward: -9.12	 Mean_loss: 0.07577761,  training time: 13.15
[683.0, 785.5, 765.75, 721.25]
Episode 840	 reward: -8.72	 Mean_loss: 0.08696916,  training time: 13.15
[(15, 5), (15, 7), (15, 9), (15, 10)]
[743.0, 785.25, 785.75, 806.25]
Episode 841	 reward: -9.50	 Mean_loss: 0.06088687,  training time: 13.28
[688.25, 759.75, 787.75, 771.5]
Episode 842	 reward: -9.20	 Mean_loss: 0.05322276,  training time: 13.17
[730.0, 752.25, 843.25, 804.75]
Episode 843	 reward: -9.96	 Mean_loss: 0.05264721,  training time: 13.17
[734.0, 744.5, 797.25, 745.0]
Episode 844	 reward: -9.45	 Mean_loss: 0.03570475,  training time: 13.16
[716.5, 694.0, 768.25, 761.0]
Episode 845	 reward: -8.82	 Mean_loss: 0.06964727,  training time: 13.17
[716.25, 780.0, 782.75, 780.5]
Episode 846	 reward: -9.12	 Mean_loss: 0.05616453,  training time: 13.17
[711.5, 745.75, 745.75, 760.0]
Episode 847	 reward: -8.93	 Mean_loss: 0.05424313,  training time: 13.16
[781.5, 764.25, 850.0, 716.5]
Episode 848	 reward: -9.40	 Mean_loss: 0.06620142,  training time: 13.16
[748.0, 700.0, 823.25, 722.75]
Episode 849	 reward: -9.20	 Mean_loss: 0.06319439,  training time: 13.18
[699.0, 727.75, 760.0, 728.25]
Episode 850	 reward: -9.16	 Mean_loss: 0.09225103,  training time: 13.15
[706.5, 692.75, 697.75, 769.5]
Episode 851	 reward: -9.03	 Mean_loss: 0.05046581,  training time: 13.18
[710.0, 771.25, 803.5, 765.5]
Episode 852	 reward: -10.16	 Mean_loss: 0.07170345,  training time: 13.18
[711.75, 673.0, 832.75, 768.0]
Episode 853	 reward: -9.35	 Mean_loss: 0.04541126,  training time: 13.14
[722.5, 686.0, 794.25, 766.0]
Episode 854	 reward: -9.69	 Mean_loss: 0.04646209,  training time: 13.16
[749.75, 781.25, 792.75, 722.25]
Episode 855	 reward: -9.86	 Mean_loss: 0.07047822,  training time: 13.14
[720.5, 718.0, 842.25, 761.0]
Episode 856	 reward: -9.19	 Mean_loss: 0.06881930,  training time: 13.15
[742.75, 726.25, 902.25, 777.75]
Episode 857	 reward: -9.84	 Mean_loss: 0.05788959,  training time: 13.13
[701.25, 722.0, 838.5, 760.5]
Episode 858	 reward: -8.96	 Mean_loss: 0.04329445,  training time: 13.18
[689.75, 735.75, 771.75, 768.75]
Episode 859	 reward: -9.84	 Mean_loss: 0.09523970,  training time: 13.15
[707.75, 697.25, 793.75, 761.75]
Episode 860	 reward: -10.07	 Mean_loss: 0.05328359,  training time: 13.18
[(15, 5), (15, 7), (15, 9), (15, 10)]
[771.5, 672.25, 763.0, 778.0]
Episode 861	 reward: -9.08	 Mean_loss: 0.09451125,  training time: 13.22
[718.75, 674.0, 716.25, 789.5]
Episode 862	 reward: -8.70	 Mean_loss: 0.04759207,  training time: 13.23
[710.25, 723.5, 781.0, 788.0]
Episode 863	 reward: -9.55	 Mean_loss: 0.06762885,  training time: 13.15
[753.5, 723.0, 712.0, 767.75]
Episode 864	 reward: -9.53	 Mean_loss: 0.06170754,  training time: 13.14
[734.75, 721.0, 731.75, 798.25]
Episode 865	 reward: -9.37	 Mean_loss: 0.07652900,  training time: 13.12
[717.0, 725.5, 704.75, 768.75]
Episode 866	 reward: -9.59	 Mean_loss: 0.06859414,  training time: 13.18
[693.25, 691.75, 699.0, 789.0]
Episode 867	 reward: -10.35	 Mean_loss: 0.04378328,  training time: 13.13
[710.75, 697.25, 741.5, 761.25]
Episode 868	 reward: -9.17	 Mean_loss: 0.06602318,  training time: 13.22
[709.25, 754.5, 725.75, 717.5]
Episode 869	 reward: -9.01	 Mean_loss: 0.05977828,  training time: 13.18
[732.75, 765.0, 732.75, 790.5]
Episode 870	 reward: -9.01	 Mean_loss: 0.06697079,  training time: 13.18
[735.75, 745.0, 768.0, 774.5]
Episode 871	 reward: -9.49	 Mean_loss: 0.06465574,  training time: 13.15
[724.0, 695.5, 708.25, 766.0]
Episode 872	 reward: -9.41	 Mean_loss: 0.07562812,  training time: 13.19
[737.75, 669.5, 704.0, 742.0]
Episode 873	 reward: -9.63	 Mean_loss: 0.05941295,  training time: 13.26
[691.5, 705.25, 702.25, 786.75]
Episode 874	 reward: -8.74	 Mean_loss: 0.04769506,  training time: 13.42
[703.25, 729.25, 755.5, 760.5]
Episode 875	 reward: -9.71	 Mean_loss: 0.08703031,  training time: 13.35
[739.25, 751.0, 729.0, 742.0]
Episode 876	 reward: -9.54	 Mean_loss: 0.06440808,  training time: 13.40
[771.0, 698.25, 734.0, 778.0]
Episode 877	 reward: -9.17	 Mean_loss: 0.05357861,  training time: 13.35
[700.75, 709.5, 759.25, 791.25]
Episode 878	 reward: -9.63	 Mean_loss: 0.05979817,  training time: 13.38
[677.25, 721.25, 751.25, 833.5]
Episode 879	 reward: -9.91	 Mean_loss: 0.12696213,  training time: 13.38
[735.0, 717.5, 736.0, 755.0]
Episode 880	 reward: -9.01	 Mean_loss: 0.06525142,  training time: 13.25
[(15, 5), (15, 7), (15, 9), (15, 10)]
[696.5, 775.25, 743.5, 794.25]
Episode 881	 reward: -9.72	 Mean_loss: 0.10224073,  training time: 13.19
[683.25, 748.75, 733.75, 717.25]
Episode 882	 reward: -9.31	 Mean_loss: 0.08360043,  training time: 13.18
[645.5, 723.5, 773.0, 776.5]
Episode 883	 reward: -9.92	 Mean_loss: 0.12797841,  training time: 13.25
[656.25, 712.75, 714.5, 784.75]
Episode 884	 reward: -9.34	 Mean_loss: 0.08554018,  training time: 13.20
[653.25, 735.25, 780.75, 763.25]
Episode 885	 reward: -8.75	 Mean_loss: 0.09502129,  training time: 13.18
[692.75, 699.5, 724.75, 764.75]
Episode 886	 reward: -9.23	 Mean_loss: 0.08959140,  training time: 13.18
[678.0, 736.25, 749.0, 756.5]
Episode 887	 reward: -9.04	 Mean_loss: 0.08078253,  training time: 13.13
[661.0, 757.0, 737.0, 738.5]
Episode 888	 reward: -9.00	 Mean_loss: 0.09877042,  training time: 13.17
[684.0, 732.5, 789.25, 765.25]
Episode 889	 reward: -9.55	 Mean_loss: 0.08448711,  training time: 13.27
[679.0, 675.75, 766.25, 775.75]
Episode 890	 reward: -9.52	 Mean_loss: 0.06980052,  training time: 13.14
[685.75, 700.0, 721.0, 795.25]
Episode 891	 reward: -9.66	 Mean_loss: 0.09573670,  training time: 13.46
[675.5, 694.75, 715.75, 790.5]
Episode 892	 reward: -9.14	 Mean_loss: 0.08340497,  training time: 13.41
[717.75, 786.0, 819.5, 702.5]
Episode 893	 reward: -9.73	 Mean_loss: 0.09272388,  training time: 13.39
[698.25, 755.25, 762.5, 770.75]
Episode 894	 reward: -9.20	 Mean_loss: 0.09159108,  training time: 13.40
[666.5, 736.75, 733.25, 765.25]
Episode 895	 reward: -9.08	 Mean_loss: 0.06405593,  training time: 13.20
[731.5, 732.0, 763.0, 772.75]
Episode 896	 reward: -9.18	 Mean_loss: 0.10232957,  training time: 13.13
[628.0, 714.25, 723.5, 707.0]
Episode 897	 reward: -9.20	 Mean_loss: 0.08485896,  training time: 13.16
[712.0, 792.25, 754.5, 765.0]
Episode 898	 reward: -9.94	 Mean_loss: 0.06481986,  training time: 13.16
[667.5, 725.25, 731.25, 768.0]
Episode 899	 reward: -9.38	 Mean_loss: 0.06136962,  training time: 13.12
[616.75, 766.0, 728.0, 835.75]
Episode 900	 reward: -9.21	 Mean_loss: 0.09688088,  training time: 13.17
[(15, 5), (15, 7), (15, 9), (15, 10)]
[752.0, 744.5, 806.75, 710.5]
Episode 901	 reward: -9.04	 Mean_loss: 0.09761433,  training time: 13.19
[761.25, 704.0, 781.5, 699.75]
Episode 902	 reward: -9.31	 Mean_loss: 0.09333298,  training time: 13.14
[686.75, 692.5, 727.25, 797.5]
Episode 903	 reward: -9.60	 Mean_loss: 0.14879073,  training time: 13.16
[699.0, 705.75, 745.5, 761.5]
Episode 904	 reward: -8.91	 Mean_loss: 0.08090984,  training time: 13.21
[724.5, 692.75, 761.5, 723.25]
Episode 905	 reward: -9.09	 Mean_loss: 0.07519057,  training time: 13.15
[734.75, 697.0, 756.25, 730.25]
Episode 906	 reward: -9.00	 Mean_loss: 0.08757359,  training time: 13.16
[737.75, 660.25, 727.0, 733.0]
Episode 907	 reward: -9.71	 Mean_loss: 0.07247206,  training time: 13.12
[725.0, 693.5, 715.25, 792.5]
Episode 908	 reward: -9.31	 Mean_loss: 0.05142115,  training time: 13.18
[713.5, 687.0, 683.0, 748.0]
Episode 909	 reward: -9.37	 Mean_loss: 0.06909438,  training time: 13.16
[690.0, 715.25, 739.25, 737.0]
Episode 910	 reward: -9.30	 Mean_loss: 0.06021032,  training time: 13.21
[738.25, 673.25, 707.75, 789.25]
Episode 911	 reward: -9.54	 Mean_loss: 0.07410298,  training time: 13.18
[699.5, 682.25, 716.25, 719.75]
Episode 912	 reward: -8.84	 Mean_loss: 0.09199262,  training time: 13.18
[701.0, 717.5, 753.75, 734.5]
Episode 913	 reward: -9.20	 Mean_loss: 0.09377006,  training time: 13.22
[746.25, 781.0, 829.75, 739.0]
Episode 914	 reward: -9.24	 Mean_loss: 0.04807879,  training time: 13.19
[691.75, 734.75, 781.0, 760.0]
Episode 915	 reward: -9.81	 Mean_loss: 0.05441587,  training time: 13.23
[787.0, 698.0, 696.5, 761.75]
Episode 916	 reward: -9.40	 Mean_loss: 0.04834656,  training time: 13.19
[720.5, 705.5, 707.25, 715.0]
Episode 917	 reward: -9.40	 Mean_loss: 0.07045344,  training time: 13.19
[678.75, 750.75, 725.75, 819.75]
Episode 918	 reward: -9.39	 Mean_loss: 0.07806735,  training time: 13.20
[702.75, 764.5, 724.0, 743.5]
Episode 919	 reward: -9.42	 Mean_loss: 0.07808425,  training time: 13.19
[698.0, 762.25, 717.25, 745.25]
Episode 920	 reward: -9.24	 Mean_loss: 0.06537033,  training time: 13.18
[(15, 5), (15, 7), (15, 9), (15, 10)]
[726.75, 671.0, 740.75, 786.0]
Episode 921	 reward: -9.61	 Mean_loss: 0.07641742,  training time: 13.25
[740.0, 648.0, 721.0, 767.75]
Episode 922	 reward: -9.19	 Mean_loss: 0.06590792,  training time: 13.20
[676.75, 695.5, 743.0, 771.25]
Episode 923	 reward: -9.67	 Mean_loss: 0.06360784,  training time: 13.20
[720.0, 743.0, 775.75, 784.75]
Episode 924	 reward: -8.92	 Mean_loss: 0.07602686,  training time: 13.19
[641.0, 700.0, 782.25, 760.0]
Episode 925	 reward: -8.76	 Mean_loss: 0.06346765,  training time: 13.26
[682.75, 681.75, 780.75, 754.0]
Episode 926	 reward: -9.25	 Mean_loss: 0.13319331,  training time: 13.16
[614.0, 692.25, 761.5, 845.0]
Episode 927	 reward: -9.35	 Mean_loss: 0.12763891,  training time: 13.15
[668.5, 686.5, 728.75, 778.5]
Episode 928	 reward: -9.66	 Mean_loss: 0.08123539,  training time: 13.16
[678.25, 669.0, 764.0, 772.0]
Episode 929	 reward: -9.24	 Mean_loss: 0.10002881,  training time: 13.18
[679.25, 648.0, 734.5, 782.25]
Episode 930	 reward: -9.15	 Mean_loss: 0.07851665,  training time: 13.16
[726.25, 670.5, 743.0, 777.5]
Episode 931	 reward: -9.42	 Mean_loss: 0.08806778,  training time: 13.34
[749.0, 698.75, 780.25, 780.5]
Episode 932	 reward: -9.09	 Mean_loss: 0.07496339,  training time: 13.37
[703.75, 664.75, 764.5, 763.5]
Episode 933	 reward: -9.40	 Mean_loss: 0.07244231,  training time: 13.37
[682.0, 680.0, 750.75, 787.75]
Episode 934	 reward: -9.55	 Mean_loss: 0.06078603,  training time: 13.40
[683.75, 708.25, 732.5, 760.5]
Episode 935	 reward: -9.19	 Mean_loss: 0.08744902,  training time: 13.43
[682.75, 709.0, 756.5, 790.5]
Episode 936	 reward: -9.32	 Mean_loss: 0.05479084,  training time: 13.13
[729.25, 688.25, 750.0, 789.0]
Episode 937	 reward: -9.20	 Mean_loss: 0.04708244,  training time: 13.21
[660.75, 690.25, 751.0, 731.0]
Episode 938	 reward: -8.94	 Mean_loss: 0.07474402,  training time: 13.32
[655.5, 694.75, 728.0, 765.25]
Episode 939	 reward: -9.01	 Mean_loss: 0.04300397,  training time: 13.18
[696.75, 702.5, 762.25, 768.25]
Episode 940	 reward: -9.48	 Mean_loss: 0.09246986,  training time: 13.43
[(15, 5), (15, 7), (15, 9), (15, 10)]
[752.5, 692.25, 730.0, 760.75]
Episode 941	 reward: -10.02	 Mean_loss: 0.09500729,  training time: 13.45
[770.0, 693.5, 691.5, 733.0]
Episode 942	 reward: -9.48	 Mean_loss: 0.07429956,  training time: 13.43
[728.0, 683.25, 714.5, 785.5]
Episode 943	 reward: -8.78	 Mean_loss: 0.05164644,  training time: 13.20
[739.25, 704.5, 770.5, 765.25]
Episode 944	 reward: -9.56	 Mean_loss: 0.08090006,  training time: 13.15
[763.0, 728.75, 791.75, 741.5]
Episode 945	 reward: -9.47	 Mean_loss: 0.09156583,  training time: 13.17
[762.0, 708.5, 757.75, 814.5]
Episode 946	 reward: -9.82	 Mean_loss: 0.10975089,  training time: 13.33
[750.5, 702.75, 710.75, 816.75]
Episode 947	 reward: -9.09	 Mean_loss: 0.08541906,  training time: 13.17
[727.0, 730.75, 746.5, 774.0]
Episode 948	 reward: -9.15	 Mean_loss: 0.05389332,  training time: 13.15
[743.5, 752.75, 717.25, 736.0]
Episode 949	 reward: -9.26	 Mean_loss: 0.04803193,  training time: 13.17
[740.75, 697.5, 726.25, 845.5]
Episode 950	 reward: -8.98	 Mean_loss: 0.10180845,  training time: 13.17
[716.75, 688.75, 758.25, 766.5]
Episode 951	 reward: -9.97	 Mean_loss: 0.07009709,  training time: 13.19
[726.25, 727.75, 741.0, 798.5]
Episode 952	 reward: -9.14	 Mean_loss: 0.06003225,  training time: 13.34
[719.0, 712.75, 786.5, 798.0]
Episode 953	 reward: -9.33	 Mean_loss: 0.04762098,  training time: 13.05
[702.75, 648.75, 721.5, 759.5]
Episode 954	 reward: -10.39	 Mean_loss: 0.09979955,  training time: 13.04
[721.0, 716.5, 806.75, 735.0]
Episode 955	 reward: -9.19	 Mean_loss: 0.09323366,  training time: 13.03
[725.0, 698.75, 732.75, 761.0]
Episode 956	 reward: -9.47	 Mean_loss: 0.06000928,  training time: 13.03
[762.25, 720.0, 722.75, 763.5]
Episode 957	 reward: -9.46	 Mean_loss: 0.05845060,  training time: 13.01
[772.25, 680.0, 756.0, 744.5]
Episode 958	 reward: -9.38	 Mean_loss: 0.04972295,  training time: 13.22
[705.25, 708.0, 705.75, 802.75]
Episode 959	 reward: -8.72	 Mean_loss: 0.04299681,  training time: 13.22
[730.75, 700.5, 756.75, 737.5]
Episode 960	 reward: -9.56	 Mean_loss: 0.07959791,  training time: 13.07
[(15, 5), (15, 7), (15, 9), (15, 10)]
[797.0, 692.25, 773.5, 742.75]
Episode 961	 reward: -8.71	 Mean_loss: 0.08142632,  training time: 13.11
[766.5, 668.5, 790.25, 771.5]
Episode 962	 reward: -8.95	 Mean_loss: 0.08515663,  training time: 13.08
[814.25, 722.0, 737.5, 698.5]
Episode 963	 reward: -9.49	 Mean_loss: 0.08036424,  training time: 13.08
[778.5, 746.0, 769.75, 781.5]
Episode 964	 reward: -8.80	 Mean_loss: 0.09834012,  training time: 13.09
[811.0, 662.25, 748.0, 746.0]
Episode 965	 reward: -8.63	 Mean_loss: 0.09955059,  training time: 13.08
[771.25, 664.0, 731.25, 734.75]
Episode 966	 reward: -8.76	 Mean_loss: 0.09615143,  training time: 13.09
[767.0, 745.5, 757.0, 756.25]
Episode 967	 reward: -8.84	 Mean_loss: 0.05299390,  training time: 13.15
[761.5, 661.75, 806.25, 755.75]
Episode 968	 reward: -9.07	 Mean_loss: 0.07240939,  training time: 13.07
[763.75, 675.5, 816.75, 727.25]
Episode 969	 reward: -8.85	 Mean_loss: 0.05302901,  training time: 13.07
[773.5, 713.75, 762.5, 736.0]
Episode 970	 reward: -9.39	 Mean_loss: 0.07313138,  training time: 13.09
[734.0, 697.75, 732.0, 766.75]
Episode 971	 reward: -9.75	 Mean_loss: 0.09306071,  training time: 13.07
[847.0, 697.0, 709.0, 781.0]
Episode 972	 reward: -9.23	 Mean_loss: 0.07568857,  training time: 13.04
[803.25, 674.75, 784.5, 699.0]
Episode 973	 reward: -9.20	 Mean_loss: 0.08249316,  training time: 13.04
[774.25, 683.75, 752.5, 727.75]
Episode 974	 reward: -9.52	 Mean_loss: 0.07134981,  training time: 13.07
[755.5, 709.5, 738.0, 774.25]
Episode 975	 reward: -9.36	 Mean_loss: 0.05387727,  training time: 13.20
[778.0, 690.75, 737.25, 749.25]
Episode 976	 reward: -8.79	 Mean_loss: 0.10389842,  training time: 13.06
[781.75, 710.0, 760.0, 744.0]
Episode 977	 reward: -9.54	 Mean_loss: 0.07166731,  training time: 13.19
[757.0, 677.0, 739.25, 762.0]
Episode 978	 reward: -8.81	 Mean_loss: 0.07200269,  training time: 13.04
[783.75, 691.5, 717.5, 844.75]
Episode 979	 reward: -9.38	 Mean_loss: 0.12471451,  training time: 13.03
[769.0, 646.75, 777.0, 760.5]
Episode 980	 reward: -8.77	 Mean_loss: 0.09128591,  training time: 13.06
[(15, 5), (15, 7), (15, 9), (15, 10)]
[726.0, 766.0, 769.5, 792.75]
Episode 981	 reward: -9.43	 Mean_loss: 0.08933519,  training time: 13.28
[678.75, 718.25, 763.0, 823.25]
Episode 982	 reward: -9.28	 Mean_loss: 0.08064400,  training time: 13.11
[777.0, 738.5, 738.0, 741.75]
Episode 983	 reward: -8.88	 Mean_loss: 0.06562620,  training time: 13.37
[748.25, 699.25, 756.75, 759.75]
Episode 984	 reward: -9.56	 Mean_loss: 0.06359811,  training time: 13.37
[692.75, 744.5, 741.75, 742.75]
Episode 985	 reward: -8.99	 Mean_loss: 0.10193130,  training time: 13.37
[734.25, 810.25, 778.5, 769.5]
Episode 986	 reward: -9.44	 Mean_loss: 0.06882928,  training time: 13.36
[781.0, 776.25, 710.75, 754.5]
Episode 987	 reward: -9.68	 Mean_loss: 0.09480301,  training time: 13.35
[687.75, 759.75, 778.0, 753.0]
Episode 988	 reward: -9.65	 Mean_loss: 0.06387345,  training time: 13.44
[735.0, 755.25, 742.75, 750.75]
Episode 989	 reward: -9.26	 Mean_loss: 0.09384491,  training time: 13.40
[726.25, 767.75, 746.75, 750.5]
Episode 990	 reward: -10.03	 Mean_loss: 0.07471652,  training time: 13.03
[716.75, 719.5, 707.75, 791.75]
Episode 991	 reward: -9.47	 Mean_loss: 0.09830759,  training time: 13.09
[748.0, 739.75, 755.5, 797.0]
Episode 992	 reward: -9.19	 Mean_loss: 0.04255316,  training time: 13.08
[772.75, 720.0, 745.25, 740.0]
Episode 993	 reward: -8.96	 Mean_loss: 0.08639676,  training time: 13.08
[760.5, 712.5, 760.75, 774.0]
Episode 994	 reward: -9.26	 Mean_loss: 0.09552938,  training time: 13.08
[728.5, 748.75, 749.75, 759.5]
Episode 995	 reward: -9.84	 Mean_loss: 0.10359997,  training time: 13.03
[745.0, 746.0, 679.75, 772.5]
Episode 996	 reward: -9.83	 Mean_loss: 0.09618464,  training time: 13.04
[775.5, 760.25, 736.5, 768.75]
Episode 997	 reward: -9.04	 Mean_loss: 0.14511332,  training time: 13.03
[731.5, 751.5, 747.25, 795.0]
Episode 998	 reward: -10.07	 Mean_loss: 0.09098437,  training time: 13.06
[747.75, 701.0, 736.25, 826.75]
Episode 999	 reward: -9.27	 Mean_loss: 0.07774287,  training time: 13.03
[695.5, 736.0, 757.0, 769.5]
Episode 1000	 reward: -9.36	 Mean_loss: 0.10720401,  training time: 13.03
+ for model in 'maml+$model_suffix'
+ echo 15,5 15,7 15,9 15,10
+ tr ' ' '\n'
+ IFS=,
+ read n_j n_m
+ python train/DAN_finetuning.py --logdir ./runs/exp14/transfer_maml+exp14_1000_64_3_15x5 --model_suffix exp11_maml+exp14_1000_64_3_15x5 --finetuning_model maml+exp14_1000_64_3 --max_updates 100 --n_j 15 --n_m 5 --num_envs 4 --hidden_dim_actor 64 --hidden_dim_critic 64 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x5+mix
save model name:  15x5+mix+exp11_maml+exp14_1000_64_3_15x5
./trained_network/SD2/maml+exp14_1000_64_3.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/maml+exp14_1000_64_3.pth
vali data :./data/data_train_vali/SD2/15x5+mix


progress:   0%|[34m          [0m| 0/100 [00:00<?, ?it/s]                                                 Episode 1	 reward: -6.44	 makespan: 637.75	 Mean_loss: 0.36011636,  training time: 2.31
progress:   0%|[34m          [0m| 0/100 [00:02<?, ?it/s]progress:   1%|[34m          [0m| 1/100 [00:02<03:48,  2.31s/it]                                                         Episode 2	 reward: -6.66	 makespan: 659.25	 Mean_loss: 0.24918897,  training time: 1.23
progress:   1%|[34m          [0m| 1/100 [00:03<03:48,  2.31s/it]progress:   2%|[34m         [0m| 2/100 [00:03<02:44,  1.68s/it]                                                         Episode 3	 reward: -6.31	 makespan: 625.00	 Mean_loss: 0.27976954,  training time: 1.19
progress:   2%|[34m         [0m| 2/100 [00:04<02:44,  1.68s/it]progress:   3%|[34m         [0m| 3/100 [00:04<02:21,  1.46s/it]                                                         Episode 4	 reward: -6.34	 makespan: 627.25	 Mean_loss: 0.24210119,  training time: 1.24
progress:   3%|[34m         [0m| 3/100 [00:05<02:21,  1.46s/it]progress:   4%|[34m         [0m| 4/100 [00:05<02:11,  1.37s/it]                                                         Episode 5	 reward: -6.07	 makespan: 600.50	 Mean_loss: 0.13016556,  training time: 1.24
progress:   4%|[34m         [0m| 4/100 [00:07<02:11,  1.37s/it]progress:   5%|[34m         [0m| 5/100 [00:07<02:05,  1.32s/it]                                                         Episode 6	 reward: -6.31	 makespan: 624.75	 Mean_loss: 0.12368048,  training time: 1.24
progress:   5%|[34m         [0m| 5/100 [00:08<02:05,  1.32s/it]progress:   6%|[34m         [0m| 6/100 [00:08<02:01,  1.29s/it]                                                         Episode 7	 reward: -5.99	 makespan: 592.75	 Mean_loss: 0.11643666,  training time: 1.22
progress:   6%|[34m         [0m| 6/100 [00:09<02:01,  1.29s/it]progress:   7%|[34m         [0m| 7/100 [00:09<01:58,  1.27s/it]                                                         Episode 8	 reward: -6.19	 makespan: 613.25	 Mean_loss: 0.16213480,  training time: 1.24
progress:   7%|[34m         [0m| 7/100 [00:10<01:58,  1.27s/it]progress:   8%|[34m         [0m| 8/100 [00:10<01:55,  1.26s/it]                                                         Episode 9	 reward: -5.89	 makespan: 583.00	 Mean_loss: 0.12980400,  training time: 1.24
progress:   8%|[34m         [0m| 8/100 [00:12<01:55,  1.26s/it]progress:   9%|[34m         [0m| 9/100 [00:12<01:54,  1.25s/it]                                                         Episode 10	 reward: -5.84	 makespan: 578.25	 Mean_loss: 0.10308505,  training time: 1.24
progress:   9%|[34m         [0m| 9/100 [00:13<01:54,  1.25s/it]progress:  10%|[34m         [0m| 10/100 [00:13<01:52,  1.25s/it]                                                          Episode 11	 reward: -5.64	 makespan: 558.75	 Mean_loss: 0.06022790,  training time: 1.22
progress:  10%|[34m         [0m| 10/100 [00:14<01:52,  1.25s/it]progress:  11%|[34m         [0m| 11/100 [00:14<01:50,  1.24s/it]                                                          Episode 12	 reward: -5.69	 makespan: 563.75	 Mean_loss: 0.06759605,  training time: 1.32
progress:  11%|[34m         [0m| 11/100 [00:15<01:50,  1.24s/it]progress:  12%|[34m        [0m| 12/100 [00:15<01:51,  1.27s/it]                                                          Episode 13	 reward: -5.57	 makespan: 551.75	 Mean_loss: 0.06467521,  training time: 1.24
progress:  12%|[34m        [0m| 12/100 [00:17<01:51,  1.27s/it]progress:  13%|[34m        [0m| 13/100 [00:17<01:49,  1.26s/it]                                                          Episode 14	 reward: -5.83	 makespan: 577.50	 Mean_loss: 0.05790637,  training time: 1.21
progress:  13%|[34m        [0m| 13/100 [00:18<01:49,  1.26s/it]progress:  14%|[34m        [0m| 14/100 [00:18<01:46,  1.24s/it]                                                          Episode 15	 reward: -5.75	 makespan: 569.50	 Mean_loss: 0.06745013,  training time: 1.23
progress:  14%|[34m        [0m| 14/100 [00:19<01:46,  1.24s/it]progress:  15%|[34m        [0m| 15/100 [00:19<01:45,  1.24s/it]                                                          Episode 16	 reward: -6.13	 makespan: 607.00	 Mean_loss: 0.05696734,  training time: 1.22
progress:  15%|[34m        [0m| 15/100 [00:20<01:45,  1.24s/it]progress:  16%|[34m        [0m| 16/100 [00:20<01:43,  1.23s/it]                                                          Episode 17	 reward: -5.82	 makespan: 576.50	 Mean_loss: 0.07299560,  training time: 1.23
progress:  16%|[34m        [0m| 16/100 [00:22<01:43,  1.23s/it]progress:  17%|[34m        [0m| 17/100 [00:22<01:42,  1.23s/it]                                                          Episode 18	 reward: -6.02	 makespan: 596.25	 Mean_loss: 0.08026297,  training time: 1.24
progress:  17%|[34m        [0m| 17/100 [00:23<01:42,  1.23s/it]progress:  18%|[34m        [0m| 18/100 [00:23<01:41,  1.24s/it]                                                          Episode 19	 reward: -6.05	 makespan: 599.00	 Mean_loss: 0.08714297,  training time: 1.21
progress:  18%|[34m        [0m| 18/100 [00:24<01:41,  1.24s/it]progress:  19%|[34m        [0m| 19/100 [00:24<01:39,  1.23s/it]                                                          Episode 20	 reward: -5.84	 makespan: 578.00	 Mean_loss: 0.05232827,  training time: 1.23
progress:  19%|[34m        [0m| 19/100 [00:25<01:39,  1.23s/it]progress:  20%|[34m        [0m| 20/100 [00:25<01:38,  1.23s/it]                                                          Episode 21	 reward: -6.29	 makespan: 622.50	 Mean_loss: 0.06998621,  training time: 1.20
progress:  20%|[34m        [0m| 20/100 [00:26<01:38,  1.23s/it]progress:  21%|[34m        [0m| 21/100 [00:26<01:36,  1.22s/it]                                                          Episode 22	 reward: -5.85	 makespan: 578.75	 Mean_loss: 0.05591151,  training time: 1.24
progress:  21%|[34m        [0m| 21/100 [00:28<01:36,  1.22s/it]progress:  22%|[34m       [0m| 22/100 [00:28<01:35,  1.23s/it]                                                          Episode 23	 reward: -5.97	 makespan: 591.00	 Mean_loss: 0.04935632,  training time: 1.23
progress:  22%|[34m       [0m| 22/100 [00:29<01:35,  1.23s/it]progress:  23%|[34m       [0m| 23/100 [00:29<01:34,  1.23s/it]                                                          Episode 24	 reward: -6.14	 makespan: 608.25	 Mean_loss: 0.06210379,  training time: 1.23
progress:  23%|[34m       [0m| 23/100 [00:30<01:34,  1.23s/it]progress:  24%|[34m       [0m| 24/100 [00:30<01:33,  1.23s/it]                                                          Episode 25	 reward: -6.15	 makespan: 608.75	 Mean_loss: 0.02189130,  training time: 1.25
progress:  24%|[34m       [0m| 24/100 [00:31<01:33,  1.23s/it]progress:  25%|[34m       [0m| 25/100 [00:31<01:32,  1.24s/it]                                                          Episode 26	 reward: -5.97	 makespan: 591.00	 Mean_loss: 0.02401433,  training time: 1.24
progress:  25%|[34m       [0m| 25/100 [00:33<01:32,  1.24s/it]progress:  26%|[34m       [0m| 26/100 [00:33<01:31,  1.24s/it]                                                          Episode 27	 reward: -5.63	 makespan: 557.50	 Mean_loss: 0.03499542,  training time: 1.24
progress:  26%|[34m       [0m| 26/100 [00:34<01:31,  1.24s/it]progress:  27%|[34m       [0m| 27/100 [00:34<01:30,  1.24s/it]                                                          Episode 28	 reward: -5.72	 makespan: 566.75	 Mean_loss: 0.01946206,  training time: 1.21
progress:  27%|[34m       [0m| 27/100 [00:35<01:30,  1.24s/it]progress:  28%|[34m       [0m| 28/100 [00:35<01:28,  1.23s/it]                                                          Episode 29	 reward: -5.69	 makespan: 563.25	 Mean_loss: 0.04304455,  training time: 1.22
progress:  28%|[34m       [0m| 28/100 [00:36<01:28,  1.23s/it]progress:  29%|[34m       [0m| 29/100 [00:36<01:27,  1.23s/it]                                                          Episode 30	 reward: -5.40	 makespan: 535.00	 Mean_loss: 0.03760671,  training time: 1.23
progress:  29%|[34m       [0m| 29/100 [00:38<01:27,  1.23s/it]progress:  30%|[34m       [0m| 30/100 [00:38<01:26,  1.23s/it]                                                          Episode 31	 reward: -5.52	 makespan: 546.75	 Mean_loss: 0.05709121,  training time: 1.25
progress:  30%|[34m       [0m| 30/100 [00:39<01:26,  1.23s/it]progress:  31%|[34m       [0m| 31/100 [00:39<01:25,  1.24s/it]                                                          Episode 32	 reward: -5.46	 makespan: 540.75	 Mean_loss: 0.02326876,  training time: 1.21
progress:  31%|[34m       [0m| 31/100 [00:40<01:25,  1.24s/it]progress:  32%|[34m      [0m| 32/100 [00:40<01:23,  1.23s/it]                                                          Episode 33	 reward: -5.61	 makespan: 555.75	 Mean_loss: 0.03674353,  training time: 1.19
progress:  32%|[34m      [0m| 32/100 [00:41<01:23,  1.23s/it]progress:  33%|[34m      [0m| 33/100 [00:41<01:21,  1.22s/it]                                                          Episode 34	 reward: -5.52	 makespan: 546.00	 Mean_loss: 0.03674972,  training time: 1.23
progress:  33%|[34m      [0m| 33/100 [00:42<01:21,  1.22s/it]progress:  34%|[34m      [0m| 34/100 [00:42<01:20,  1.22s/it]                                                          Episode 35	 reward: -5.80	 makespan: 574.25	 Mean_loss: 0.05075725,  training time: 1.23
progress:  34%|[34m      [0m| 34/100 [00:44<01:20,  1.22s/it]progress:  35%|[34m      [0m| 35/100 [00:44<01:19,  1.22s/it]                                                          Episode 36	 reward: -5.78	 makespan: 572.50	 Mean_loss: 0.01683298,  training time: 1.22
progress:  35%|[34m      [0m| 35/100 [00:45<01:19,  1.22s/it]progress:  36%|[34m      [0m| 36/100 [00:45<01:18,  1.22s/it]                                                          Episode 37	 reward: -5.68	 makespan: 562.25	 Mean_loss: 0.03867671,  training time: 1.21
progress:  36%|[34m      [0m| 36/100 [00:46<01:18,  1.22s/it]progress:  37%|[34m      [0m| 37/100 [00:46<01:16,  1.22s/it]                                                          Episode 38	 reward: -5.96	 makespan: 589.75	 Mean_loss: 0.04418464,  training time: 1.35
progress:  37%|[34m      [0m| 37/100 [00:47<01:16,  1.22s/it]progress:  38%|[34m      [0m| 38/100 [00:47<01:18,  1.26s/it]                                                          Episode 39	 reward: -5.42	 makespan: 536.50	 Mean_loss: 0.05567176,  training time: 1.23
progress:  38%|[34m      [0m| 38/100 [00:49<01:18,  1.26s/it]progress:  39%|[34m      [0m| 39/100 [00:49<01:16,  1.25s/it]                                                          Episode 40	 reward: -5.43	 makespan: 537.25	 Mean_loss: 0.02847306,  training time: 1.20
progress:  39%|[34m      [0m| 39/100 [00:50<01:16,  1.25s/it]progress:  40%|[34m      [0m| 40/100 [00:50<01:14,  1.23s/it]                                                          Episode 41	 reward: -5.64	 makespan: 558.50	 Mean_loss: 0.02456368,  training time: 1.24
progress:  40%|[34m      [0m| 40/100 [00:51<01:14,  1.23s/it]progress:  41%|[34m      [0m| 41/100 [00:51<01:12,  1.24s/it]                                                          Episode 42	 reward: -5.72	 makespan: 566.50	 Mean_loss: 0.02667190,  training time: 1.23
progress:  41%|[34m      [0m| 41/100 [00:52<01:12,  1.24s/it]progress:  42%|[34m     [0m| 42/100 [00:52<01:11,  1.24s/it]                                                          Episode 43	 reward: -5.60	 makespan: 554.00	 Mean_loss: 0.03492031,  training time: 1.24
progress:  42%|[34m     [0m| 42/100 [00:54<01:11,  1.24s/it]progress:  43%|[34m     [0m| 43/100 [00:54<01:10,  1.24s/it]                                                          Episode 44	 reward: -5.45	 makespan: 539.75	 Mean_loss: 0.03244771,  training time: 1.24
progress:  43%|[34m     [0m| 43/100 [00:55<01:10,  1.24s/it]progress:  44%|[34m     [0m| 44/100 [00:55<01:09,  1.24s/it]                                                          Episode 45	 reward: -5.46	 makespan: 540.75	 Mean_loss: 0.05439583,  training time: 1.23
progress:  44%|[34m     [0m| 44/100 [00:56<01:09,  1.24s/it]progress:  45%|[34m     [0m| 45/100 [00:56<01:07,  1.23s/it]                                                          Episode 46	 reward: -5.20	 makespan: 514.50	 Mean_loss: 0.02697688,  training time: 1.19
progress:  45%|[34m     [0m| 45/100 [00:57<01:07,  1.23s/it]progress:  46%|[34m     [0m| 46/100 [00:57<01:06,  1.22s/it]                                                          Episode 47	 reward: -5.33	 makespan: 527.25	 Mean_loss: 0.01548454,  training time: 1.23
progress:  46%|[34m     [0m| 46/100 [00:59<01:06,  1.22s/it]progress:  47%|[34m     [0m| 47/100 [00:59<01:04,  1.23s/it]                                                          Episode 48	 reward: -5.46	 makespan: 540.25	 Mean_loss: 0.01948404,  training time: 1.22
progress:  47%|[34m     [0m| 47/100 [01:00<01:04,  1.23s/it]progress:  48%|[34m     [0m| 48/100 [01:00<01:03,  1.22s/it]                                                          Episode 49	 reward: -5.57	 makespan: 551.25	 Mean_loss: 0.06243316,  training time: 1.25
progress:  48%|[34m     [0m| 48/100 [01:01<01:03,  1.22s/it]progress:  49%|[34m     [0m| 49/100 [01:01<01:02,  1.23s/it]                                                          Episode 50	 reward: -5.45	 makespan: 539.25	 Mean_loss: 0.04155933,  training time: 1.21
progress:  49%|[34m     [0m| 49/100 [01:02<01:02,  1.23s/it]progress:  50%|[34m     [0m| 50/100 [01:02<01:01,  1.22s/it]                                                          Episode 51	 reward: -5.27	 makespan: 521.75	 Mean_loss: 0.03022517,  training time: 1.21
progress:  50%|[34m     [0m| 50/100 [01:03<01:01,  1.22s/it]progress:  51%|[34m     [0m| 51/100 [01:03<00:59,  1.22s/it]                                                          Episode 52	 reward: -5.41	 makespan: 535.75	 Mean_loss: 0.03335482,  training time: 1.23
progress:  51%|[34m     [0m| 51/100 [01:05<00:59,  1.22s/it]progress:  52%|[34m    [0m| 52/100 [01:05<00:58,  1.22s/it]                                                          Episode 53	 reward: -5.52	 makespan: 546.50	 Mean_loss: 0.01843227,  training time: 1.23
progress:  52%|[34m    [0m| 52/100 [01:06<00:58,  1.22s/it]progress:  53%|[34m    [0m| 53/100 [01:06<00:57,  1.22s/it]                                                          Episode 54	 reward: -5.61	 makespan: 555.50	 Mean_loss: 0.05205025,  training time: 1.24
progress:  53%|[34m    [0m| 53/100 [01:07<00:57,  1.22s/it]progress:  54%|[34m    [0m| 54/100 [01:07<00:56,  1.23s/it]                                                          Episode 55	 reward: -5.40	 makespan: 534.50	 Mean_loss: 0.02627854,  training time: 1.22
progress:  54%|[34m    [0m| 54/100 [01:08<00:56,  1.23s/it]progress:  55%|[34m    [0m| 55/100 [01:08<00:55,  1.23s/it]                                                          Episode 56	 reward: -5.71	 makespan: 565.75	 Mean_loss: 0.04181148,  training time: 1.23
progress:  55%|[34m    [0m| 55/100 [01:10<00:55,  1.23s/it]progress:  56%|[34m    [0m| 56/100 [01:10<00:54,  1.23s/it]                                                          Episode 57	 reward: -5.52	 makespan: 546.50	 Mean_loss: 0.04083489,  training time: 1.24
progress:  56%|[34m    [0m| 56/100 [01:11<00:54,  1.23s/it]progress:  57%|[34m    [0m| 57/100 [01:11<00:52,  1.23s/it]                                                          Episode 58	 reward: -5.44	 makespan: 538.50	 Mean_loss: 0.01877150,  training time: 1.22
progress:  57%|[34m    [0m| 57/100 [01:12<00:52,  1.23s/it]progress:  58%|[34m    [0m| 58/100 [01:12<00:51,  1.23s/it]                                                          Episode 59	 reward: -5.35	 makespan: 529.25	 Mean_loss: 0.01551533,  training time: 1.25
progress:  58%|[34m    [0m| 58/100 [01:13<00:51,  1.23s/it]progress:  59%|[34m    [0m| 59/100 [01:13<00:50,  1.23s/it]                                                          Episode 60	 reward: -5.45	 makespan: 540.00	 Mean_loss: 0.03787443,  training time: 1.23
progress:  59%|[34m    [0m| 59/100 [01:14<00:50,  1.23s/it]progress:  60%|[34m    [0m| 60/100 [01:14<00:49,  1.23s/it]                                                          Episode 61	 reward: -5.18	 makespan: 512.75	 Mean_loss: 0.03692031,  training time: 1.25
progress:  60%|[34m    [0m| 60/100 [01:16<00:49,  1.23s/it]progress:  61%|[34m    [0m| 61/100 [01:16<00:48,  1.24s/it]                                                          Episode 62	 reward: -5.24	 makespan: 518.75	 Mean_loss: 0.01658932,  training time: 1.23
progress:  61%|[34m    [0m| 61/100 [01:17<00:48,  1.24s/it]progress:  62%|[34m   [0m| 62/100 [01:17<00:46,  1.23s/it]                                                          Episode 63	 reward: -5.33	 makespan: 527.75	 Mean_loss: 0.02138103,  training time: 1.22
progress:  62%|[34m   [0m| 62/100 [01:18<00:46,  1.23s/it]progress:  63%|[34m   [0m| 63/100 [01:18<00:45,  1.23s/it]                                                          Episode 64	 reward: -5.43	 makespan: 537.50	 Mean_loss: 0.01947726,  training time: 1.23
progress:  63%|[34m   [0m| 63/100 [01:19<00:45,  1.23s/it]progress:  64%|[34m   [0m| 64/100 [01:19<00:44,  1.23s/it]                                                          Episode 65	 reward: -5.60	 makespan: 554.75	 Mean_loss: 0.00568585,  training time: 1.21
progress:  64%|[34m   [0m| 64/100 [01:21<00:44,  1.23s/it]progress:  65%|[34m   [0m| 65/100 [01:21<00:42,  1.22s/it]                                                          Episode 66	 reward: -5.25	 makespan: 519.50	 Mean_loss: 0.01322959,  training time: 1.23
progress:  65%|[34m   [0m| 65/100 [01:22<00:42,  1.22s/it]progress:  66%|[34m   [0m| 66/100 [01:22<00:41,  1.23s/it]                                                          Episode 67	 reward: -5.44	 makespan: 538.50	 Mean_loss: 0.02004279,  training time: 1.20
progress:  66%|[34m   [0m| 66/100 [01:23<00:41,  1.23s/it]progress:  67%|[34m   [0m| 67/100 [01:23<00:40,  1.22s/it]                                                          Episode 68	 reward: -5.66	 makespan: 560.25	 Mean_loss: 0.01832374,  training time: 1.23
progress:  67%|[34m   [0m| 67/100 [01:24<00:40,  1.22s/it]progress:  68%|[34m   [0m| 68/100 [01:24<00:39,  1.22s/it]                                                          Episode 69	 reward: -5.31	 makespan: 525.75	 Mean_loss: 0.02345855,  training time: 1.22
progress:  68%|[34m   [0m| 68/100 [01:26<00:39,  1.22s/it]progress:  69%|[34m   [0m| 69/100 [01:26<00:37,  1.22s/it]                                                          Episode 70	 reward: -5.65	 makespan: 559.25	 Mean_loss: 0.01632494,  training time: 1.28
progress:  69%|[34m   [0m| 69/100 [01:27<00:37,  1.22s/it]progress:  70%|[34m   [0m| 70/100 [01:27<00:37,  1.24s/it]                                                          Episode 71	 reward: -5.45	 makespan: 540.00	 Mean_loss: 0.02058137,  training time: 1.19
progress:  70%|[34m   [0m| 70/100 [01:28<00:37,  1.24s/it]progress:  71%|[34m   [0m| 71/100 [01:28<00:35,  1.23s/it]                                                          Episode 72	 reward: -5.41	 makespan: 535.25	 Mean_loss: 0.01850516,  training time: 1.22
progress:  71%|[34m   [0m| 71/100 [01:29<00:35,  1.23s/it]progress:  72%|[34m  [0m| 72/100 [01:29<00:34,  1.22s/it]                                                          Episode 73	 reward: -5.10	 makespan: 504.75	 Mean_loss: 0.02544043,  training time: 1.24
progress:  72%|[34m  [0m| 72/100 [01:30<00:34,  1.22s/it]progress:  73%|[34m  [0m| 73/100 [01:30<00:33,  1.23s/it]                                                          Episode 74	 reward: -5.23	 makespan: 517.50	 Mean_loss: 0.01818139,  training time: 1.21
progress:  73%|[34m  [0m| 73/100 [01:32<00:33,  1.23s/it]progress:  74%|[34m  [0m| 74/100 [01:32<00:31,  1.22s/it]                                                          Episode 75	 reward: -5.29	 makespan: 523.75	 Mean_loss: 0.01065026,  training time: 1.27
progress:  74%|[34m  [0m| 74/100 [01:33<00:31,  1.22s/it]progress:  75%|[34m  [0m| 75/100 [01:33<00:30,  1.24s/it]                                                          Episode 76	 reward: -5.26	 makespan: 521.00	 Mean_loss: 0.01244366,  training time: 1.26
progress:  75%|[34m  [0m| 75/100 [01:34<00:30,  1.24s/it]progress:  76%|[34m  [0m| 76/100 [01:34<00:29,  1.24s/it]                                                          Episode 77	 reward: -5.28	 makespan: 523.00	 Mean_loss: 0.00517004,  training time: 1.21
progress:  76%|[34m  [0m| 76/100 [01:35<00:29,  1.24s/it]progress:  77%|[34m  [0m| 77/100 [01:35<00:28,  1.23s/it]                                                          Episode 78	 reward: -5.32	 makespan: 527.00	 Mean_loss: 0.02147788,  training time: 1.21
progress:  77%|[34m  [0m| 77/100 [01:37<00:28,  1.23s/it]progress:  78%|[34m  [0m| 78/100 [01:37<00:27,  1.23s/it]                                                          Episode 79	 reward: -5.33	 makespan: 527.50	 Mean_loss: 0.01759669,  training time: 1.21
progress:  78%|[34m  [0m| 78/100 [01:38<00:27,  1.23s/it]progress:  79%|[34m  [0m| 79/100 [01:38<00:25,  1.22s/it]                                                          Episode 80	 reward: -5.35	 makespan: 529.25	 Mean_loss: 0.03333699,  training time: 1.23
progress:  79%|[34m  [0m| 79/100 [01:39<00:25,  1.22s/it]progress:  80%|[34m  [0m| 80/100 [01:39<00:24,  1.22s/it]                                                          Episode 81	 reward: -5.58	 makespan: 552.25	 Mean_loss: 0.01668984,  training time: 1.21
progress:  80%|[34m  [0m| 80/100 [01:40<00:24,  1.22s/it]progress:  81%|[34m  [0m| 81/100 [01:40<00:23,  1.22s/it]                                                          Episode 82	 reward: -5.07	 makespan: 501.50	 Mean_loss: 0.03713237,  training time: 1.22
progress:  81%|[34m  [0m| 81/100 [01:41<00:23,  1.22s/it]progress:  82%|[34m [0m| 82/100 [01:41<00:21,  1.22s/it]                                                          Episode 83	 reward: -5.46	 makespan: 540.25	 Mean_loss: 0.02639608,  training time: 1.23
progress:  82%|[34m [0m| 82/100 [01:43<00:21,  1.22s/it]progress:  83%|[34m [0m| 83/100 [01:43<00:20,  1.22s/it]                                                          Episode 84	 reward: -5.21	 makespan: 516.00	 Mean_loss: 0.02290395,  training time: 1.23
progress:  83%|[34m [0m| 83/100 [01:44<00:20,  1.22s/it]progress:  84%|[34m [0m| 84/100 [01:44<00:19,  1.23s/it]                                                          Episode 85	 reward: -5.20	 makespan: 514.75	 Mean_loss: 0.02396308,  training time: 1.21
progress:  84%|[34m [0m| 84/100 [01:45<00:19,  1.23s/it]progress:  85%|[34m [0m| 85/100 [01:45<00:18,  1.22s/it]                                                          Episode 86	 reward: -5.09	 makespan: 503.75	 Mean_loss: 0.00175852,  training time: 1.23
progress:  85%|[34m [0m| 85/100 [01:46<00:18,  1.22s/it]progress:  86%|[34m [0m| 86/100 [01:46<00:17,  1.23s/it]                                                          Episode 87	 reward: -5.16	 makespan: 511.25	 Mean_loss: 0.00652528,  training time: 1.22
progress:  86%|[34m [0m| 86/100 [01:48<00:17,  1.23s/it]progress:  87%|[34m [0m| 87/100 [01:48<00:15,  1.23s/it]                                                          Episode 88	 reward: -5.32	 makespan: 526.50	 Mean_loss: 0.02817071,  training time: 1.30
progress:  87%|[34m [0m| 87/100 [01:49<00:15,  1.23s/it]progress:  88%|[34m [0m| 88/100 [01:49<00:14,  1.25s/it]                                                          Episode 89	 reward: -5.08	 makespan: 502.75	 Mean_loss: 0.01256311,  training time: 1.22
progress:  88%|[34m [0m| 88/100 [01:50<00:14,  1.25s/it]progress:  89%|[34m [0m| 89/100 [01:50<00:13,  1.24s/it]                                                          Episode 90	 reward: -5.48	 makespan: 542.25	 Mean_loss: 0.03332259,  training time: 1.19
progress:  89%|[34m [0m| 89/100 [01:51<00:13,  1.24s/it]progress:  90%|[34m [0m| 90/100 [01:51<00:12,  1.22s/it]                                                          Episode 91	 reward: -5.14	 makespan: 508.75	 Mean_loss: 0.02070057,  training time: 1.21
progress:  90%|[34m [0m| 90/100 [01:53<00:12,  1.22s/it]progress:  91%|[34m [0m| 91/100 [01:53<00:10,  1.22s/it]                                                          Episode 92	 reward: -5.48	 makespan: 543.00	 Mean_loss: 0.04198894,  training time: 1.22
progress:  91%|[34m [0m| 91/100 [01:54<00:10,  1.22s/it]progress:  92%|[34m[0m| 92/100 [01:54<00:09,  1.22s/it]                                                          Episode 93	 reward: -5.01	 makespan: 496.25	 Mean_loss: 0.05519260,  training time: 1.22
progress:  92%|[34m[0m| 92/100 [01:55<00:09,  1.22s/it]progress:  93%|[34m[0m| 93/100 [01:55<00:08,  1.22s/it]                                                          Episode 94	 reward: -5.31	 makespan: 525.25	 Mean_loss: 0.02913641,  training time: 1.22
progress:  93%|[34m[0m| 93/100 [01:56<00:08,  1.22s/it]progress:  94%|[34m[0m| 94/100 [01:56<00:07,  1.22s/it]                                                          Episode 95	 reward: -5.43	 makespan: 538.00	 Mean_loss: 0.01931836,  training time: 1.24
progress:  94%|[34m[0m| 94/100 [01:57<00:07,  1.22s/it]progress:  95%|[34m[0m| 95/100 [01:57<00:06,  1.23s/it]                                                          Episode 96	 reward: -5.22	 makespan: 516.75	 Mean_loss: 0.03562287,  training time: 1.22
progress:  95%|[34m[0m| 95/100 [01:59<00:06,  1.23s/it]progress:  96%|[34m[0m| 96/100 [01:59<00:04,  1.23s/it]                                                          Episode 97	 reward: -5.70	 makespan: 564.75	 Mean_loss: 0.04241195,  training time: 1.21
progress:  96%|[34m[0m| 96/100 [02:00<00:04,  1.23s/it]progress:  97%|[34m[0m| 97/100 [02:00<00:03,  1.22s/it]                                                          Episode 98	 reward: -5.47	 makespan: 541.75	 Mean_loss: 0.01861216,  training time: 1.22
progress:  97%|[34m[0m| 97/100 [02:01<00:03,  1.22s/it]progress:  98%|[34m[0m| 98/100 [02:01<00:02,  1.22s/it]                                                          Episode 99	 reward: -5.47	 makespan: 541.25	 Mean_loss: 0.02247962,  training time: 1.22
progress:  98%|[34m[0m| 98/100 [02:02<00:02,  1.22s/it]progress:  99%|[34m[0m| 99/100 [02:02<00:01,  1.22s/it]                                                          Episode 100	 reward: -5.44	 makespan: 539.00	 Mean_loss: 0.02250299,  training time: 1.21
progress:  99%|[34m[0m| 99/100 [02:04<00:01,  1.22s/it]progress: 100%|[34m[0m| 100/100 [02:04<00:00,  1.22s/it]progress: 100%|[34m[0m| 100/100 [02:04<00:00,  1.24s/it]
+ IFS=,
+ read n_j n_m
+ python train/DAN_finetuning.py --logdir ./runs/exp14/transfer_maml+exp14_1000_64_3_15x7 --model_suffix exp11_maml+exp14_1000_64_3_15x7 --finetuning_model maml+exp14_1000_64_3 --max_updates 100 --n_j 15 --n_m 7 --num_envs 4 --hidden_dim_actor 64 --hidden_dim_critic 64 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x7+mix
save model name:  15x7+mix+exp11_maml+exp14_1000_64_3_15x7
./trained_network/SD2/maml+exp14_1000_64_3.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/maml+exp14_1000_64_3.pth
vali data :./data/data_train_vali/SD2/15x7+mix


progress:   0%|[34m          [0m| 0/100 [00:00<?, ?it/s]                                                 Episode 1	 reward: -7.43	 makespan: 735.25	 Mean_loss: 0.35859790,  training time: 2.74
progress:   0%|[34m          [0m| 0/100 [00:02<?, ?it/s]progress:   1%|[34m          [0m| 1/100 [00:02<04:31,  2.74s/it]                                                         Episode 2	 reward: -6.80	 makespan: 673.50	 Mean_loss: 0.27690777,  training time: 1.72
progress:   1%|[34m          [0m| 1/100 [00:04<04:31,  2.74s/it]progress:   2%|[34m         [0m| 2/100 [00:04<03:30,  2.14s/it]                                                         Episode 3	 reward: -7.07	 makespan: 700.25	 Mean_loss: 0.23843962,  training time: 1.71
progress:   2%|[34m         [0m| 2/100 [00:06<03:30,  2.14s/it]progress:   3%|[34m         [0m| 3/100 [00:06<03:08,  1.95s/it]                                                         Episode 4	 reward: -7.66	 makespan: 758.75	 Mean_loss: 0.25210124,  training time: 1.73
progress:   3%|[34m         [0m| 3/100 [00:07<03:08,  1.95s/it]progress:   4%|[34m         [0m| 4/100 [00:07<02:58,  1.86s/it]                                                         Episode 5	 reward: -6.51	 makespan: 644.50	 Mean_loss: 0.35016912,  training time: 1.75
progress:   4%|[34m         [0m| 4/100 [00:09<02:58,  1.86s/it]progress:   5%|[34m         [0m| 5/100 [00:09<02:53,  1.82s/it]                                                         Episode 6	 reward: -6.63	 makespan: 656.00	 Mean_loss: 0.22627227,  training time: 1.80
progress:   5%|[34m         [0m| 5/100 [00:11<02:53,  1.82s/it]progress:   6%|[34m         [0m| 6/100 [00:11<02:50,  1.82s/it]                                                         Episode 7	 reward: -7.00	 makespan: 693.25	 Mean_loss: 0.19199865,  training time: 1.70
progress:   6%|[34m         [0m| 6/100 [00:13<02:50,  1.82s/it]progress:   7%|[34m         [0m| 7/100 [00:13<02:45,  1.78s/it]                                                         Episode 8	 reward: -6.55	 makespan: 648.00	 Mean_loss: 0.18136838,  training time: 1.78
progress:   7%|[34m         [0m| 7/100 [00:14<02:45,  1.78s/it]progress:   8%|[34m         [0m| 8/100 [00:14<02:43,  1.78s/it]                                                         Episode 9	 reward: -6.27	 makespan: 620.25	 Mean_loss: 0.11758050,  training time: 1.69
progress:   8%|[34m         [0m| 8/100 [00:16<02:43,  1.78s/it]progress:   9%|[34m         [0m| 9/100 [00:16<02:39,  1.75s/it]                                                         Episode 10	 reward: -6.38	 makespan: 631.75	 Mean_loss: 0.11377702,  training time: 1.70
progress:   9%|[34m         [0m| 9/100 [00:18<02:39,  1.75s/it]progress:  10%|[34m         [0m| 10/100 [00:18<02:36,  1.74s/it]                                                          Episode 11	 reward: -6.25	 makespan: 618.50	 Mean_loss: 0.11302237,  training time: 1.69
progress:  10%|[34m         [0m| 10/100 [00:20<02:36,  1.74s/it]progress:  11%|[34m         [0m| 11/100 [00:20<02:33,  1.72s/it]                                                          Episode 12	 reward: -6.82	 makespan: 674.75	 Mean_loss: 0.12831937,  training time: 1.67
progress:  11%|[34m         [0m| 11/100 [00:21<02:33,  1.72s/it]progress:  12%|[34m        [0m| 12/100 [00:21<02:30,  1.71s/it]                                                          Episode 13	 reward: -6.91	 makespan: 684.25	 Mean_loss: 0.12452441,  training time: 1.79
progress:  12%|[34m        [0m| 12/100 [00:23<02:30,  1.71s/it]progress:  13%|[34m        [0m| 13/100 [00:23<02:30,  1.73s/it]                                                          Episode 14	 reward: -6.53	 makespan: 646.50	 Mean_loss: 0.08960696,  training time: 1.67
progress:  13%|[34m        [0m| 13/100 [00:25<02:30,  1.73s/it]progress:  14%|[34m        [0m| 14/100 [00:25<02:27,  1.72s/it]                                                          Episode 15	 reward: -6.68	 makespan: 661.00	 Mean_loss: 0.14747262,  training time: 1.67
progress:  14%|[34m        [0m| 14/100 [00:26<02:27,  1.72s/it]progress:  15%|[34m        [0m| 15/100 [00:26<02:24,  1.70s/it]                                                          Episode 16	 reward: -6.23	 makespan: 617.25	 Mean_loss: 0.09164426,  training time: 1.78
progress:  15%|[34m        [0m| 15/100 [00:28<02:24,  1.70s/it]progress:  16%|[34m        [0m| 16/100 [00:28<02:25,  1.73s/it]                                                          Episode 17	 reward: -7.12	 makespan: 704.75	 Mean_loss: 0.11422361,  training time: 1.71
progress:  16%|[34m        [0m| 16/100 [00:30<02:25,  1.73s/it]progress:  17%|[34m        [0m| 17/100 [00:30<02:22,  1.72s/it]                                                          Episode 18	 reward: -6.85	 makespan: 678.00	 Mean_loss: 0.11281058,  training time: 1.70
progress:  17%|[34m        [0m| 17/100 [00:32<02:22,  1.72s/it]progress:  18%|[34m        [0m| 18/100 [00:32<02:20,  1.71s/it]                                                          Episode 19	 reward: -7.37	 makespan: 730.00	 Mean_loss: 0.07852390,  training time: 1.72
progress:  18%|[34m        [0m| 18/100 [00:33<02:20,  1.71s/it]progress:  19%|[34m        [0m| 19/100 [00:33<02:19,  1.72s/it]                                                          Episode 20	 reward: -6.73	 makespan: 666.50	 Mean_loss: 0.11636103,  training time: 1.71
progress:  19%|[34m        [0m| 19/100 [00:35<02:19,  1.72s/it]progress:  20%|[34m        [0m| 20/100 [00:35<02:17,  1.72s/it]                                                          Episode 21	 reward: -6.55	 makespan: 648.00	 Mean_loss: 0.06804143,  training time: 1.85
progress:  20%|[34m        [0m| 20/100 [00:37<02:17,  1.72s/it]progress:  21%|[34m        [0m| 21/100 [00:37<02:18,  1.76s/it]                                                          Episode 22	 reward: -6.26	 makespan: 619.25	 Mean_loss: 0.06119678,  training time: 1.77
progress:  21%|[34m        [0m| 21/100 [00:39<02:18,  1.76s/it]progress:  22%|[34m       [0m| 22/100 [00:39<02:17,  1.76s/it]                                                          Episode 23	 reward: -6.47	 makespan: 641.00	 Mean_loss: 0.06494528,  training time: 1.65
progress:  22%|[34m       [0m| 22/100 [00:40<02:17,  1.76s/it]progress:  23%|[34m       [0m| 23/100 [00:40<02:12,  1.73s/it]                                                          Episode 24	 reward: -6.15	 makespan: 609.25	 Mean_loss: 0.05173573,  training time: 1.66
progress:  23%|[34m       [0m| 23/100 [00:42<02:12,  1.73s/it]progress:  24%|[34m       [0m| 24/100 [00:42<02:09,  1.71s/it]                                                          Episode 25	 reward: -5.97	 makespan: 591.50	 Mean_loss: 0.03944403,  training time: 1.66
progress:  24%|[34m       [0m| 24/100 [00:44<02:09,  1.71s/it]progress:  25%|[34m       [0m| 25/100 [00:44<02:06,  1.69s/it]                                                          Episode 26	 reward: -6.02	 makespan: 595.75	 Mean_loss: 0.04690566,  training time: 1.66
progress:  25%|[34m       [0m| 25/100 [00:45<02:06,  1.69s/it]progress:  26%|[34m       [0m| 26/100 [00:45<02:04,  1.68s/it]                                                          Episode 27	 reward: -6.09	 makespan: 603.25	 Mean_loss: 0.03746428,  training time: 1.66
progress:  26%|[34m       [0m| 26/100 [00:47<02:04,  1.68s/it]progress:  27%|[34m       [0m| 27/100 [00:47<02:02,  1.68s/it]                                                          Episode 28	 reward: -5.84	 makespan: 577.75	 Mean_loss: 0.05348638,  training time: 1.66
progress:  27%|[34m       [0m| 27/100 [00:49<02:02,  1.68s/it]progress:  28%|[34m       [0m| 28/100 [00:49<02:00,  1.67s/it]                                                          Episode 29	 reward: -5.83	 makespan: 577.50	 Mean_loss: 0.03443150,  training time: 1.66
progress:  28%|[34m       [0m| 28/100 [00:50<02:00,  1.67s/it]progress:  29%|[34m       [0m| 29/100 [00:50<01:58,  1.67s/it]                                                          Episode 30	 reward: -6.12	 makespan: 606.25	 Mean_loss: 0.05913019,  training time: 1.68
progress:  29%|[34m       [0m| 29/100 [00:52<01:58,  1.67s/it]progress:  30%|[34m       [0m| 30/100 [00:52<01:56,  1.67s/it]                                                          Episode 31	 reward: -6.02	 makespan: 596.25	 Mean_loss: 0.03847222,  training time: 1.67
progress:  30%|[34m       [0m| 30/100 [00:54<01:56,  1.67s/it]progress:  31%|[34m       [0m| 31/100 [00:54<01:55,  1.67s/it]                                                          Episode 32	 reward: -5.92	 makespan: 586.00	 Mean_loss: 0.03082223,  training time: 1.65
progress:  31%|[34m       [0m| 31/100 [00:55<01:55,  1.67s/it]progress:  32%|[34m      [0m| 32/100 [00:55<01:53,  1.67s/it]                                                          Episode 33	 reward: -5.80	 makespan: 574.00	 Mean_loss: 0.03182981,  training time: 1.66
progress:  32%|[34m      [0m| 32/100 [00:57<01:53,  1.67s/it]progress:  33%|[34m      [0m| 33/100 [00:57<01:51,  1.66s/it]                                                          Episode 34	 reward: -5.88	 makespan: 581.75	 Mean_loss: 0.03075813,  training time: 1.66
progress:  33%|[34m      [0m| 33/100 [00:59<01:51,  1.66s/it]progress:  34%|[34m      [0m| 34/100 [00:59<01:49,  1.66s/it]                                                          Episode 35	 reward: -5.79	 makespan: 573.50	 Mean_loss: 0.01366301,  training time: 1.68
progress:  34%|[34m      [0m| 34/100 [01:00<01:49,  1.66s/it]progress:  35%|[34m      [0m| 35/100 [01:00<01:48,  1.67s/it]                                                          Episode 36	 reward: -6.05	 makespan: 598.75	 Mean_loss: 0.03900956,  training time: 1.67
progress:  35%|[34m      [0m| 35/100 [01:02<01:48,  1.67s/it]progress:  36%|[34m      [0m| 36/100 [01:02<01:46,  1.67s/it]                                                          Episode 37	 reward: -5.70	 makespan: 564.75	 Mean_loss: 0.00732404,  training time: 1.67
progress:  36%|[34m      [0m| 36/100 [01:04<01:46,  1.67s/it]progress:  37%|[34m      [0m| 37/100 [01:04<01:45,  1.67s/it]                                                          Episode 38	 reward: -5.62	 makespan: 556.00	 Mean_loss: 0.03354542,  training time: 1.66
progress:  37%|[34m      [0m| 37/100 [01:05<01:45,  1.67s/it]progress:  38%|[34m      [0m| 38/100 [01:05<01:43,  1.67s/it]                                                          Episode 39	 reward: -5.73	 makespan: 567.00	 Mean_loss: 0.02990587,  training time: 1.67
progress:  38%|[34m      [0m| 38/100 [01:07<01:43,  1.67s/it]progress:  39%|[34m      [0m| 39/100 [01:07<01:41,  1.67s/it]                                                          Episode 40	 reward: -5.90	 makespan: 584.00	 Mean_loss: 0.06281809,  training time: 1.67
progress:  39%|[34m      [0m| 39/100 [01:09<01:41,  1.67s/it]progress:  40%|[34m      [0m| 40/100 [01:09<01:40,  1.67s/it]                                                          Episode 41	 reward: -5.95	 makespan: 589.50	 Mean_loss: 0.02960932,  training time: 1.67
progress:  40%|[34m      [0m| 40/100 [01:10<01:40,  1.67s/it]progress:  41%|[34m      [0m| 41/100 [01:10<01:38,  1.67s/it]                                                          Episode 42	 reward: -5.81	 makespan: 575.00	 Mean_loss: 0.03436898,  training time: 1.65
progress:  41%|[34m      [0m| 41/100 [01:12<01:38,  1.67s/it]progress:  42%|[34m     [0m| 42/100 [01:12<01:36,  1.66s/it]                                                          Episode 43	 reward: -5.67	 makespan: 561.50	 Mean_loss: 0.04087890,  training time: 1.66
progress:  42%|[34m     [0m| 42/100 [01:14<01:36,  1.66s/it]progress:  43%|[34m     [0m| 43/100 [01:14<01:34,  1.66s/it]                                                          Episode 44	 reward: -5.90	 makespan: 584.25	 Mean_loss: 0.02542293,  training time: 1.65
progress:  43%|[34m     [0m| 43/100 [01:15<01:34,  1.66s/it]progress:  44%|[34m     [0m| 44/100 [01:15<01:33,  1.66s/it]                                                          Episode 45	 reward: -5.65	 makespan: 559.75	 Mean_loss: 0.01988458,  training time: 1.66
progress:  44%|[34m     [0m| 44/100 [01:17<01:33,  1.66s/it]progress:  45%|[34m     [0m| 45/100 [01:17<01:31,  1.66s/it]                                                          Episode 46	 reward: -5.96	 makespan: 589.75	 Mean_loss: 0.04819054,  training time: 1.67
progress:  45%|[34m     [0m| 45/100 [01:19<01:31,  1.66s/it]progress:  46%|[34m     [0m| 46/100 [01:19<01:29,  1.66s/it]                                                          Episode 47	 reward: -5.90	 makespan: 584.50	 Mean_loss: 0.02529312,  training time: 1.68
progress:  46%|[34m     [0m| 46/100 [01:20<01:29,  1.66s/it]progress:  47%|[34m     [0m| 47/100 [01:20<01:28,  1.67s/it]                                                          Episode 48	 reward: -5.64	 makespan: 558.00	 Mean_loss: 0.03192933,  training time: 1.73
progress:  47%|[34m     [0m| 47/100 [01:22<01:28,  1.67s/it]progress:  48%|[34m     [0m| 48/100 [01:22<01:27,  1.69s/it]                                                          Episode 49	 reward: -5.76	 makespan: 570.25	 Mean_loss: 0.01754353,  training time: 1.66
progress:  48%|[34m     [0m| 48/100 [01:24<01:27,  1.69s/it]progress:  49%|[34m     [0m| 49/100 [01:24<01:25,  1.68s/it]                                                          Episode 50	 reward: -5.75	 makespan: 569.00	 Mean_loss: 0.02210302,  training time: 1.66
progress:  49%|[34m     [0m| 49/100 [01:25<01:25,  1.68s/it]progress:  50%|[34m     [0m| 50/100 [01:25<01:23,  1.67s/it]                                                          Episode 51	 reward: -5.86	 makespan: 580.00	 Mean_loss: 0.02252736,  training time: 1.67
progress:  50%|[34m     [0m| 50/100 [01:27<01:23,  1.67s/it]progress:  51%|[34m     [0m| 51/100 [01:27<01:22,  1.67s/it]                                                          Episode 52	 reward: -5.78	 makespan: 571.75	 Mean_loss: 0.02548951,  training time: 1.68
progress:  51%|[34m     [0m| 51/100 [01:29<01:22,  1.67s/it]progress:  52%|[34m    [0m| 52/100 [01:29<01:20,  1.68s/it]                                                          Episode 53	 reward: -5.59	 makespan: 553.75	 Mean_loss: 0.01956282,  training time: 1.68
progress:  52%|[34m    [0m| 52/100 [01:30<01:20,  1.68s/it]progress:  53%|[34m    [0m| 53/100 [01:30<01:18,  1.68s/it]                                                          Episode 54	 reward: -5.66	 makespan: 560.25	 Mean_loss: 0.01280285,  training time: 1.68
progress:  53%|[34m    [0m| 53/100 [01:32<01:18,  1.68s/it]progress:  54%|[34m    [0m| 54/100 [01:32<01:17,  1.68s/it]                                                          Episode 55	 reward: -5.98	 makespan: 592.00	 Mean_loss: 0.02152909,  training time: 1.69
progress:  54%|[34m    [0m| 54/100 [01:34<01:17,  1.68s/it]progress:  55%|[34m    [0m| 55/100 [01:34<01:15,  1.68s/it]                                                          Episode 56	 reward: -5.76	 makespan: 569.75	 Mean_loss: 0.02694330,  training time: 1.68
progress:  55%|[34m    [0m| 55/100 [01:35<01:15,  1.68s/it]progress:  56%|[34m    [0m| 56/100 [01:35<01:13,  1.68s/it]                                                          Episode 57	 reward: -5.65	 makespan: 559.50	 Mean_loss: 0.03095216,  training time: 1.69
progress:  56%|[34m    [0m| 56/100 [01:37<01:13,  1.68s/it]progress:  57%|[34m    [0m| 57/100 [01:37<01:12,  1.68s/it]                                                          Episode 58	 reward: -5.82	 makespan: 575.75	 Mean_loss: 0.01969850,  training time: 1.68
progress:  57%|[34m    [0m| 57/100 [01:39<01:12,  1.68s/it]progress:  58%|[34m    [0m| 58/100 [01:39<01:10,  1.68s/it]                                                          Episode 59	 reward: -5.79	 makespan: 572.75	 Mean_loss: 0.03097984,  training time: 1.67
progress:  58%|[34m    [0m| 58/100 [01:40<01:10,  1.68s/it]progress:  59%|[34m    [0m| 59/100 [01:40<01:08,  1.68s/it]                                                          Episode 60	 reward: -5.73	 makespan: 567.25	 Mean_loss: 0.03191973,  training time: 1.66
progress:  59%|[34m    [0m| 59/100 [01:42<01:08,  1.68s/it]progress:  60%|[34m    [0m| 60/100 [01:42<01:07,  1.68s/it]                                                          Episode 61	 reward: -5.56	 makespan: 550.25	 Mean_loss: 0.02339476,  training time: 1.69
progress:  60%|[34m    [0m| 60/100 [01:44<01:07,  1.68s/it]progress:  61%|[34m    [0m| 61/100 [01:44<01:05,  1.68s/it]                                                          Episode 62	 reward: -6.10	 makespan: 604.00	 Mean_loss: 0.04796544,  training time: 1.72
progress:  61%|[34m    [0m| 61/100 [01:45<01:05,  1.68s/it]progress:  62%|[34m   [0m| 62/100 [01:45<01:04,  1.69s/it]                                                          Episode 63	 reward: -5.90	 makespan: 584.50	 Mean_loss: 0.01670961,  training time: 1.67
progress:  62%|[34m   [0m| 62/100 [01:47<01:04,  1.69s/it]progress:  63%|[34m   [0m| 63/100 [01:47<01:02,  1.68s/it]                                                          Episode 64	 reward: -5.78	 makespan: 572.25	 Mean_loss: 0.03487176,  training time: 1.66
progress:  63%|[34m   [0m| 63/100 [01:49<01:02,  1.68s/it]progress:  64%|[34m   [0m| 64/100 [01:49<01:00,  1.68s/it]                                                          Episode 65	 reward: -5.84	 makespan: 578.50	 Mean_loss: 0.02724483,  training time: 1.66
progress:  64%|[34m   [0m| 64/100 [01:50<01:00,  1.68s/it]progress:  65%|[34m   [0m| 65/100 [01:50<00:58,  1.67s/it]                                                          Episode 66	 reward: -6.11	 makespan: 604.75	 Mean_loss: 0.04772277,  training time: 1.66
progress:  65%|[34m   [0m| 65/100 [01:52<00:58,  1.67s/it]progress:  66%|[34m   [0m| 66/100 [01:52<00:56,  1.67s/it]                                                          Episode 67	 reward: -6.05	 makespan: 599.25	 Mean_loss: 0.02628766,  training time: 1.66
progress:  66%|[34m   [0m| 66/100 [01:54<00:56,  1.67s/it]progress:  67%|[34m   [0m| 67/100 [01:54<00:55,  1.67s/it]                                                          Episode 68	 reward: -5.58	 makespan: 552.00	 Mean_loss: 0.02592609,  training time: 1.66
progress:  67%|[34m   [0m| 67/100 [01:55<00:55,  1.67s/it]progress:  68%|[34m   [0m| 68/100 [01:55<00:53,  1.67s/it]                                                          Episode 69	 reward: -5.59	 makespan: 553.50	 Mean_loss: 0.02025483,  training time: 1.67
progress:  68%|[34m   [0m| 68/100 [01:57<00:53,  1.67s/it]progress:  69%|[34m   [0m| 69/100 [01:57<00:51,  1.67s/it]                                                          Episode 70	 reward: -5.61	 makespan: 555.75	 Mean_loss: 0.03827783,  training time: 1.66
progress:  69%|[34m   [0m| 69/100 [01:59<00:51,  1.67s/it]progress:  70%|[34m   [0m| 70/100 [01:59<00:49,  1.67s/it]                                                          Episode 71	 reward: -5.57	 makespan: 551.75	 Mean_loss: 0.01623695,  training time: 1.67
progress:  70%|[34m   [0m| 70/100 [02:00<00:49,  1.67s/it]progress:  71%|[34m   [0m| 71/100 [02:00<00:48,  1.67s/it]                                                          Episode 72	 reward: -5.69	 makespan: 563.50	 Mean_loss: 0.02653940,  training time: 1.66
progress:  71%|[34m   [0m| 71/100 [02:02<00:48,  1.67s/it]progress:  72%|[34m  [0m| 72/100 [02:02<00:46,  1.67s/it]                                                          Episode 73	 reward: -5.89	 makespan: 582.75	 Mean_loss: 0.02825743,  training time: 1.74
progress:  72%|[34m  [0m| 72/100 [02:04<00:46,  1.67s/it]progress:  73%|[34m  [0m| 73/100 [02:04<00:45,  1.69s/it]                                                          Episode 74	 reward: -5.71	 makespan: 565.50	 Mean_loss: 0.01862530,  training time: 1.67
progress:  73%|[34m  [0m| 73/100 [02:06<00:45,  1.69s/it]progress:  74%|[34m  [0m| 74/100 [02:06<00:43,  1.69s/it]                                                          Episode 75	 reward: -5.67	 makespan: 561.75	 Mean_loss: 0.02270357,  training time: 1.66
progress:  74%|[34m  [0m| 74/100 [02:07<00:43,  1.69s/it]progress:  75%|[34m  [0m| 75/100 [02:07<00:41,  1.68s/it]                                                          Episode 76	 reward: -5.74	 makespan: 568.25	 Mean_loss: 0.02009911,  training time: 1.67
progress:  75%|[34m  [0m| 75/100 [02:09<00:41,  1.68s/it]progress:  76%|[34m  [0m| 76/100 [02:09<00:40,  1.68s/it]                                                          Episode 77	 reward: -5.66	 makespan: 560.00	 Mean_loss: 0.02295606,  training time: 1.68
progress:  76%|[34m  [0m| 76/100 [02:11<00:40,  1.68s/it]progress:  77%|[34m  [0m| 77/100 [02:11<00:38,  1.68s/it]                                                          Episode 78	 reward: -5.62	 makespan: 556.25	 Mean_loss: 0.01555566,  training time: 1.66
progress:  77%|[34m  [0m| 77/100 [02:12<00:38,  1.68s/it]progress:  78%|[34m  [0m| 78/100 [02:12<00:36,  1.67s/it]                                                          Episode 79	 reward: -5.82	 makespan: 576.25	 Mean_loss: 0.03260029,  training time: 1.67
progress:  78%|[34m  [0m| 78/100 [02:14<00:36,  1.67s/it]progress:  79%|[34m  [0m| 79/100 [02:14<00:35,  1.67s/it]                                                          Episode 80	 reward: -6.13	 makespan: 607.00	 Mean_loss: 0.03668207,  training time: 1.67
progress:  79%|[34m  [0m| 79/100 [02:16<00:35,  1.67s/it]progress:  80%|[34m  [0m| 80/100 [02:16<00:33,  1.67s/it]                                                          Episode 81	 reward: -5.90	 makespan: 584.25	 Mean_loss: 0.02050212,  training time: 1.72
progress:  80%|[34m  [0m| 80/100 [02:17<00:33,  1.67s/it]progress:  81%|[34m  [0m| 81/100 [02:17<00:32,  1.69s/it]                                                          Episode 82	 reward: -5.63	 makespan: 557.50	 Mean_loss: 0.02734436,  training time: 1.67
progress:  81%|[34m  [0m| 81/100 [02:19<00:32,  1.69s/it]progress:  82%|[34m [0m| 82/100 [02:19<00:30,  1.69s/it]                                                          Episode 83	 reward: -6.03	 makespan: 597.00	 Mean_loss: 0.02379123,  training time: 1.68
progress:  82%|[34m [0m| 82/100 [02:21<00:30,  1.69s/it]progress:  83%|[34m [0m| 83/100 [02:21<00:28,  1.69s/it]                                                          Episode 84	 reward: -5.94	 makespan: 587.75	 Mean_loss: 0.03872989,  training time: 1.68
progress:  83%|[34m [0m| 83/100 [02:22<00:28,  1.69s/it]progress:  84%|[34m [0m| 84/100 [02:22<00:26,  1.69s/it]                                                          Episode 85	 reward: -6.00	 makespan: 593.75	 Mean_loss: 0.03464124,  training time: 1.68
progress:  84%|[34m [0m| 84/100 [02:24<00:26,  1.69s/it]progress:  85%|[34m [0m| 85/100 [02:24<00:25,  1.68s/it]                                                          Episode 86	 reward: -5.94	 makespan: 588.50	 Mean_loss: 0.01227874,  training time: 1.68
progress:  85%|[34m [0m| 85/100 [02:26<00:25,  1.68s/it]progress:  86%|[34m [0m| 86/100 [02:26<00:23,  1.68s/it]                                                          Episode 87	 reward: -5.83	 makespan: 576.75	 Mean_loss: 0.02806875,  training time: 1.68
progress:  86%|[34m [0m| 86/100 [02:27<00:23,  1.68s/it]progress:  87%|[34m [0m| 87/100 [02:27<00:21,  1.68s/it]                                                          Episode 88	 reward: -6.33	 makespan: 626.25	 Mean_loss: 0.02109030,  training time: 1.68
progress:  87%|[34m [0m| 87/100 [02:29<00:21,  1.68s/it]progress:  88%|[34m [0m| 88/100 [02:29<00:20,  1.68s/it]                                                          Episode 89	 reward: -5.68	 makespan: 562.00	 Mean_loss: 0.02776546,  training time: 1.69
progress:  88%|[34m [0m| 88/100 [02:31<00:20,  1.68s/it]progress:  89%|[34m [0m| 89/100 [02:31<00:18,  1.68s/it]                                                          Episode 90	 reward: -5.61	 makespan: 555.25	 Mean_loss: 0.02779489,  training time: 1.65
progress:  89%|[34m [0m| 89/100 [02:32<00:18,  1.68s/it]progress:  90%|[34m [0m| 90/100 [02:32<00:16,  1.67s/it]                                                          Episode 91	 reward: -5.94	 makespan: 588.50	 Mean_loss: 0.03746847,  training time: 1.66
progress:  90%|[34m [0m| 90/100 [02:34<00:16,  1.67s/it]progress:  91%|[34m [0m| 91/100 [02:34<00:15,  1.67s/it]                                                          Episode 92	 reward: -5.78	 makespan: 572.25	 Mean_loss: 0.01347556,  training time: 1.67
progress:  91%|[34m [0m| 91/100 [02:36<00:15,  1.67s/it]progress:  92%|[34m[0m| 92/100 [02:36<00:13,  1.67s/it]                                                          Episode 93	 reward: -5.49	 makespan: 543.50	 Mean_loss: 0.01927613,  training time: 1.66
progress:  92%|[34m[0m| 92/100 [02:37<00:13,  1.67s/it]progress:  93%|[34m[0m| 93/100 [02:37<00:11,  1.67s/it]                                                          Episode 94	 reward: -6.05	 makespan: 599.00	 Mean_loss: 0.02815386,  training time: 1.67
progress:  93%|[34m[0m| 93/100 [02:39<00:11,  1.67s/it]progress:  94%|[34m[0m| 94/100 [02:39<00:09,  1.67s/it]                                                          Episode 95	 reward: -5.97	 makespan: 591.00	 Mean_loss: 0.02024730,  training time: 1.64
progress:  94%|[34m[0m| 94/100 [02:41<00:09,  1.67s/it]progress:  95%|[34m[0m| 95/100 [02:41<00:08,  1.66s/it]                                                          Episode 96	 reward: -5.60	 makespan: 554.00	 Mean_loss: 0.03758697,  training time: 1.66
progress:  95%|[34m[0m| 95/100 [02:42<00:08,  1.66s/it]progress:  96%|[34m[0m| 96/100 [02:42<00:06,  1.66s/it]                                                          Episode 97	 reward: -6.17	 makespan: 611.25	 Mean_loss: 0.04008006,  training time: 1.68
progress:  96%|[34m[0m| 96/100 [02:44<00:06,  1.66s/it]progress:  97%|[34m[0m| 97/100 [02:44<00:04,  1.67s/it]                                                          Episode 98	 reward: -6.08	 makespan: 602.00	 Mean_loss: 0.02477579,  training time: 1.67
progress:  97%|[34m[0m| 97/100 [02:46<00:04,  1.67s/it]progress:  98%|[34m[0m| 98/100 [02:46<00:03,  1.67s/it]                                                          Episode 99	 reward: -6.13	 makespan: 606.75	 Mean_loss: 0.03342532,  training time: 1.69
progress:  98%|[34m[0m| 98/100 [02:47<00:03,  1.67s/it]progress:  99%|[34m[0m| 99/100 [02:47<00:01,  1.67s/it]                                                          Episode 100	 reward: -5.75	 makespan: 569.50	 Mean_loss: 0.02577883,  training time: 1.67
progress:  99%|[34m[0m| 99/100 [02:49<00:01,  1.67s/it]progress: 100%|[34m[0m| 100/100 [02:49<00:00,  1.67s/it]progress: 100%|[34m[0m| 100/100 [02:49<00:00,  1.70s/it]
+ IFS=,
+ read n_j n_m
+ python train/DAN_finetuning.py --logdir ./runs/exp14/transfer_maml+exp14_1000_64_3_15x9 --model_suffix exp11_maml+exp14_1000_64_3_15x9 --finetuning_model maml+exp14_1000_64_3 --max_updates 100 --n_j 15 --n_m 9 --num_envs 4 --hidden_dim_actor 64 --hidden_dim_critic 64 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x9+mix
save model name:  15x9+mix+exp11_maml+exp14_1000_64_3_15x9
./trained_network/SD2/maml+exp14_1000_64_3.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/maml+exp14_1000_64_3.pth
vali data :./data/data_train_vali/SD2/15x9+mix


progress:   0%|[34m          [0m| 0/100 [00:00<?, ?it/s]                                                 Episode 1	 reward: -7.40	 makespan: 732.50	 Mean_loss: 0.22179975,  training time: 3.15
progress:   0%|[34m          [0m| 0/100 [00:03<?, ?it/s]progress:   1%|[34m          [0m| 1/100 [00:03<05:11,  3.15s/it]                                                         Episode 2	 reward: -7.60	 makespan: 752.50	 Mean_loss: 0.21857753,  training time: 2.14
progress:   1%|[34m          [0m| 1/100 [00:05<05:11,  3.15s/it]progress:   2%|[34m         [0m| 2/100 [00:05<04:10,  2.56s/it]                                                         Episode 3	 reward: -7.79	 makespan: 770.75	 Mean_loss: 0.33207813,  training time: 2.08
progress:   2%|[34m         [0m| 2/100 [00:07<04:10,  2.56s/it]progress:   3%|[34m         [0m| 3/100 [00:07<03:46,  2.34s/it]                                                         Episode 4	 reward: -7.28	 makespan: 720.25	 Mean_loss: 0.37213495,  training time: 2.08
progress:   3%|[34m         [0m| 3/100 [00:09<03:46,  2.34s/it]progress:   4%|[34m         [0m| 4/100 [00:09<03:34,  2.23s/it]                                                         Episode 5	 reward: -7.83	 makespan: 775.00	 Mean_loss: 0.28380322,  training time: 2.06
progress:   4%|[34m         [0m| 4/100 [00:11<03:34,  2.23s/it]progress:   5%|[34m         [0m| 5/100 [00:11<03:26,  2.17s/it]                                                         Episode 6	 reward: -7.88	 makespan: 780.25	 Mean_loss: 0.34932208,  training time: 2.17
progress:   5%|[34m         [0m| 5/100 [00:13<03:26,  2.17s/it]progress:   6%|[34m         [0m| 6/100 [00:13<03:24,  2.17s/it]                                                         Episode 7	 reward: -8.48	 makespan: 839.75	 Mean_loss: 0.35493362,  training time: 2.15
progress:   6%|[34m         [0m| 6/100 [00:15<03:24,  2.17s/it]progress:   7%|[34m         [0m| 7/100 [00:15<03:21,  2.16s/it]                                                         Episode 8	 reward: -7.61	 makespan: 753.50	 Mean_loss: 0.32957494,  training time: 2.06
progress:   7%|[34m         [0m| 7/100 [00:17<03:21,  2.16s/it]progress:   8%|[34m         [0m| 8/100 [00:17<03:15,  2.13s/it]                                                         Episode 9	 reward: -7.60	 makespan: 752.75	 Mean_loss: 0.28526598,  training time: 2.16
progress:   8%|[34m         [0m| 8/100 [00:20<03:15,  2.13s/it]progress:   9%|[34m         [0m| 9/100 [00:20<03:14,  2.14s/it]                                                         Episode 10	 reward: -7.10	 makespan: 703.25	 Mean_loss: 0.18232338,  training time: 2.13
progress:   9%|[34m         [0m| 9/100 [00:22<03:14,  2.14s/it]progress:  10%|[34m         [0m| 10/100 [00:22<03:12,  2.14s/it]                                                          Episode 11	 reward: -7.32	 makespan: 724.50	 Mean_loss: 0.12987086,  training time: 2.14
progress:  10%|[34m         [0m| 10/100 [00:24<03:12,  2.14s/it]progress:  11%|[34m         [0m| 11/100 [00:24<03:10,  2.14s/it]                                                          Episode 12	 reward: -7.19	 makespan: 712.00	 Mean_loss: 0.17775217,  training time: 2.12
progress:  11%|[34m         [0m| 11/100 [00:26<03:10,  2.14s/it]progress:  12%|[34m        [0m| 12/100 [00:26<03:07,  2.13s/it]                                                          Episode 13	 reward: -6.86	 makespan: 679.25	 Mean_loss: 0.14021567,  training time: 2.12
progress:  12%|[34m        [0m| 12/100 [00:28<03:07,  2.13s/it]progress:  13%|[34m        [0m| 13/100 [00:28<03:05,  2.13s/it]                                                          Episode 14	 reward: -6.88	 makespan: 681.00	 Mean_loss: 0.08821654,  training time: 2.13
progress:  13%|[34m        [0m| 13/100 [00:30<03:05,  2.13s/it]progress:  14%|[34m        [0m| 14/100 [00:30<03:03,  2.13s/it]                                                          Episode 15	 reward: -6.44	 makespan: 637.75	 Mean_loss: 0.07121019,  training time: 2.13
progress:  14%|[34m        [0m| 14/100 [00:32<03:03,  2.13s/it]progress:  15%|[34m        [0m| 15/100 [00:32<03:01,  2.13s/it]                                                          Episode 16	 reward: -6.46	 makespan: 640.00	 Mean_loss: 0.07192540,  training time: 2.08
progress:  15%|[34m        [0m| 15/100 [00:34<03:01,  2.13s/it]progress:  16%|[34m        [0m| 16/100 [00:34<02:57,  2.12s/it]                                                          Episode 17	 reward: -6.73	 makespan: 666.00	 Mean_loss: 0.04690652,  training time: 2.10
progress:  16%|[34m        [0m| 16/100 [00:37<02:57,  2.12s/it]progress:  17%|[34m        [0m| 17/100 [00:37<02:55,  2.11s/it]                                                          Episode 18	 reward: -6.71	 makespan: 664.75	 Mean_loss: 0.04608736,  training time: 2.10
progress:  17%|[34m        [0m| 17/100 [00:39<02:55,  2.11s/it]progress:  18%|[34m        [0m| 18/100 [00:39<02:52,  2.11s/it]                                                          Episode 19	 reward: -7.03	 makespan: 695.75	 Mean_loss: 0.07502338,  training time: 2.08
progress:  18%|[34m        [0m| 18/100 [00:41<02:52,  2.11s/it]progress:  19%|[34m        [0m| 19/100 [00:41<02:50,  2.10s/it]                                                          Episode 20	 reward: -6.46	 makespan: 639.25	 Mean_loss: 0.08559147,  training time: 2.08
progress:  19%|[34m        [0m| 19/100 [00:43<02:50,  2.10s/it]progress:  20%|[34m        [0m| 20/100 [00:43<02:47,  2.09s/it]                                                          Episode 21	 reward: -6.43	 makespan: 637.00	 Mean_loss: 0.08120039,  training time: 2.07
progress:  20%|[34m        [0m| 20/100 [00:45<02:47,  2.09s/it]progress:  21%|[34m        [0m| 21/100 [00:45<02:44,  2.09s/it]                                                          Episode 22	 reward: -6.46	 makespan: 639.50	 Mean_loss: 0.05631710,  training time: 2.08
progress:  21%|[34m        [0m| 21/100 [00:47<02:44,  2.09s/it]progress:  22%|[34m       [0m| 22/100 [00:47<02:42,  2.09s/it]                                                          Episode 23	 reward: -6.35	 makespan: 628.75	 Mean_loss: 0.06275377,  training time: 2.07
progress:  22%|[34m       [0m| 22/100 [00:49<02:42,  2.09s/it]progress:  23%|[34m       [0m| 23/100 [00:49<02:40,  2.08s/it]                                                          Episode 24	 reward: -6.46	 makespan: 639.25	 Mean_loss: 0.06792632,  training time: 2.08
progress:  23%|[34m       [0m| 23/100 [00:51<02:40,  2.08s/it]progress:  24%|[34m       [0m| 24/100 [00:51<02:38,  2.08s/it]                                                          Episode 25	 reward: -6.48	 makespan: 642.00	 Mean_loss: 0.04420009,  training time: 2.08
progress:  24%|[34m       [0m| 24/100 [00:53<02:38,  2.08s/it]progress:  25%|[34m       [0m| 25/100 [00:53<02:36,  2.08s/it]                                                          Episode 26	 reward: -6.43	 makespan: 636.50	 Mean_loss: 0.04758132,  training time: 2.08
progress:  25%|[34m       [0m| 25/100 [00:55<02:36,  2.08s/it]progress:  26%|[34m       [0m| 26/100 [00:55<02:34,  2.08s/it]                                                          Episode 27	 reward: -6.10	 makespan: 603.75	 Mean_loss: 0.03703490,  training time: 2.07
progress:  26%|[34m       [0m| 26/100 [00:57<02:34,  2.08s/it]progress:  27%|[34m       [0m| 27/100 [00:57<02:31,  2.08s/it]                                                          Episode 28	 reward: -6.66	 makespan: 659.25	 Mean_loss: 0.07025024,  training time: 2.08
progress:  27%|[34m       [0m| 27/100 [00:59<02:31,  2.08s/it]progress:  28%|[34m       [0m| 28/100 [00:59<02:29,  2.08s/it]                                                          Episode 29	 reward: -6.04	 makespan: 597.50	 Mean_loss: 0.03661821,  training time: 2.08
progress:  28%|[34m       [0m| 28/100 [01:01<02:29,  2.08s/it]progress:  29%|[34m       [0m| 29/100 [01:01<02:27,  2.08s/it]                                                          Episode 30	 reward: -6.48	 makespan: 641.75	 Mean_loss: 0.07532371,  training time: 2.09
progress:  29%|[34m       [0m| 29/100 [01:04<02:27,  2.08s/it]progress:  30%|[34m       [0m| 30/100 [01:04<02:25,  2.08s/it]                                                          Episode 31	 reward: -5.91	 makespan: 584.75	 Mean_loss: 0.03460766,  training time: 2.08
progress:  30%|[34m       [0m| 30/100 [01:06<02:25,  2.08s/it]progress:  31%|[34m       [0m| 31/100 [01:06<02:23,  2.08s/it]                                                          Episode 32	 reward: -6.41	 makespan: 634.75	 Mean_loss: 0.05438539,  training time: 2.09
progress:  31%|[34m       [0m| 31/100 [01:08<02:23,  2.08s/it]progress:  32%|[34m      [0m| 32/100 [01:08<02:21,  2.09s/it]                                                          Episode 33	 reward: -5.98	 makespan: 592.25	 Mean_loss: 0.05475861,  training time: 2.07
progress:  32%|[34m      [0m| 32/100 [01:10<02:21,  2.09s/it]progress:  33%|[34m      [0m| 33/100 [01:10<02:19,  2.08s/it]                                                          Episode 34	 reward: -5.88	 makespan: 582.00	 Mean_loss: 0.03939785,  training time: 2.08
progress:  33%|[34m      [0m| 33/100 [01:12<02:19,  2.08s/it]progress:  34%|[34m      [0m| 34/100 [01:12<02:17,  2.08s/it]                                                          Episode 35	 reward: -6.08	 makespan: 602.00	 Mean_loss: 0.02689446,  training time: 2.08
progress:  34%|[34m      [0m| 34/100 [01:14<02:17,  2.08s/it]progress:  35%|[34m      [0m| 35/100 [01:14<02:15,  2.08s/it]                                                          Episode 36	 reward: -6.02	 makespan: 596.00	 Mean_loss: 0.03571719,  training time: 2.03
progress:  35%|[34m      [0m| 35/100 [01:16<02:15,  2.08s/it]progress:  36%|[34m      [0m| 36/100 [01:16<02:12,  2.07s/it]                                                          Episode 37	 reward: -6.19	 makespan: 612.50	 Mean_loss: 0.04972704,  training time: 2.08
progress:  36%|[34m      [0m| 36/100 [01:18<02:12,  2.07s/it]progress:  37%|[34m      [0m| 37/100 [01:18<02:10,  2.07s/it]                                                          Episode 38	 reward: -6.14	 makespan: 608.25	 Mean_loss: 0.03245701,  training time: 2.04
progress:  37%|[34m      [0m| 37/100 [01:20<02:10,  2.07s/it]progress:  38%|[34m      [0m| 38/100 [01:20<02:07,  2.06s/it]                                                          Episode 39	 reward: -6.10	 makespan: 603.75	 Mean_loss: 0.03354971,  training time: 2.09
progress:  38%|[34m      [0m| 38/100 [01:22<02:07,  2.06s/it]progress:  39%|[34m      [0m| 39/100 [01:22<02:06,  2.07s/it]                                                          Episode 40	 reward: -6.01	 makespan: 595.00	 Mean_loss: 0.04304023,  training time: 2.09
progress:  39%|[34m      [0m| 39/100 [01:24<02:06,  2.07s/it]progress:  40%|[34m      [0m| 40/100 [01:24<02:04,  2.08s/it]                                                          Episode 41	 reward: -6.03	 makespan: 597.25	 Mean_loss: 0.04647518,  training time: 2.08
progress:  40%|[34m      [0m| 40/100 [01:26<02:04,  2.08s/it]progress:  41%|[34m      [0m| 41/100 [01:26<02:02,  2.08s/it]                                                          Episode 42	 reward: -6.04	 makespan: 597.75	 Mean_loss: 0.03264392,  training time: 2.09
progress:  41%|[34m      [0m| 41/100 [01:29<02:02,  2.08s/it]progress:  42%|[34m     [0m| 42/100 [01:29<02:00,  2.08s/it]                                                          Episode 43	 reward: -6.06	 makespan: 599.50	 Mean_loss: 0.02550777,  training time: 2.10
progress:  42%|[34m     [0m| 42/100 [01:31<02:00,  2.08s/it]progress:  43%|[34m     [0m| 43/100 [01:31<01:58,  2.09s/it]                                                          Episode 44	 reward: -6.10	 makespan: 603.50	 Mean_loss: 0.02179449,  training time: 2.08
progress:  43%|[34m     [0m| 43/100 [01:33<01:58,  2.09s/it]progress:  44%|[34m     [0m| 44/100 [01:33<01:56,  2.09s/it]                                                          Episode 45	 reward: -6.02	 makespan: 596.25	 Mean_loss: 0.02894754,  training time: 2.05
progress:  44%|[34m     [0m| 44/100 [01:35<01:56,  2.09s/it]progress:  45%|[34m     [0m| 45/100 [01:35<01:54,  2.08s/it]                                                          Episode 46	 reward: -6.36	 makespan: 630.00	 Mean_loss: 0.05259325,  training time: 2.06
progress:  45%|[34m     [0m| 45/100 [01:37<01:54,  2.08s/it]progress:  46%|[34m     [0m| 46/100 [01:37<01:51,  2.07s/it]                                                          Episode 47	 reward: -5.93	 makespan: 587.00	 Mean_loss: 0.03238054,  training time: 2.07
progress:  46%|[34m     [0m| 46/100 [01:39<01:51,  2.07s/it]progress:  47%|[34m     [0m| 47/100 [01:39<01:49,  2.07s/it]                                                          Episode 48	 reward: -6.02	 makespan: 595.50	 Mean_loss: 0.03610934,  training time: 2.07
progress:  47%|[34m     [0m| 47/100 [01:41<01:49,  2.07s/it]progress:  48%|[34m     [0m| 48/100 [01:41<01:47,  2.07s/it]                                                          Episode 49	 reward: -6.13	 makespan: 607.25	 Mean_loss: 0.03928942,  training time: 2.08
progress:  48%|[34m     [0m| 48/100 [01:43<01:47,  2.07s/it]progress:  49%|[34m     [0m| 49/100 [01:43<01:45,  2.07s/it]                                                          Episode 50	 reward: -5.86	 makespan: 579.75	 Mean_loss: 0.01355691,  training time: 2.06
progress:  49%|[34m     [0m| 49/100 [01:45<01:45,  2.07s/it]progress:  50%|[34m     [0m| 50/100 [01:45<01:43,  2.07s/it]                                                          Episode 51	 reward: -6.02	 makespan: 595.75	 Mean_loss: 0.02472623,  training time: 2.07
progress:  50%|[34m     [0m| 50/100 [01:47<01:43,  2.07s/it]progress:  51%|[34m     [0m| 51/100 [01:47<01:41,  2.07s/it]                                                          Episode 52	 reward: -6.60	 makespan: 653.75	 Mean_loss: 0.05478390,  training time: 2.08
progress:  51%|[34m     [0m| 51/100 [01:49<01:41,  2.07s/it]progress:  52%|[34m    [0m| 52/100 [01:49<01:39,  2.07s/it]                                                          Episode 53	 reward: -6.25	 makespan: 618.75	 Mean_loss: 0.02862718,  training time: 2.05
progress:  52%|[34m    [0m| 52/100 [01:51<01:39,  2.07s/it]progress:  53%|[34m    [0m| 53/100 [01:51<01:37,  2.07s/it]                                                          Episode 54	 reward: -6.16	 makespan: 609.75	 Mean_loss: 0.04811231,  training time: 2.06
progress:  53%|[34m    [0m| 53/100 [01:53<01:37,  2.07s/it]progress:  54%|[34m    [0m| 54/100 [01:53<01:35,  2.07s/it]                                                          Episode 55	 reward: -6.20	 makespan: 614.25	 Mean_loss: 0.03541352,  training time: 2.05
progress:  54%|[34m    [0m| 54/100 [01:55<01:35,  2.07s/it]progress:  55%|[34m    [0m| 55/100 [01:55<01:32,  2.06s/it]                                                          Episode 56	 reward: -6.23	 makespan: 617.00	 Mean_loss: 0.02508175,  training time: 2.07
progress:  55%|[34m    [0m| 55/100 [01:57<01:32,  2.06s/it]progress:  56%|[34m    [0m| 56/100 [01:57<01:30,  2.06s/it]                                                          Episode 57	 reward: -6.60	 makespan: 653.75	 Mean_loss: 0.07580365,  training time: 2.07
progress:  56%|[34m    [0m| 56/100 [02:00<01:30,  2.06s/it]progress:  57%|[34m    [0m| 57/100 [02:00<01:28,  2.07s/it]                                                          Episode 58	 reward: -5.88	 makespan: 582.25	 Mean_loss: 0.03167421,  training time: 2.11
progress:  57%|[34m    [0m| 57/100 [02:02<01:28,  2.07s/it]progress:  58%|[34m    [0m| 58/100 [02:02<01:27,  2.08s/it]                                                          Episode 59	 reward: -6.56	 makespan: 649.00	 Mean_loss: 0.04315722,  training time: 2.07
progress:  58%|[34m    [0m| 58/100 [02:04<01:27,  2.08s/it]progress:  59%|[34m    [0m| 59/100 [02:04<01:25,  2.08s/it]                                                          Episode 60	 reward: -6.01	 makespan: 594.50	 Mean_loss: 0.03038304,  training time: 2.07
progress:  59%|[34m    [0m| 59/100 [02:06<01:25,  2.08s/it]progress:  60%|[34m    [0m| 60/100 [02:06<01:23,  2.08s/it]                                                          Episode 61	 reward: -5.79	 makespan: 573.00	 Mean_loss: 0.03312520,  training time: 2.07
progress:  60%|[34m    [0m| 60/100 [02:08<01:23,  2.08s/it]progress:  61%|[34m    [0m| 61/100 [02:08<01:20,  2.07s/it]                                                          Episode 62	 reward: -5.79	 makespan: 572.75	 Mean_loss: 0.01724444,  training time: 2.08
progress:  61%|[34m    [0m| 61/100 [02:10<01:20,  2.07s/it]progress:  62%|[34m   [0m| 62/100 [02:10<01:18,  2.08s/it]                                                          Episode 63	 reward: -5.81	 makespan: 575.00	 Mean_loss: 0.02763193,  training time: 2.07
progress:  62%|[34m   [0m| 62/100 [02:12<01:18,  2.08s/it]progress:  63%|[34m   [0m| 63/100 [02:12<01:16,  2.07s/it]                                                          Episode 64	 reward: -5.85	 makespan: 579.25	 Mean_loss: 0.02073809,  training time: 2.08
progress:  63%|[34m   [0m| 63/100 [02:14<01:16,  2.07s/it]progress:  64%|[34m   [0m| 64/100 [02:14<01:14,  2.07s/it]                                                          Episode 65	 reward: -6.10	 makespan: 603.75	 Mean_loss: 0.03088959,  training time: 2.07
progress:  64%|[34m   [0m| 64/100 [02:16<01:14,  2.07s/it]progress:  65%|[34m   [0m| 65/100 [02:16<01:12,  2.07s/it]                                                          Episode 66	 reward: -6.03	 makespan: 597.00	 Mean_loss: 0.02540932,  training time: 2.06
progress:  65%|[34m   [0m| 65/100 [02:18<01:12,  2.07s/it]progress:  66%|[34m   [0m| 66/100 [02:18<01:10,  2.07s/it]                                                          Episode 67	 reward: -6.37	 makespan: 630.75	 Mean_loss: 0.05061789,  training time: 2.07
progress:  66%|[34m   [0m| 66/100 [02:20<01:10,  2.07s/it]progress:  67%|[34m   [0m| 67/100 [02:20<01:08,  2.07s/it]                                                          Episode 68	 reward: -6.31	 makespan: 624.50	 Mean_loss: 0.02600113,  training time: 2.07
progress:  67%|[34m   [0m| 67/100 [02:22<01:08,  2.07s/it]progress:  68%|[34m   [0m| 68/100 [02:22<01:06,  2.07s/it]                                                          Episode 69	 reward: -6.16	 makespan: 609.50	 Mean_loss: 0.02185418,  training time: 2.07
progress:  68%|[34m   [0m| 68/100 [02:24<01:06,  2.07s/it]progress:  69%|[34m   [0m| 69/100 [02:24<01:04,  2.07s/it]                                                          Episode 70	 reward: -6.11	 makespan: 605.00	 Mean_loss: 0.03180024,  training time: 2.07
progress:  69%|[34m   [0m| 69/100 [02:26<01:04,  2.07s/it]progress:  70%|[34m   [0m| 70/100 [02:27<01:02,  2.07s/it]                                                          Episode 71	 reward: -5.86	 makespan: 579.75	 Mean_loss: 0.02757676,  training time: 2.07
progress:  70%|[34m   [0m| 70/100 [02:29<01:02,  2.07s/it]progress:  71%|[34m   [0m| 71/100 [02:29<01:00,  2.07s/it]                                                          Episode 72	 reward: -5.93	 makespan: 587.50	 Mean_loss: 0.02295267,  training time: 2.13
progress:  71%|[34m   [0m| 71/100 [02:31<01:00,  2.07s/it]progress:  72%|[34m  [0m| 72/100 [02:31<00:58,  2.09s/it]                                                          Episode 73	 reward: -5.70	 makespan: 564.00	 Mean_loss: 0.01915886,  training time: 2.06
progress:  72%|[34m  [0m| 72/100 [02:33<00:58,  2.09s/it]progress:  73%|[34m  [0m| 73/100 [02:33<00:56,  2.08s/it]                                                          Episode 74	 reward: -6.02	 makespan: 595.75	 Mean_loss: 0.03805216,  training time: 2.07
progress:  73%|[34m  [0m| 73/100 [02:35<00:56,  2.08s/it]progress:  74%|[34m  [0m| 74/100 [02:35<00:53,  2.08s/it]                                                          Episode 75	 reward: -5.69	 makespan: 563.75	 Mean_loss: 0.02084219,  training time: 2.16
progress:  74%|[34m  [0m| 74/100 [02:37<00:53,  2.08s/it]progress:  75%|[34m  [0m| 75/100 [02:37<00:52,  2.10s/it]                                                          Episode 76	 reward: -5.72	 makespan: 566.75	 Mean_loss: 0.02483038,  training time: 2.06
progress:  75%|[34m  [0m| 75/100 [02:39<00:52,  2.10s/it]progress:  76%|[34m  [0m| 76/100 [02:39<00:50,  2.09s/it]                                                          Episode 77	 reward: -5.94	 makespan: 587.75	 Mean_loss: 0.04149530,  training time: 2.06
progress:  76%|[34m  [0m| 76/100 [02:41<00:50,  2.09s/it]progress:  77%|[34m  [0m| 77/100 [02:41<00:47,  2.08s/it]                                                          Episode 78	 reward: -5.86	 makespan: 580.25	 Mean_loss: 0.01799863,  training time: 2.06
progress:  77%|[34m  [0m| 77/100 [02:43<00:47,  2.08s/it]progress:  78%|[34m  [0m| 78/100 [02:43<00:45,  2.07s/it]                                                          Episode 79	 reward: -5.67	 makespan: 561.00	 Mean_loss: 0.01662443,  training time: 2.05
progress:  78%|[34m  [0m| 78/100 [02:45<00:45,  2.07s/it]progress:  79%|[34m  [0m| 79/100 [02:45<00:43,  2.07s/it]                                                          Episode 80	 reward: -5.62	 makespan: 556.00	 Mean_loss: 0.01485369,  training time: 2.04
progress:  79%|[34m  [0m| 79/100 [02:47<00:43,  2.07s/it]progress:  80%|[34m  [0m| 80/100 [02:47<00:41,  2.06s/it]                                                          Episode 81	 reward: -5.78	 makespan: 572.25	 Mean_loss: 0.02130772,  training time: 2.10
progress:  80%|[34m  [0m| 80/100 [02:49<00:41,  2.06s/it]progress:  81%|[34m  [0m| 81/100 [02:49<00:39,  2.07s/it]                                                          Episode 82	 reward: -5.67	 makespan: 561.50	 Mean_loss: 0.02276632,  training time: 2.07
progress:  81%|[34m  [0m| 81/100 [02:51<00:39,  2.07s/it]progress:  82%|[34m [0m| 82/100 [02:51<00:37,  2.07s/it]                                                          Episode 83	 reward: -5.55	 makespan: 549.75	 Mean_loss: 0.01765263,  training time: 2.07
progress:  82%|[34m [0m| 82/100 [02:53<00:37,  2.07s/it]progress:  83%|[34m [0m| 83/100 [02:54<00:35,  2.07s/it]                                                          Episode 84	 reward: -6.02	 makespan: 595.50	 Mean_loss: 0.03033191,  training time: 2.07
progress:  83%|[34m [0m| 83/100 [02:56<00:35,  2.07s/it]progress:  84%|[34m [0m| 84/100 [02:56<00:33,  2.07s/it]                                                          Episode 85	 reward: -5.69	 makespan: 563.25	 Mean_loss: 0.03065475,  training time: 2.07
progress:  84%|[34m [0m| 84/100 [02:58<00:33,  2.07s/it]progress:  85%|[34m [0m| 85/100 [02:58<00:31,  2.07s/it]                                                          Episode 86	 reward: -5.73	 makespan: 567.00	 Mean_loss: 0.02398111,  training time: 2.08
progress:  85%|[34m [0m| 85/100 [03:00<00:31,  2.07s/it]progress:  86%|[34m [0m| 86/100 [03:00<00:29,  2.07s/it]                                                          Episode 87	 reward: -5.46	 makespan: 541.00	 Mean_loss: 0.01366531,  training time: 2.06
progress:  86%|[34m [0m| 86/100 [03:02<00:29,  2.07s/it]progress:  87%|[34m [0m| 87/100 [03:02<00:26,  2.07s/it]                                                          Episode 88	 reward: -5.58	 makespan: 552.25	 Mean_loss: 0.01960099,  training time: 2.07
progress:  87%|[34m [0m| 87/100 [03:04<00:26,  2.07s/it]progress:  88%|[34m [0m| 88/100 [03:04<00:24,  2.07s/it]                                                          Episode 89	 reward: -5.91	 makespan: 585.00	 Mean_loss: 0.02953634,  training time: 2.07
progress:  88%|[34m [0m| 88/100 [03:06<00:24,  2.07s/it]progress:  89%|[34m [0m| 89/100 [03:06<00:22,  2.07s/it]                                                          Episode 90	 reward: -5.37	 makespan: 531.75	 Mean_loss: 0.02110293,  training time: 2.06
progress:  89%|[34m [0m| 89/100 [03:08<00:22,  2.07s/it]progress:  90%|[34m [0m| 90/100 [03:08<00:20,  2.07s/it]                                                          Episode 91	 reward: -5.60	 makespan: 554.75	 Mean_loss: 0.01202987,  training time: 2.06
progress:  90%|[34m [0m| 90/100 [03:10<00:20,  2.07s/it]progress:  91%|[34m [0m| 91/100 [03:10<00:18,  2.06s/it]                                                          Episode 92	 reward: -5.51	 makespan: 545.00	 Mean_loss: 0.00932031,  training time: 2.07
progress:  91%|[34m [0m| 91/100 [03:12<00:18,  2.06s/it]progress:  92%|[34m[0m| 92/100 [03:12<00:16,  2.07s/it]                                                          Episode 93	 reward: -5.71	 makespan: 565.00	 Mean_loss: 0.00643015,  training time: 2.07
progress:  92%|[34m[0m| 92/100 [03:14<00:16,  2.07s/it]progress:  93%|[34m[0m| 93/100 [03:14<00:14,  2.07s/it]                                                          Episode 94	 reward: -5.70	 makespan: 564.25	 Mean_loss: 0.00812043,  training time: 2.13
progress:  93%|[34m[0m| 93/100 [03:16<00:14,  2.07s/it]progress:  94%|[34m[0m| 94/100 [03:16<00:12,  2.09s/it]                                                          Episode 95	 reward: -5.89	 makespan: 583.00	 Mean_loss: 0.01683142,  training time: 2.06
progress:  94%|[34m[0m| 94/100 [03:18<00:12,  2.09s/it]progress:  95%|[34m[0m| 95/100 [03:18<00:10,  2.08s/it]                                                          Episode 96	 reward: -5.67	 makespan: 561.75	 Mean_loss: 0.01688549,  training time: 2.09
progress:  95%|[34m[0m| 95/100 [03:20<00:10,  2.08s/it]progress:  96%|[34m[0m| 96/100 [03:20<00:08,  2.08s/it]                                                          Episode 97	 reward: -5.54	 makespan: 548.00	 Mean_loss: 0.01826223,  training time: 2.05
progress:  96%|[34m[0m| 96/100 [03:23<00:08,  2.08s/it]progress:  97%|[34m[0m| 97/100 [03:23<00:06,  2.07s/it]                                                          Episode 98	 reward: -5.68	 makespan: 562.75	 Mean_loss: 0.01674108,  training time: 2.08
progress:  97%|[34m[0m| 97/100 [03:25<00:06,  2.07s/it]progress:  98%|[34m[0m| 98/100 [03:25<00:04,  2.08s/it]                                                          Episode 99	 reward: -5.80	 makespan: 574.00	 Mean_loss: 0.00960649,  training time: 2.20
progress:  98%|[34m[0m| 98/100 [03:27<00:04,  2.08s/it]progress:  99%|[34m[0m| 99/100 [03:27<00:02,  2.11s/it]                                                          Episode 100	 reward: -5.62	 makespan: 556.75	 Mean_loss: 0.02185797,  training time: 2.16
progress:  99%|[34m[0m| 99/100 [03:29<00:02,  2.11s/it]progress: 100%|[34m[0m| 100/100 [03:29<00:00,  2.13s/it]progress: 100%|[34m[0m| 100/100 [03:29<00:00,  2.09s/it]
+ IFS=,
+ read n_j n_m
+ python train/DAN_finetuning.py --logdir ./runs/exp14/transfer_maml+exp14_1000_64_3_15x10 --model_suffix exp11_maml+exp14_1000_64_3_15x10 --finetuning_model maml+exp14_1000_64_3 --max_updates 100 --n_j 15 --n_m 10 --num_envs 4 --hidden_dim_actor 64 --hidden_dim_critic 64 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x10+mix
save model name:  15x10+mix+exp11_maml+exp14_1000_64_3_15x10
./trained_network/SD2/maml+exp14_1000_64_3.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/maml+exp14_1000_64_3.pth
vali data :./data/data_train_vali/SD2/15x10+mix


progress:   0%|[34m          [0m| 0/100 [00:00<?, ?it/s]                                                 Episode 1	 reward: -7.61	 makespan: 753.25	 Mean_loss: 0.19736624,  training time: 3.47
progress:   0%|[34m          [0m| 0/100 [00:03<?, ?it/s]progress:   1%|[34m          [0m| 1/100 [00:03<05:43,  3.47s/it]                                                         Episode 2	 reward: -7.91	 makespan: 783.25	 Mean_loss: 0.21011290,  training time: 2.36
progress:   1%|[34m          [0m| 1/100 [00:05<05:43,  3.47s/it]progress:   2%|[34m         [0m| 2/100 [00:05<04:36,  2.82s/it]                                                         Episode 3	 reward: -7.49	 makespan: 741.25	 Mean_loss: 0.30375254,  training time: 2.41
progress:   2%|[34m         [0m| 2/100 [00:08<04:36,  2.82s/it]progress:   3%|[34m         [0m| 3/100 [00:08<04:15,  2.63s/it]                                                         Episode 4	 reward: -7.67	 makespan: 759.50	 Mean_loss: 0.25219539,  training time: 2.37
progress:   3%|[34m         [0m| 3/100 [00:10<04:15,  2.63s/it]progress:   4%|[34m         [0m| 4/100 [00:10<04:02,  2.53s/it]                                                         Episode 5	 reward: -7.44	 makespan: 736.25	 Mean_loss: 0.23476960,  training time: 2.39
progress:   4%|[34m         [0m| 4/100 [00:13<04:02,  2.53s/it]progress:   5%|[34m         [0m| 5/100 [00:13<03:55,  2.48s/it]                                                         Episode 6	 reward: -7.23	 makespan: 716.00	 Mean_loss: 0.27308729,  training time: 2.53
progress:   5%|[34m         [0m| 5/100 [00:15<03:55,  2.48s/it]progress:   6%|[34m         [0m| 6/100 [00:15<03:54,  2.50s/it]                                                         Episode 7	 reward: -7.23	 makespan: 715.75	 Mean_loss: 0.21608099,  training time: 2.38
progress:   6%|[34m         [0m| 6/100 [00:17<03:54,  2.50s/it]progress:   7%|[34m         [0m| 7/100 [00:17<03:48,  2.46s/it]                                                         Episode 8	 reward: -6.84	 makespan: 677.00	 Mean_loss: 0.17721988,  training time: 2.33
progress:   7%|[34m         [0m| 7/100 [00:20<03:48,  2.46s/it]progress:   8%|[34m         [0m| 8/100 [00:20<03:42,  2.42s/it]                                                         Episode 9	 reward: -7.03	 makespan: 696.00	 Mean_loss: 0.09870032,  training time: 2.34
progress:   8%|[34m         [0m| 8/100 [00:22<03:42,  2.42s/it]progress:   9%|[34m         [0m| 9/100 [00:22<03:38,  2.40s/it]                                                         Episode 10	 reward: -7.07	 makespan: 699.75	 Mean_loss: 0.05364032,  training time: 2.46
progress:   9%|[34m         [0m| 9/100 [00:25<03:38,  2.40s/it]progress:  10%|[34m         [0m| 10/100 [00:25<03:37,  2.42s/it]                                                          Episode 11	 reward: -6.93	 makespan: 685.75	 Mean_loss: 0.05926755,  training time: 2.40
progress:  10%|[34m         [0m| 10/100 [00:27<03:37,  2.42s/it]progress:  11%|[34m         [0m| 11/100 [00:27<03:34,  2.41s/it]                                                          Episode 12	 reward: -6.64	 makespan: 657.75	 Mean_loss: 0.04640204,  training time: 2.39
progress:  11%|[34m         [0m| 11/100 [00:29<03:34,  2.41s/it]progress:  12%|[34m        [0m| 12/100 [00:29<03:31,  2.40s/it]                                                          Episode 13	 reward: -6.64	 makespan: 657.00	 Mean_loss: 0.07935949,  training time: 2.39
progress:  12%|[34m        [0m| 12/100 [00:32<03:31,  2.40s/it]progress:  13%|[34m        [0m| 13/100 [00:32<03:28,  2.40s/it]                                                          Episode 14	 reward: -6.87	 makespan: 680.50	 Mean_loss: 0.07695837,  training time: 2.39
progress:  13%|[34m        [0m| 13/100 [00:34<03:28,  2.40s/it]progress:  14%|[34m        [0m| 14/100 [00:34<03:26,  2.40s/it]                                                          Episode 15	 reward: -6.76	 makespan: 669.50	 Mean_loss: 0.06210501,  training time: 2.38
progress:  14%|[34m        [0m| 14/100 [00:37<03:26,  2.40s/it]progress:  15%|[34m        [0m| 15/100 [00:37<03:23,  2.39s/it]                                                          Episode 16	 reward: -6.89	 makespan: 682.00	 Mean_loss: 0.06098919,  training time: 2.39
progress:  15%|[34m        [0m| 15/100 [00:39<03:23,  2.39s/it]progress:  16%|[34m        [0m| 16/100 [00:39<03:21,  2.39s/it]                                                          Episode 17	 reward: -7.16	 makespan: 708.75	 Mean_loss: 0.06326756,  training time: 2.37
progress:  16%|[34m        [0m| 16/100 [00:41<03:21,  2.39s/it]progress:  17%|[34m        [0m| 17/100 [00:41<03:18,  2.39s/it]                                                          Episode 18	 reward: -6.76	 makespan: 669.25	 Mean_loss: 0.07459494,  training time: 2.38
progress:  17%|[34m        [0m| 17/100 [00:44<03:18,  2.39s/it]progress:  18%|[34m        [0m| 18/100 [00:44<03:15,  2.38s/it]                                                          Episode 19	 reward: -6.85	 makespan: 678.25	 Mean_loss: 0.07291569,  training time: 2.40
progress:  18%|[34m        [0m| 18/100 [00:46<03:15,  2.38s/it]progress:  19%|[34m        [0m| 19/100 [00:46<03:13,  2.39s/it]                                                          Episode 20	 reward: -6.76	 makespan: 668.75	 Mean_loss: 0.05853337,  training time: 2.33
progress:  19%|[34m        [0m| 19/100 [00:48<03:13,  2.39s/it]progress:  20%|[34m        [0m| 20/100 [00:48<03:09,  2.37s/it]                                                          Episode 21	 reward: -7.13	 makespan: 706.25	 Mean_loss: 0.03947324,  training time: 2.34
progress:  20%|[34m        [0m| 20/100 [00:51<03:09,  2.37s/it]progress:  21%|[34m        [0m| 21/100 [00:51<03:06,  2.36s/it]                                                          Episode 22	 reward: -6.86	 makespan: 679.25	 Mean_loss: 0.05738189,  training time: 2.36
progress:  21%|[34m        [0m| 21/100 [00:53<03:06,  2.36s/it]progress:  22%|[34m       [0m| 22/100 [00:53<03:04,  2.36s/it]                                                          Episode 23	 reward: -6.87	 makespan: 680.25	 Mean_loss: 0.05395084,  training time: 2.34
progress:  22%|[34m       [0m| 22/100 [00:55<03:04,  2.36s/it]progress:  23%|[34m       [0m| 23/100 [00:55<03:01,  2.36s/it]                                                          Episode 24	 reward: -6.92	 makespan: 684.75	 Mean_loss: 0.06818700,  training time: 2.31
progress:  23%|[34m       [0m| 23/100 [00:58<03:01,  2.36s/it]progress:  24%|[34m       [0m| 24/100 [00:58<02:58,  2.34s/it]                                                          Episode 25	 reward: -7.20	 makespan: 712.50	 Mean_loss: 0.05860292,  training time: 2.46
progress:  24%|[34m       [0m| 24/100 [01:00<02:58,  2.34s/it]progress:  25%|[34m       [0m| 25/100 [01:00<02:58,  2.38s/it]                                                          Episode 26	 reward: -7.03	 makespan: 696.25	 Mean_loss: 0.07895431,  training time: 2.35
progress:  25%|[34m       [0m| 25/100 [01:03<02:58,  2.38s/it]progress:  26%|[34m       [0m| 26/100 [01:03<02:55,  2.37s/it]                                                          Episode 27	 reward: -7.12	 makespan: 704.50	 Mean_loss: 0.10064298,  training time: 2.34
progress:  26%|[34m       [0m| 26/100 [01:05<02:55,  2.37s/it]progress:  27%|[34m       [0m| 27/100 [01:05<02:52,  2.36s/it]                                                          Episode 28	 reward: -6.82	 makespan: 674.75	 Mean_loss: 0.02944740,  training time: 2.34
progress:  27%|[34m       [0m| 27/100 [01:07<02:52,  2.36s/it]progress:  28%|[34m       [0m| 28/100 [01:07<02:49,  2.36s/it]                                                          Episode 29	 reward: -7.33	 makespan: 725.25	 Mean_loss: 0.13088065,  training time: 2.35
progress:  28%|[34m       [0m| 28/100 [01:10<02:49,  2.36s/it]progress:  29%|[34m       [0m| 29/100 [01:10<02:47,  2.35s/it]                                                          Episode 30	 reward: -6.58	 makespan: 651.00	 Mean_loss: 0.06281720,  training time: 2.34
progress:  29%|[34m       [0m| 29/100 [01:12<02:47,  2.35s/it]progress:  30%|[34m       [0m| 30/100 [01:12<02:44,  2.35s/it]                                                          Episode 31	 reward: -7.03	 makespan: 696.25	 Mean_loss: 0.07651120,  training time: 2.32
progress:  30%|[34m       [0m| 30/100 [01:14<02:44,  2.35s/it]progress:  31%|[34m       [0m| 31/100 [01:14<02:41,  2.34s/it]                                                          Episode 32	 reward: -6.70	 makespan: 663.75	 Mean_loss: 0.04465978,  training time: 2.34
progress:  31%|[34m       [0m| 31/100 [01:17<02:41,  2.34s/it]progress:  32%|[34m      [0m| 32/100 [01:17<02:39,  2.34s/it]                                                          Episode 33	 reward: -6.77	 makespan: 669.75	 Mean_loss: 0.05821000,  training time: 2.32
progress:  32%|[34m      [0m| 32/100 [01:19<02:39,  2.34s/it]progress:  33%|[34m      [0m| 33/100 [01:19<02:36,  2.34s/it]                                                          Episode 34	 reward: -6.75	 makespan: 668.25	 Mean_loss: 0.04002287,  training time: 2.35
progress:  33%|[34m      [0m| 33/100 [01:21<02:36,  2.34s/it]progress:  34%|[34m      [0m| 34/100 [01:21<02:34,  2.34s/it]                                                          Episode 35	 reward: -6.54	 makespan: 647.00	 Mean_loss: 0.03098229,  training time: 2.35
progress:  34%|[34m      [0m| 34/100 [01:24<02:34,  2.34s/it]progress:  35%|[34m      [0m| 35/100 [01:24<02:32,  2.34s/it]                                                          Episode 36	 reward: -6.67	 makespan: 660.75	 Mean_loss: 0.05195311,  training time: 2.35
progress:  35%|[34m      [0m| 35/100 [01:26<02:32,  2.34s/it]progress:  36%|[34m      [0m| 36/100 [01:26<02:30,  2.35s/it]                                                          Episode 37	 reward: -6.97	 makespan: 690.00	 Mean_loss: 0.04035973,  training time: 2.35
progress:  36%|[34m      [0m| 36/100 [01:28<02:30,  2.35s/it]progress:  37%|[34m      [0m| 37/100 [01:28<02:27,  2.35s/it]                                                          Episode 38	 reward: -7.03	 makespan: 696.25	 Mean_loss: 0.04130467,  training time: 2.34
progress:  37%|[34m      [0m| 37/100 [01:31<02:27,  2.35s/it]progress:  38%|[34m      [0m| 38/100 [01:31<02:25,  2.34s/it]                                                          Episode 39	 reward: -6.74	 makespan: 667.00	 Mean_loss: 0.03816521,  training time: 2.34
progress:  38%|[34m      [0m| 38/100 [01:33<02:25,  2.34s/it]progress:  39%|[34m      [0m| 39/100 [01:33<02:23,  2.34s/it]                                                          Episode 40	 reward: -6.59	 makespan: 652.00	 Mean_loss: 0.04559391,  training time: 2.34
progress:  39%|[34m      [0m| 39/100 [01:35<02:23,  2.34s/it]progress:  40%|[34m      [0m| 40/100 [01:35<02:20,  2.34s/it]                                                          Episode 41	 reward: -7.46	 makespan: 738.75	 Mean_loss: 0.06413212,  training time: 2.36
progress:  40%|[34m      [0m| 40/100 [01:38<02:20,  2.34s/it]progress:  41%|[34m      [0m| 41/100 [01:38<02:18,  2.35s/it]                                                          Episode 42	 reward: -6.81	 makespan: 673.75	 Mean_loss: 0.07620418,  training time: 2.35
progress:  41%|[34m      [0m| 41/100 [01:40<02:18,  2.35s/it]progress:  42%|[34m     [0m| 42/100 [01:40<02:16,  2.35s/it]                                                          Episode 43	 reward: -7.24	 makespan: 716.75	 Mean_loss: 0.05278452,  training time: 2.32
progress:  42%|[34m     [0m| 42/100 [01:42<02:16,  2.35s/it]progress:  43%|[34m     [0m| 43/100 [01:42<02:13,  2.34s/it]                                                          Episode 44	 reward: -6.92	 makespan: 684.75	 Mean_loss: 0.06136547,  training time: 2.34
progress:  43%|[34m     [0m| 43/100 [01:45<02:13,  2.34s/it]progress:  44%|[34m     [0m| 44/100 [01:45<02:11,  2.34s/it]                                                          Episode 45	 reward: -6.83	 makespan: 676.50	 Mean_loss: 0.07407212,  training time: 2.33
progress:  44%|[34m     [0m| 44/100 [01:47<02:11,  2.34s/it]progress:  45%|[34m     [0m| 45/100 [01:47<02:08,  2.34s/it]                                                          Episode 46	 reward: -6.41	 makespan: 634.25	 Mean_loss: 0.04588988,  training time: 2.33
progress:  45%|[34m     [0m| 45/100 [01:49<02:08,  2.34s/it]progress:  46%|[34m     [0m| 46/100 [01:49<02:06,  2.33s/it]                                                          Episode 47	 reward: -6.57	 makespan: 650.50	 Mean_loss: 0.02827936,  training time: 2.32
progress:  46%|[34m     [0m| 46/100 [01:52<02:06,  2.33s/it]progress:  47%|[34m     [0m| 47/100 [01:52<02:03,  2.33s/it]                                                          Episode 48	 reward: -6.84	 makespan: 677.50	 Mean_loss: 0.04659181,  training time: 2.33
progress:  47%|[34m     [0m| 47/100 [01:54<02:03,  2.33s/it]progress:  48%|[34m     [0m| 48/100 [01:54<02:01,  2.33s/it]                                                          Episode 49	 reward: -6.87	 makespan: 680.50	 Mean_loss: 0.03160649,  training time: 2.33
progress:  48%|[34m     [0m| 48/100 [01:56<02:01,  2.33s/it]progress:  49%|[34m     [0m| 49/100 [01:56<01:58,  2.33s/it]                                                          Episode 50	 reward: -6.64	 makespan: 657.00	 Mean_loss: 0.03244390,  training time: 2.39
progress:  49%|[34m     [0m| 49/100 [01:59<01:58,  2.33s/it]progress:  50%|[34m     [0m| 50/100 [01:59<01:57,  2.35s/it]                                                          Episode 51	 reward: -6.60	 makespan: 653.50	 Mean_loss: 0.02746008,  training time: 2.33
progress:  50%|[34m     [0m| 50/100 [02:01<01:57,  2.35s/it]progress:  51%|[34m     [0m| 51/100 [02:01<01:54,  2.34s/it]                                                          Episode 52	 reward: -6.46	 makespan: 639.50	 Mean_loss: 0.02095504,  training time: 2.34
progress:  51%|[34m     [0m| 51/100 [02:03<01:54,  2.34s/it]progress:  52%|[34m    [0m| 52/100 [02:03<01:52,  2.34s/it]                                                          Episode 53	 reward: -6.61	 makespan: 654.00	 Mean_loss: 0.03128760,  training time: 2.33
progress:  52%|[34m    [0m| 52/100 [02:06<01:52,  2.34s/it]progress:  53%|[34m    [0m| 53/100 [02:06<01:50,  2.34s/it]                                                          Episode 54	 reward: -6.60	 makespan: 653.75	 Mean_loss: 0.02229737,  training time: 2.34
progress:  53%|[34m    [0m| 53/100 [02:08<01:50,  2.34s/it]progress:  54%|[34m    [0m| 54/100 [02:08<01:47,  2.34s/it]                                                          Episode 55	 reward: -6.61	 makespan: 654.00	 Mean_loss: 0.03138934,  training time: 2.44
progress:  54%|[34m    [0m| 54/100 [02:11<01:47,  2.34s/it]progress:  55%|[34m    [0m| 55/100 [02:11<01:46,  2.37s/it]                                                          Episode 56	 reward: -6.77	 makespan: 670.25	 Mean_loss: 0.02688931,  training time: 2.33
progress:  55%|[34m    [0m| 55/100 [02:13<01:46,  2.37s/it]progress:  56%|[34m    [0m| 56/100 [02:13<01:43,  2.36s/it]                                                          Episode 57	 reward: -6.33	 makespan: 627.00	 Mean_loss: 0.03004954,  training time: 2.33
progress:  56%|[34m    [0m| 56/100 [02:15<01:43,  2.36s/it]progress:  57%|[34m    [0m| 57/100 [02:15<01:41,  2.35s/it]                                                          Episode 58	 reward: -6.39	 makespan: 633.00	 Mean_loss: 0.01913512,  training time: 2.35
progress:  57%|[34m    [0m| 57/100 [02:18<01:41,  2.35s/it]progress:  58%|[34m    [0m| 58/100 [02:18<01:38,  2.35s/it]                                                          Episode 59	 reward: -6.46	 makespan: 639.75	 Mean_loss: 0.01785657,  training time: 2.33
progress:  58%|[34m    [0m| 58/100 [02:20<01:38,  2.35s/it]progress:  59%|[34m    [0m| 59/100 [02:20<01:36,  2.34s/it]                                                          Episode 60	 reward: -6.57	 makespan: 650.25	 Mean_loss: 0.02066306,  training time: 2.35
progress:  59%|[34m    [0m| 59/100 [02:22<01:36,  2.34s/it]progress:  60%|[34m    [0m| 60/100 [02:22<01:33,  2.35s/it]                                                          Episode 61	 reward: -6.70	 makespan: 663.75	 Mean_loss: 0.04598653,  training time: 2.37
progress:  60%|[34m    [0m| 60/100 [02:25<01:33,  2.35s/it]progress:  61%|[34m    [0m| 61/100 [02:25<01:31,  2.35s/it]                                                          Episode 62	 reward: -6.69	 makespan: 662.00	 Mean_loss: 0.02709163,  training time: 2.33
progress:  61%|[34m    [0m| 61/100 [02:27<01:31,  2.35s/it]progress:  62%|[34m   [0m| 62/100 [02:27<01:29,  2.35s/it]                                                          Episode 63	 reward: -6.59	 makespan: 652.00	 Mean_loss: 0.01077269,  training time: 2.32
progress:  62%|[34m   [0m| 62/100 [02:29<01:29,  2.35s/it]progress:  63%|[34m   [0m| 63/100 [02:29<01:26,  2.34s/it]                                                          Episode 64	 reward: -6.61	 makespan: 654.50	 Mean_loss: 0.02243441,  training time: 2.33
progress:  63%|[34m   [0m| 63/100 [02:32<01:26,  2.34s/it]progress:  64%|[34m   [0m| 64/100 [02:32<01:24,  2.34s/it]                                                          Episode 65	 reward: -6.67	 makespan: 660.50	 Mean_loss: 0.03047086,  training time: 2.32
progress:  64%|[34m   [0m| 64/100 [02:34<01:24,  2.34s/it]progress:  65%|[34m   [0m| 65/100 [02:34<01:21,  2.33s/it]                                                          Episode 66	 reward: -6.52	 makespan: 645.75	 Mean_loss: 0.01156990,  training time: 2.32
progress:  65%|[34m   [0m| 65/100 [02:36<01:21,  2.33s/it]progress:  66%|[34m   [0m| 66/100 [02:36<01:19,  2.33s/it]                                                          Episode 67	 reward: -6.41	 makespan: 635.00	 Mean_loss: 0.02073592,  training time: 2.33
progress:  66%|[34m   [0m| 66/100 [02:39<01:19,  2.33s/it]progress:  67%|[34m   [0m| 67/100 [02:39<01:16,  2.33s/it]                                                          Episode 68	 reward: -6.62	 makespan: 655.25	 Mean_loss: 0.01889638,  training time: 2.33
progress:  67%|[34m   [0m| 67/100 [02:41<01:16,  2.33s/it]progress:  68%|[34m   [0m| 68/100 [02:41<01:14,  2.33s/it]                                                          Episode 69	 reward: -6.46	 makespan: 639.75	 Mean_loss: 0.02372239,  training time: 2.32
progress:  68%|[34m   [0m| 68/100 [02:43<01:14,  2.33s/it]progress:  69%|[34m   [0m| 69/100 [02:43<01:12,  2.33s/it]                                                          Episode 70	 reward: -6.41	 makespan: 635.00	 Mean_loss: 0.02596376,  training time: 2.34
progress:  69%|[34m   [0m| 69/100 [02:46<01:12,  2.33s/it]progress:  70%|[34m   [0m| 70/100 [02:46<01:09,  2.33s/it]                                                          Episode 71	 reward: -6.51	 makespan: 644.50	 Mean_loss: 0.01044930,  training time: 2.32
progress:  70%|[34m   [0m| 70/100 [02:48<01:09,  2.33s/it]progress:  71%|[34m   [0m| 71/100 [02:48<01:07,  2.33s/it]                                                          Episode 72	 reward: -6.62	 makespan: 655.75	 Mean_loss: 0.02512690,  training time: 2.31
progress:  71%|[34m   [0m| 71/100 [02:50<01:07,  2.33s/it]progress:  72%|[34m  [0m| 72/100 [02:50<01:05,  2.32s/it]                                                          Episode 73	 reward: -6.61	 makespan: 654.25	 Mean_loss: 0.03370568,  training time: 2.32
progress:  72%|[34m  [0m| 72/100 [02:53<01:05,  2.32s/it]progress:  73%|[34m  [0m| 73/100 [02:53<01:02,  2.32s/it]                                                          Episode 74	 reward: -6.51	 makespan: 644.00	 Mean_loss: 0.03346331,  training time: 2.32
progress:  73%|[34m  [0m| 73/100 [02:55<01:02,  2.32s/it]progress:  74%|[34m  [0m| 74/100 [02:55<01:00,  2.32s/it]                                                          Episode 75	 reward: -6.87	 makespan: 679.75	 Mean_loss: 0.03336180,  training time: 2.31
progress:  74%|[34m  [0m| 74/100 [02:57<01:00,  2.32s/it]progress:  75%|[34m  [0m| 75/100 [02:57<00:58,  2.32s/it]                                                          Episode 76	 reward: -6.63	 makespan: 656.00	 Mean_loss: 0.03490069,  training time: 2.32
progress:  75%|[34m  [0m| 75/100 [02:59<00:58,  2.32s/it]progress:  76%|[34m  [0m| 76/100 [02:59<00:55,  2.32s/it]                                                          Episode 77	 reward: -6.76	 makespan: 669.25	 Mean_loss: 0.05318285,  training time: 2.34
progress:  76%|[34m  [0m| 76/100 [03:02<00:55,  2.32s/it]progress:  77%|[34m  [0m| 77/100 [03:02<00:53,  2.33s/it]                                                          Episode 78	 reward: -6.50	 makespan: 643.25	 Mean_loss: 0.04828521,  training time: 2.32
progress:  77%|[34m  [0m| 77/100 [03:04<00:53,  2.33s/it]progress:  78%|[34m  [0m| 78/100 [03:04<00:51,  2.33s/it]                                                          Episode 79	 reward: -6.59	 makespan: 652.00	 Mean_loss: 0.02485536,  training time: 2.33
progress:  78%|[34m  [0m| 78/100 [03:06<00:51,  2.33s/it]progress:  79%|[34m  [0m| 79/100 [03:06<00:48,  2.33s/it]                                                          Episode 80	 reward: -6.55	 makespan: 648.75	 Mean_loss: 0.02502504,  training time: 2.33
progress:  79%|[34m  [0m| 79/100 [03:09<00:48,  2.33s/it]progress:  80%|[34m  [0m| 80/100 [03:09<00:46,  2.33s/it]                                                          Episode 81	 reward: -6.83	 makespan: 676.00	 Mean_loss: 0.02805795,  training time: 2.32
progress:  80%|[34m  [0m| 80/100 [03:11<00:46,  2.33s/it]progress:  81%|[34m  [0m| 81/100 [03:11<00:44,  2.33s/it]                                                          Episode 82	 reward: -6.46	 makespan: 639.75	 Mean_loss: 0.04054174,  training time: 2.33
progress:  81%|[34m  [0m| 81/100 [03:13<00:44,  2.33s/it]progress:  82%|[34m [0m| 82/100 [03:13<00:41,  2.33s/it]                                                          Episode 83	 reward: -6.67	 makespan: 660.75	 Mean_loss: 0.02202433,  training time: 2.40
progress:  82%|[34m [0m| 82/100 [03:16<00:41,  2.33s/it]progress:  83%|[34m [0m| 83/100 [03:16<00:39,  2.35s/it]                                                          Episode 84	 reward: -6.73	 makespan: 666.00	 Mean_loss: 0.01883944,  training time: 2.30
progress:  83%|[34m [0m| 83/100 [03:18<00:39,  2.35s/it]progress:  84%|[34m [0m| 84/100 [03:18<00:37,  2.33s/it]                                                          Episode 85	 reward: -6.77	 makespan: 670.25	 Mean_loss: 0.02337352,  training time: 2.30
progress:  84%|[34m [0m| 84/100 [03:20<00:37,  2.33s/it]progress:  85%|[34m [0m| 85/100 [03:20<00:34,  2.32s/it]                                                          Episode 86	 reward: -6.56	 makespan: 649.25	 Mean_loss: 0.02856119,  training time: 2.32
progress:  85%|[34m [0m| 85/100 [03:23<00:34,  2.32s/it]progress:  86%|[34m [0m| 86/100 [03:23<00:32,  2.32s/it]                                                          Episode 87	 reward: -6.45	 makespan: 638.50	 Mean_loss: 0.02136626,  training time: 2.37
progress:  86%|[34m [0m| 86/100 [03:25<00:32,  2.32s/it]progress:  87%|[34m [0m| 87/100 [03:25<00:30,  2.34s/it]                                                          Episode 88	 reward: -6.57	 makespan: 650.25	 Mean_loss: 0.02464731,  training time: 2.29
progress:  87%|[34m [0m| 87/100 [03:27<00:30,  2.34s/it]progress:  88%|[34m [0m| 88/100 [03:27<00:27,  2.32s/it]                                                          Episode 89	 reward: -6.40	 makespan: 633.50	 Mean_loss: 0.01491038,  training time: 2.31
progress:  88%|[34m [0m| 88/100 [03:30<00:27,  2.32s/it]progress:  89%|[34m [0m| 89/100 [03:30<00:25,  2.32s/it]                                                          Episode 90	 reward: -6.38	 makespan: 631.50	 Mean_loss: 0.01627469,  training time: 2.32
progress:  89%|[34m [0m| 89/100 [03:32<00:25,  2.32s/it]progress:  90%|[34m [0m| 90/100 [03:32<00:23,  2.32s/it]                                                          Episode 91	 reward: -6.44	 makespan: 637.25	 Mean_loss: 0.01592530,  training time: 2.32
progress:  90%|[34m [0m| 90/100 [03:34<00:23,  2.32s/it]progress:  91%|[34m [0m| 91/100 [03:34<00:20,  2.32s/it]                                                          Episode 92	 reward: -6.45	 makespan: 638.50	 Mean_loss: 0.03189663,  training time: 2.32
progress:  91%|[34m [0m| 91/100 [03:37<00:20,  2.32s/it]progress:  92%|[34m[0m| 92/100 [03:37<00:18,  2.32s/it]                                                          Episode 93	 reward: -6.39	 makespan: 632.50	 Mean_loss: 0.03333509,  training time: 2.38
progress:  92%|[34m[0m| 92/100 [03:39<00:18,  2.32s/it]progress:  93%|[34m[0m| 93/100 [03:39<00:16,  2.34s/it]                                                          Episode 94	 reward: -6.64	 makespan: 657.00	 Mean_loss: 0.03994067,  training time: 2.34
progress:  93%|[34m[0m| 93/100 [03:41<00:16,  2.34s/it]progress:  94%|[34m[0m| 94/100 [03:41<00:14,  2.34s/it]                                                          Episode 95	 reward: -6.59	 makespan: 652.75	 Mean_loss: 0.01801532,  training time: 2.30
progress:  94%|[34m[0m| 94/100 [03:44<00:14,  2.34s/it]progress:  95%|[34m[0m| 95/100 [03:44<00:11,  2.33s/it]                                                          Episode 96	 reward: -6.84	 makespan: 677.00	 Mean_loss: 0.05643799,  training time: 2.34
progress:  95%|[34m[0m| 95/100 [03:46<00:11,  2.33s/it]progress:  96%|[34m[0m| 96/100 [03:46<00:09,  2.33s/it]                                                          Episode 97	 reward: -6.85	 makespan: 678.50	 Mean_loss: 0.03395200,  training time: 2.34
progress:  96%|[34m[0m| 96/100 [03:48<00:09,  2.33s/it]progress:  97%|[34m[0m| 97/100 [03:48<00:07,  2.33s/it]                                                          Episode 98	 reward: -6.62	 makespan: 655.50	 Mean_loss: 0.02736855,  training time: 2.34
progress:  97%|[34m[0m| 97/100 [03:51<00:07,  2.33s/it]progress:  98%|[34m[0m| 98/100 [03:51<00:04,  2.34s/it]                                                          Episode 99	 reward: -6.69	 makespan: 662.25	 Mean_loss: 0.01523449,  training time: 2.45
progress:  98%|[34m[0m| 98/100 [03:53<00:04,  2.34s/it]progress:  99%|[34m[0m| 99/100 [03:53<00:02,  2.37s/it]                                                          Episode 100	 reward: -6.57	 makespan: 650.00	 Mean_loss: 0.02427606,  training time: 2.33
progress:  99%|[34m[0m| 99/100 [03:56<00:02,  2.37s/it]progress: 100%|[34m[0m| 100/100 [03:56<00:00,  2.36s/it]progress: 100%|[34m[0m| 100/100 [03:56<00:00,  2.36s/it]
+ IFS=,
+ read n_j n_m

+ source /work/home/lxx_hzau/.bashrc
++ export CUDA_HOME=/usr/local/cuda-11.1/
++ CUDA_HOME=/usr/local/cuda-11.1/
++ export PATH=/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
++ PATH=/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
++ '[' -f /etc/bashrc ']'
++ . /etc/bashrc
+++ '[' '' ']'
+++ shopt -q login_shell
+++ '[' 5235 -gt 199 ']'
++++ /usr/bin/id -gn
++++ /usr/bin/id -un
+++ '[' ac6owqc808 = lxx_hzau ']'
+++ umask 022
+++ SHELL=/bin/bash
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/256term.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/256term.sh
++++ local256=truecolor
++++ '[' -n truecolor ']'
++++ case "$TERM" in
++++ export TERM
++++ '[' -n '' ']'
++++ unset local256
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/abrt-console-notification.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/abrt-console-notification.sh
++++ tty -s
++++ return 0
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/bash_completion.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/bash_completion.sh
++++ '[' -z '4.2.46(2)-release' -o -z '' -o -n '' ']'
++++ return
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/check.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/check.sh
++++ export CHECK_HOME=/opt/hpc/setfreq/
++++ CHECK_HOME=/opt/hpc/setfreq/
++++ export PATH=/opt/hpc/setfreq/:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
++++ PATH=/opt/hpc/setfreq/:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/clusconf-env.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/clusconf-env.sh
++++ export CLUSCONF_HOME=/opt/clusconf
++++ CLUSCONF_HOME=/opt/clusconf
++++ export PATH=/opt/clusconf/bin:/opt/hpc/setfreq/:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
++++ PATH=/opt/clusconf/bin:/opt/hpc/setfreq/:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
++++ export PATH=/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq/:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
++++ PATH=/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq/:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
++++ export NFSCONF=/opt/clusconf/etc/nfs.cfg
++++ NFSCONF=/opt/clusconf/etc/nfs.cfg
++++ export IPMICONF=/opt/clusconf/etc/ipmi.cfg
++++ IPMICONF=/opt/clusconf/etc/ipmi.cfg
++++ export AUTOCLUSCONF=/opt/clusconf/etc/autoconf.cfg
++++ AUTOCLUSCONF=/opt/clusconf/etc/autoconf.cfg
++++ export STARTWAITTIME=300
++++ STARTWAITTIME=300
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/colorgrep.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/colorgrep.sh
++++ /usr/libexec/grepconf.sh -c
++++ alias 'grep=grep --color=auto'
++++ alias 'egrep=egrep --color=auto'
++++ alias 'fgrep=fgrep --color=auto'
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/colorls.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/colorls.sh
++++ '[' '!' -t 0 ']'
++++ return
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/dawning.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/dawning.sh
++++ export GRIDVIEW_HOME=/opt/gridview
++++ GRIDVIEW_HOME=/opt/gridview
++++ export MUNGE_HOME=/opt/gridview/munge
++++ MUNGE_HOME=/opt/gridview/munge
++++ export PATH=/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq/:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
++++ PATH=/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq/:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
++++ export LD_LIBRARY_PATH=/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/sharp/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/lib:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:
++++ LD_LIBRARY_PATH=/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/sharp/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/lib:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:
++++ export MANPATH=/opt/gridview/munge/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/share/man:/opt/rh/devtoolset-7/root/usr/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:
++++ MANPATH=/opt/gridview/munge/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/share/man:/opt/rh/devtoolset-7/root/usr/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:
++++ export C_INCLUDE_PATH=/opt/gridview/munge/include:/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/rh/devtoolset-7/root/usr/include/c++/7:/opt/rh/devtoolset-7/root/usr/include/c++/7/x86_64-redhat-linux:/opt/gridview/slurm/include:/opt/gridview/munge/include:
++++ C_INCLUDE_PATH=/opt/gridview/munge/include:/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/rh/devtoolset-7/root/usr/include/c++/7:/opt/rh/devtoolset-7/root/usr/include/c++/7/x86_64-redhat-linux:/opt/gridview/slurm/include:/opt/gridview/munge/include:
++++ export SLURM_HOME=/opt/gridview/slurm
++++ SLURM_HOME=/opt/gridview/slurm
++++ export PATH=/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq/:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
++++ PATH=/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq/:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
++++ export LD_LIBRARY_PATH=/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/sharp/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/lib:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:
++++ LD_LIBRARY_PATH=/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/sharp/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/lib:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:
++++ export MANPATH=/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/share/man:/opt/rh/devtoolset-7/root/usr/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:
++++ MANPATH=/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/share/man:/opt/rh/devtoolset-7/root/usr/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:
++++ export C_INCLUDE_PATH=/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/rh/devtoolset-7/root/usr/include/c++/7:/opt/rh/devtoolset-7/root/usr/include/c++/7/x86_64-redhat-linux:/opt/gridview/slurm/include:/opt/gridview/munge/include:
++++ C_INCLUDE_PATH=/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/rh/devtoolset-7/root/usr/include/c++/7:/opt/rh/devtoolset-7/root/usr/include/c++/7/x86_64-redhat-linux:/opt/gridview/slurm/include:/opt/gridview/munge/include:
++++ export SLURM_PMIX_DIRECT_CONN=true
++++ SLURM_PMIX_DIRECT_CONN=true
++++ export SLURM_PMIX_DIRECT_CONN_UCX=true
++++ SLURM_PMIX_DIRECT_CONN_UCX=true
++++ export SLURM_PMIX_DIRECT_CONN_EARLY=true
++++ SLURM_PMIX_DIRECT_CONN_EARLY=true
++++ export SLURM_PMIX_TIMEOUT=3000
++++ SLURM_PMIX_TIMEOUT=3000
++++ '[' lxx_hzau '!=' root ']'
++++ export SQUEUE_USERS=lxx_hzau
++++ SQUEUE_USERS=lxx_hzau
++++ export SCONTROL_JOB_USERS=lxx_hzau
++++ SCONTROL_JOB_USERS=lxx_hzau
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/flatpak.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/flatpak.sh
++++ '[' /exports/share:/var/lib/flatpak/exports/share:/usr/local/share:/usr/share = /work/home/lxx_hzau/.local/share/flatpak/exports/share:/var/lib/flatpak/exports/share:/usr/local/share:/usr/share ']'
++++ export XDG_DATA_DIRS
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/kde.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/kde.sh
++++ '[' -z /usr ']'
++++ '[' -z '' ']'
++++ grep --color=auto -qs '^PRELINKING=yes' /etc/sysconfig/prelink
++++ for libdir in /usr/lib64 /usr/lib
++++ '[' -n /usr/lib64/kde4/plugins:/usr/lib/kde4/plugins ']'
++++ echo /usr/lib64/kde4/plugins:/usr/lib/kde4/plugins
++++ /bin/grep -q /usr/lib64/kde4/plugins
++++ for libdir in /usr/lib64 /usr/lib
++++ '[' -n /usr/lib64/kde4/plugins:/usr/lib/kde4/plugins ']'
++++ echo /usr/lib64/kde4/plugins:/usr/lib/kde4/plugins
++++ /bin/grep -q /usr/lib/kde4/plugins
++++ '[' '!' -d /work/home/lxx_hzau/.local/share ']'
++++ unset libdir
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/lang.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/lang.sh
++++ sourced=0
++++ '[' -n en_US.UTF-8 ']'
++++ saved_lang=en_US.UTF-8
++++ '[' -f /work/home/lxx_hzau/.i18n ']'
++++ LANG=en_US.UTF-8
++++ unset saved_lang
++++ '[' 0 = 1 ']'
++++ unset sourced
++++ unset langfile
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/less.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/less.sh
++++ '[' -x /usr/bin/lesspipe.sh ']'
++++ export 'LESSOPEN=||/usr/bin/lesspipe.sh %s'
++++ LESSOPEN='||/usr/bin/lesspipe.sh %s'
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/modules.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/modules.sh
++++++ /bin/ps -p 64633 -ocomm=
+++++ /bin/basename slurm_script
++++ shell=slurm_script
++++ '[' -f /usr/share/Modules/init/slurm_script ']'
++++ . /usr/share/Modules/init/sh
+++++ MODULESHOME=/usr/share/Modules
+++++ export MODULESHOME
+++++ '[' compiler/devtoolset/7.3.1:mpi/hpcx/gcc-7.3.1 = '' ']'
+++++ '[' /public/software/modules/base:/public/software/modules/apps = '' ']'
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/mpi-selector.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/mpi-selector.sh
++++ mpi_selector_dir=/var/mpi-selector/data
++++ mpi_selector_homefile=/work/home/lxx_hzau/.mpi-selector
++++ mpi_selector_sysfile=/etc/sysconfig/mpi-selector
++++ mpi_selection=
++++ test -f /work/home/lxx_hzau/.mpi-selector
++++ test -f /etc/sysconfig/mpi-selector
++++ test '' '!=' '' -a -f /var/mpi-selector/data/.sh
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/PackageKit.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/PackageKit.sh
++++ [[ -n '' ]]
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/perl-homedir.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/perl-homedir.sh
++++ PERL_HOMEDIR=1
++++ '[' -f /etc/sysconfig/perl-homedir ']'
++++ '[' -f /work/home/lxx_hzau/.perl-homedir ']'
++++ alias 'perlll=eval `perl -Mlocal::lib`'
++++ '[' x1 = x1 ']'
+++++ perl -Mlocal::lib
++++ eval
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/profile_pmix.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/profile_pmix.sh
++++ export PMIX_HOME=/opt/gridview/pmix
++++ PMIX_HOME=/opt/gridview/pmix
++++ export PATH=/opt/gridview/pmix/bin:/opt/gridview/pmix/sbin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq/:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
++++ PATH=/opt/gridview/pmix/bin:/opt/gridview/pmix/sbin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq/:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
++++ export LD_LIBRARY_PATH=/opt/gridview/pmix/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/sharp/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/lib:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:
++++ LD_LIBRARY_PATH=/opt/gridview/pmix/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/sharp/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/lib:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:
++++ export MANPATH=/opt/gridview/pmix/share/pmix:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/share/man:/opt/rh/devtoolset-7/root/usr/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:
++++ MANPATH=/opt/gridview/pmix/share/pmix:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/share/man:/opt/rh/devtoolset-7/root/usr/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:
++++ export C_INCLUDE_PATH=/opt/gridview/pmix/include:/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/rh/devtoolset-7/root/usr/include/c++/7:/opt/rh/devtoolset-7/root/usr/include/c++/7/x86_64-redhat-linux:/opt/gridview/slurm/include:/opt/gridview/munge/include:
++++ C_INCLUDE_PATH=/opt/gridview/pmix/include:/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/rh/devtoolset-7/root/usr/include/c++/7:/opt/rh/devtoolset-7/root/usr/include/c++/7/x86_64-redhat-linux:/opt/gridview/slurm/include:/opt/gridview/munge/include:
++++ export SLURM_PMIX_DIRECT_CONN=true
++++ SLURM_PMIX_DIRECT_CONN=true
++++ export SLURM_PMIX_DIRECT_CONN_UCX=false
++++ SLURM_PMIX_DIRECT_CONN_UCX=false
++++ export SLURM_PMIX_DIRECT_CONN_EARLY=false
++++ SLURM_PMIX_DIRECT_CONN_EARLY=false
++++ export SLURM_PMIX_TIMEOUT=3000
++++ SLURM_PMIX_TIMEOUT=3000
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/qt-graphicssystem.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/qt-graphicssystem.sh
++++ '[' -z 1 -a -z '' ']'
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/qt.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/qt.sh
++++ '[' -z /usr/lib64/qt-3.3 ']'
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/rocm-gcc-mpi-default.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/rocm-gcc-mpi-default.sh
++++ [[ lxx_hzau =~ root|xhadmin ]]
++++ module use /public/software/modules/apps
+++++ /usr/bin/modulecmd sh use /public/software/modules/apps
++++ eval
++++ module use /public/software/modules/base
+++++ /usr/bin/modulecmd sh use /public/software/modules/base
++++ eval
++++ module unuse /opt/hpc/software/modules
+++++ /usr/bin/modulecmd sh unuse /opt/hpc/software/modules
++++ eval
++++ module load compiler/devtoolset/7.3.1
+++++ /usr/bin/modulecmd sh load compiler/devtoolset/7.3.1
++++ eval
++++ module load mpi/hpcx/gcc-7.3.1
+++++ /usr/bin/modulecmd sh load mpi/hpcx/gcc-7.3.1
++++ eval
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/vim.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/vim.sh
++++ '[' -n '4.2.46(2)-release' -o -n '' -o -n '' ']'
++++ '[' -x /usr/bin/id ']'
+++++ /usr/bin/id -u
++++ ID=5235
++++ '[' -n 5235 -a 5235 -le 200 ']'
++++ alias vi
++++ alias vi=vim
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/vte.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/vte.sh
++++ '[' -n '4.2.46(2)-release' -o -n '' ']'
++++ [[ hxB == *i* ]]
++++ return 0
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/which2.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/which2.sh
++++ alias 'which=alias | /usr/bin/which --tty-only --read-alias --show-dot --show-tilde'
+++ unset i
+++ unset -f pathmunge
+++ /work/home/lxx_hzau/miniconda3/bin/conda shell.bash hook
++ __conda_setup='export CONDA_EXE='\''/work/home/lxx_hzau/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/work/home/lxx_hzau/miniconda3/bin/python'\''

# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
__conda_exe() (
    "$CONDA_EXE" $_CE_M $_CE_CONDA "$@"
)

__conda_hashr() {
    if [ -n "${ZSH_VERSION:+x}" ]; then
        \rehash
    elif [ -n "${POSH_VERSION:+x}" ]; then
        :  # pass
    else
        \hash -r
    fi
}

__conda_activate() {
    if [ -n "${CONDA_PS1_BACKUP:+x}" ]; then
        # Handle transition from shell activated with conda <= 4.3 to a subsequent activation
        # after conda updated to >= 4.4. See issue #6173.
        PS1="$CONDA_PS1_BACKUP"
        \unset CONDA_PS1_BACKUP
    fi
    \local ask_conda
    ask_conda="$(PS1="${PS1:-}" __conda_exe shell.posix "$@")" || \return
    \eval "$ask_conda"
    __conda_hashr
}

__conda_reactivate() {
    \local ask_conda
    ask_conda="$(PS1="${PS1:-}" __conda_exe shell.posix reactivate)" || \return
    \eval "$ask_conda"
    __conda_hashr
}

conda() {
    \local cmd="${1-__missing__}"
    case "$cmd" in
        activate|deactivate)
            __conda_activate "$@"
            ;;
        install|update|upgrade|remove|uninstall)
            __conda_exe "$@" || \return
            __conda_reactivate
            ;;
        *)
            __conda_exe "$@"
            ;;
    esac
}

if [ -z "${CONDA_SHLVL+x}" ]; then
    \export CONDA_SHLVL=0
    # In dev-mode CONDA_EXE is python.exe and on Windows
    # it is in a different relative location to condabin.
    if [ -n "${_CE_CONDA:+x}" ] && [ -n "${WINDIR+x}" ]; then
        PATH="$(\dirname "$CONDA_EXE")/condabin${PATH:+":${PATH}"}"
    else
        PATH="$(\dirname "$(\dirname "$CONDA_EXE")")/condabin${PATH:+":${PATH}"}"
    fi
    \export PATH

    # We'\''re not allowing PS1 to be unbound. It must at least be set.
    # However, we'\''re not exporting it, which can cause problems when starting a second shell
    # via a first shell (i.e. starting zsh from bash).
    if [ -z "${PS1+x}" ]; then
        PS1=
    fi
fi

conda activate base'
++ '[' 0 -eq 0 ']'
++ eval 'export CONDA_EXE='\''/work/home/lxx_hzau/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/work/home/lxx_hzau/miniconda3/bin/python'\''

# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
__conda_exe() (
    "$CONDA_EXE" $_CE_M $_CE_CONDA "$@"
)

__conda_hashr() {
    if [ -n "${ZSH_VERSION:+x}" ]; then
        \rehash
    elif [ -n "${POSH_VERSION:+x}" ]; then
        :  # pass
    else
        \hash -r
    fi
}

__conda_activate() {
    if [ -n "${CONDA_PS1_BACKUP:+x}" ]; then
        # Handle transition from shell activated with conda <= 4.3 to a subsequent activation
        # after conda updated to >= 4.4. See issue #6173.
        PS1="$CONDA_PS1_BACKUP"
        \unset CONDA_PS1_BACKUP
    fi
    \local ask_conda
    ask_conda="$(PS1="${PS1:-}" __conda_exe shell.posix "$@")" || \return
    \eval "$ask_conda"
    __conda_hashr
}

__conda_reactivate() {
    \local ask_conda
    ask_conda="$(PS1="${PS1:-}" __conda_exe shell.posix reactivate)" || \return
    \eval "$ask_conda"
    __conda_hashr
}

conda() {
    \local cmd="${1-__missing__}"
    case "$cmd" in
        activate|deactivate)
            __conda_activate "$@"
            ;;
        install|update|upgrade|remove|uninstall)
            __conda_exe "$@" || \return
            __conda_reactivate
            ;;
        *)
            __conda_exe "$@"
            ;;
    esac
}

if [ -z "${CONDA_SHLVL+x}" ]; then
    \export CONDA_SHLVL=0
    # In dev-mode CONDA_EXE is python.exe and on Windows
    # it is in a different relative location to condabin.
    if [ -n "${_CE_CONDA:+x}" ] && [ -n "${WINDIR+x}" ]; then
        PATH="$(\dirname "$CONDA_EXE")/condabin${PATH:+":${PATH}"}"
    else
        PATH="$(\dirname "$(\dirname "$CONDA_EXE")")/condabin${PATH:+":${PATH}"}"
    fi
    \export PATH

    # We'\''re not allowing PS1 to be unbound. It must at least be set.
    # However, we'\''re not exporting it, which can cause problems when starting a second shell
    # via a first shell (i.e. starting zsh from bash).
    if [ -z "${PS1+x}" ]; then
        PS1=
    fi
fi

conda activate base'
+++ export CONDA_EXE=/work/home/lxx_hzau/miniconda3/bin/conda
+++ CONDA_EXE=/work/home/lxx_hzau/miniconda3/bin/conda
+++ export _CE_M=
+++ _CE_M=
+++ export _CE_CONDA=
+++ _CE_CONDA=
+++ export CONDA_PYTHON_EXE=/work/home/lxx_hzau/miniconda3/bin/python
+++ CONDA_PYTHON_EXE=/work/home/lxx_hzau/miniconda3/bin/python
+++ '[' -z x ']'
+++ conda activate base
+++ local cmd=activate
+++ case "$cmd" in
+++ __conda_activate activate base
+++ '[' -n '' ']'
+++ local ask_conda
++++ PS1=
++++ __conda_exe shell.posix activate base
++++ /work/home/lxx_hzau/miniconda3/bin/conda shell.posix activate base
+++ ask_conda='PS1='\''(base) '\''
export PATH='\''/opt/gridview/pmix/bin:/opt/gridview/pmix/sbin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin'\''
export CONDA_SHLVL='\''1'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export PYTHONPATH='\''/work/home/lxx_hzau/project/FJSP-DRL-main'\'''
+++ eval 'PS1='\''(base) '\''
export PATH='\''/opt/gridview/pmix/bin:/opt/gridview/pmix/sbin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin'\''
export CONDA_SHLVL='\''1'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export PYTHONPATH='\''/work/home/lxx_hzau/project/FJSP-DRL-main'\'''
++++ PS1='(base) '
++++ export PATH=/opt/gridview/pmix/bin:/opt/gridview/pmix/sbin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin
++++ PATH=/opt/gridview/pmix/bin:/opt/gridview/pmix/sbin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin
++++ export CONDA_SHLVL=1
++++ CONDA_SHLVL=1
++++ export 'CONDA_PROMPT_MODIFIER=(base) '
++++ CONDA_PROMPT_MODIFIER='(base) '
++++ export PYTHONPATH=/work/home/lxx_hzau/project/FJSP-DRL-main
++++ PYTHONPATH=/work/home/lxx_hzau/project/FJSP-DRL-main
+++ __conda_hashr
+++ '[' -n '' ']'
+++ '[' -n '' ']'
+++ hash -r
++ unset __conda_setup
++ export PATH=/work/home/lxx_hzau/local/bin:/opt/gridview/pmix/bin:/opt/gridview/pmix/sbin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin
++ PATH=/work/home/lxx_hzau/local/bin:/opt/gridview/pmix/bin:/opt/gridview/pmix/sbin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin
+ module load nvidia/cuda/11.6
++ /usr/bin/modulecmd sh load nvidia/cuda/11.6
+ eval CUDA_HOME=/public/software/compiler/nvidia/cuda/11.6.0:/usr/local/cuda-11.1/ ';export' 'CUDA_HOME;CUDA_ROOT=/public/software/compiler/nvidia/cuda/11.6.0' ';export' 'CUDA_ROOT;INCLUDE=/public/software/compiler/nvidia/cuda/11.6.0/include:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/include:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/include:/opt/hpc/software/mpi/hpcx/v2.11.0/sharp/include:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/include' ';export' 'INCLUDE;LD_LIBRARY_PATH=/public/software/compiler/nvidia/cuda/11.6.0/lib64:/opt/gridview/pmix/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/sharp/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/lib:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:' ';export' 'LD_LIBRARY_PATH;LOADEDMODULES=compiler/devtoolset/7.3.1:mpi/hpcx/gcc-7.3.1:nvidia/cuda/11.6' ';export' 'LOADEDMODULES;PATH=/public/software/compiler/nvidia/cuda/11.6.0/bin:/work/home/lxx_hzau/local/bin:/opt/gridview/pmix/bin:/opt/gridview/pmix/sbin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin' ';export' 'PATH;_LMFILES_=/public/software/modules/base/compiler/devtoolset/7.3.1:/public/software/modules/base/mpi/hpcx/gcc-7.3.1:/public/software/modules/base/nvidia/cuda/11.6' ';export' '_LMFILES_;'
++ CUDA_HOME=/public/software/compiler/nvidia/cuda/11.6.0:/usr/local/cuda-11.1/
++ export CUDA_HOME
++ CUDA_ROOT=/public/software/compiler/nvidia/cuda/11.6.0
++ export CUDA_ROOT
++ INCLUDE=/public/software/compiler/nvidia/cuda/11.6.0/include:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/include:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/include:/opt/hpc/software/mpi/hpcx/v2.11.0/sharp/include:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/include
++ export INCLUDE
++ LD_LIBRARY_PATH=/public/software/compiler/nvidia/cuda/11.6.0/lib64:/opt/gridview/pmix/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/sharp/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/lib:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:
++ export LD_LIBRARY_PATH
++ LOADEDMODULES=compiler/devtoolset/7.3.1:mpi/hpcx/gcc-7.3.1:nvidia/cuda/11.6
++ export LOADEDMODULES
++ PATH=/public/software/compiler/nvidia/cuda/11.6.0/bin:/work/home/lxx_hzau/local/bin:/opt/gridview/pmix/bin:/opt/gridview/pmix/sbin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin
++ export PATH
++ _LMFILES_=/public/software/modules/base/compiler/devtoolset/7.3.1:/public/software/modules/base/mpi/hpcx/gcc-7.3.1:/public/software/modules/base/nvidia/cuda/11.6
++ export _LMFILES_
+ conda activate RL-torch
+ local cmd=activate
+ case "$cmd" in
+ __conda_activate activate RL-torch
+ '[' -n '' ']'
+ local ask_conda
++ PS1='(base) '
++ __conda_exe shell.posix activate RL-torch
++ /work/home/lxx_hzau/miniconda3/bin/conda shell.posix activate RL-torch
+ ask_conda='PS1='\''(RL-torch) '\''
export PATH='\''/public/software/compiler/nvidia/cuda/11.6.0/bin:/work/home/lxx_hzau/local/bin:/opt/gridview/pmix/bin:/opt/gridview/pmix/sbin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/envs/RL-torch/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin'\''
export CONDA_PREFIX='\''/work/home/lxx_hzau/miniconda3/envs/RL-torch'\''
export CONDA_SHLVL='\''2'\''
export CONDA_DEFAULT_ENV='\''RL-torch'\''
export CONDA_PROMPT_MODIFIER='\''(RL-torch) '\''
export PYTHONPATH='\''/work/home/lxx_hzau/project/FJSP-DRL-main'\''
export CONDA_PREFIX_1='\''/work/home/lxx_hzau/miniconda3'\''
export CONDA_EXE='\''/work/home/lxx_hzau/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/work/home/lxx_hzau/miniconda3/bin/python'\'''
+ eval 'PS1='\''(RL-torch) '\''
export PATH='\''/public/software/compiler/nvidia/cuda/11.6.0/bin:/work/home/lxx_hzau/local/bin:/opt/gridview/pmix/bin:/opt/gridview/pmix/sbin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/envs/RL-torch/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin'\''
export CONDA_PREFIX='\''/work/home/lxx_hzau/miniconda3/envs/RL-torch'\''
export CONDA_SHLVL='\''2'\''
export CONDA_DEFAULT_ENV='\''RL-torch'\''
export CONDA_PROMPT_MODIFIER='\''(RL-torch) '\''
export PYTHONPATH='\''/work/home/lxx_hzau/project/FJSP-DRL-main'\''
export CONDA_PREFIX_1='\''/work/home/lxx_hzau/miniconda3'\''
export CONDA_EXE='\''/work/home/lxx_hzau/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/work/home/lxx_hzau/miniconda3/bin/python'\'''
++ PS1='(RL-torch) '
++ export PATH=/public/software/compiler/nvidia/cuda/11.6.0/bin:/work/home/lxx_hzau/local/bin:/opt/gridview/pmix/bin:/opt/gridview/pmix/sbin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/envs/RL-torch/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin
++ PATH=/public/software/compiler/nvidia/cuda/11.6.0/bin:/work/home/lxx_hzau/local/bin:/opt/gridview/pmix/bin:/opt/gridview/pmix/sbin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/envs/RL-torch/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin
++ export CONDA_PREFIX=/work/home/lxx_hzau/miniconda3/envs/RL-torch
++ CONDA_PREFIX=/work/home/lxx_hzau/miniconda3/envs/RL-torch
++ export CONDA_SHLVL=2
++ CONDA_SHLVL=2
++ export CONDA_DEFAULT_ENV=RL-torch
++ CONDA_DEFAULT_ENV=RL-torch
++ export 'CONDA_PROMPT_MODIFIER=(RL-torch) '
++ CONDA_PROMPT_MODIFIER='(RL-torch) '
++ export PYTHONPATH=/work/home/lxx_hzau/project/FJSP-DRL-main
++ PYTHONPATH=/work/home/lxx_hzau/project/FJSP-DRL-main
++ export CONDA_PREFIX_1=/work/home/lxx_hzau/miniconda3
++ CONDA_PREFIX_1=/work/home/lxx_hzau/miniconda3
++ export CONDA_EXE=/work/home/lxx_hzau/miniconda3/bin/conda
++ CONDA_EXE=/work/home/lxx_hzau/miniconda3/bin/conda
++ export _CE_M=
++ _CE_M=
++ export _CE_CONDA=
++ _CE_CONDA=
++ export CONDA_PYTHON_EXE=/work/home/lxx_hzau/miniconda3/bin/python
++ CONDA_PYTHON_EXE=/work/home/lxx_hzau/miniconda3/bin/python
+ __conda_hashr
+ '[' -n '' ']'
+ '[' -n '' ']'
+ hash -r
+ t=train
+ exp=exp14
+ echo exp14
exp14
+ cat
MAML
MAML
+ n_j_options='15 15 15 15'
+ n_m_options='5  7  9  10'
+ logdir=./runs/exp14
+ hidden_dim_actor=512
+ hidden_dim_critic=512
+ num_mlp_layers_actor=3
+ num_mlp_layers_critic=3
+ num_envs=4
+ meta_iterations=1000
+ max_updates_maml=1000
+ num_tasks=4
+ max_updates_finetune=50
+ lr=0.003
+ data='15,5 15,7 15,9 15,10'
+ logdir_dan=./runs/exp14/DAN
+ model_suffix=exp14_1000_512_3
+ logdir_maml=./runs/exp14/maml
+ python train/multi_task_maml_exp14.py --logdir ./runs/exp14/maml --model_suffix exp14_1000_512_3 --maml_model True --meta_iterations 1000 --num_tasks 4 --max_updates 1000 --num_envs 4 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --n_j_options 15 15 15 15 --n_m_options 5 7 9 10
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/10x5+mix
save model name:  maml+exp14_1000_512_3
self.n_js [15, 15, 15, 15]
[5, 7, 9, 10]
[(15, 5), (15, 7), (15, 9), (15, 10)]
[839.75, 881.5, 883.0, 893.5]
Episode 1	 reward: -9.51	 Mean_loss: 1.06210601,  training time: 16.72
[877.75, 885.5, 1066.75, 970.0]
Episode 2	 reward: -9.48	 Mean_loss: 1.35726595,  training time: 13.74
[851.0, 880.25, 859.5, 908.0]
Episode 3	 reward: -9.29	 Mean_loss: 1.05429184,  training time: 13.71
[848.0, 894.0, 945.0, 929.25]
Episode 4	 reward: -9.72	 Mean_loss: 1.09167099,  training time: 13.72
[869.75, 865.0, 901.5, 893.0]
Episode 5	 reward: -8.83	 Mean_loss: 0.82939804,  training time: 13.75
[780.25, 859.5, 966.5, 953.25]
Episode 6	 reward: -9.52	 Mean_loss: 0.97106898,  training time: 13.72
[846.75, 825.75, 877.75, 885.75]
Episode 7	 reward: -8.86	 Mean_loss: 0.77969933,  training time: 13.74
[783.25, 887.25, 831.5, 919.5]
Episode 8	 reward: -9.42	 Mean_loss: 0.73674089,  training time: 13.77
[832.75, 888.75, 916.5, 897.75]
Episode 9	 reward: -9.77	 Mean_loss: 0.69403589,  training time: 13.76
[875.25, 897.75, 913.25, 876.5]
Episode 10	 reward: -10.25	 Mean_loss: 0.52530986,  training time: 13.65
[886.0, 903.75, 935.25, 911.5]
Episode 11	 reward: -9.63	 Mean_loss: 0.76146346,  training time: 13.67
[810.5, 877.75, 850.75, 973.0]
Episode 12	 reward: -10.17	 Mean_loss: 0.72833985,  training time: 13.66
[859.5, 939.0, 880.25, 898.25]
Episode 13	 reward: -9.85	 Mean_loss: 0.56907403,  training time: 13.64
[763.0, 884.25, 942.75, 938.75]
Episode 14	 reward: -10.04	 Mean_loss: 0.61110485,  training time: 13.68
[777.5, 883.75, 901.25, 937.75]
Episode 15	 reward: -9.34	 Mean_loss: 0.48908478,  training time: 13.72
[756.25, 902.5, 840.0, 898.25]
Episode 16	 reward: -9.42	 Mean_loss: 0.48555946,  training time: 13.70
[826.75, 874.75, 849.5, 909.0]
Episode 17	 reward: -8.94	 Mean_loss: 0.54308373,  training time: 13.72
[750.75, 837.0, 852.0, 908.75]
Episode 18	 reward: -9.05	 Mean_loss: 0.49016535,  training time: 13.73
[781.75, 831.25, 803.75, 843.0]
Episode 19	 reward: -9.02	 Mean_loss: 0.52121490,  training time: 13.69
[754.0, 773.75, 816.25, 956.5]
Episode 20	 reward: -8.93	 Mean_loss: 0.64445192,  training time: 13.71
[818.0, 810.5, 817.75, 858.5]
Episode 21	 reward: -10.35	 Mean_loss: 0.44406560,  training time: 13.77
[840.75, 868.75, 842.5, 827.75]
Episode 22	 reward: -9.32	 Mean_loss: 0.59070092,  training time: 13.82
[808.25, 820.25, 864.0, 918.0]
Episode 23	 reward: -9.09	 Mean_loss: 0.44785932,  training time: 13.69
[828.75, 949.5, 798.0, 858.5]
Episode 24	 reward: -9.09	 Mean_loss: 0.53368938,  training time: 13.71
[821.5, 854.5, 860.5, 873.5]
Episode 25	 reward: -9.38	 Mean_loss: 0.57377118,  training time: 13.69
[809.75, 817.75, 799.25, 879.75]
Episode 26	 reward: -9.59	 Mean_loss: 0.48796901,  training time: 13.69
[791.75, 809.25, 834.25, 902.75]
Episode 27	 reward: -10.11	 Mean_loss: 0.60432911,  training time: 13.70
[796.5, 837.25, 820.75, 820.5]
Episode 28	 reward: -9.19	 Mean_loss: 0.76490653,  training time: 13.76
[797.75, 832.0, 803.75, 885.0]
Episode 29	 reward: -9.66	 Mean_loss: 0.58860457,  training time: 13.77
[774.5, 824.0, 844.0, 840.75]
Episode 30	 reward: -9.50	 Mean_loss: 0.69989872,  training time: 13.72
[797.5, 862.25, 836.75, 934.25]
Episode 31	 reward: -9.83	 Mean_loss: 0.62555617,  training time: 13.69
[792.0, 873.5, 778.5, 877.25]
Episode 32	 reward: -10.19	 Mean_loss: 0.55707854,  training time: 13.76
[859.5, 764.25, 846.75, 792.75]
Episode 33	 reward: -9.82	 Mean_loss: 0.70545155,  training time: 13.70
[758.75, 842.0, 872.75, 823.75]
Episode 34	 reward: -9.13	 Mean_loss: 0.55701506,  training time: 13.71
[843.0, 878.0, 814.5, 901.25]
Episode 35	 reward: -9.09	 Mean_loss: 0.59342003,  training time: 13.74
[793.25, 826.5, 801.0, 833.75]
Episode 36	 reward: -10.22	 Mean_loss: 0.63369244,  training time: 13.72
[860.25, 840.25, 801.5, 868.0]
Episode 37	 reward: -9.20	 Mean_loss: 0.68276554,  training time: 13.71
[805.75, 792.25, 768.5, 884.5]
Episode 38	 reward: -9.01	 Mean_loss: 0.61288524,  training time: 13.66
[752.75, 812.5, 729.25, 853.5]
Episode 39	 reward: -9.15	 Mean_loss: 0.46876878,  training time: 13.69
[740.0, 787.75, 757.75, 862.5]
Episode 40	 reward: -9.73	 Mean_loss: 0.54413456,  training time: 13.68
[793.5, 855.5, 815.5, 795.5]
Episode 41	 reward: -9.52	 Mean_loss: 0.58931744,  training time: 13.73
[742.5, 848.75, 793.5, 792.25]
Episode 42	 reward: -9.79	 Mean_loss: 0.50732750,  training time: 13.65
[777.75, 888.25, 798.25, 833.75]
Episode 43	 reward: -9.35	 Mean_loss: 0.52929300,  training time: 13.76
[714.75, 779.25, 769.75, 816.25]
Episode 44	 reward: -9.73	 Mean_loss: 0.43003267,  training time: 13.66
[730.75, 816.0, 766.75, 813.0]
Episode 45	 reward: -8.95	 Mean_loss: 0.46974713,  training time: 13.67
[768.75, 908.0, 712.25, 833.75]
Episode 46	 reward: -9.40	 Mean_loss: 0.41119787,  training time: 13.68
[804.0, 770.25, 773.5, 777.25]
Episode 47	 reward: -9.34	 Mean_loss: 0.55315983,  training time: 13.67
[765.75, 767.0, 752.0, 818.75]
Episode 48	 reward: -9.46	 Mean_loss: 0.42991406,  training time: 13.66
[766.0, 806.25, 788.75, 786.25]
Episode 49	 reward: -9.57	 Mean_loss: 0.49376741,  training time: 13.67
[737.0, 783.5, 786.5, 748.0]
Episode 50	 reward: -10.13	 Mean_loss: 0.41088969,  training time: 13.67
[757.0, 807.25, 754.25, 747.5]
Episode 51	 reward: -9.65	 Mean_loss: 0.36848116,  training time: 13.68
[770.5, 740.0, 778.75, 720.75]
Episode 52	 reward: -9.76	 Mean_loss: 0.39591518,  training time: 13.64
[773.25, 793.25, 740.25, 831.0]
Episode 53	 reward: -9.25	 Mean_loss: 0.38802209,  training time: 13.65
[761.5, 837.5, 762.0, 765.0]
Episode 54	 reward: -9.54	 Mean_loss: 0.37400532,  training time: 13.64
[725.25, 769.5, 754.5, 827.25]
Episode 55	 reward: -9.37	 Mean_loss: 0.38635316,  training time: 13.71
[789.25, 820.0, 765.5, 756.25]
Episode 56	 reward: -9.45	 Mean_loss: 0.38614535,  training time: 13.67
[739.0, 768.25, 732.5, 744.25]
Episode 57	 reward: -9.57	 Mean_loss: 0.33309799,  training time: 13.67
[718.5, 723.25, 766.25, 781.5]
Episode 58	 reward: -9.65	 Mean_loss: 0.29168952,  training time: 13.64
[750.25, 801.5, 803.0, 799.0]
Episode 59	 reward: -9.23	 Mean_loss: 0.30382061,  training time: 13.67
[714.25, 818.0, 715.25, 747.25]
Episode 60	 reward: -8.96	 Mean_loss: 0.33822981,  training time: 13.68
[737.0, 713.25, 698.0, 738.5]
Episode 61	 reward: -10.12	 Mean_loss: 0.34704939,  training time: 13.72
[742.25, 723.75, 683.25, 790.75]
Episode 62	 reward: -10.05	 Mean_loss: 0.25494778,  training time: 13.66
[740.5, 708.5, 738.5, 743.25]
Episode 63	 reward: -9.79	 Mean_loss: 0.33251369,  training time: 13.69
[757.0, 715.5, 709.25, 798.25]
Episode 64	 reward: -10.22	 Mean_loss: 0.34616080,  training time: 13.74
[776.0, 718.75, 699.25, 799.5]
Episode 65	 reward: -10.11	 Mean_loss: 0.29242235,  training time: 13.67
[740.75, 684.0, 722.75, 757.75]
Episode 66	 reward: -9.80	 Mean_loss: 0.31782514,  training time: 13.70
[755.25, 714.25, 740.5, 812.25]
Episode 67	 reward: -9.68	 Mean_loss: 0.27826300,  training time: 13.68
[744.25, 672.5, 716.25, 800.75]
Episode 68	 reward: -9.89	 Mean_loss: 0.25736237,  training time: 13.67
[780.0, 743.5, 710.75, 799.5]
Episode 69	 reward: -9.76	 Mean_loss: 0.28600436,  training time: 13.66
[737.75, 716.0, 709.5, 789.5]
Episode 70	 reward: -9.56	 Mean_loss: 0.25079092,  training time: 13.68
[749.75, 693.0, 749.75, 792.25]
Episode 71	 reward: -10.01	 Mean_loss: 0.28580633,  training time: 13.66
[754.25, 709.5, 659.0, 766.0]
Episode 72	 reward: -9.58	 Mean_loss: 0.26163509,  training time: 13.66
[711.75, 705.5, 691.0, 718.0]
Episode 73	 reward: -9.62	 Mean_loss: 0.25817919,  training time: 13.67
[733.75, 724.0, 697.25, 786.0]
Episode 74	 reward: -9.30	 Mean_loss: 0.26614401,  training time: 13.66
[711.5, 742.0, 732.0, 779.5]
Episode 75	 reward: -9.72	 Mean_loss: 0.24040614,  training time: 13.65
[741.0, 680.25, 692.75, 726.0]
Episode 76	 reward: -9.64	 Mean_loss: 0.28984129,  training time: 13.65
[752.0, 637.5, 734.0, 745.0]
Episode 77	 reward: -10.45	 Mean_loss: 0.23789096,  training time: 13.65
[706.25, 699.5, 644.75, 801.0]
Episode 78	 reward: -9.28	 Mean_loss: 0.21441765,  training time: 13.65
[750.75, 693.5, 732.25, 723.5]
Episode 79	 reward: -9.73	 Mean_loss: 0.26522288,  training time: 13.68
[736.75, 675.25, 637.5, 775.0]
Episode 80	 reward: -9.47	 Mean_loss: 0.24734414,  training time: 13.66
[654.25, 657.75, 716.25, 765.0]
Episode 81	 reward: -9.77	 Mean_loss: 0.20900880,  training time: 13.71
[675.25, 653.5, 712.75, 827.25]
Episode 82	 reward: -9.84	 Mean_loss: 0.30197677,  training time: 13.69
[650.0, 673.5, 713.25, 748.0]
Episode 83	 reward: -9.59	 Mean_loss: 0.24619927,  training time: 13.68
[652.5, 692.75, 688.5, 706.5]
Episode 84	 reward: -9.78	 Mean_loss: 0.24549465,  training time: 13.64
[696.0, 633.5, 681.25, 767.5]
Episode 85	 reward: -9.50	 Mean_loss: 0.19024357,  training time: 13.80
[670.0, 638.5, 725.5, 767.5]
Episode 86	 reward: -10.22	 Mean_loss: 0.23301782,  training time: 13.62
[670.0, 652.25, 712.5, 756.75]
Episode 87	 reward: -9.59	 Mean_loss: 0.17552589,  training time: 13.67
[691.0, 671.25, 722.5, 765.5]
Episode 88	 reward: -9.23	 Mean_loss: 0.14977667,  training time: 13.68
[666.25, 663.75, 731.5, 811.5]
Episode 89	 reward: -9.32	 Mean_loss: 0.13172491,  training time: 13.66
[677.75, 653.5, 741.25, 733.25]
Episode 90	 reward: -9.86	 Mean_loss: 0.18657018,  training time: 13.67
[683.5, 619.75, 696.5, 763.5]
Episode 91	 reward: -9.25	 Mean_loss: 0.18115172,  training time: 13.64
[638.5, 629.0, 678.75, 752.5]
Episode 92	 reward: -9.83	 Mean_loss: 0.17388609,  training time: 13.67
[639.75, 652.25, 715.0, 755.0]
Episode 93	 reward: -10.02	 Mean_loss: 0.20559976,  training time: 13.64
[632.0, 643.5, 709.25, 751.75]
Episode 94	 reward: -9.84	 Mean_loss: 0.15423051,  training time: 13.68
[664.0, 632.25, 687.75, 737.0]
Episode 95	 reward: -9.10	 Mean_loss: 0.18878865,  training time: 13.67
[658.5, 666.0, 694.0, 746.0]
Episode 96	 reward: -9.52	 Mean_loss: 0.17357123,  training time: 13.66
[674.25, 645.5, 727.5, 749.0]
Episode 97	 reward: -9.35	 Mean_loss: 0.18043190,  training time: 13.64
[662.0, 639.75, 680.75, 704.5]
Episode 98	 reward: -9.44	 Mean_loss: 0.11444750,  training time: 13.63
[689.0, 637.5, 712.5, 814.75]
Episode 99	 reward: -9.26	 Mean_loss: 0.14589806,  training time: 13.65
[684.0, 622.75, 671.25, 753.25]
Episode 100	 reward: -9.71	 Mean_loss: 0.17701972,  training time: 13.69
[637.25, 663.0, 756.75, 708.25]
Episode 101	 reward: -9.50	 Mean_loss: 0.15238327,  training time: 13.72
[607.25, 729.5, 680.5, 686.75]
Episode 102	 reward: -9.71	 Mean_loss: 0.18058084,  training time: 13.66
[615.0, 674.5, 683.5, 691.5]
Episode 103	 reward: -9.13	 Mean_loss: 0.17779072,  training time: 13.65
[597.75, 656.25, 670.5, 701.25]
Episode 104	 reward: -10.41	 Mean_loss: 0.14332528,  training time: 13.70
[653.25, 686.75, 716.75, 736.5]
Episode 105	 reward: -9.02	 Mean_loss: 0.18257897,  training time: 13.64
[644.0, 670.5, 691.75, 722.75]
Episode 106	 reward: -9.44	 Mean_loss: 0.15685287,  training time: 13.70
[630.75, 653.5, 696.25, 698.5]
Episode 107	 reward: -9.12	 Mean_loss: 0.15859230,  training time: 13.69
[664.0, 677.75, 657.0, 723.0]
Episode 108	 reward: -9.07	 Mean_loss: 0.17434354,  training time: 13.70
[642.25, 658.5, 693.0, 682.5]
Episode 109	 reward: -9.44	 Mean_loss: 0.17710429,  training time: 13.66
[646.5, 636.5, 690.25, 707.0]
Episode 110	 reward: -9.11	 Mean_loss: 0.17505893,  training time: 13.67
[612.75, 694.25, 718.75, 748.0]
Episode 111	 reward: -8.84	 Mean_loss: 0.14311807,  training time: 13.64
[648.5, 651.25, 677.5, 704.5]
Episode 112	 reward: -9.57	 Mean_loss: 0.21707001,  training time: 13.67
[622.25, 659.75, 670.75, 698.75]
Episode 113	 reward: -9.22	 Mean_loss: 0.13825576,  training time: 13.64
[616.75, 707.75, 692.25, 712.25]
Episode 114	 reward: -9.51	 Mean_loss: 0.17163366,  training time: 13.66
[617.5, 690.0, 691.0, 712.75]
Episode 115	 reward: -9.19	 Mean_loss: 0.16265677,  training time: 13.65
[600.0, 641.25, 682.75, 709.25]
Episode 116	 reward: -10.20	 Mean_loss: 0.16187085,  training time: 13.63
[612.25, 680.5, 678.75, 700.0]
Episode 117	 reward: -9.17	 Mean_loss: 0.15680939,  training time: 13.66
[570.5, 656.75, 654.5, 679.75]
Episode 118	 reward: -9.88	 Mean_loss: 0.19014539,  training time: 13.67
[632.25, 650.75, 659.0, 751.25]
Episode 119	 reward: -9.85	 Mean_loss: 0.19498226,  training time: 13.65
[594.0, 653.75, 658.5, 692.75]
Episode 120	 reward: -8.88	 Mean_loss: 0.16290152,  training time: 13.68
[626.5, 611.75, 665.0, 666.0]
Episode 121	 reward: -9.64	 Mean_loss: 0.12507826,  training time: 13.73
[615.25, 658.5, 670.25, 696.75]
Episode 122	 reward: -8.65	 Mean_loss: 0.14006345,  training time: 13.66
[629.5, 621.0, 667.25, 670.0]
Episode 123	 reward: -8.97	 Mean_loss: 0.12342301,  training time: 13.64
[650.0, 601.5, 682.5, 656.5]
Episode 124	 reward: -9.38	 Mean_loss: 0.12219096,  training time: 13.63
[631.5, 653.25, 663.25, 697.75]
Episode 125	 reward: -8.78	 Mean_loss: 0.11262345,  training time: 13.62
[622.75, 617.75, 661.75, 654.75]
Episode 126	 reward: -9.02	 Mean_loss: 0.13142772,  training time: 13.67
[613.75, 633.0, 663.75, 708.25]
Episode 127	 reward: -9.20	 Mean_loss: 0.15388498,  training time: 13.73
[610.0, 666.0, 621.75, 682.75]
Episode 128	 reward: -8.74	 Mean_loss: 0.14519985,  training time: 13.67
[608.5, 643.25, 625.5, 648.75]
Episode 129	 reward: -8.93	 Mean_loss: 0.13705505,  training time: 13.68
[641.25, 628.0, 658.25, 683.25]
Episode 130	 reward: -9.12	 Mean_loss: 0.13387263,  training time: 13.68
[616.5, 625.25, 644.5, 698.75]
Episode 131	 reward: -8.57	 Mean_loss: 0.12391255,  training time: 13.63
[601.0, 638.0, 655.5, 706.75]
Episode 132	 reward: -9.76	 Mean_loss: 0.13560902,  training time: 13.62
[589.25, 652.0, 678.0, 703.5]
Episode 133	 reward: -9.14	 Mean_loss: 0.16474688,  training time: 13.62
[627.75, 636.0, 660.75, 669.5]
Episode 134	 reward: -9.25	 Mean_loss: 0.12032892,  training time: 13.67
[617.5, 653.5, 653.5, 709.0]
Episode 135	 reward: -9.03	 Mean_loss: 0.11522947,  training time: 13.70
[639.75, 633.0, 660.75, 672.0]
Episode 136	 reward: -9.23	 Mean_loss: 0.15675643,  training time: 13.65
[584.5, 589.75, 652.0, 709.25]
Episode 137	 reward: -9.41	 Mean_loss: 0.08711646,  training time: 13.67
[627.75, 630.75, 657.5, 686.0]
Episode 138	 reward: -8.91	 Mean_loss: 0.12089755,  training time: 13.64
[636.25, 623.5, 682.25, 663.75]
Episode 139	 reward: -8.84	 Mean_loss: 0.10444810,  training time: 13.65
[587.75, 602.5, 661.5, 713.75]
Episode 140	 reward: -9.15	 Mean_loss: 0.10339075,  training time: 13.65
[606.5, 608.5, 676.5, 669.5]
Episode 141	 reward: -9.85	 Mean_loss: 0.12763074,  training time: 13.69
[626.25, 618.5, 667.0, 684.0]
Episode 142	 reward: -9.39	 Mean_loss: 0.13535993,  training time: 13.64
[636.25, 591.5, 659.25, 670.0]
Episode 143	 reward: -9.60	 Mean_loss: 0.11696274,  training time: 13.65
[611.75, 622.75, 646.75, 716.75]
Episode 144	 reward: -10.89	 Mean_loss: 0.08717407,  training time: 13.64
[586.25, 601.75, 673.5, 722.5]
Episode 145	 reward: -10.14	 Mean_loss: 0.14829035,  training time: 13.68
[633.75, 622.0, 668.75, 674.0]
Episode 146	 reward: -9.77	 Mean_loss: 0.13990083,  training time: 13.66
[566.75, 595.25, 657.75, 670.25]
Episode 147	 reward: -9.30	 Mean_loss: 0.14646572,  training time: 13.67
[600.0, 642.0, 685.5, 718.25]
Episode 148	 reward: -8.98	 Mean_loss: 0.09900307,  training time: 13.75
[605.5, 607.75, 634.75, 681.75]
Episode 149	 reward: -9.32	 Mean_loss: 0.11076114,  training time: 13.68
[583.25, 572.75, 618.0, 692.5]
Episode 150	 reward: -9.53	 Mean_loss: 0.11127434,  training time: 13.64
[599.25, 586.0, 693.0, 702.25]
Episode 151	 reward: -9.32	 Mean_loss: 0.14196539,  training time: 13.65
[594.0, 579.0, 689.5, 664.25]
Episode 152	 reward: -9.10	 Mean_loss: 0.15937066,  training time: 13.64
[619.25, 584.25, 679.75, 672.0]
Episode 153	 reward: -9.05	 Mean_loss: 0.10922315,  training time: 13.67
[567.5, 610.25, 635.75, 669.75]
Episode 154	 reward: -9.20	 Mean_loss: 0.10218088,  training time: 13.66
[611.25, 621.25, 617.25, 695.0]
Episode 155	 reward: -10.04	 Mean_loss: 0.12911876,  training time: 13.68
[589.5, 617.25, 659.25, 742.5]
Episode 156	 reward: -9.29	 Mean_loss: 0.13499260,  training time: 13.64
[579.0, 589.0, 648.0, 708.25]
Episode 157	 reward: -9.80	 Mean_loss: 0.08995175,  training time: 13.64
[619.0, 619.0, 659.75, 654.75]
Episode 158	 reward: -10.45	 Mean_loss: 0.11661980,  training time: 13.65
[575.25, 591.5, 641.5, 700.75]
Episode 159	 reward: -9.60	 Mean_loss: 0.13382465,  training time: 13.66
[608.25, 590.75, 664.25, 682.75]
Episode 160	 reward: -9.54	 Mean_loss: 0.10126885,  training time: 13.69
[599.25, 657.75, 652.75, 645.75]
Episode 161	 reward: -9.55	 Mean_loss: 0.05297172,  training time: 13.68
[589.5, 639.75, 617.0, 657.5]
Episode 162	 reward: -9.75	 Mean_loss: 0.06636742,  training time: 13.67
[598.5, 631.5, 679.0, 642.25]
Episode 163	 reward: -9.30	 Mean_loss: 0.06396709,  training time: 13.64
[623.5, 631.25, 626.25, 667.75]
Episode 164	 reward: -8.70	 Mean_loss: 0.07743480,  training time: 13.65
[619.5, 641.75, 630.25, 662.0]
Episode 165	 reward: -9.17	 Mean_loss: 0.07520202,  training time: 13.66
[585.0, 650.5, 628.5, 615.75]
Episode 166	 reward: -8.87	 Mean_loss: 0.10307487,  training time: 13.68
[583.25, 647.25, 644.5, 638.0]
Episode 167	 reward: -9.30	 Mean_loss: 0.05233089,  training time: 13.64
[574.75, 647.75, 640.0, 720.5]
Episode 168	 reward: -9.08	 Mean_loss: 0.10886341,  training time: 13.65
[577.5, 661.25, 620.5, 621.25]
Episode 169	 reward: -9.86	 Mean_loss: 0.07107496,  training time: 13.75
[611.5, 648.5, 645.75, 699.75]
Episode 170	 reward: -9.31	 Mean_loss: 0.05358676,  training time: 13.64
[617.5, 658.0, 648.0, 662.75]
Episode 171	 reward: -9.21	 Mean_loss: 0.05628102,  training time: 13.67
[601.5, 637.75, 616.0, 659.75]
Episode 172	 reward: -9.27	 Mean_loss: 0.10151812,  training time: 13.65
[607.5, 642.75, 647.75, 664.25]
Episode 173	 reward: -8.87	 Mean_loss: 0.09710470,  training time: 13.68
[628.25, 629.5, 658.5, 683.5]
Episode 174	 reward: -9.49	 Mean_loss: 0.05662728,  training time: 13.66
[582.0, 642.75, 665.5, 668.5]
Episode 175	 reward: -9.10	 Mean_loss: 0.06704377,  training time: 13.67
[611.75, 634.25, 634.25, 662.25]
Episode 176	 reward: -9.63	 Mean_loss: 0.07810654,  training time: 13.64
[588.0, 622.75, 611.25, 677.75]
Episode 177	 reward: -9.86	 Mean_loss: 0.08117141,  training time: 13.69
[579.75, 627.0, 649.75, 672.75]
Episode 178	 reward: -9.39	 Mean_loss: 0.05525357,  training time: 13.68
[607.5, 648.0, 646.25, 689.0]
Episode 179	 reward: -9.43	 Mean_loss: 0.08418203,  training time: 13.65
[587.25, 613.75, 674.5, 680.5]
Episode 180	 reward: -9.86	 Mean_loss: 0.06649489,  training time: 13.64
[663.25, 572.0, 695.25, 689.25]
Episode 181	 reward: -9.28	 Mean_loss: 0.06481023,  training time: 13.73
[658.5, 585.25, 685.0, 683.5]
Episode 182	 reward: -9.52	 Mean_loss: 0.09739623,  training time: 13.68
[677.5, 595.75, 645.75, 680.0]
Episode 183	 reward: -9.04	 Mean_loss: 0.08140697,  training time: 13.68
[655.75, 564.0, 678.25, 659.75]
Episode 184	 reward: -9.57	 Mean_loss: 0.09217938,  training time: 13.68
[654.0, 584.0, 687.5, 669.75]
Episode 185	 reward: -9.77	 Mean_loss: 0.07516873,  training time: 13.68
[631.5, 610.25, 650.75, 672.5]
Episode 186	 reward: -9.53	 Mean_loss: 0.08633833,  training time: 13.64
[641.5, 612.75, 714.0, 670.0]
Episode 187	 reward: -8.83	 Mean_loss: 0.11433781,  training time: 13.64
[635.0, 591.5, 685.75, 703.75]
Episode 188	 reward: -9.44	 Mean_loss: 0.10510140,  training time: 13.64
[682.0, 584.75, 666.75, 684.0]
Episode 189	 reward: -9.21	 Mean_loss: 0.11191846,  training time: 13.65
[650.25, 594.25, 669.5, 717.0]
Episode 190	 reward: -10.41	 Mean_loss: 0.06656978,  training time: 13.76
[665.75, 597.0, 700.5, 648.0]
Episode 191	 reward: -9.12	 Mean_loss: 0.11120205,  training time: 13.68
[641.75, 597.0, 687.0, 695.25]
Episode 192	 reward: -9.38	 Mean_loss: 0.09746458,  training time: 13.64
[636.75, 585.0, 673.0, 654.5]
Episode 193	 reward: -8.53	 Mean_loss: 0.11834770,  training time: 13.65
[645.5, 591.0, 681.25, 683.25]
Episode 194	 reward: -9.02	 Mean_loss: 0.08075301,  training time: 13.64
[654.25, 619.75, 675.75, 679.5]
Episode 195	 reward: -9.39	 Mean_loss: 0.08045737,  training time: 13.63
[634.25, 574.0, 721.25, 681.0]
Episode 196	 reward: -9.41	 Mean_loss: 0.11467120,  training time: 13.67
[658.0, 629.5, 684.0, 683.25]
Episode 197	 reward: -9.32	 Mean_loss: 0.08178697,  training time: 13.68
[613.5, 595.5, 642.0, 637.5]
Episode 198	 reward: -9.15	 Mean_loss: 0.11183121,  training time: 13.68
[639.75, 572.5, 721.75, 656.0]
Episode 199	 reward: -9.77	 Mean_loss: 0.09243128,  training time: 13.62
[638.75, 610.0, 695.75, 705.5]
Episode 200	 reward: -9.39	 Mean_loss: 0.10151365,  training time: 13.63
[640.0, 656.25, 670.5, 667.25]
Episode 201	 reward: -9.49	 Mean_loss: 0.09996447,  training time: 13.71
[666.25, 654.75, 658.75, 678.75]
Episode 202	 reward: -9.90	 Mean_loss: 0.10711366,  training time: 13.64
[618.0, 643.75, 668.75, 675.25]
Episode 203	 reward: -9.64	 Mean_loss: 0.09474717,  training time: 13.67
[641.0, 651.25, 669.25, 701.75]
Episode 204	 reward: -9.76	 Mean_loss: 0.11611941,  training time: 13.66
[650.0, 620.5, 662.5, 680.0]
Episode 205	 reward: -9.21	 Mean_loss: 0.09498483,  training time: 13.67
[643.5, 646.0, 672.0, 715.5]
Episode 206	 reward: -10.16	 Mean_loss: 0.06470267,  training time: 13.72
[635.25, 671.5, 657.25, 665.25]
Episode 207	 reward: -9.24	 Mean_loss: 0.06646431,  training time: 13.69
[639.75, 647.5, 649.5, 686.75]
Episode 208	 reward: -10.44	 Mean_loss: 0.05694261,  training time: 13.68
[636.0, 658.5, 654.0, 691.25]
Episode 209	 reward: -9.86	 Mean_loss: 0.10395804,  training time: 13.68
[631.0, 619.0, 669.5, 698.25]
Episode 210	 reward: -9.51	 Mean_loss: 0.08183470,  training time: 13.66
[648.25, 688.75, 660.75, 718.75]
Episode 211	 reward: -9.09	 Mean_loss: 0.07930385,  training time: 13.75
[650.5, 627.0, 708.5, 688.0]
Episode 212	 reward: -9.28	 Mean_loss: 0.09175056,  training time: 13.70
[700.75, 649.25, 681.25, 666.75]
Episode 213	 reward: -9.67	 Mean_loss: 0.10359956,  training time: 13.62
[658.25, 674.75, 678.25, 698.25]
Episode 214	 reward: -9.43	 Mean_loss: 0.08876738,  training time: 13.69
[632.0, 626.75, 682.5, 679.25]
Episode 215	 reward: -9.28	 Mean_loss: 0.07532223,  training time: 13.68
[630.25, 727.0, 662.0, 683.25]
Episode 216	 reward: -9.63	 Mean_loss: 0.12995814,  training time: 13.65
[670.25, 655.0, 677.25, 684.0]
Episode 217	 reward: -9.76	 Mean_loss: 0.06860351,  training time: 13.64
[652.25, 656.5, 716.75, 678.5]
Episode 218	 reward: -8.81	 Mean_loss: 0.09557395,  training time: 13.67
[652.5, 660.5, 660.0, 671.25]
Episode 219	 reward: -9.64	 Mean_loss: 0.08450162,  training time: 13.64
[652.5, 657.0, 664.75, 663.75]
Episode 220	 reward: -9.44	 Mean_loss: 0.09480067,  training time: 13.68
[616.25, 627.75, 659.5, 700.75]
Episode 221	 reward: -9.35	 Mean_loss: 0.06929557,  training time: 13.71
[614.25, 655.25, 667.75, 707.0]
Episode 222	 reward: -9.41	 Mean_loss: 0.10457166,  training time: 13.67
[590.5, 634.5, 672.5, 711.75]
Episode 223	 reward: -9.48	 Mean_loss: 0.09334542,  training time: 13.66
[609.5, 640.25, 668.5, 709.75]
Episode 224	 reward: -9.88	 Mean_loss: 0.08610027,  training time: 13.64
[627.5, 635.75, 667.5, 724.5]
Episode 225	 reward: -9.34	 Mean_loss: 0.08638311,  training time: 13.58
[651.0, 654.5, 654.25, 720.25]
Episode 226	 reward: -8.89	 Mean_loss: 0.06837475,  training time: 13.63
[623.5, 639.75, 649.25, 723.0]
Episode 227	 reward: -9.85	 Mean_loss: 0.08409162,  training time: 13.60
[625.75, 630.25, 698.0, 675.25]
Episode 228	 reward: -9.70	 Mean_loss: 0.13366701,  training time: 13.60
[601.0, 642.5, 644.25, 666.75]
Episode 229	 reward: -9.45	 Mean_loss: 0.09552498,  training time: 13.62
[602.25, 595.5, 638.0, 695.25]
Episode 230	 reward: -9.44	 Mean_loss: 0.06020349,  training time: 13.62
[664.5, 641.5, 655.25, 692.75]
Episode 231	 reward: -9.51	 Mean_loss: 0.08449777,  training time: 13.64
[611.0, 609.5, 680.5, 722.25]
Episode 232	 reward: -9.54	 Mean_loss: 0.12935725,  training time: 13.67
[608.5, 645.25, 658.5, 733.25]
Episode 233	 reward: -9.07	 Mean_loss: 0.08701993,  training time: 13.59
[637.25, 661.75, 668.75, 698.5]
Episode 234	 reward: -10.30	 Mean_loss: 0.09066731,  training time: 13.65
[603.5, 612.75, 649.0, 727.25]
Episode 235	 reward: -9.37	 Mean_loss: 0.12182414,  training time: 13.61
[593.0, 605.5, 660.0, 728.0]
Episode 236	 reward: -9.76	 Mean_loss: 0.08635239,  training time: 13.63
[618.75, 641.75, 686.25, 685.0]
Episode 237	 reward: -10.36	 Mean_loss: 0.08761199,  training time: 13.60
[608.5, 627.5, 655.25, 680.5]
Episode 238	 reward: -10.70	 Mean_loss: 0.09822157,  training time: 13.58
[626.75, 611.75, 683.25, 685.75]
Episode 239	 reward: -9.61	 Mean_loss: 0.09250578,  training time: 13.63
[618.5, 618.75, 624.25, 706.0]
Episode 240	 reward: -10.20	 Mean_loss: 0.09906693,  training time: 13.58
[629.25, 630.75, 637.0, 711.25]
Episode 241	 reward: -10.29	 Mean_loss: 0.09431029,  training time: 13.64
[603.25, 591.75, 651.5, 670.25]
Episode 242	 reward: -9.63	 Mean_loss: 0.06746195,  training time: 13.65
[634.75, 628.25, 640.0, 728.0]
Episode 243	 reward: -10.11	 Mean_loss: 0.08139376,  training time: 13.56
[629.5, 647.25, 659.75, 692.75]
Episode 244	 reward: -9.70	 Mean_loss: 0.07995639,  training time: 13.59
[586.25, 605.5, 680.0, 695.5]
Episode 245	 reward: -9.63	 Mean_loss: 0.07186154,  training time: 13.60
[613.75, 615.0, 650.0, 664.75]
Episode 246	 reward: -9.05	 Mean_loss: 0.06876198,  training time: 13.56
[590.75, 611.5, 678.75, 729.25]
Episode 247	 reward: -9.28	 Mean_loss: 0.08532635,  training time: 13.61
[616.0, 610.25, 660.25, 722.75]
Episode 248	 reward: -9.17	 Mean_loss: 0.07134989,  training time: 13.60
[603.0, 635.75, 691.75, 713.75]
Episode 249	 reward: -9.35	 Mean_loss: 0.11261027,  training time: 13.62
[616.0, 574.75, 668.25, 710.0]
Episode 250	 reward: -9.93	 Mean_loss: 0.06239581,  training time: 13.60
[597.5, 679.75, 667.0, 739.5]
Episode 251	 reward: -10.04	 Mean_loss: 0.08369991,  training time: 13.58
[610.0, 628.0, 660.0, 692.0]
Episode 252	 reward: -9.78	 Mean_loss: 0.07762419,  training time: 13.58
[640.75, 640.25, 649.75, 721.25]
Episode 253	 reward: -9.95	 Mean_loss: 0.06427051,  training time: 13.65
[612.75, 659.5, 647.0, 694.0]
Episode 254	 reward: -9.07	 Mean_loss: 0.07054720,  training time: 13.56
[619.0, 627.5, 673.5, 703.5]
Episode 255	 reward: -8.70	 Mean_loss: 0.07009266,  training time: 13.60
[617.5, 614.0, 661.25, 711.5]
Episode 256	 reward: -9.68	 Mean_loss: 0.08063717,  training time: 13.61
[604.5, 616.5, 627.0, 724.5]
Episode 257	 reward: -9.41	 Mean_loss: 0.07689760,  training time: 13.61
[607.0, 611.75, 654.5, 703.25]
Episode 258	 reward: -9.65	 Mean_loss: 0.06448369,  training time: 13.60
[606.0, 585.0, 659.0, 712.25]
Episode 259	 reward: -9.29	 Mean_loss: 0.07542793,  training time: 13.61
[619.5, 609.75, 639.25, 709.25]
Episode 260	 reward: -9.59	 Mean_loss: 0.11023500,  training time: 13.57
[625.5, 654.75, 721.75, 664.5]
Episode 261	 reward: -9.43	 Mean_loss: 0.06019850,  training time: 13.66
[647.5, 654.0, 659.0, 671.5]
Episode 262	 reward: -8.93	 Mean_loss: 0.05490394,  training time: 13.57
[616.0, 628.75, 707.25, 651.75]
Episode 263	 reward: -9.11	 Mean_loss: 0.08384535,  training time: 13.59
[652.75, 660.5, 673.5, 680.75]
Episode 264	 reward: -8.79	 Mean_loss: 0.07591656,  training time: 13.58
[650.0, 653.25, 691.75, 678.25]
Episode 265	 reward: -9.52	 Mean_loss: 0.08615967,  training time: 13.59
[685.25, 617.25, 697.25, 651.25]
Episode 266	 reward: -8.68	 Mean_loss: 0.11819436,  training time: 13.58
[653.75, 634.75, 713.0, 637.5]
Episode 267	 reward: -9.17	 Mean_loss: 0.07222764,  training time: 13.56
[667.0, 615.5, 702.0, 648.25]
Episode 268	 reward: -8.89	 Mean_loss: 0.09658944,  training time: 13.59
[694.0, 672.0, 676.75, 645.5]
Episode 269	 reward: -8.98	 Mean_loss: 0.07742044,  training time: 13.61
[617.0, 608.0, 657.25, 686.75]
Episode 270	 reward: -9.98	 Mean_loss: 0.05792924,  training time: 13.58
[678.5, 626.25, 663.25, 676.75]
Episode 271	 reward: -9.63	 Mean_loss: 0.07145657,  training time: 13.56
[660.75, 631.0, 728.0, 661.25]
Episode 272	 reward: -9.19	 Mean_loss: 0.08599624,  training time: 13.58
[645.5, 647.5, 704.25, 656.5]
Episode 273	 reward: -9.72	 Mean_loss: 0.08945729,  training time: 13.60
[648.0, 612.75, 667.25, 671.5]
Episode 274	 reward: -8.95	 Mean_loss: 0.10624134,  training time: 13.64
[658.0, 602.75, 692.0, 692.25]
Episode 275	 reward: -9.10	 Mean_loss: 0.06454560,  training time: 13.57
[638.0, 631.75, 695.5, 625.5]
Episode 276	 reward: -9.30	 Mean_loss: 0.09527845,  training time: 13.60
[658.0, 618.25, 687.75, 670.75]
Episode 277	 reward: -9.04	 Mean_loss: 0.07693528,  training time: 13.63
[639.5, 634.5, 689.25, 615.25]
Episode 278	 reward: -9.19	 Mean_loss: 0.10343877,  training time: 13.54
[639.25, 624.5, 674.0, 679.25]
Episode 279	 reward: -9.89	 Mean_loss: 0.07465940,  training time: 13.63
[660.25, 609.5, 672.75, 632.75]
Episode 280	 reward: -9.02	 Mean_loss: 0.07921828,  training time: 13.62
[556.25, 646.0, 723.0, 642.25]
Episode 281	 reward: -9.62	 Mean_loss: 0.08867225,  training time: 13.67
[619.0, 663.0, 705.5, 641.75]
Episode 282	 reward: -8.95	 Mean_loss: 0.07218914,  training time: 13.60
[615.75, 633.0, 662.0, 660.75]
Episode 283	 reward: -9.53	 Mean_loss: 0.06512117,  training time: 13.56
[573.0, 669.0, 667.75, 653.5]
Episode 284	 reward: -8.74	 Mean_loss: 0.08396588,  training time: 13.55
[598.0, 660.0, 711.75, 658.75]
Episode 285	 reward: -9.36	 Mean_loss: 0.06116080,  training time: 13.57
[591.5, 646.5, 697.25, 678.0]
Episode 286	 reward: -9.45	 Mean_loss: 0.08549099,  training time: 13.61
[581.75, 689.5, 712.5, 650.0]
Episode 287	 reward: -9.24	 Mean_loss: 0.05761251,  training time: 13.55
[572.75, 679.0, 657.0, 695.5]
Episode 288	 reward: -10.01	 Mean_loss: 0.09834985,  training time: 13.57
[594.5, 650.75, 674.5, 652.75]
Episode 289	 reward: -9.63	 Mean_loss: 0.06077372,  training time: 13.59
[563.25, 632.75, 677.0, 665.0]
Episode 290	 reward: -9.72	 Mean_loss: 0.04768477,  training time: 13.56
[589.5, 629.75, 678.0, 668.5]
Episode 291	 reward: -9.96	 Mean_loss: 0.07141645,  training time: 13.57
[554.75, 681.75, 638.5, 636.0]
Episode 292	 reward: -10.10	 Mean_loss: 0.06789001,  training time: 13.56
[568.25, 692.75, 677.75, 658.5]
Episode 293	 reward: -9.24	 Mean_loss: 0.06232374,  training time: 13.55
[581.75, 656.75, 689.75, 643.25]
Episode 294	 reward: -9.97	 Mean_loss: 0.06498380,  training time: 13.57
[567.0, 653.0, 643.0, 611.0]
Episode 295	 reward: -9.77	 Mean_loss: 0.07308246,  training time: 13.66
[557.25, 658.75, 724.75, 668.75]
Episode 296	 reward: -9.07	 Mean_loss: 0.06692625,  training time: 13.54
[557.75, 652.0, 713.25, 671.25]
Episode 297	 reward: -9.41	 Mean_loss: 0.08469364,  training time: 13.53
[594.0, 661.5, 697.0, 634.75]
Episode 298	 reward: -9.84	 Mean_loss: 0.07126115,  training time: 13.54
[618.5, 692.5, 677.0, 667.25]
Episode 299	 reward: -9.52	 Mean_loss: 0.07472500,  training time: 13.53
[567.5, 648.75, 711.25, 689.75]
Episode 300	 reward: -10.08	 Mean_loss: 0.08465284,  training time: 13.55
[603.5, 615.5, 612.75, 650.75]
Episode 301	 reward: -9.21	 Mean_loss: 0.06861404,  training time: 13.58
[663.5, 619.25, 613.75, 651.75]
Episode 302	 reward: -9.34	 Mean_loss: 0.04879028,  training time: 13.58
[645.25, 621.75, 655.75, 635.75]
Episode 303	 reward: -9.06	 Mean_loss: 0.05399901,  training time: 13.58
[656.75, 593.25, 597.0, 681.75]
Episode 304	 reward: -8.95	 Mean_loss: 0.06526353,  training time: 13.56
[633.5, 611.5, 614.5, 646.5]
Episode 305	 reward: -8.51	 Mean_loss: 0.04538272,  training time: 13.58
[632.75, 598.75, 638.25, 651.75]
Episode 306	 reward: -9.32	 Mean_loss: 0.05516952,  training time: 13.55
[650.5, 599.75, 608.0, 623.75]
Episode 307	 reward: -9.22	 Mean_loss: 0.06537962,  training time: 13.58
[659.25, 558.5, 596.75, 657.75]
Episode 308	 reward: -9.09	 Mean_loss: 0.08982589,  training time: 13.57
[644.0, 622.5, 627.0, 655.0]
Episode 309	 reward: -9.42	 Mean_loss: 0.04265625,  training time: 13.56
[661.0, 584.0, 637.5, 643.5]
Episode 310	 reward: -9.08	 Mean_loss: 0.04796056,  training time: 13.59
[643.0, 607.0, 673.0, 639.5]
Episode 311	 reward: -9.18	 Mean_loss: 0.06176946,  training time: 13.58
[645.25, 594.75, 648.75, 646.75]
Episode 312	 reward: -8.67	 Mean_loss: 0.08182482,  training time: 13.54
[658.75, 620.0, 626.25, 621.0]
Episode 313	 reward: -8.71	 Mean_loss: 0.04923018,  training time: 13.54
[637.75, 589.75, 630.75, 665.0]
Episode 314	 reward: -9.43	 Mean_loss: 0.04680961,  training time: 13.57
[634.75, 616.25, 629.25, 649.75]
Episode 315	 reward: -8.49	 Mean_loss: 0.05062827,  training time: 13.56
[628.5, 598.75, 596.75, 675.25]
Episode 316	 reward: -9.05	 Mean_loss: 0.05288047,  training time: 13.61
[634.25, 590.5, 596.75, 621.75]
Episode 317	 reward: -9.09	 Mean_loss: 0.06051937,  training time: 13.56
[636.5, 578.0, 641.0, 632.25]
Episode 318	 reward: -9.01	 Mean_loss: 0.04606877,  training time: 13.58
[686.5, 615.75, 603.25, 631.0]
Episode 319	 reward: -8.61	 Mean_loss: 0.05356585,  training time: 13.57
[638.25, 572.25, 636.0, 670.75]
Episode 320	 reward: -9.26	 Mean_loss: 0.05841127,  training time: 13.53
[609.5, 610.0, 624.25, 667.25]
Episode 321	 reward: -9.47	 Mean_loss: 0.08353392,  training time: 13.59
[619.0, 643.75, 610.25, 674.75]
Episode 322	 reward: -9.34	 Mean_loss: 0.06663194,  training time: 13.53
[625.75, 631.75, 661.5, 686.75]
Episode 323	 reward: -8.79	 Mean_loss: 0.07003181,  training time: 13.54
[625.5, 656.0, 645.25, 703.25]
Episode 324	 reward: -9.86	 Mean_loss: 0.07471730,  training time: 13.54
[609.0, 629.5, 645.75, 650.0]
Episode 325	 reward: -8.93	 Mean_loss: 0.05902587,  training time: 13.59
[590.25, 643.5, 682.25, 657.0]
Episode 326	 reward: -9.31	 Mean_loss: 0.08930292,  training time: 13.54
[592.0, 617.25, 657.0, 698.75]
Episode 327	 reward: -9.49	 Mean_loss: 0.05552599,  training time: 13.56
[586.25, 647.5, 620.75, 686.75]
Episode 328	 reward: -8.98	 Mean_loss: 0.04552984,  training time: 13.55
[585.75, 611.75, 655.0, 684.25]
Episode 329	 reward: -9.31	 Mean_loss: 0.06372751,  training time: 13.54
[603.25, 678.75, 707.25, 711.5]
Episode 330	 reward: -9.71	 Mean_loss: 0.05601805,  training time: 13.56
[617.0, 623.0, 633.5, 701.0]
Episode 331	 reward: -9.13	 Mean_loss: 0.05727740,  training time: 13.53
[622.75, 642.5, 692.25, 721.5]
Episode 332	 reward: -9.61	 Mean_loss: 0.05250007,  training time: 13.61
[623.75, 652.25, 673.75, 640.25]
Episode 333	 reward: -9.32	 Mean_loss: 0.05554447,  training time: 13.56
[604.25, 603.0, 659.75, 667.25]
Episode 334	 reward: -9.75	 Mean_loss: 0.04954911,  training time: 13.57
[586.25, 600.75, 632.75, 661.75]
Episode 335	 reward: -8.76	 Mean_loss: 0.06823657,  training time: 13.58
[615.25, 647.5, 658.25, 704.5]
Episode 336	 reward: -10.43	 Mean_loss: 0.06321335,  training time: 13.57
[641.25, 631.5, 679.25, 667.0]
Episode 337	 reward: -9.25	 Mean_loss: 0.05824182,  training time: 13.60
[604.5, 640.5, 637.25, 686.75]
Episode 338	 reward: -9.61	 Mean_loss: 0.05710570,  training time: 13.53
[586.5, 627.5, 631.75, 644.25]
Episode 339	 reward: -9.56	 Mean_loss: 0.06173221,  training time: 13.53
[622.25, 634.5, 650.5, 661.25]
Episode 340	 reward: -9.50	 Mean_loss: 0.05641370,  training time: 13.56
[607.0, 643.0, 696.0, 743.75]
Episode 341	 reward: -10.06	 Mean_loss: 0.07478268,  training time: 13.61
[583.25, 651.25, 673.5, 753.5]
Episode 342	 reward: -9.11	 Mean_loss: 0.08174385,  training time: 13.56
[632.25, 643.25, 674.5, 699.25]
Episode 343	 reward: -10.27	 Mean_loss: 0.06295909,  training time: 13.58
[605.5, 663.0, 663.25, 687.5]
Episode 344	 reward: -9.57	 Mean_loss: 0.05184736,  training time: 13.57
[610.5, 661.75, 661.75, 697.5]
Episode 345	 reward: -9.57	 Mean_loss: 0.06210535,  training time: 13.54
[588.75, 662.5, 669.75, 725.25]
Episode 346	 reward: -9.74	 Mean_loss: 0.08476788,  training time: 13.53
[648.25, 641.75, 714.75, 732.5]
Episode 347	 reward: -9.17	 Mean_loss: 0.06988894,  training time: 13.52
[625.75, 640.75, 652.75, 715.75]
Episode 348	 reward: -9.22	 Mean_loss: 0.07102870,  training time: 13.53
[591.0, 638.5, 677.75, 730.75]
Episode 349	 reward: -9.21	 Mean_loss: 0.09633965,  training time: 13.50
[647.75, 632.5, 665.25, 713.5]
Episode 350	 reward: -9.29	 Mean_loss: 0.08354617,  training time: 13.49
[599.0, 637.75, 646.75, 720.75]
Episode 351	 reward: -9.50	 Mean_loss: 0.09053397,  training time: 13.53
[630.5, 640.25, 671.0, 733.25]
Episode 352	 reward: -9.70	 Mean_loss: 0.08525137,  training time: 13.50
[626.25, 653.25, 669.25, 705.5]
Episode 353	 reward: -9.57	 Mean_loss: 0.08568066,  training time: 13.48
[633.75, 642.75, 651.0, 726.0]
Episode 354	 reward: -10.02	 Mean_loss: 0.05527457,  training time: 13.48
[617.75, 650.5, 673.75, 697.25]
Episode 355	 reward: -9.66	 Mean_loss: 0.08579408,  training time: 13.53
[656.5, 656.0, 674.75, 698.75]
Episode 356	 reward: -9.77	 Mean_loss: 0.05779862,  training time: 13.49
[626.5, 628.25, 655.75, 754.0]
Episode 357	 reward: -9.96	 Mean_loss: 0.07886382,  training time: 13.51
[606.0, 632.5, 671.75, 716.0]
Episode 358	 reward: -9.96	 Mean_loss: 0.06425098,  training time: 13.58
[626.75, 659.5, 640.5, 710.75]
Episode 359	 reward: -9.77	 Mean_loss: 0.05187489,  training time: 13.44
[615.0, 664.75, 642.75, 718.25]
Episode 360	 reward: -9.64	 Mean_loss: 0.05939355,  training time: 13.51
[636.5, 605.0, 627.5, 720.5]
Episode 361	 reward: -9.64	 Mean_loss: 0.05992320,  training time: 13.51
[627.75, 608.25, 628.25, 724.25]
Episode 362	 reward: -10.24	 Mean_loss: 0.09870204,  training time: 13.48
[606.75, 622.5, 655.25, 699.0]
Episode 363	 reward: -10.10	 Mean_loss: 0.05929173,  training time: 13.49
[628.25, 612.25, 633.75, 707.0]
Episode 364	 reward: -9.95	 Mean_loss: 0.05802107,  training time: 13.46
[616.0, 619.0, 616.5, 698.25]
Episode 365	 reward: -9.70	 Mean_loss: 0.04318543,  training time: 13.47
[579.75, 632.5, 626.0, 681.5]
Episode 366	 reward: -9.74	 Mean_loss: 0.08327018,  training time: 13.52
[595.75, 606.5, 610.75, 698.5]
Episode 367	 reward: -9.91	 Mean_loss: 0.04322522,  training time: 13.51
[596.25, 608.75, 675.5, 682.5]
Episode 368	 reward: -9.59	 Mean_loss: 0.06237349,  training time: 13.50
[596.5, 616.0, 619.0, 672.5]
Episode 369	 reward: -9.89	 Mean_loss: 0.06376844,  training time: 13.48
[586.5, 634.25, 647.0, 679.5]
Episode 370	 reward: -9.37	 Mean_loss: 0.04452603,  training time: 13.48
[595.25, 640.5, 623.25, 722.25]
Episode 371	 reward: -9.26	 Mean_loss: 0.08222656,  training time: 13.50
[597.5, 577.75, 604.75, 712.5]
Episode 372	 reward: -9.30	 Mean_loss: 0.06445481,  training time: 13.47
[606.0, 627.75, 639.25, 699.0]
Episode 373	 reward: -9.65	 Mean_loss: 0.07865495,  training time: 13.53
[618.5, 620.0, 646.0, 710.75]
Episode 374	 reward: -9.31	 Mean_loss: 0.05616036,  training time: 13.48
[613.5, 613.5, 641.5, 730.0]
Episode 375	 reward: -9.68	 Mean_loss: 0.05944781,  training time: 13.48
[606.0, 615.0, 626.75, 696.25]
Episode 376	 reward: -9.32	 Mean_loss: 0.05234828,  training time: 13.51
[621.5, 619.5, 637.75, 681.75]
Episode 377	 reward: -9.93	 Mean_loss: 0.07301750,  training time: 13.54
[604.25, 646.0, 663.5, 690.75]
Episode 378	 reward: -9.73	 Mean_loss: 0.04889291,  training time: 13.53
[606.75, 629.75, 666.0, 709.25]
Episode 379	 reward: -9.96	 Mean_loss: 0.06838304,  training time: 13.56
[606.25, 624.0, 614.5, 683.25]
Episode 380	 reward: -10.11	 Mean_loss: 0.05580490,  training time: 13.49
[599.0, 646.0, 666.0, 674.75]
Episode 381	 reward: -10.20	 Mean_loss: 0.06496117,  training time: 13.52
[585.5, 625.75, 646.0, 667.75]
Episode 382	 reward: -9.25	 Mean_loss: 0.05961862,  training time: 13.53
[572.25, 650.0, 652.0, 683.25]
Episode 383	 reward: -9.54	 Mean_loss: 0.05917053,  training time: 13.52
[581.25, 628.0, 658.75, 680.0]
Episode 384	 reward: -9.07	 Mean_loss: 0.07124236,  training time: 13.49
[604.5, 626.0, 627.25, 662.0]
Episode 385	 reward: -9.55	 Mean_loss: 0.05681076,  training time: 13.47
[611.0, 609.25, 649.0, 673.75]
Episode 386	 reward: -9.79	 Mean_loss: 0.04903956,  training time: 13.51
[625.75, 602.0, 632.25, 695.0]
Episode 387	 reward: -9.75	 Mean_loss: 0.05927174,  training time: 13.51
[616.0, 621.5, 640.5, 656.75]
Episode 388	 reward: -9.49	 Mean_loss: 0.06459837,  training time: 13.48
[582.75, 639.5, 611.75, 674.0]
Episode 389	 reward: -8.95	 Mean_loss: 0.06152471,  training time: 13.47
[589.25, 624.5, 656.5, 682.5]
Episode 390	 reward: -9.01	 Mean_loss: 0.06730127,  training time: 13.49
[625.5, 642.0, 656.5, 679.0]
Episode 391	 reward: -9.86	 Mean_loss: 0.06892554,  training time: 13.46
[560.5, 653.0, 618.25, 681.5]
Episode 392	 reward: -9.20	 Mean_loss: 0.04939875,  training time: 13.47
[617.5, 636.75, 629.25, 677.5]
Episode 393	 reward: -9.66	 Mean_loss: 0.06342959,  training time: 13.51
[598.75, 649.0, 608.0, 640.25]
Episode 394	 reward: -9.47	 Mean_loss: 0.04129863,  training time: 13.47
[559.25, 622.5, 648.75, 652.0]
Episode 395	 reward: -9.73	 Mean_loss: 0.05126496,  training time: 13.46
[606.25, 615.75, 641.25, 687.75]
Episode 396	 reward: -9.32	 Mean_loss: 0.08260588,  training time: 13.49
[598.75, 652.25, 640.25, 668.25]
Episode 397	 reward: -9.02	 Mean_loss: 0.06088172,  training time: 13.46
[583.75, 630.25, 645.25, 677.25]
Episode 398	 reward: -9.59	 Mean_loss: 0.06309169,  training time: 13.50
[617.25, 631.5, 618.75, 663.0]
Episode 399	 reward: -9.05	 Mean_loss: 0.06348608,  training time: 13.51
[592.5, 599.0, 663.25, 684.25]
Episode 400	 reward: -9.94	 Mean_loss: 0.05023501,  training time: 13.56
[614.25, 566.5, 646.75, 707.0]
Episode 401	 reward: -10.05	 Mean_loss: 0.08144540,  training time: 13.52
[598.0, 583.75, 648.0, 662.5]
Episode 402	 reward: -10.22	 Mean_loss: 0.06179376,  training time: 13.52
[581.25, 575.5, 633.0, 699.5]
Episode 403	 reward: -9.98	 Mean_loss: 0.05782970,  training time: 13.46
[606.0, 560.75, 611.75, 651.75]
Episode 404	 reward: -9.27	 Mean_loss: 0.07307429,  training time: 13.51
[578.75, 575.25, 625.5, 658.75]
Episode 405	 reward: -9.52	 Mean_loss: 0.05184174,  training time: 13.53
[585.0, 551.75, 632.5, 655.25]
Episode 406	 reward: -9.84	 Mean_loss: 0.05686726,  training time: 13.48
[586.0, 555.75, 638.0, 714.25]
Episode 407	 reward: -9.86	 Mean_loss: 0.11702444,  training time: 13.48
[601.5, 596.75, 633.75, 680.75]
Episode 408	 reward: -9.63	 Mean_loss: 0.05568476,  training time: 13.49
[586.5, 565.5, 647.25, 660.25]
Episode 409	 reward: -9.32	 Mean_loss: 0.08760828,  training time: 13.48
[565.25, 608.5, 628.25, 715.25]
Episode 410	 reward: -9.79	 Mean_loss: 0.09617025,  training time: 13.53
[596.5, 568.5, 604.0, 670.0]
Episode 411	 reward: -9.96	 Mean_loss: 0.08891829,  training time: 13.52
[597.25, 564.75, 599.5, 673.0]
Episode 412	 reward: -9.79	 Mean_loss: 0.06269637,  training time: 13.48
[581.25, 565.0, 641.75, 653.0]
Episode 413	 reward: -9.71	 Mean_loss: 0.06762192,  training time: 13.51
[569.25, 581.0, 615.75, 718.5]
Episode 414	 reward: -9.15	 Mean_loss: 0.14114204,  training time: 13.51
[613.75, 564.5, 619.25, 661.0]
Episode 415	 reward: -9.27	 Mean_loss: 0.06073258,  training time: 13.50
[565.25, 595.5, 636.5, 691.0]
Episode 416	 reward: -9.45	 Mean_loss: 0.08794428,  training time: 13.48
[588.25, 569.75, 637.5, 681.75]
Episode 417	 reward: -9.36	 Mean_loss: 0.05402073,  training time: 13.53
[575.75, 548.5, 617.5, 675.0]
Episode 418	 reward: -9.86	 Mean_loss: 0.06040914,  training time: 13.50
[614.75, 567.0, 633.25, 687.75]
Episode 419	 reward: -9.48	 Mean_loss: 0.07347305,  training time: 13.51
[572.75, 558.75, 636.0, 685.25]
Episode 420	 reward: -9.12	 Mean_loss: 0.06152133,  training time: 13.54
[591.0, 594.25, 637.0, 625.75]
Episode 421	 reward: -8.70	 Mean_loss: 0.04540958,  training time: 13.66
[564.5, 589.0, 635.75, 631.25]
Episode 422	 reward: -9.30	 Mean_loss: 0.05811929,  training time: 13.48
[607.75, 576.75, 641.5, 636.75]
Episode 423	 reward: -9.25	 Mean_loss: 0.05015889,  training time: 13.51
[593.75, 600.5, 622.25, 631.75]
Episode 424	 reward: -9.11	 Mean_loss: 0.04923090,  training time: 13.48
[591.25, 576.0, 614.75, 600.5]
Episode 425	 reward: -9.53	 Mean_loss: 0.03345193,  training time: 13.52
[596.25, 583.5, 616.25, 596.75]
Episode 426	 reward: -9.88	 Mean_loss: 0.04049475,  training time: 13.51
[592.5, 595.0, 636.0, 610.0]
Episode 427	 reward: -8.83	 Mean_loss: 0.04188068,  training time: 13.49
[625.75, 600.5, 648.0, 600.25]
Episode 428	 reward: -8.86	 Mean_loss: 0.03871998,  training time: 13.53
[592.25, 557.5, 620.5, 624.5]
Episode 429	 reward: -9.57	 Mean_loss: 0.03870190,  training time: 13.50
[598.75, 587.25, 629.25, 633.25]
Episode 430	 reward: -10.19	 Mean_loss: 0.04054285,  training time: 13.51
[593.5, 590.0, 635.25, 615.25]
Episode 431	 reward: -9.38	 Mean_loss: 0.03590927,  training time: 13.57
[577.5, 573.25, 642.5, 633.5]
Episode 432	 reward: -9.29	 Mean_loss: 0.06778788,  training time: 13.50
[593.75, 583.75, 641.0, 608.5]
Episode 433	 reward: -9.78	 Mean_loss: 0.03524765,  training time: 13.50
[594.5, 622.75, 620.0, 637.5]
Episode 434	 reward: -9.87	 Mean_loss: 0.05609661,  training time: 13.50
[617.75, 592.5, 640.5, 621.75]
Episode 435	 reward: -10.55	 Mean_loss: 0.02790166,  training time: 13.52
[604.25, 587.0, 587.25, 607.5]
Episode 436	 reward: -9.01	 Mean_loss: 0.04147904,  training time: 13.50
[588.0, 582.0, 646.25, 610.5]
Episode 437	 reward: -9.26	 Mean_loss: 0.03535404,  training time: 13.51
[587.5, 580.0, 656.0, 607.5]
Episode 438	 reward: -9.26	 Mean_loss: 0.03990519,  training time: 13.49
[583.5, 589.5, 584.5, 622.25]
Episode 439	 reward: -8.83	 Mean_loss: 0.04004891,  training time: 13.48
[568.75, 613.0, 642.25, 582.0]
Episode 440	 reward: -8.80	 Mean_loss: 0.03860704,  training time: 13.51
[598.25, 598.0, 593.75, 649.5]
Episode 441	 reward: -9.18	 Mean_loss: 0.05066723,  training time: 13.52
[565.75, 615.0, 607.25, 645.0]
Episode 442	 reward: -8.95	 Mean_loss: 0.04106244,  training time: 13.59
[538.0, 634.25, 632.5, 661.5]
Episode 443	 reward: -8.91	 Mean_loss: 0.04375441,  training time: 13.51
[592.75, 628.75, 649.0, 665.25]
Episode 444	 reward: -9.93	 Mean_loss: 0.05210866,  training time: 13.51
[564.5, 615.75, 636.5, 622.25]
Episode 445	 reward: -9.71	 Mean_loss: 0.04212223,  training time: 13.51
[546.75, 632.0, 607.25, 637.75]
Episode 446	 reward: -9.03	 Mean_loss: 0.03427031,  training time: 13.54
[566.25, 646.0, 612.0, 667.5]
Episode 447	 reward: -9.91	 Mean_loss: 0.03950696,  training time: 13.55
[568.25, 640.25, 650.5, 641.5]
Episode 448	 reward: -9.04	 Mean_loss: 0.04569951,  training time: 13.52
[565.5, 605.5, 611.75, 628.75]
Episode 449	 reward: -9.27	 Mean_loss: 0.04004999,  training time: 13.51
[611.0, 622.25, 623.75, 621.5]
Episode 450	 reward: -9.89	 Mean_loss: 0.05422989,  training time: 13.52
[593.75, 647.25, 633.75, 662.25]
Episode 451	 reward: -9.62	 Mean_loss: 0.04992263,  training time: 13.57
[569.75, 645.75, 633.5, 637.5]
Episode 452	 reward: -9.84	 Mean_loss: 0.05558782,  training time: 13.56
[569.0, 630.25, 621.5, 629.25]
Episode 453	 reward: -9.95	 Mean_loss: 0.06891617,  training time: 13.50
[555.5, 624.75, 620.0, 643.75]
Episode 454	 reward: -9.08	 Mean_loss: 0.06421123,  training time: 13.52
[568.75, 637.0, 617.5, 675.5]
Episode 455	 reward: -10.78	 Mean_loss: 0.05804094,  training time: 13.51
[578.75, 637.5, 620.0, 636.5]
Episode 456	 reward: -9.25	 Mean_loss: 0.05634676,  training time: 13.53
[564.0, 632.25, 643.25, 640.0]
Episode 457	 reward: -8.86	 Mean_loss: 0.04563145,  training time: 13.54
[572.5, 653.0, 594.75, 630.5]
Episode 458	 reward: -9.19	 Mean_loss: 0.05220202,  training time: 13.50
[553.5, 663.5, 641.0, 654.5]
Episode 459	 reward: -9.37	 Mean_loss: 0.04730155,  training time: 13.53
[564.5, 621.0, 627.5, 645.0]
Episode 460	 reward: -9.43	 Mean_loss: 0.05793104,  training time: 13.54
[630.5, 581.25, 634.75, 631.0]
Episode 461	 reward: -9.28	 Mean_loss: 0.05313899,  training time: 13.58
[632.5, 576.0, 605.5, 638.5]
Episode 462	 reward: -9.08	 Mean_loss: 0.04560120,  training time: 13.53
[602.0, 591.75, 642.5, 612.0]
Episode 463	 reward: -9.22	 Mean_loss: 0.04026133,  training time: 13.60
[644.5, 607.0, 639.0, 620.25]
Episode 464	 reward: -8.99	 Mean_loss: 0.05198321,  training time: 13.51
[648.5, 595.75, 639.0, 615.5]
Episode 465	 reward: -8.72	 Mean_loss: 0.06257703,  training time: 13.51
[592.75, 571.5, 627.0, 645.75]
Episode 466	 reward: -9.03	 Mean_loss: 0.07740767,  training time: 13.52
[621.25, 613.75, 640.75, 616.5]
Episode 467	 reward: -8.86	 Mean_loss: 0.07257043,  training time: 13.50
[620.5, 595.75, 665.0, 623.5]
Episode 468	 reward: -8.65	 Mean_loss: 0.06177219,  training time: 13.50
[619.75, 600.5, 619.0, 680.25]
Episode 469	 reward: -9.02	 Mean_loss: 0.08963792,  training time: 13.55
[622.5, 635.25, 629.5, 599.75]
Episode 470	 reward: -9.57	 Mean_loss: 0.04064840,  training time: 13.54
[625.25, 596.75, 648.5, 635.0]
Episode 471	 reward: -9.39	 Mean_loss: 0.06566764,  training time: 13.52
[571.75, 592.0, 632.25, 617.0]
Episode 472	 reward: -8.68	 Mean_loss: 0.05473944,  training time: 13.48
[632.0, 623.25, 625.0, 606.5]
Episode 473	 reward: -9.06	 Mean_loss: 0.05644128,  training time: 13.52
[615.25, 592.5, 629.0, 644.0]
Episode 474	 reward: -8.78	 Mean_loss: 0.06299233,  training time: 13.52
[610.75, 615.25, 613.75, 627.25]
Episode 475	 reward: -9.44	 Mean_loss: 0.04601993,  training time: 13.52
[611.25, 619.0, 678.0, 656.5]
Episode 476	 reward: -8.36	 Mean_loss: 0.09723871,  training time: 13.49
[625.5, 583.25, 585.0, 633.5]
Episode 477	 reward: -8.75	 Mean_loss: 0.06927624,  training time: 13.51
[630.25, 594.75, 613.5, 605.0]
Episode 478	 reward: -9.15	 Mean_loss: 0.04800563,  training time: 13.48
[601.25, 572.75, 637.75, 638.5]
Episode 479	 reward: -8.65	 Mean_loss: 0.04253694,  training time: 13.52
[619.25, 575.25, 627.25, 596.25]
Episode 480	 reward: -9.19	 Mean_loss: 0.05603731,  training time: 13.50
[633.75, 549.0, 599.25, 712.5]
Episode 481	 reward: -9.35	 Mean_loss: 0.08198508,  training time: 13.56
[625.75, 533.0, 624.75, 660.5]
Episode 482	 reward: -9.53	 Mean_loss: 0.05420256,  training time: 13.52
[648.75, 536.75, 625.25, 650.25]
Episode 483	 reward: -9.91	 Mean_loss: 0.06881405,  training time: 13.52
[610.5, 533.75, 602.0, 645.0]
Episode 484	 reward: -9.26	 Mean_loss: 0.06304477,  training time: 13.56
[598.25, 564.5, 609.0, 641.5]
Episode 485	 reward: -9.57	 Mean_loss: 0.06957480,  training time: 13.48
[620.0, 556.0, 624.0, 668.75]
Episode 486	 reward: -10.49	 Mean_loss: 0.03833867,  training time: 13.53
[610.0, 552.75, 642.75, 650.0]
Episode 487	 reward: -8.78	 Mean_loss: 0.04892638,  training time: 13.54
[620.75, 551.25, 645.75, 674.25]
Episode 488	 reward: -9.39	 Mean_loss: 0.06489831,  training time: 13.53
[614.0, 527.75, 638.0, 662.75]
Episode 489	 reward: -9.62	 Mean_loss: 0.06123525,  training time: 13.52
[621.75, 546.0, 639.5, 648.25]
Episode 490	 reward: -9.44	 Mean_loss: 0.04479192,  training time: 13.53
[601.5, 580.75, 656.5, 678.75]
Episode 491	 reward: -9.45	 Mean_loss: 0.08900087,  training time: 13.54
[603.0, 522.25, 613.0, 650.25]
Episode 492	 reward: -9.44	 Mean_loss: 0.07283775,  training time: 13.52
[611.0, 551.25, 630.25, 693.0]
Episode 493	 reward: -9.82	 Mean_loss: 0.05811909,  training time: 13.51
[627.25, 544.75, 632.25, 639.75]
Episode 494	 reward: -9.73	 Mean_loss: 0.07504321,  training time: 13.50
[640.0, 540.5, 605.0, 655.5]
Episode 495	 reward: -9.29	 Mean_loss: 0.03661574,  training time: 13.48
[610.25, 539.0, 623.25, 641.75]
Episode 496	 reward: -9.82	 Mean_loss: 0.06308329,  training time: 13.48
[613.5, 575.25, 626.75, 657.5]
Episode 497	 reward: -9.46	 Mean_loss: 0.06852999,  training time: 13.50
[606.5, 558.0, 606.0, 652.25]
Episode 498	 reward: -8.85	 Mean_loss: 0.06082972,  training time: 13.48
[590.75, 554.25, 629.5, 650.5]
Episode 499	 reward: -9.95	 Mean_loss: 0.05085884,  training time: 13.51
[608.5, 545.25, 648.5, 659.5]
Episode 500	 reward: -9.40	 Mean_loss: 0.05255318,  training time: 13.52
[569.25, 595.5, 632.25, 663.75]
Episode 501	 reward: -9.86	 Mean_loss: 0.05311716,  training time: 13.57
[592.25, 602.0, 641.5, 664.75]
Episode 502	 reward: -9.94	 Mean_loss: 0.04147514,  training time: 13.50
[547.0, 576.0, 603.25, 651.5]
Episode 503	 reward: -9.27	 Mean_loss: 0.04780953,  training time: 13.53
[601.25, 577.0, 587.5, 687.0]
Episode 504	 reward: -9.68	 Mean_loss: 0.03946906,  training time: 13.51
[554.25, 622.25, 658.25, 677.25]
Episode 505	 reward: -9.83	 Mean_loss: 0.06858315,  training time: 13.62
[565.0, 622.5, 601.25, 699.25]
Episode 506	 reward: -10.00	 Mean_loss: 0.06177839,  training time: 13.55
[552.5, 588.5, 633.75, 661.75]
Episode 507	 reward: -9.75	 Mean_loss: 0.03823379,  training time: 13.51
[568.75, 611.5, 649.25, 667.5]
Episode 508	 reward: -9.19	 Mean_loss: 0.05247525,  training time: 13.48
[540.75, 589.0, 611.25, 677.0]
Episode 509	 reward: -9.57	 Mean_loss: 0.04641739,  training time: 13.50
[580.75, 586.75, 637.75, 721.25]
Episode 510	 reward: -9.30	 Mean_loss: 0.08492047,  training time: 13.51
[576.5, 593.25, 640.0, 669.5]
Episode 511	 reward: -9.35	 Mean_loss: 0.05982092,  training time: 13.51
[549.0, 611.25, 643.5, 688.0]
Episode 512	 reward: -9.81	 Mean_loss: 0.04013747,  training time: 13.48
[552.0, 583.0, 631.25, 683.25]
Episode 513	 reward: -9.38	 Mean_loss: 0.05174077,  training time: 13.52
[601.5, 595.25, 662.75, 695.5]
Episode 514	 reward: -10.49	 Mean_loss: 0.06807040,  training time: 13.52
[553.75, 598.75, 619.5, 694.0]
Episode 515	 reward: -9.23	 Mean_loss: 0.04580020,  training time: 13.53
[568.0, 590.75, 626.75, 664.5]
Episode 516	 reward: -8.87	 Mean_loss: 0.03482284,  training time: 13.50
[579.0, 568.25, 647.0, 663.75]
Episode 517	 reward: -10.38	 Mean_loss: 0.03524402,  training time: 13.48
[550.25, 592.25, 642.5, 681.0]
Episode 518	 reward: -9.32	 Mean_loss: 0.04576087,  training time: 13.53
[564.75, 604.0, 618.25, 663.5]
Episode 519	 reward: -9.94	 Mean_loss: 0.05712565,  training time: 13.48
[587.25, 600.25, 609.75, 643.75]
Episode 520	 reward: -10.34	 Mean_loss: 0.05850895,  training time: 13.51
[573.0, 569.25, 653.25, 664.25]
Episode 521	 reward: -9.44	 Mean_loss: 0.03642011,  training time: 13.57
[557.75, 601.0, 631.25, 633.0]
Episode 522	 reward: -9.51	 Mean_loss: 0.04170851,  training time: 13.53
[586.0, 561.75, 630.75, 670.5]
Episode 523	 reward: -8.83	 Mean_loss: 0.06344391,  training time: 13.49
[565.5, 561.75, 602.75, 637.75]
Episode 524	 reward: -9.32	 Mean_loss: 0.04067850,  training time: 13.51
[563.75, 541.75, 611.0, 677.75]
Episode 525	 reward: -9.63	 Mean_loss: 0.04779195,  training time: 13.49
[599.75, 584.75, 612.5, 675.25]
Episode 526	 reward: -9.74	 Mean_loss: 0.05130523,  training time: 13.59
[578.0, 522.25, 589.5, 650.5]
Episode 527	 reward: -9.61	 Mean_loss: 0.04555735,  training time: 13.50
[555.0, 538.5, 614.5, 661.5]
Episode 528	 reward: -9.79	 Mean_loss: 0.05585135,  training time: 13.50
[590.25, 540.5, 638.25, 638.25]
Episode 529	 reward: -9.35	 Mean_loss: 0.04029521,  training time: 13.50
[576.5, 571.75, 642.5, 708.75]
Episode 530	 reward: -10.03	 Mean_loss: 0.07285018,  training time: 13.53
[585.25, 557.0, 606.5, 677.75]
Episode 531	 reward: -10.00	 Mean_loss: 0.05472547,  training time: 13.50
[579.25, 549.75, 633.5, 638.25]
Episode 532	 reward: -9.75	 Mean_loss: 0.04456438,  training time: 13.50
[589.5, 533.25, 593.75, 669.0]
Episode 533	 reward: -9.63	 Mean_loss: 0.04496054,  training time: 13.51
[575.75, 571.5, 648.25, 669.0]
Episode 534	 reward: -9.38	 Mean_loss: 0.05499354,  training time: 13.50
[577.75, 551.5, 628.0, 679.25]
Episode 535	 reward: -9.27	 Mean_loss: 0.03592044,  training time: 13.53
[586.0, 560.0, 630.0, 648.5]
Episode 536	 reward: -9.74	 Mean_loss: 0.04471896,  training time: 13.50
[586.75, 573.75, 597.75, 647.0]
Episode 537	 reward: -9.98	 Mean_loss: 0.04964604,  training time: 13.50
[578.75, 594.25, 632.75, 667.25]
Episode 538	 reward: -9.71	 Mean_loss: 0.04465937,  training time: 13.53
[609.25, 537.0, 656.0, 678.0]
Episode 539	 reward: -9.58	 Mean_loss: 0.07135040,  training time: 13.49
[573.5, 538.75, 602.25, 638.5]
Episode 540	 reward: -10.26	 Mean_loss: 0.04672973,  training time: 13.49
[556.0, 611.5, 634.0, 672.75]
Episode 541	 reward: -8.73	 Mean_loss: 0.03535154,  training time: 13.57
[590.5, 574.75, 633.25, 670.75]
Episode 542	 reward: -9.52	 Mean_loss: 0.04338869,  training time: 13.51
[582.25, 607.5, 632.0, 663.0]
Episode 543	 reward: -9.70	 Mean_loss: 0.04360136,  training time: 13.51
[573.75, 618.5, 650.25, 686.5]
Episode 544	 reward: -9.75	 Mean_loss: 0.03296334,  training time: 13.53
[592.0, 593.25, 631.75, 655.75]
Episode 545	 reward: -9.68	 Mean_loss: 0.03803253,  training time: 13.52
[594.0, 604.0, 654.75, 650.75]
Episode 546	 reward: -9.22	 Mean_loss: 0.06101066,  training time: 13.50
[586.75, 563.5, 639.75, 668.0]
Episode 547	 reward: -9.88	 Mean_loss: 0.04419495,  training time: 13.59
[576.0, 570.25, 637.75, 664.5]
Episode 548	 reward: -9.68	 Mean_loss: 0.04869706,  training time: 13.50
[591.25, 589.0, 667.5, 655.5]
Episode 549	 reward: -9.60	 Mean_loss: 0.05483533,  training time: 13.50
[583.0, 561.75, 659.25, 695.75]
Episode 550	 reward: -9.82	 Mean_loss: 0.04380836,  training time: 13.49
[579.5, 611.0, 645.0, 696.25]
Episode 551	 reward: -9.64	 Mean_loss: 0.03763020,  training time: 13.50
[577.25, 598.75, 617.75, 693.75]
Episode 552	 reward: -9.49	 Mean_loss: 0.03489249,  training time: 13.51
[573.75, 598.5, 656.75, 681.0]
Episode 553	 reward: -9.90	 Mean_loss: 0.04650225,  training time: 13.53
[579.75, 591.0, 652.25, 676.0]
Episode 554	 reward: -9.03	 Mean_loss: 0.04441645,  training time: 13.48
[588.0, 584.25, 601.5, 680.25]
Episode 555	 reward: -9.24	 Mean_loss: 0.05220853,  training time: 13.53
[558.25, 615.5, 617.25, 651.25]
Episode 556	 reward: -9.57	 Mean_loss: 0.04200463,  training time: 13.48
[565.0, 586.0, 675.25, 682.5]
Episode 557	 reward: -9.58	 Mean_loss: 0.05075672,  training time: 13.48
[580.0, 574.0, 633.0, 660.0]
Episode 558	 reward: -9.50	 Mean_loss: 0.04613800,  training time: 13.47
[569.5, 589.5, 624.5, 700.5]
Episode 559	 reward: -9.40	 Mean_loss: 0.04879030,  training time: 13.48
[593.0, 601.75, 644.0, 680.5]
Episode 560	 reward: -9.06	 Mean_loss: 0.04863210,  training time: 13.51
[631.0, 604.75, 619.25, 639.5]
Episode 561	 reward: -9.90	 Mean_loss: 0.04577008,  training time: 13.56
[593.5, 583.5, 617.25, 649.5]
Episode 562	 reward: -10.30	 Mean_loss: 0.04882529,  training time: 13.53
[592.25, 608.5, 625.0, 667.5]
Episode 563	 reward: -9.42	 Mean_loss: 0.04371129,  training time: 13.49
[574.25, 562.0, 660.25, 638.0]
Episode 564	 reward: -9.60	 Mean_loss: 0.03058767,  training time: 13.51
[624.5, 602.25, 597.0, 648.0]
Episode 565	 reward: -9.60	 Mean_loss: 0.02809635,  training time: 13.52
[624.25, 611.0, 601.75, 646.5]
Episode 566	 reward: -9.40	 Mean_loss: 0.02191164,  training time: 13.50
[618.75, 601.75, 628.5, 667.0]
Episode 567	 reward: -9.43	 Mean_loss: 0.04217421,  training time: 13.48
[592.0, 630.0, 668.25, 660.25]
Episode 568	 reward: -9.12	 Mean_loss: 0.04658844,  training time: 13.58
[609.25, 578.0, 666.75, 632.0]
Episode 569	 reward: -9.33	 Mean_loss: 0.05125558,  training time: 13.50
[625.75, 611.25, 632.0, 631.5]
Episode 570	 reward: -9.01	 Mean_loss: 0.04221310,  training time: 13.51
[603.75, 586.75, 628.0, 629.0]
Episode 571	 reward: -9.74	 Mean_loss: 0.04781195,  training time: 13.49
[626.5, 584.25, 635.75, 636.75]
Episode 572	 reward: -10.22	 Mean_loss: 0.04000213,  training time: 13.51
[610.5, 597.25, 623.5, 636.5]
Episode 573	 reward: -9.81	 Mean_loss: 0.05055546,  training time: 13.50
[588.75, 600.25, 604.75, 645.5]
Episode 574	 reward: -9.68	 Mean_loss: 0.03974959,  training time: 13.51
[632.5, 597.25, 597.75, 653.75]
Episode 575	 reward: -8.84	 Mean_loss: 0.02056777,  training time: 13.49
[586.75, 611.75, 607.5, 669.0]
Episode 576	 reward: -9.53	 Mean_loss: 0.05824324,  training time: 13.51
[596.5, 597.0, 654.75, 644.25]
Episode 577	 reward: -9.62	 Mean_loss: 0.05708988,  training time: 13.52
[604.25, 635.75, 637.0, 645.5]
Episode 578	 reward: -9.19	 Mean_loss: 0.02729235,  training time: 13.49
[604.5, 612.0, 627.5, 645.0]
Episode 579	 reward: -9.55	 Mean_loss: 0.04262362,  training time: 13.47
[620.5, 570.5, 638.5, 632.75]
Episode 580	 reward: -9.67	 Mean_loss: 0.04083440,  training time: 13.50
[611.5, 598.5, 626.5, 637.5]
Episode 581	 reward: -9.39	 Mean_loss: 0.03400370,  training time: 13.53
[634.75, 615.0, 647.75, 616.75]
Episode 582	 reward: -8.66	 Mean_loss: 0.02660581,  training time: 13.53
[626.25, 571.75, 623.25, 628.75]
Episode 583	 reward: -9.08	 Mean_loss: 0.05192662,  training time: 13.48
[609.0, 575.75, 639.75, 630.25]
Episode 584	 reward: -8.77	 Mean_loss: 0.04760474,  training time: 13.53
[614.5, 598.0, 608.75, 616.25]
Episode 585	 reward: -9.25	 Mean_loss: 0.04658071,  training time: 13.52
[615.5, 576.75, 643.0, 634.25]
Episode 586	 reward: -8.90	 Mean_loss: 0.04917039,  training time: 13.50
[641.5, 564.25, 641.5, 591.25]
Episode 587	 reward: -9.26	 Mean_loss: 0.04100370,  training time: 13.48
[624.0, 604.25, 676.25, 622.75]
Episode 588	 reward: -8.78	 Mean_loss: 0.05486232,  training time: 13.49
[624.0, 582.25, 629.5, 652.25]
Episode 589	 reward: -9.40	 Mean_loss: 0.04378793,  training time: 13.59
[613.75, 599.5, 634.75, 627.0]
Episode 590	 reward: -9.06	 Mean_loss: 0.05562470,  training time: 13.54
[588.25, 574.5, 635.5, 656.0]
Episode 591	 reward: -8.77	 Mean_loss: 0.03724498,  training time: 13.43
[616.25, 572.75, 619.75, 631.5]
Episode 592	 reward: -10.06	 Mean_loss: 0.06691082,  training time: 13.42
[606.75, 589.25, 641.0, 643.25]
Episode 593	 reward: -9.88	 Mean_loss: 0.03465999,  training time: 13.45
[598.75, 575.75, 636.25, 666.25]
Episode 594	 reward: -8.92	 Mean_loss: 0.03477984,  training time: 13.42
[608.0, 611.5, 618.0, 655.5]
Episode 595	 reward: -9.26	 Mean_loss: 0.03196558,  training time: 13.43
[609.25, 587.25, 631.0, 659.25]
Episode 596	 reward: -9.00	 Mean_loss: 0.04823350,  training time: 13.38
[601.75, 576.25, 662.75, 644.5]
Episode 597	 reward: -8.80	 Mean_loss: 0.03368728,  training time: 13.43
[618.0, 619.5, 641.25, 622.25]
Episode 598	 reward: -9.14	 Mean_loss: 0.04062851,  training time: 13.38
[629.25, 557.5, 640.75, 616.5]
Episode 599	 reward: -9.85	 Mean_loss: 0.02625373,  training time: 13.41
[624.0, 592.0, 673.75, 635.25]
Episode 600	 reward: -8.90	 Mean_loss: 0.05596142,  training time: 13.41
[591.75, 614.0, 649.75, 680.5]
Episode 601	 reward: -9.31	 Mean_loss: 0.04545239,  training time: 13.48
[591.75, 621.25, 632.75, 654.25]
Episode 602	 reward: -9.39	 Mean_loss: 0.03644231,  training time: 13.45
[582.0, 616.0, 622.25, 664.25]
Episode 603	 reward: -9.71	 Mean_loss: 0.04023300,  training time: 13.42
[568.0, 590.25, 636.5, 655.75]
Episode 604	 reward: -9.37	 Mean_loss: 0.05200498,  training time: 13.42
[652.0, 604.0, 599.5, 672.75]
Episode 605	 reward: -8.82	 Mean_loss: 0.05280332,  training time: 13.45
[586.75, 601.5, 622.0, 676.25]
Episode 606	 reward: -9.68	 Mean_loss: 0.03764797,  training time: 13.42
[562.0, 608.5, 613.25, 668.25]
Episode 607	 reward: -9.05	 Mean_loss: 0.04829452,  training time: 13.42
[579.5, 590.75, 608.75, 678.75]
Episode 608	 reward: -9.05	 Mean_loss: 0.04477856,  training time: 13.39
[618.25, 597.25, 655.0, 637.5]
Episode 609	 reward: -9.53	 Mean_loss: 0.04224405,  training time: 13.38
[596.5, 562.0, 653.75, 634.75]
Episode 610	 reward: -9.68	 Mean_loss: 0.04542013,  training time: 13.51
[596.75, 632.25, 671.0, 623.5]
Episode 611	 reward: -9.94	 Mean_loss: 0.03053031,  training time: 13.40
[598.5, 584.0, 627.0, 641.75]
Episode 612	 reward: -9.23	 Mean_loss: 0.05037205,  training time: 13.44
[613.75, 593.5, 668.0, 674.5]
Episode 613	 reward: -9.37	 Mean_loss: 0.05926916,  training time: 13.44
[581.75, 610.75, 648.0, 647.75]
Episode 614	 reward: -9.13	 Mean_loss: 0.04447844,  training time: 13.42
[585.25, 569.75, 627.75, 666.25]
Episode 615	 reward: -9.73	 Mean_loss: 0.05512850,  training time: 13.42
[594.75, 591.0, 629.0, 688.5]
Episode 616	 reward: -8.97	 Mean_loss: 0.03507678,  training time: 13.44
[611.75, 594.5, 656.0, 670.0]
Episode 617	 reward: -9.31	 Mean_loss: 0.03981888,  training time: 13.41
[574.5, 595.25, 628.75, 666.0]
Episode 618	 reward: -9.00	 Mean_loss: 0.05334388,  training time: 13.42
[599.75, 609.25, 661.5, 636.0]
Episode 619	 reward: -9.49	 Mean_loss: 0.05181867,  training time: 13.43
[592.0, 607.25, 633.5, 671.25]
Episode 620	 reward: -9.30	 Mean_loss: 0.04673695,  training time: 13.44
[602.75, 552.75, 651.75, 634.25]
Episode 621	 reward: -9.35	 Mean_loss: 0.04700981,  training time: 13.50
[571.25, 580.0, 671.0, 631.0]
Episode 622	 reward: -8.62	 Mean_loss: 0.03741029,  training time: 13.47
[579.25, 546.0, 601.75, 636.25]
Episode 623	 reward: -9.55	 Mean_loss: 0.04049664,  training time: 13.41
[536.75, 568.25, 638.75, 633.25]
Episode 624	 reward: -9.71	 Mean_loss: 0.04584731,  training time: 13.43
[592.5, 569.0, 613.75, 643.75]
Episode 625	 reward: -9.35	 Mean_loss: 0.03925842,  training time: 13.42
[564.25, 583.0, 625.75, 640.75]
Episode 626	 reward: -9.88	 Mean_loss: 0.03524803,  training time: 13.49
[573.5, 551.5, 642.75, 656.5]
Episode 627	 reward: -9.09	 Mean_loss: 0.04869307,  training time: 13.43
[589.0, 560.0, 678.25, 617.0]
Episode 628	 reward: -8.76	 Mean_loss: 0.03736758,  training time: 13.42
[552.75, 598.75, 640.5, 658.5]
Episode 629	 reward: -9.65	 Mean_loss: 0.03056383,  training time: 13.41
[592.0, 570.25, 609.75, 633.0]
Episode 630	 reward: -9.75	 Mean_loss: 0.04116223,  training time: 13.43
[578.0, 573.0, 626.5, 646.5]
Episode 631	 reward: -9.96	 Mean_loss: 0.02886589,  training time: 13.50
[559.75, 550.75, 626.5, 620.25]
Episode 632	 reward: -9.43	 Mean_loss: 0.02823246,  training time: 13.44
[592.25, 567.75, 653.75, 648.5]
Episode 633	 reward: -9.84	 Mean_loss: 0.05197527,  training time: 13.46
[559.25, 573.25, 666.0, 681.0]
Episode 634	 reward: -9.55	 Mean_loss: 0.03580853,  training time: 13.42
[588.5, 571.5, 652.75, 656.25]
Episode 635	 reward: -9.16	 Mean_loss: 0.02977042,  training time: 13.43
[585.5, 581.25, 634.5, 632.5]
Episode 636	 reward: -9.89	 Mean_loss: 0.03165427,  training time: 13.44
[535.25, 574.0, 618.75, 623.5]
Episode 637	 reward: -9.48	 Mean_loss: 0.04102534,  training time: 13.43
[556.25, 567.75, 622.25, 613.5]
Episode 638	 reward: -9.08	 Mean_loss: 0.05601946,  training time: 13.41
[544.75, 567.75, 633.0, 625.5]
Episode 639	 reward: -9.29	 Mean_loss: 0.04036114,  training time: 13.42
[556.75, 576.75, 637.75, 662.0]
Episode 640	 reward: -9.53	 Mean_loss: 0.04316663,  training time: 13.44
[575.25, 551.75, 649.75, 663.5]
Episode 641	 reward: -8.91	 Mean_loss: 0.06017792,  training time: 13.46
[568.25, 569.25, 663.0, 672.5]
Episode 642	 reward: -9.05	 Mean_loss: 0.02853706,  training time: 13.42
[585.0, 544.75, 662.75, 703.5]
Episode 643	 reward: -8.97	 Mean_loss: 0.06273191,  training time: 13.47
[583.0, 534.75, 624.75, 653.75]
Episode 644	 reward: -10.20	 Mean_loss: 0.03603421,  training time: 13.48
[601.5, 537.25, 605.5, 689.0]
Episode 645	 reward: -9.76	 Mean_loss: 0.05804105,  training time: 13.44
[583.25, 562.0, 635.5, 673.75]
Episode 646	 reward: -9.81	 Mean_loss: 0.04080271,  training time: 13.41
[600.5, 542.0, 647.5, 672.5]
Episode 647	 reward: -10.49	 Mean_loss: 0.06770945,  training time: 13.47
[621.75, 564.25, 662.0, 679.0]
Episode 648	 reward: -9.82	 Mean_loss: 0.04271317,  training time: 13.44
[597.5, 566.75, 642.0, 671.5]
Episode 649	 reward: -9.74	 Mean_loss: 0.04263380,  training time: 13.44
[610.5, 528.0, 629.75, 666.75]
Episode 650	 reward: -9.36	 Mean_loss: 0.04504228,  training time: 13.38
[584.5, 564.5, 651.5, 682.75]
Episode 651	 reward: -9.19	 Mean_loss: 0.03250024,  training time: 13.41
[596.0, 566.0, 658.0, 699.5]
Episode 652	 reward: -9.85	 Mean_loss: 0.06300392,  training time: 13.47
[592.25, 546.5, 645.0, 643.25]
Episode 653	 reward: -9.82	 Mean_loss: 0.05312051,  training time: 13.40
[580.25, 535.5, 646.5, 676.75]
Episode 654	 reward: -9.14	 Mean_loss: 0.03951139,  training time: 13.40
[601.0, 559.25, 633.25, 658.25]
Episode 655	 reward: -8.91	 Mean_loss: 0.03691762,  training time: 13.45
[586.75, 524.25, 627.5, 676.0]
Episode 656	 reward: -9.35	 Mean_loss: 0.03816704,  training time: 13.42
[592.5, 532.75, 621.25, 667.25]
Episode 657	 reward: -9.14	 Mean_loss: 0.06382350,  training time: 13.42
[602.0, 524.25, 641.75, 693.25]
Episode 658	 reward: -9.72	 Mean_loss: 0.04941988,  training time: 13.43
[574.0, 529.25, 638.0, 693.75]
Episode 659	 reward: -9.53	 Mean_loss: 0.05753285,  training time: 13.39
[585.25, 545.5, 666.0, 680.75]
Episode 660	 reward: -9.65	 Mean_loss: 0.05295563,  training time: 13.41
[558.25, 637.25, 570.25, 648.25]
Episode 661	 reward: -9.65	 Mean_loss: 0.03732753,  training time: 13.47
[550.5, 659.5, 567.5, 655.0]
Episode 662	 reward: -9.16	 Mean_loss: 0.04621356,  training time: 13.40
[568.5, 609.25, 578.0, 602.75]
Episode 663	 reward: -9.00	 Mean_loss: 0.03726994,  training time: 13.40
[565.25, 640.75, 580.75, 639.5]
Episode 664	 reward: -10.23	 Mean_loss: 0.04994022,  training time: 13.33
[560.25, 640.0, 587.75, 642.5]
Episode 665	 reward: -8.91	 Mean_loss: 0.06179185,  training time: 13.53
[574.5, 653.5, 563.25, 649.75]
Episode 666	 reward: -9.10	 Mean_loss: 0.03866521,  training time: 13.55
[575.25, 610.25, 541.75, 615.75]
Episode 667	 reward: -9.29	 Mean_loss: 0.03413818,  training time: 13.39
[566.0, 627.5, 577.0, 649.0]
Episode 668	 reward: -9.60	 Mean_loss: 0.04009527,  training time: 13.40
[572.0, 621.75, 566.5, 649.25]
Episode 669	 reward: -9.58	 Mean_loss: 0.03653068,  training time: 13.43
[561.5, 635.25, 613.0, 648.75]
Episode 670	 reward: -9.43	 Mean_loss: 0.03790070,  training time: 13.41
[568.25, 637.75, 562.5, 626.5]
Episode 671	 reward: -8.92	 Mean_loss: 0.03225778,  training time: 13.44
[541.0, 623.25, 582.0, 651.5]
Episode 672	 reward: -9.77	 Mean_loss: 0.06175468,  training time: 13.37
[573.0, 617.5, 559.5, 650.75]
Episode 673	 reward: -9.43	 Mean_loss: 0.03986174,  training time: 13.47
[575.0, 622.25, 570.5, 679.5]
Episode 674	 reward: -9.37	 Mean_loss: 0.05939009,  training time: 13.38
[573.75, 615.0, 561.5, 632.75]
Episode 675	 reward: -9.71	 Mean_loss: 0.07228535,  training time: 13.39
[557.5, 638.5, 574.5, 644.0]
Episode 676	 reward: -9.36	 Mean_loss: 0.02920484,  training time: 13.38
[571.0, 622.0, 595.5, 626.75]
Episode 677	 reward: -9.85	 Mean_loss: 0.05061210,  training time: 13.38
[559.5, 657.25, 576.5, 661.75]
Episode 678	 reward: -8.77	 Mean_loss: 0.05427611,  training time: 13.39
[558.75, 605.5, 570.5, 627.0]
Episode 679	 reward: -9.16	 Mean_loss: 0.04004952,  training time: 13.41
[573.75, 639.75, 589.0, 642.25]
Episode 680	 reward: -9.64	 Mean_loss: 0.04090214,  training time: 13.44
[564.5, 588.0, 664.5, 667.75]
Episode 681	 reward: -9.74	 Mean_loss: 0.04386263,  training time: 13.48
[563.25, 607.25, 610.25, 667.75]
Episode 682	 reward: -9.35	 Mean_loss: 0.07362465,  training time: 13.40
[566.75, 605.75, 617.25, 654.0]
Episode 683	 reward: -8.76	 Mean_loss: 0.04190226,  training time: 13.43
[550.75, 607.0, 633.25, 673.25]
Episode 684	 reward: -9.12	 Mean_loss: 0.04822483,  training time: 13.40
[548.5, 589.0, 611.25, 654.0]
Episode 685	 reward: -9.19	 Mean_loss: 0.04817289,  training time: 13.38
[570.25, 620.5, 647.5, 705.5]
Episode 686	 reward: -9.02	 Mean_loss: 0.10491689,  training time: 13.41
[555.0, 623.25, 652.25, 639.75]
Episode 687	 reward: -9.53	 Mean_loss: 0.03447043,  training time: 13.42
[560.5, 641.75, 620.5, 641.5]
Episode 688	 reward: -9.96	 Mean_loss: 0.05804099,  training time: 13.45
[566.5, 611.75, 610.0, 630.5]
Episode 689	 reward: -9.36	 Mean_loss: 0.04402384,  training time: 13.43
[560.0, 581.75, 628.75, 641.75]
Episode 690	 reward: -10.13	 Mean_loss: 0.02707931,  training time: 13.41
[572.25, 607.75, 668.5, 642.25]
Episode 691	 reward: -9.37	 Mean_loss: 0.04032555,  training time: 13.39
[534.0, 616.75, 657.25, 633.75]
Episode 692	 reward: -9.16	 Mean_loss: 0.02945689,  training time: 13.40
[569.75, 595.0, 640.5, 644.75]
Episode 693	 reward: -9.16	 Mean_loss: 0.05564731,  training time: 13.37
[545.5, 600.0, 628.25, 664.75]
Episode 694	 reward: -10.39	 Mean_loss: 0.05275004,  training time: 13.51
[571.25, 610.0, 621.75, 625.5]
Episode 695	 reward: -9.17	 Mean_loss: 0.04261274,  training time: 13.42
[566.0, 633.75, 659.75, 655.25]
Episode 696	 reward: -10.01	 Mean_loss: 0.03800876,  training time: 13.39
[583.0, 613.0, 617.0, 667.0]
Episode 697	 reward: -9.22	 Mean_loss: 0.04107666,  training time: 13.38
[560.0, 591.5, 637.75, 656.25]
Episode 698	 reward: -8.95	 Mean_loss: 0.06691642,  training time: 13.37
[568.25, 598.5, 649.0, 657.5]
Episode 699	 reward: -9.30	 Mean_loss: 0.06142704,  training time: 13.38
[544.75, 623.75, 607.75, 655.25]
Episode 700	 reward: -9.58	 Mean_loss: 0.03174616,  training time: 13.39
[597.0, 572.75, 632.5, 623.0]
Episode 701	 reward: -9.33	 Mean_loss: 0.03104015,  training time: 13.45
[580.75, 606.75, 623.75, 655.0]
Episode 702	 reward: -8.65	 Mean_loss: 0.06491467,  training time: 13.43
[601.75, 580.5, 575.75, 672.5]
Episode 703	 reward: -9.07	 Mean_loss: 0.05088709,  training time: 13.44
[603.0, 584.75, 629.5, 648.0]
Episode 704	 reward: -9.86	 Mean_loss: 0.05488014,  training time: 13.52
[566.25, 561.0, 611.25, 633.25]
Episode 705	 reward: -8.67	 Mean_loss: 0.04044100,  training time: 13.41
[601.5, 585.75, 627.75, 683.0]
Episode 706	 reward: -9.70	 Mean_loss: 0.09685535,  training time: 13.45
[594.75, 595.5, 604.0, 650.5]
Episode 707	 reward: -9.14	 Mean_loss: 0.04420189,  training time: 13.40
[599.5, 555.5, 579.5, 638.5]
Episode 708	 reward: -9.49	 Mean_loss: 0.04708884,  training time: 13.38
[565.0, 554.25, 628.75, 622.0]
Episode 709	 reward: -9.37	 Mean_loss: 0.02498055,  training time: 13.39
[597.0, 567.5, 613.25, 690.0]
Episode 710	 reward: -9.78	 Mean_loss: 0.07026082,  training time: 13.34
[577.0, 558.75, 617.75, 631.0]
Episode 711	 reward: -8.80	 Mean_loss: 0.03425473,  training time: 13.36
[581.5, 558.25, 617.25, 657.25]
Episode 712	 reward: -9.26	 Mean_loss: 0.03200110,  training time: 13.38
[617.0, 568.0, 591.25, 645.0]
Episode 713	 reward: -8.62	 Mean_loss: 0.02422768,  training time: 13.43
[582.75, 559.75, 584.0, 612.0]
Episode 714	 reward: -9.16	 Mean_loss: 0.02914190,  training time: 13.43
[578.25, 568.5, 608.75, 613.5]
Episode 715	 reward: -9.34	 Mean_loss: 0.03803303,  training time: 13.48
[577.75, 571.0, 628.25, 659.25]
Episode 716	 reward: -9.40	 Mean_loss: 0.06943537,  training time: 13.38
[581.75, 602.5, 656.25, 659.0]
Episode 717	 reward: -10.06	 Mean_loss: 0.04578160,  training time: 13.37
[604.0, 588.5, 609.75, 638.75]
Episode 718	 reward: -9.26	 Mean_loss: 0.06269597,  training time: 13.38
[582.75, 551.0, 607.5, 638.25]
Episode 719	 reward: -9.52	 Mean_loss: 0.06264317,  training time: 13.43
[592.25, 602.25, 587.25, 659.0]
Episode 720	 reward: -8.56	 Mean_loss: 0.05149235,  training time: 13.34
[616.0, 580.5, 626.75, 684.25]
Episode 721	 reward: -9.78	 Mean_loss: 0.03828777,  training time: 13.49
[625.5, 604.0, 628.5, 677.25]
Episode 722	 reward: -9.07	 Mean_loss: 0.03311209,  training time: 13.36
[610.25, 593.25, 600.0, 689.0]
Episode 723	 reward: -9.46	 Mean_loss: 0.04817435,  training time: 13.40
[596.0, 608.5, 613.0, 671.75]
Episode 724	 reward: -9.47	 Mean_loss: 0.03712037,  training time: 13.39
[612.25, 572.75, 618.0, 680.0]
Episode 725	 reward: -9.22	 Mean_loss: 0.06576481,  training time: 13.45
[578.0, 577.5, 627.5, 679.5]
Episode 726	 reward: -8.81	 Mean_loss: 0.04014355,  training time: 13.44
[588.75, 600.25, 668.75, 689.25]
Episode 727	 reward: -9.37	 Mean_loss: 0.04851782,  training time: 13.42
[581.5, 600.75, 626.25, 681.0]
Episode 728	 reward: -9.88	 Mean_loss: 0.04133397,  training time: 13.40
[616.25, 603.5, 630.0, 676.5]
Episode 729	 reward: -9.45	 Mean_loss: 0.04541035,  training time: 13.35
[610.25, 573.0, 631.5, 687.25]
Episode 730	 reward: -9.09	 Mean_loss: 0.04103700,  training time: 13.36
[597.5, 588.25, 645.75, 686.25]
Episode 731	 reward: -9.77	 Mean_loss: 0.03559019,  training time: 13.38
[597.75, 579.5, 640.25, 649.5]
Episode 732	 reward: -9.25	 Mean_loss: 0.06456347,  training time: 13.43
[611.5, 570.5, 656.25, 667.75]
Episode 733	 reward: -9.39	 Mean_loss: 0.02535734,  training time: 13.37
[593.5, 568.25, 629.25, 659.25]
Episode 734	 reward: -10.00	 Mean_loss: 0.02997213,  training time: 13.40
[609.0, 594.0, 654.5, 678.75]
Episode 735	 reward: -9.74	 Mean_loss: 0.04231527,  training time: 13.40
[611.25, 568.5, 625.0, 674.25]
Episode 736	 reward: -9.47	 Mean_loss: 0.04606459,  training time: 13.48
[581.75, 581.0, 622.5, 654.5]
Episode 737	 reward: -9.85	 Mean_loss: 0.05335181,  training time: 13.39
[621.0, 574.0, 655.0, 666.75]
Episode 738	 reward: -8.84	 Mean_loss: 0.05027783,  training time: 13.37
[598.0, 578.5, 647.5, 633.25]
Episode 739	 reward: -9.44	 Mean_loss: 0.04996581,  training time: 13.39
[605.0, 601.25, 651.75, 675.25]
Episode 740	 reward: -10.17	 Mean_loss: 0.03687534,  training time: 13.37
[598.75, 559.5, 598.75, 667.25]
Episode 741	 reward: -8.83	 Mean_loss: 0.04976637,  training time: 13.48
[595.75, 512.25, 595.25, 667.0]
Episode 742	 reward: -9.40	 Mean_loss: 0.04366965,  training time: 13.48
[598.25, 534.0, 596.5, 671.25]
Episode 743	 reward: -9.77	 Mean_loss: 0.04418224,  training time: 13.34
[609.5, 509.0, 641.25, 660.0]
Episode 744	 reward: -9.14	 Mean_loss: 0.04801000,  training time: 13.34
[597.75, 522.0, 601.25, 671.0]
Episode 745	 reward: -9.41	 Mean_loss: 0.06304512,  training time: 13.36
[582.25, 527.75, 622.75, 660.0]
Episode 746	 reward: -9.17	 Mean_loss: 0.03474234,  training time: 13.36
[603.75, 529.0, 614.75, 628.0]
Episode 747	 reward: -9.37	 Mean_loss: 0.05628282,  training time: 13.39
[601.0, 565.0, 582.75, 682.75]
Episode 748	 reward: -8.98	 Mean_loss: 0.05192017,  training time: 13.34
[609.0, 528.25, 666.75, 645.25]
Episode 749	 reward: -9.61	 Mean_loss: 0.04025367,  training time: 13.37
[606.75, 534.25, 624.25, 647.75]
Episode 750	 reward: -9.81	 Mean_loss: 0.05512726,  training time: 13.35
[625.0, 531.5, 611.5, 675.75]
Episode 751	 reward: -9.69	 Mean_loss: 0.05802573,  training time: 13.36
[609.5, 538.5, 620.75, 664.75]
Episode 752	 reward: -9.36	 Mean_loss: 0.04315797,  training time: 13.37
[621.75, 535.75, 602.25, 672.25]
Episode 753	 reward: -8.95	 Mean_loss: 0.05824701,  training time: 13.34
[599.0, 543.75, 591.75, 661.25]
Episode 754	 reward: -9.55	 Mean_loss: 0.04872376,  training time: 13.37
[601.75, 556.75, 622.75, 674.75]
Episode 755	 reward: -9.18	 Mean_loss: 0.07175098,  training time: 13.35
[605.5, 534.25, 621.0, 673.0]
Episode 756	 reward: -9.73	 Mean_loss: 0.04862131,  training time: 13.37
[591.25, 531.25, 633.75, 644.75]
Episode 757	 reward: -9.80	 Mean_loss: 0.05765576,  training time: 13.47
[587.75, 538.5, 636.0, 662.75]
Episode 758	 reward: -9.33	 Mean_loss: 0.04489763,  training time: 13.40
[587.0, 538.25, 601.5, 650.75]
Episode 759	 reward: -9.09	 Mean_loss: 0.04739524,  training time: 13.37
[615.25, 541.0, 608.5, 641.25]
Episode 760	 reward: -8.99	 Mean_loss: 0.07192289,  training time: 13.43
[538.5, 531.5, 632.25, 628.0]
Episode 761	 reward: -9.55	 Mean_loss: 0.04865802,  training time: 13.40
[541.0, 563.5, 637.0, 661.75]
Episode 762	 reward: -9.11	 Mean_loss: 0.05100278,  training time: 13.34
[566.25, 560.25, 641.5, 632.75]
Episode 763	 reward: -9.21	 Mean_loss: 0.05763600,  training time: 13.33
[569.0, 578.5, 649.25, 668.75]
Episode 764	 reward: -9.36	 Mean_loss: 0.05979367,  training time: 13.35
[572.0, 547.25, 616.25, 644.75]
Episode 765	 reward: -9.97	 Mean_loss: 0.04609225,  training time: 13.36
[596.25, 550.5, 590.75, 664.25]
Episode 766	 reward: -9.46	 Mean_loss: 0.05094383,  training time: 13.34
[563.25, 544.75, 604.0, 609.25]
Episode 767	 reward: -9.81	 Mean_loss: 0.03840502,  training time: 13.33
[564.5, 560.75, 619.0, 651.5]
Episode 768	 reward: -9.86	 Mean_loss: 0.08416911,  training time: 13.34
[558.0, 539.75, 624.75, 641.25]
Episode 769	 reward: -9.12	 Mean_loss: 0.03333525,  training time: 13.39
[560.0, 514.5, 607.75, 638.25]
Episode 770	 reward: -9.41	 Mean_loss: 0.03760489,  training time: 13.34
[553.25, 554.0, 609.5, 666.75]
Episode 771	 reward: -9.04	 Mean_loss: 0.03988112,  training time: 13.35
[568.5, 537.5, 634.0, 686.5]
Episode 772	 reward: -9.09	 Mean_loss: 0.05584114,  training time: 13.36
[566.75, 556.0, 632.75, 638.75]
Episode 773	 reward: -9.32	 Mean_loss: 0.05752394,  training time: 13.40
[591.5, 528.25, 618.5, 616.0]
Episode 774	 reward: -10.09	 Mean_loss: 0.05973525,  training time: 13.42
[563.5, 534.5, 630.0, 607.25]
Episode 775	 reward: -8.74	 Mean_loss: 0.04503173,  training time: 13.40
[607.0, 553.75, 642.75, 656.5]
Episode 776	 reward: -10.19	 Mean_loss: 0.05387381,  training time: 13.37
[585.5, 538.0, 618.25, 620.5]
Episode 777	 reward: -9.58	 Mean_loss: 0.06927348,  training time: 13.37
[543.0, 552.0, 616.75, 622.5]
Episode 778	 reward: -9.19	 Mean_loss: 0.02469228,  training time: 13.42
[575.5, 572.0, 595.25, 638.75]
Episode 779	 reward: -9.06	 Mean_loss: 0.04168214,  training time: 13.34
[585.0, 534.0, 616.75, 655.0]
Episode 780	 reward: -9.13	 Mean_loss: 0.03080338,  training time: 13.38
[613.0, 579.0, 640.25, 686.25]
Episode 781	 reward: -9.55	 Mean_loss: 0.04869572,  training time: 13.42
[605.75, 552.25, 623.5, 686.25]
Episode 782	 reward: -10.31	 Mean_loss: 0.05418753,  training time: 13.36
[613.75, 605.75, 633.75, 729.5]
Episode 783	 reward: -9.50	 Mean_loss: 0.06535950,  training time: 13.38
[634.25, 603.25, 634.75, 696.75]
Episode 784	 reward: -9.61	 Mean_loss: 0.04646120,  training time: 13.39
[619.5, 597.5, 626.75, 700.0]
Episode 785	 reward: -9.92	 Mean_loss: 0.04300478,  training time: 13.43
[597.75, 554.5, 638.5, 703.75]
Episode 786	 reward: -9.43	 Mean_loss: 0.03455283,  training time: 13.38
[599.5, 606.75, 605.0, 709.5]
Episode 787	 reward: -9.78	 Mean_loss: 0.04685942,  training time: 13.37
[604.25, 597.5, 664.75, 695.5]
Episode 788	 reward: -9.86	 Mean_loss: 0.04422365,  training time: 13.45
[600.5, 578.75, 629.75, 703.75]
Episode 789	 reward: -10.26	 Mean_loss: 0.03058475,  training time: 13.37
[626.0, 564.5, 624.5, 683.25]
Episode 790	 reward: -10.52	 Mean_loss: 0.06087377,  training time: 13.41
[626.0, 560.5, 613.0, 689.25]
Episode 791	 reward: -10.08	 Mean_loss: 0.05525189,  training time: 13.39
[620.25, 598.5, 642.0, 712.0]
Episode 792	 reward: -9.70	 Mean_loss: 0.03031396,  training time: 13.36
[624.75, 593.0, 627.0, 692.5]
Episode 793	 reward: -9.51	 Mean_loss: 0.05379464,  training time: 13.36
[646.5, 570.0, 630.5, 696.75]
Episode 794	 reward: -9.72	 Mean_loss: 0.05110691,  training time: 13.36
[613.5, 617.5, 631.25, 682.25]
Episode 795	 reward: -10.22	 Mean_loss: 0.04404950,  training time: 13.41
[618.25, 592.5, 657.0, 707.0]
Episode 796	 reward: -9.73	 Mean_loss: 0.03055317,  training time: 13.36
[609.25, 571.75, 616.25, 710.0]
Episode 797	 reward: -9.87	 Mean_loss: 0.02792260,  training time: 13.37
[577.0, 569.25, 665.5, 717.25]
Episode 798	 reward: -9.57	 Mean_loss: 0.05149292,  training time: 13.37
[620.0, 599.75, 638.75, 669.25]
Episode 799	 reward: -10.39	 Mean_loss: 0.05105143,  training time: 13.42
[617.0, 569.0, 629.75, 686.5]
Episode 800	 reward: -9.83	 Mean_loss: 0.03821424,  training time: 13.39
[606.5, 564.75, 635.25, 663.25]
Episode 801	 reward: -10.48	 Mean_loss: 0.05292280,  training time: 13.46
[586.0, 562.25, 595.0, 638.75]
Episode 802	 reward: -9.71	 Mean_loss: 0.05635489,  training time: 13.35
[601.25, 563.75, 630.5, 655.75]
Episode 803	 reward: -9.16	 Mean_loss: 0.04461214,  training time: 13.40
[579.25, 580.5, 623.5, 628.5]
Episode 804	 reward: -9.48	 Mean_loss: 0.04094705,  training time: 13.38
[574.0, 581.75, 630.0, 674.0]
Episode 805	 reward: -9.16	 Mean_loss: 0.04437869,  training time: 13.38
[592.0, 575.0, 654.25, 686.75]
Episode 806	 reward: -9.42	 Mean_loss: 0.05792802,  training time: 13.40
[591.75, 565.75, 589.5, 639.5]
Episode 807	 reward: -10.09	 Mean_loss: 0.03543503,  training time: 13.40
[578.0, 565.25, 634.25, 618.25]
Episode 808	 reward: -8.62	 Mean_loss: 0.04148385,  training time: 13.37
[575.5, 571.0, 614.75, 630.5]
Episode 809	 reward: -9.72	 Mean_loss: 0.05439198,  training time: 13.34
[610.75, 555.25, 603.0, 634.75]
Episode 810	 reward: -10.02	 Mean_loss: 0.03229835,  training time: 13.37
[580.75, 587.5, 649.25, 663.5]
Episode 811	 reward: -9.56	 Mean_loss: 0.05100507,  training time: 13.40
[567.5, 589.5, 632.0, 680.25]
Episode 812	 reward: -9.53	 Mean_loss: 0.04761808,  training time: 13.41
[555.25, 570.75, 623.5, 676.25]
Episode 813	 reward: -10.80	 Mean_loss: 0.04338282,  training time: 13.37
[576.25, 555.25, 661.75, 674.0]
Episode 814	 reward: -9.27	 Mean_loss: 0.04741824,  training time: 13.42
[578.75, 571.5, 655.75, 668.5]
Episode 815	 reward: -8.99	 Mean_loss: 0.03684485,  training time: 13.41
[572.25, 597.25, 625.25, 665.5]
Episode 816	 reward: -9.59	 Mean_loss: 0.04411492,  training time: 13.38
[604.0, 572.25, 651.5, 688.75]
Episode 817	 reward: -9.26	 Mean_loss: 0.05326578,  training time: 13.36
[575.75, 574.5, 625.25, 674.5]
Episode 818	 reward: -9.34	 Mean_loss: 0.05272719,  training time: 13.38
[545.0, 565.25, 612.5, 656.0]
Episode 819	 reward: -10.84	 Mean_loss: 0.05269640,  training time: 13.37
[574.75, 547.25, 632.0, 668.25]
Episode 820	 reward: -10.23	 Mean_loss: 0.04462978,  training time: 13.49
[596.5, 619.0, 603.75, 644.0]
Episode 821	 reward: -9.13	 Mean_loss: 0.04029514,  training time: 13.48
[575.0, 616.25, 586.75, 654.0]
Episode 822	 reward: -9.00	 Mean_loss: 0.03980274,  training time: 13.44
[587.25, 636.0, 602.75, 637.25]
Episode 823	 reward: -9.16	 Mean_loss: 0.04395063,  training time: 13.41
[594.5, 659.25, 605.5, 616.5]
Episode 824	 reward: -8.92	 Mean_loss: 0.03294116,  training time: 13.41
[577.25, 623.25, 641.0, 669.75]
Episode 825	 reward: -9.85	 Mean_loss: 0.05782118,  training time: 13.41
[581.25, 608.5, 633.5, 661.5]
Episode 826	 reward: -9.11	 Mean_loss: 0.04226003,  training time: 13.43
[602.75, 616.25, 581.75, 631.5]
Episode 827	 reward: -9.61	 Mean_loss: 0.03343707,  training time: 13.40
[583.75, 616.75, 620.75, 637.75]
Episode 828	 reward: -8.74	 Mean_loss: 0.03770670,  training time: 13.41
[595.25, 626.25, 623.0, 657.75]
Episode 829	 reward: -10.55	 Mean_loss: 0.04848019,  training time: 13.42
[583.0, 601.0, 612.25, 661.25]
Episode 830	 reward: -9.30	 Mean_loss: 0.04731368,  training time: 13.38
[560.25, 628.0, 624.25, 650.0]
Episode 831	 reward: -9.24	 Mean_loss: 0.03512901,  training time: 13.43
[572.25, 632.75, 615.5, 626.75]
Episode 832	 reward: -9.71	 Mean_loss: 0.06488290,  training time: 13.42
[590.25, 633.5, 593.0, 653.5]
Episode 833	 reward: -9.38	 Mean_loss: 0.03479688,  training time: 13.44
[577.25, 630.0, 670.0, 647.5]
Episode 834	 reward: -9.56	 Mean_loss: 0.03920932,  training time: 13.46
[575.75, 600.75, 613.5, 635.75]
Episode 835	 reward: -9.42	 Mean_loss: 0.04568005,  training time: 13.42
[578.75, 636.75, 587.0, 635.0]
Episode 836	 reward: -9.37	 Mean_loss: 0.04869159,  training time: 13.45
[571.5, 628.25, 637.5, 631.5]
Episode 837	 reward: -9.47	 Mean_loss: 0.05356026,  training time: 13.40
[582.25, 626.25, 606.0, 629.5]
Episode 838	 reward: -9.78	 Mean_loss: 0.05336848,  training time: 13.45
[611.5, 651.5, 612.0, 644.75]
Episode 839	 reward: -9.08	 Mean_loss: 0.03421091,  training time: 13.44
[586.0, 642.75, 593.0, 668.75]
Episode 840	 reward: -9.08	 Mean_loss: 0.04017117,  training time: 13.45
[636.25, 590.5, 622.25, 661.75]
Episode 841	 reward: -9.80	 Mean_loss: 0.03726667,  training time: 13.58
[620.0, 600.25, 651.75, 654.75]
Episode 842	 reward: -9.02	 Mean_loss: 0.03838113,  training time: 13.43
[621.75, 580.0, 645.5, 651.0]
Episode 843	 reward: -9.41	 Mean_loss: 0.04682494,  training time: 13.40
[610.0, 564.5, 609.75, 647.5]
Episode 844	 reward: -9.62	 Mean_loss: 0.04032103,  training time: 13.41
[593.25, 572.25, 645.0, 666.25]
Episode 845	 reward: -9.86	 Mean_loss: 0.03331080,  training time: 13.39
[595.0, 574.25, 637.5, 646.75]
Episode 846	 reward: -9.02	 Mean_loss: 0.03299045,  training time: 13.39
[578.75, 595.0, 684.25, 685.75]
Episode 847	 reward: -10.13	 Mean_loss: 0.03618503,  training time: 13.40
[602.75, 590.25, 662.0, 673.5]
Episode 848	 reward: -9.90	 Mean_loss: 0.04758234,  training time: 13.40
[611.0, 611.25, 622.75, 650.0]
Episode 849	 reward: -10.18	 Mean_loss: 0.04804602,  training time: 13.46
[613.75, 578.75, 659.0, 667.0]
Episode 850	 reward: -9.26	 Mean_loss: 0.04116957,  training time: 13.40
[609.0, 580.25, 669.0, 647.25]
Episode 851	 reward: -8.72	 Mean_loss: 0.02920141,  training time: 13.36
[601.0, 580.25, 628.0, 628.25]
Episode 852	 reward: -10.04	 Mean_loss: 0.03853520,  training time: 13.35
[590.0, 572.5, 642.5, 652.25]
Episode 853	 reward: -10.21	 Mean_loss: 0.04127165,  training time: 13.41
[588.75, 578.75, 634.25, 662.25]
Episode 854	 reward: -9.19	 Mean_loss: 0.04653343,  training time: 13.36
[597.0, 578.75, 613.5, 677.5]
Episode 855	 reward: -9.30	 Mean_loss: 0.04870654,  training time: 13.41
[597.75, 565.75, 648.0, 625.5]
Episode 856	 reward: -9.39	 Mean_loss: 0.04155078,  training time: 13.40
[600.75, 569.75, 675.5, 661.25]
Episode 857	 reward: -9.05	 Mean_loss: 0.04759720,  training time: 13.48
[585.25, 588.75, 616.75, 656.25]
Episode 858	 reward: -9.40	 Mean_loss: 0.05238825,  training time: 13.40
[592.25, 569.75, 633.25, 659.0]
Episode 859	 reward: -9.97	 Mean_loss: 0.04825402,  training time: 13.39
[605.25, 571.25, 635.25, 651.25]
Episode 860	 reward: -9.44	 Mean_loss: 0.04507828,  training time: 13.39
[593.75, 580.0, 647.0, 668.25]
Episode 861	 reward: -9.44	 Mean_loss: 0.02844484,  training time: 13.44
[606.5, 590.5, 655.75, 683.25]
Episode 862	 reward: -10.17	 Mean_loss: 0.05369826,  training time: 13.46
[624.75, 588.5, 640.5, 692.25]
Episode 863	 reward: -9.22	 Mean_loss: 0.04240348,  training time: 13.39
[606.0, 587.75, 645.0, 655.25]
Episode 864	 reward: -9.39	 Mean_loss: 0.02464829,  training time: 13.36
[593.0, 575.75, 622.5, 686.75]
Episode 865	 reward: -9.34	 Mean_loss: 0.04276061,  training time: 13.42
[605.0, 594.25, 621.25, 700.0]
Episode 866	 reward: -9.18	 Mean_loss: 0.04254939,  training time: 13.38
[618.25, 567.0, 638.75, 665.5]
Episode 867	 reward: -9.18	 Mean_loss: 0.04459920,  training time: 13.39
[559.25, 570.25, 635.5, 698.25]
Episode 868	 reward: -10.32	 Mean_loss: 0.05002494,  training time: 13.41
[625.25, 592.5, 634.5, 696.75]
Episode 869	 reward: -9.94	 Mean_loss: 0.05146464,  training time: 13.43
[591.5, 588.25, 626.0, 649.75]
Episode 870	 reward: -9.10	 Mean_loss: 0.05185108,  training time: 13.39
[584.0, 560.25, 641.25, 683.75]
Episode 871	 reward: -9.40	 Mean_loss: 0.06050940,  training time: 13.43
[590.0, 575.5, 626.25, 652.75]
Episode 872	 reward: -9.11	 Mean_loss: 0.04733779,  training time: 13.45
[579.75, 585.25, 620.5, 654.25]
Episode 873	 reward: -9.98	 Mean_loss: 0.04678847,  training time: 13.47
[627.75, 556.5, 646.5, 668.0]
Episode 874	 reward: -9.40	 Mean_loss: 0.03923555,  training time: 13.43
[603.25, 584.75, 652.5, 632.0]
Episode 875	 reward: -9.61	 Mean_loss: 0.05870047,  training time: 13.47
[620.75, 594.5, 645.0, 652.75]
Episode 876	 reward: -8.83	 Mean_loss: 0.05276525,  training time: 13.44
[599.0, 613.75, 626.5, 676.5]
Episode 877	 reward: -9.04	 Mean_loss: 0.03606619,  training time: 13.43
[599.25, 581.0, 621.5, 680.5]
Episode 878	 reward: -8.99	 Mean_loss: 0.04426772,  training time: 13.43
[607.25, 555.0, 623.25, 640.25]
Episode 879	 reward: -8.41	 Mean_loss: 0.05126947,  training time: 13.46
[609.0, 598.75, 599.0, 685.25]
Episode 880	 reward: -9.39	 Mean_loss: 0.05661165,  training time: 13.42
[560.0, 600.25, 628.75, 649.5]
Episode 881	 reward: -9.86	 Mean_loss: 0.06018887,  training time: 13.48
[600.0, 605.0, 571.0, 655.0]
Episode 882	 reward: -9.34	 Mean_loss: 0.04240586,  training time: 13.41
[550.75, 583.5, 635.25, 711.75]
Episode 883	 reward: -9.59	 Mean_loss: 0.06366421,  training time: 13.48
[569.5, 575.25, 618.5, 629.75]
Episode 884	 reward: -9.27	 Mean_loss: 0.05047908,  training time: 13.38
[572.25, 642.75, 619.5, 675.0]
Episode 885	 reward: -9.31	 Mean_loss: 0.04465557,  training time: 13.43
[550.0, 614.0, 626.5, 625.75]
Episode 886	 reward: -9.54	 Mean_loss: 0.05990256,  training time: 13.40
[555.25, 581.25, 616.0, 666.0]
Episode 887	 reward: -9.23	 Mean_loss: 0.05639539,  training time: 13.43
[574.25, 600.75, 608.0, 644.0]
Episode 888	 reward: -10.17	 Mean_loss: 0.05445996,  training time: 13.41
[577.5, 623.0, 596.25, 677.5]
Episode 889	 reward: -9.10	 Mean_loss: 0.06965974,  training time: 13.40
[593.75, 598.0, 629.5, 630.75]
Episode 890	 reward: -9.73	 Mean_loss: 0.05942446,  training time: 13.43
[558.75, 579.75, 622.25, 656.0]
Episode 891	 reward: -9.55	 Mean_loss: 0.05472129,  training time: 13.44
[590.25, 610.5, 606.25, 628.25]
Episode 892	 reward: -9.53	 Mean_loss: 0.05415114,  training time: 13.43
[567.5, 611.75, 654.0, 643.0]
Episode 893	 reward: -9.14	 Mean_loss: 0.03536046,  training time: 13.41
[574.75, 626.25, 640.0, 652.25]
Episode 894	 reward: -10.00	 Mean_loss: 0.04842404,  training time: 13.43
[585.0, 610.5, 622.5, 698.0]
Episode 895	 reward: -8.74	 Mean_loss: 0.04385545,  training time: 13.41
[565.75, 590.5, 632.5, 655.0]
Episode 896	 reward: -9.38	 Mean_loss: 0.05736690,  training time: 13.42
[537.75, 596.5, 607.75, 662.75]
Episode 897	 reward: -9.04	 Mean_loss: 0.06861252,  training time: 13.45
[587.25, 579.75, 613.75, 667.25]
Episode 898	 reward: -9.96	 Mean_loss: 0.06279667,  training time: 13.48
[543.25, 593.75, 618.5, 666.5]
Episode 899	 reward: -9.94	 Mean_loss: 0.04321289,  training time: 13.46
[548.0, 633.75, 595.0, 649.75]
Episode 900	 reward: -8.93	 Mean_loss: 0.04482836,  training time: 13.42
[587.75, 560.0, 629.25, 606.0]
Episode 901	 reward: -9.84	 Mean_loss: 0.04690482,  training time: 13.48
[603.0, 589.5, 624.75, 606.5]
Episode 902	 reward: -8.88	 Mean_loss: 0.04789734,  training time: 13.38
[634.5, 558.75, 606.25, 632.75]
Episode 903	 reward: -9.07	 Mean_loss: 0.06251507,  training time: 13.41
[608.75, 565.75, 628.5, 678.5]
Episode 904	 reward: -9.39	 Mean_loss: 0.06413460,  training time: 13.44
[596.25, 557.5, 633.25, 630.0]
Episode 905	 reward: -9.69	 Mean_loss: 0.06112042,  training time: 13.45
[614.0, 567.5, 633.75, 635.75]
Episode 906	 reward: -9.02	 Mean_loss: 0.04636839,  training time: 13.45
[608.5, 565.0, 615.75, 618.5]
Episode 907	 reward: -9.07	 Mean_loss: 0.04289155,  training time: 13.41
[593.75, 572.75, 601.75, 601.75]
Episode 908	 reward: -8.63	 Mean_loss: 0.05139047,  training time: 13.40
[614.0, 579.25, 613.25, 644.0]
Episode 909	 reward: -8.85	 Mean_loss: 0.04826887,  training time: 13.40
[601.5, 571.25, 588.75, 634.5]
Episode 910	 reward: -9.08	 Mean_loss: 0.04083754,  training time: 13.43
[606.25, 577.25, 640.0, 633.5]
Episode 911	 reward: -9.23	 Mean_loss: 0.05040398,  training time: 13.44
[604.0, 555.25, 626.25, 625.0]
Episode 912	 reward: -10.11	 Mean_loss: 0.04233469,  training time: 13.43
[598.25, 595.0, 612.25, 659.25]
Episode 913	 reward: -9.69	 Mean_loss: 0.06166116,  training time: 13.41
[613.0, 561.25, 588.0, 633.0]
Episode 914	 reward: -9.87	 Mean_loss: 0.04163505,  training time: 13.43
[622.75, 574.5, 641.25, 623.75]
Episode 915	 reward: -8.34	 Mean_loss: 0.03791427,  training time: 13.40
[586.75, 568.75, 626.25, 668.75]
Episode 916	 reward: -9.39	 Mean_loss: 0.05049428,  training time: 13.41
[621.75, 585.75, 615.75, 648.5]
Episode 917	 reward: -9.27	 Mean_loss: 0.03797766,  training time: 13.38
[605.0, 573.0, 624.75, 649.75]
Episode 918	 reward: -9.73	 Mean_loss: 0.06768427,  training time: 13.43
[596.0, 578.25, 635.25, 644.25]
Episode 919	 reward: -8.74	 Mean_loss: 0.03615446,  training time: 13.42
[598.75, 577.25, 588.0, 628.75]
Episode 920	 reward: -8.84	 Mean_loss: 0.05745687,  training time: 13.41
[554.25, 582.25, 618.5, 660.5]
Episode 921	 reward: -9.69	 Mean_loss: 0.05914593,  training time: 13.50
[536.75, 564.75, 639.75, 673.0]
Episode 922	 reward: -9.79	 Mean_loss: 0.06093466,  training time: 13.46
[561.75, 582.5, 633.75, 667.25]
Episode 923	 reward: -9.91	 Mean_loss: 0.06326453,  training time: 13.43
[575.75, 571.5, 632.0, 686.0]
Episode 924	 reward: -9.09	 Mean_loss: 0.04314416,  training time: 13.42
[558.75, 559.25, 604.75, 617.0]
Episode 925	 reward: -9.00	 Mean_loss: 0.05780436,  training time: 13.51
[552.75, 553.0, 654.5, 674.25]
Episode 926	 reward: -9.60	 Mean_loss: 0.03082116,  training time: 13.41
[578.0, 575.75, 607.75, 652.75]
Episode 927	 reward: -9.83	 Mean_loss: 0.03393988,  training time: 13.44
[550.25, 570.25, 621.5, 650.0]
Episode 928	 reward: -9.07	 Mean_loss: 0.03535588,  training time: 13.44
[571.0, 571.5, 650.75, 660.5]
Episode 929	 reward: -9.13	 Mean_loss: 0.06278165,  training time: 13.46
[596.25, 547.0, 614.75, 656.0]
Episode 930	 reward: -10.36	 Mean_loss: 0.04303213,  training time: 13.44
[588.75, 573.25, 614.0, 664.0]
Episode 931	 reward: -9.85	 Mean_loss: 0.06153142,  training time: 13.36
[524.0, 576.75, 643.25, 644.0]
Episode 932	 reward: -10.34	 Mean_loss: 0.04748069,  training time: 13.41
[548.0, 583.0, 596.25, 673.75]
Episode 933	 reward: -9.07	 Mean_loss: 0.04366700,  training time: 13.49
[548.25, 579.5, 641.0, 661.0]
Episode 934	 reward: -9.46	 Mean_loss: 0.04170708,  training time: 13.42
[544.5, 543.5, 636.25, 657.5]
Episode 935	 reward: -9.98	 Mean_loss: 0.04177833,  training time: 13.42
[560.0, 603.25, 641.0, 671.75]
Episode 936	 reward: -10.21	 Mean_loss: 0.05607643,  training time: 13.45
[570.75, 567.0, 641.5, 660.0]
Episode 937	 reward: -9.37	 Mean_loss: 0.04758464,  training time: 13.44
[567.75, 568.5, 629.75, 665.5]
Episode 938	 reward: -8.86	 Mean_loss: 0.04678936,  training time: 13.39
[557.25, 551.5, 617.25, 644.75]
Episode 939	 reward: -9.69	 Mean_loss: 0.04046752,  training time: 13.36
[541.25, 569.25, 596.0, 680.5]
Episode 940	 reward: -9.43	 Mean_loss: 0.05462978,  training time: 13.40
[614.25, 557.75, 608.25, 641.75]
Episode 941	 reward: -9.17	 Mean_loss: 0.04252262,  training time: 13.43
[617.0, 576.75, 623.75, 673.25]
Episode 942	 reward: -9.29	 Mean_loss: 0.05022819,  training time: 13.44
[636.25, 561.0, 633.75, 665.0]
Episode 943	 reward: -9.23	 Mean_loss: 0.02846790,  training time: 13.38
[640.5, 564.75, 664.0, 663.5]
Episode 944	 reward: -9.48	 Mean_loss: 0.05586442,  training time: 13.42
[604.25, 574.25, 606.0, 637.5]
Episode 945	 reward: -9.23	 Mean_loss: 0.03667995,  training time: 13.38
[616.75, 567.25, 626.25, 636.0]
Episode 946	 reward: -8.89	 Mean_loss: 0.03893073,  training time: 13.45
[619.25, 554.75, 660.0, 644.5]
Episode 947	 reward: -8.91	 Mean_loss: 0.04111732,  training time: 13.36
[603.0, 553.5, 615.25, 687.75]
Episode 948	 reward: -9.35	 Mean_loss: 0.04115766,  training time: 13.38
[638.0, 571.5, 653.5, 660.75]
Episode 949	 reward: -9.67	 Mean_loss: 0.03321123,  training time: 13.37
[629.0, 593.25, 623.5, 650.5]
Episode 950	 reward: -10.09	 Mean_loss: 0.03356958,  training time: 13.43
[624.25, 565.5, 617.5, 681.0]
Episode 951	 reward: -9.00	 Mean_loss: 0.04709707,  training time: 13.37
[635.25, 561.5, 649.0, 680.25]
Episode 952	 reward: -9.18	 Mean_loss: 0.04694194,  training time: 13.36
[672.0, 554.75, 614.25, 637.0]
Episode 953	 reward: -9.47	 Mean_loss: 0.05355611,  training time: 13.41
[629.75, 562.75, 614.0, 646.75]
Episode 954	 reward: -9.76	 Mean_loss: 0.03695542,  training time: 13.36
[632.75, 569.75, 610.5, 659.75]
Episode 955	 reward: -9.94	 Mean_loss: 0.03149419,  training time: 13.36
[616.25, 589.75, 587.5, 670.0]
Episode 956	 reward: -9.58	 Mean_loss: 0.03905510,  training time: 13.43
[617.5, 549.5, 598.0, 672.5]
Episode 957	 reward: -9.47	 Mean_loss: 0.04559006,  training time: 13.45
[624.5, 543.5, 617.5, 665.5]
Episode 958	 reward: -9.51	 Mean_loss: 0.04222746,  training time: 13.47
[642.5, 565.5, 634.75, 684.0]
Episode 959	 reward: -9.44	 Mean_loss: 0.04677260,  training time: 13.46
[637.75, 562.75, 638.0, 670.0]
Episode 960	 reward: -9.50	 Mean_loss: 0.03967657,  training time: 13.42
[655.75, 574.5, 675.75, 648.5]
Episode 961	 reward: -8.83	 Mean_loss: 0.03434059,  training time: 13.44
[656.25, 565.25, 635.0, 626.0]
Episode 962	 reward: -9.04	 Mean_loss: 0.03902647,  training time: 13.37
[678.0, 539.75, 609.5, 633.25]
Episode 963	 reward: -9.31	 Mean_loss: 0.02778061,  training time: 13.41
[650.75, 583.25, 646.0, 624.75]
Episode 964	 reward: -9.64	 Mean_loss: 0.03286798,  training time: 13.40
[649.75, 581.0, 625.0, 630.75]
Episode 965	 reward: -8.83	 Mean_loss: 0.03905107,  training time: 13.42
[697.75, 567.0, 641.75, 655.0]
Episode 966	 reward: -9.49	 Mean_loss: 0.05432241,  training time: 13.36
[681.25, 568.5, 622.5, 632.25]
Episode 967	 reward: -8.64	 Mean_loss: 0.02984636,  training time: 13.44
[684.5, 562.25, 639.0, 636.0]
Episode 968	 reward: -8.80	 Mean_loss: 0.04633555,  training time: 13.36
[669.0, 582.0, 653.25, 613.0]
Episode 969	 reward: -8.87	 Mean_loss: 0.02853035,  training time: 13.40
[655.5, 571.5, 611.25, 665.5]
Episode 970	 reward: -8.90	 Mean_loss: 0.03748257,  training time: 13.37
[662.0, 584.0, 625.5, 595.75]
Episode 971	 reward: -9.27	 Mean_loss: 0.05524727,  training time: 13.42
[669.0, 563.75, 624.25, 645.25]
Episode 972	 reward: -8.81	 Mean_loss: 0.03175025,  training time: 13.43
[669.5, 554.0, 622.75, 589.0]
Episode 973	 reward: -8.80	 Mean_loss: 0.03727511,  training time: 13.47
[637.75, 592.25, 630.5, 617.75]
Episode 974	 reward: -9.26	 Mean_loss: 0.03556838,  training time: 13.42
[670.75, 601.5, 665.5, 625.75]
Episode 975	 reward: -8.88	 Mean_loss: 0.03392986,  training time: 13.42
[631.25, 550.0, 648.25, 626.75]
Episode 976	 reward: -8.68	 Mean_loss: 0.04022364,  training time: 13.45
[672.75, 534.25, 611.75, 638.5]
Episode 977	 reward: -8.87	 Mean_loss: 0.03059408,  training time: 13.41
[664.5, 571.75, 687.75, 637.0]
Episode 978	 reward: -9.54	 Mean_loss: 0.03545171,  training time: 13.46
[634.75, 579.0, 612.25, 603.25]
Episode 979	 reward: -9.19	 Mean_loss: 0.03959089,  training time: 13.45
[662.0, 559.0, 622.25, 612.75]
Episode 980	 reward: -9.21	 Mean_loss: 0.03879616,  training time: 13.38
[640.75, 591.75, 611.25, 653.5]
Episode 981	 reward: -9.91	 Mean_loss: 0.04366970,  training time: 13.48
[580.5, 619.25, 601.0, 678.5]
Episode 982	 reward: -10.00	 Mean_loss: 0.04982859,  training time: 13.43
[584.75, 589.5, 636.25, 663.5]
Episode 983	 reward: -9.66	 Mean_loss: 0.04207410,  training time: 13.44
[606.0, 611.0, 623.25, 676.25]
Episode 984	 reward: -9.59	 Mean_loss: 0.06628876,  training time: 13.44
[592.5, 631.0, 616.25, 676.75]
Episode 985	 reward: -8.79	 Mean_loss: 0.07630274,  training time: 13.43
[631.25, 608.25, 620.75, 650.0]
Episode 986	 reward: -9.11	 Mean_loss: 0.04182272,  training time: 13.49
[582.25, 617.75, 599.75, 676.5]
Episode 987	 reward: -9.40	 Mean_loss: 0.06086662,  training time: 13.38
[587.25, 602.5, 618.5, 655.75]
Episode 988	 reward: -8.88	 Mean_loss: 0.05362350,  training time: 13.49
[585.0, 631.25, 614.75, 679.5]
Episode 989	 reward: -9.35	 Mean_loss: 0.05571594,  training time: 13.38
[613.0, 588.75, 616.75, 670.5]
Episode 990	 reward: -9.86	 Mean_loss: 0.04636302,  training time: 13.42
[635.25, 603.25, 615.25, 656.75]
Episode 991	 reward: -8.86	 Mean_loss: 0.04287828,  training time: 13.37
[606.5, 598.75, 602.5, 682.5]
Episode 992	 reward: -10.24	 Mean_loss: 0.05788311,  training time: 13.40
[613.0, 586.75, 649.25, 641.0]
Episode 993	 reward: -9.45	 Mean_loss: 0.06701334,  training time: 13.39
[584.25, 596.5, 674.5, 645.5]
Episode 994	 reward: -9.37	 Mean_loss: 0.05534313,  training time: 13.40
[624.5, 630.5, 628.75, 660.0]
Episode 995	 reward: -9.64	 Mean_loss: 0.06101345,  training time: 13.40
[588.0, 593.5, 615.75, 637.75]
Episode 996	 reward: -9.54	 Mean_loss: 0.03194988,  training time: 13.44
[599.0, 592.25, 609.5, 657.25]
Episode 997	 reward: -8.91	 Mean_loss: 0.04895706,  training time: 13.41
[593.5, 606.25, 619.25, 674.5]
Episode 998	 reward: -9.90	 Mean_loss: 0.06235892,  training time: 13.37
[592.5, 639.0, 667.75, 669.0]
Episode 999	 reward: -10.26	 Mean_loss: 0.05352707,  training time: 13.42
[593.25, 625.75, 669.25, 649.25]
Episode 1000	 reward: -9.28	 Mean_loss: 0.06089521,  training time: 13.40
+ logdir_maml_finetuning=./runs/exp14/maml_finetuning
+ for model in 'maml+$model_suffix'
+ echo 15,5 15,7 15,9 15,10
+ tr ' ' '\n'
+ IFS=,
+ read n_j n_m
+ python train/DAN_finetuning.py --logdir ./runs/exp14/maml_finetuning/transfer_maml+exp14_1000_512_3/15x5 --model_suffix free --finetuning_model maml+exp14_1000_512_3 --max_updates 50 --n_j 15 --n_m 5 --num_envs 4 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x5+mix
save model name:  15x5+mix+free
./trained_network/SD2/maml+exp14_1000_512_3.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/maml+exp14_1000_512_3.pth
vali data :./data/data_train_vali/SD2/15x5+mix


progress:   0%|[34m          [0m| 0/50 [00:00<?, ?it/s]                                                Episode 1	 reward: -5.46	 makespan: 540.75	 Mean_loss: 2.70395303,  training time: 2.34
progress:   0%|[34m          [0m| 0/50 [00:02<?, ?it/s]progress:   2%|[34m         [0m| 1/50 [00:02<01:54,  2.34s/it]                                                        Episode 2	 reward: -5.66	 makespan: 560.00	 Mean_loss: 0.70347822,  training time: 1.24
progress:   2%|[34m         [0m| 1/50 [00:03<01:54,  2.34s/it]progress:   4%|[34m         [0m| 2/50 [00:03<01:21,  1.70s/it]                                                        Episode 3	 reward: -5.98	 makespan: 592.50	 Mean_loss: 0.58210528,  training time: 1.20
progress:   4%|[34m         [0m| 2/50 [00:04<01:21,  1.70s/it]progress:   6%|[34m         [0m| 3/50 [00:04<01:09,  1.47s/it]                                                        Episode 4	 reward: -5.63	 makespan: 557.25	 Mean_loss: 0.50766891,  training time: 1.19
progress:   6%|[34m         [0m| 3/50 [00:05<01:09,  1.47s/it]progress:   8%|[34m         [0m| 4/50 [00:05<01:02,  1.36s/it]                                                        Episode 5	 reward: -5.92	 makespan: 586.25	 Mean_loss: 0.23584069,  training time: 1.20
progress:   8%|[34m         [0m| 4/50 [00:07<01:02,  1.36s/it]progress:  10%|[34m         [0m| 5/50 [00:07<00:58,  1.30s/it]                                                        Episode 6	 reward: -5.97	 makespan: 591.25	 Mean_loss: 0.25213614,  training time: 1.20
progress:  10%|[34m         [0m| 5/50 [00:08<00:58,  1.30s/it]progress:  12%|[34m        [0m| 6/50 [00:08<00:55,  1.27s/it]                                                        Episode 7	 reward: -5.88	 makespan: 581.75	 Mean_loss: 0.16082831,  training time: 1.19
progress:  12%|[34m        [0m| 6/50 [00:09<00:55,  1.27s/it]progress:  14%|[34m        [0m| 7/50 [00:09<00:53,  1.24s/it]                                                        Episode 8	 reward: -6.11	 makespan: 605.00	 Mean_loss: 0.17459238,  training time: 1.20
progress:  14%|[34m        [0m| 7/50 [00:10<00:53,  1.24s/it]progress:  16%|[34m        [0m| 8/50 [00:10<00:51,  1.23s/it]                                                        Episode 9	 reward: -5.64	 makespan: 558.50	 Mean_loss: 0.18862553,  training time: 1.21
progress:  16%|[34m        [0m| 8/50 [00:11<00:51,  1.23s/it]progress:  18%|[34m        [0m| 9/50 [00:11<00:50,  1.22s/it]                                                        Episode 10	 reward: -5.78	 makespan: 571.75	 Mean_loss: 0.12080120,  training time: 1.22
progress:  18%|[34m        [0m| 9/50 [00:13<00:50,  1.22s/it]progress:  20%|[34m        [0m| 10/50 [00:13<00:48,  1.22s/it]                                                         Episode 11	 reward: -5.93	 makespan: 587.50	 Mean_loss: 0.17966817,  training time: 1.25
progress:  20%|[34m        [0m| 10/50 [00:14<00:48,  1.22s/it]progress:  22%|[34m       [0m| 11/50 [00:14<00:47,  1.23s/it]                                                         Episode 12	 reward: -5.52	 makespan: 546.00	 Mean_loss: 0.11621617,  training time: 1.33
progress:  22%|[34m       [0m| 11/50 [00:15<00:47,  1.23s/it]progress:  24%|[34m       [0m| 12/50 [00:15<00:47,  1.26s/it]                                                         Episode 13	 reward: -5.82	 makespan: 575.75	 Mean_loss: 0.13473928,  training time: 1.22
progress:  24%|[34m       [0m| 12/50 [00:16<00:47,  1.26s/it]progress:  26%|[34m       [0m| 13/50 [00:17<00:46,  1.25s/it]                                                         Episode 14	 reward: -5.72	 makespan: 566.00	 Mean_loss: 0.09316523,  training time: 1.24
progress:  26%|[34m       [0m| 13/50 [00:18<00:46,  1.25s/it]progress:  28%|[34m       [0m| 14/50 [00:18<00:44,  1.25s/it]                                                         Episode 15	 reward: -5.49	 makespan: 544.00	 Mean_loss: 0.10033184,  training time: 1.22
progress:  28%|[34m       [0m| 14/50 [00:19<00:44,  1.25s/it]progress:  30%|[34m       [0m| 15/50 [00:19<00:43,  1.24s/it]                                                         Episode 16	 reward: -5.67	 makespan: 561.25	 Mean_loss: 0.06252164,  training time: 1.23
progress:  30%|[34m       [0m| 15/50 [00:20<00:43,  1.24s/it]progress:  32%|[34m      [0m| 16/50 [00:20<00:42,  1.24s/it]                                                         Episode 17	 reward: -5.87	 makespan: 581.50	 Mean_loss: 0.10514014,  training time: 1.25
progress:  32%|[34m      [0m| 16/50 [00:21<00:42,  1.24s/it]progress:  34%|[34m      [0m| 17/50 [00:21<00:40,  1.24s/it]                                                         Episode 18	 reward: -5.71	 makespan: 565.25	 Mean_loss: 0.06875271,  training time: 1.23
progress:  34%|[34m      [0m| 17/50 [00:23<00:40,  1.24s/it]progress:  36%|[34m      [0m| 18/50 [00:23<00:39,  1.24s/it]                                                         Episode 19	 reward: -5.48	 makespan: 542.25	 Mean_loss: 0.08485165,  training time: 1.24
progress:  36%|[34m      [0m| 18/50 [00:24<00:39,  1.24s/it]progress:  38%|[34m      [0m| 19/50 [00:24<00:38,  1.24s/it]                                                         Episode 20	 reward: -5.56	 makespan: 550.00	 Mean_loss: 0.04160203,  training time: 1.24
progress:  38%|[34m      [0m| 19/50 [00:25<00:38,  1.24s/it]progress:  40%|[34m      [0m| 20/50 [00:25<00:37,  1.24s/it]                                                         Episode 21	 reward: -5.48	 makespan: 542.25	 Mean_loss: 0.08349517,  training time: 1.24
progress:  40%|[34m      [0m| 20/50 [00:26<00:37,  1.24s/it]progress:  42%|[34m     [0m| 21/50 [00:26<00:35,  1.24s/it]                                                         Episode 22	 reward: -5.43	 makespan: 537.75	 Mean_loss: 0.04888110,  training time: 1.22
progress:  42%|[34m     [0m| 21/50 [00:28<00:35,  1.24s/it]progress:  44%|[34m     [0m| 22/50 [00:28<00:34,  1.23s/it]                                                         Episode 23	 reward: -5.37	 makespan: 531.25	 Mean_loss: 0.07622311,  training time: 1.24
progress:  44%|[34m     [0m| 22/50 [00:29<00:34,  1.23s/it]progress:  46%|[34m     [0m| 23/50 [00:29<00:33,  1.24s/it]                                                         Episode 24	 reward: -5.35	 makespan: 529.50	 Mean_loss: 0.05009963,  training time: 1.24
progress:  46%|[34m     [0m| 23/50 [00:30<00:33,  1.24s/it]progress:  48%|[34m     [0m| 24/50 [00:30<00:32,  1.24s/it]                                                         Episode 25	 reward: -5.68	 makespan: 562.25	 Mean_loss: 0.04561649,  training time: 1.25
progress:  48%|[34m     [0m| 24/50 [00:31<00:32,  1.24s/it]progress:  50%|[34m     [0m| 25/50 [00:31<00:31,  1.24s/it]                                                         Episode 26	 reward: -5.35	 makespan: 529.25	 Mean_loss: 0.05655856,  training time: 1.21
progress:  50%|[34m     [0m| 25/50 [00:33<00:31,  1.24s/it]progress:  52%|[34m    [0m| 26/50 [00:33<00:29,  1.23s/it]                                                         Episode 27	 reward: -5.44	 makespan: 538.50	 Mean_loss: 0.10825573,  training time: 1.25
progress:  52%|[34m    [0m| 26/50 [00:34<00:29,  1.23s/it]progress:  54%|[34m    [0m| 27/50 [00:34<00:28,  1.24s/it]                                                         Episode 28	 reward: -5.54	 makespan: 548.75	 Mean_loss: 0.03965162,  training time: 1.28
progress:  54%|[34m    [0m| 27/50 [00:35<00:28,  1.24s/it]progress:  56%|[34m    [0m| 28/50 [00:35<00:27,  1.25s/it]                                                         Episode 29	 reward: -5.53	 makespan: 547.50	 Mean_loss: 0.04037699,  training time: 1.22
progress:  56%|[34m    [0m| 28/50 [00:36<00:27,  1.25s/it]progress:  58%|[34m    [0m| 29/50 [00:36<00:26,  1.24s/it]                                                         Episode 30	 reward: -5.81	 makespan: 575.50	 Mean_loss: 0.10088377,  training time: 1.18
progress:  58%|[34m    [0m| 29/50 [00:37<00:26,  1.24s/it]progress:  60%|[34m    [0m| 30/50 [00:37<00:24,  1.22s/it]                                                         Episode 31	 reward: -5.53	 makespan: 547.75	 Mean_loss: 0.05597793,  training time: 1.24
progress:  60%|[34m    [0m| 30/50 [00:39<00:24,  1.22s/it]progress:  62%|[34m   [0m| 31/50 [00:39<00:23,  1.23s/it]                                                         Episode 32	 reward: -5.66	 makespan: 560.00	 Mean_loss: 0.07116283,  training time: 1.30
progress:  62%|[34m   [0m| 31/50 [00:40<00:23,  1.23s/it]progress:  64%|[34m   [0m| 32/50 [00:40<00:22,  1.25s/it]                                                         Episode 33	 reward: -5.28	 makespan: 522.75	 Mean_loss: 0.05422557,  training time: 1.21
progress:  64%|[34m   [0m| 32/50 [00:41<00:22,  1.25s/it]progress:  66%|[34m   [0m| 33/50 [00:41<00:21,  1.24s/it]                                                         Episode 34	 reward: -5.40	 makespan: 534.75	 Mean_loss: 0.02900056,  training time: 1.21
progress:  66%|[34m   [0m| 33/50 [00:42<00:21,  1.24s/it]progress:  68%|[34m   [0m| 34/50 [00:42<00:19,  1.23s/it]                                                         Episode 35	 reward: -5.53	 makespan: 547.50	 Mean_loss: 0.03673663,  training time: 1.20
progress:  68%|[34m   [0m| 34/50 [00:44<00:19,  1.23s/it]progress:  70%|[34m   [0m| 35/50 [00:44<00:18,  1.22s/it]                                                         Episode 36	 reward: -5.20	 makespan: 514.75	 Mean_loss: 0.04176984,  training time: 1.22
progress:  70%|[34m   [0m| 35/50 [00:45<00:18,  1.22s/it]progress:  72%|[34m  [0m| 36/50 [00:45<00:17,  1.22s/it]                                                         Episode 37	 reward: -5.34	 makespan: 528.75	 Mean_loss: 0.04546584,  training time: 1.22
progress:  72%|[34m  [0m| 36/50 [00:46<00:17,  1.22s/it]progress:  74%|[34m  [0m| 37/50 [00:46<00:15,  1.22s/it]                                                         Episode 38	 reward: -5.35	 makespan: 529.75	 Mean_loss: 0.02848141,  training time: 1.23
progress:  74%|[34m  [0m| 37/50 [00:47<00:15,  1.22s/it]progress:  76%|[34m  [0m| 38/50 [00:47<00:14,  1.22s/it]                                                         Episode 39	 reward: -5.61	 makespan: 555.50	 Mean_loss: 0.03910470,  training time: 1.27
progress:  76%|[34m  [0m| 38/50 [00:49<00:14,  1.22s/it]progress:  78%|[34m  [0m| 39/50 [00:49<00:13,  1.24s/it]                                                         Episode 40	 reward: -5.27	 makespan: 521.50	 Mean_loss: 0.03593696,  training time: 1.33
progress:  78%|[34m  [0m| 39/50 [00:50<00:13,  1.24s/it]progress:  80%|[34m  [0m| 40/50 [00:50<00:12,  1.27s/it]                                                         Episode 41	 reward: -5.21	 makespan: 515.50	 Mean_loss: 0.02619174,  training time: 1.23
progress:  80%|[34m  [0m| 40/50 [00:51<00:12,  1.27s/it]progress:  82%|[34m [0m| 41/50 [00:51<00:11,  1.26s/it]                                                         Episode 42	 reward: -5.23	 makespan: 517.50	 Mean_loss: 0.02772444,  training time: 1.24
progress:  82%|[34m [0m| 41/50 [00:52<00:11,  1.26s/it]progress:  84%|[34m [0m| 42/50 [00:52<00:10,  1.25s/it]                                                         Episode 43	 reward: -5.46	 makespan: 540.25	 Mean_loss: 0.02798297,  training time: 1.24
progress:  84%|[34m [0m| 42/50 [00:54<00:10,  1.25s/it]progress:  86%|[34m [0m| 43/50 [00:54<00:08,  1.25s/it]                                                         Episode 44	 reward: -5.33	 makespan: 527.75	 Mean_loss: 0.03250719,  training time: 1.23
progress:  86%|[34m [0m| 43/50 [00:55<00:08,  1.25s/it]progress:  88%|[34m [0m| 44/50 [00:55<00:07,  1.24s/it]                                                         Episode 45	 reward: -5.25	 makespan: 519.75	 Mean_loss: 0.03754492,  training time: 1.26
progress:  88%|[34m [0m| 44/50 [00:56<00:07,  1.24s/it]progress:  90%|[34m [0m| 45/50 [00:56<00:06,  1.25s/it]                                                         Episode 46	 reward: -5.45	 makespan: 540.00	 Mean_loss: 0.03760115,  training time: 1.23
progress:  90%|[34m [0m| 45/50 [00:57<00:06,  1.25s/it]progress:  92%|[34m[0m| 46/50 [00:57<00:04,  1.24s/it]                                                         Episode 47	 reward: -5.69	 makespan: 563.00	 Mean_loss: 0.04040644,  training time: 1.23
progress:  92%|[34m[0m| 46/50 [00:59<00:04,  1.24s/it]progress:  94%|[34m[0m| 47/50 [00:59<00:03,  1.24s/it]                                                         Episode 48	 reward: -5.27	 makespan: 521.75	 Mean_loss: 0.03436789,  training time: 1.22
progress:  94%|[34m[0m| 47/50 [01:00<00:03,  1.24s/it]progress:  96%|[34m[0m| 48/50 [01:00<00:02,  1.23s/it]                                                         Episode 49	 reward: -5.38	 makespan: 533.00	 Mean_loss: 0.02765104,  training time: 1.22
progress:  96%|[34m[0m| 48/50 [01:01<00:02,  1.23s/it]progress:  98%|[34m[0m| 49/50 [01:01<00:01,  1.23s/it]                                                         Episode 50	 reward: -5.41	 makespan: 535.75	 Mean_loss: 0.03171603,  training time: 1.28
progress:  98%|[34m[0m| 49/50 [01:02<00:01,  1.23s/it]progress: 100%|[34m[0m| 50/50 [01:02<00:00,  1.24s/it]progress: 100%|[34m[0m| 50/50 [01:02<00:00,  1.26s/it]
+ IFS=,
+ read n_j n_m
+ python train/DAN_finetuning.py --logdir ./runs/exp14/maml_finetuning/transfer_maml+exp14_1000_512_3/15x7 --model_suffix free --finetuning_model maml+exp14_1000_512_3 --max_updates 50 --n_j 15 --n_m 7 --num_envs 4 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x7+mix
save model name:  15x7+mix+free
./trained_network/SD2/maml+exp14_1000_512_3.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/maml+exp14_1000_512_3.pth
vali data :./data/data_train_vali/SD2/15x7+mix


progress:   0%|[34m          [0m| 0/50 [00:00<?, ?it/s]                                                Episode 1	 reward: -5.91	 makespan: 585.25	 Mean_loss: 4.33019924,  training time: 2.84
progress:   0%|[34m          [0m| 0/50 [00:02<?, ?it/s]progress:   2%|[34m         [0m| 1/50 [00:02<02:19,  2.84s/it]                                                        Episode 2	 reward: -5.69	 makespan: 563.00	 Mean_loss: 0.40546808,  training time: 1.71
progress:   2%|[34m         [0m| 1/50 [00:04<02:19,  2.84s/it]progress:   4%|[34m         [0m| 2/50 [00:04<01:44,  2.17s/it]                                                        Episode 3	 reward: -6.00	 makespan: 594.00	 Mean_loss: 0.44634777,  training time: 1.72
progress:   4%|[34m         [0m| 2/50 [00:06<01:44,  2.17s/it]progress:   6%|[34m         [0m| 3/50 [00:06<01:32,  1.97s/it]                                                        Episode 4	 reward: -5.92	 makespan: 586.25	 Mean_loss: 0.40599766,  training time: 1.71
progress:   6%|[34m         [0m| 3/50 [00:07<01:32,  1.97s/it]progress:   8%|[34m         [0m| 4/50 [00:07<01:25,  1.86s/it]                                                        Episode 5	 reward: -5.89	 makespan: 583.00	 Mean_loss: 0.29796115,  training time: 1.71
progress:   8%|[34m         [0m| 4/50 [00:09<01:25,  1.86s/it]progress:  10%|[34m         [0m| 5/50 [00:09<01:21,  1.81s/it]                                                        Episode 6	 reward: -5.89	 makespan: 583.00	 Mean_loss: 0.19206733,  training time: 1.71
progress:  10%|[34m         [0m| 5/50 [00:11<01:21,  1.81s/it]progress:  12%|[34m        [0m| 6/50 [00:11<01:18,  1.78s/it]                                                        Episode 7	 reward: -5.72	 makespan: 566.50	 Mean_loss: 0.19771148,  training time: 1.79
progress:  12%|[34m        [0m| 6/50 [00:13<01:18,  1.78s/it]progress:  14%|[34m        [0m| 7/50 [00:13<01:16,  1.78s/it]                                                        Episode 8	 reward: -6.21	 makespan: 615.00	 Mean_loss: 0.12957224,  training time: 1.74
progress:  14%|[34m        [0m| 7/50 [00:14<01:16,  1.78s/it]progress:  16%|[34m        [0m| 8/50 [00:14<01:14,  1.77s/it]                                                        Episode 9	 reward: -5.79	 makespan: 573.25	 Mean_loss: 0.13863085,  training time: 1.74
progress:  16%|[34m        [0m| 8/50 [00:16<01:14,  1.77s/it]progress:  18%|[34m        [0m| 9/50 [00:16<01:12,  1.76s/it]                                                        Episode 10	 reward: -6.26	 makespan: 619.50	 Mean_loss: 0.10611627,  training time: 1.74
progress:  18%|[34m        [0m| 9/50 [00:18<01:12,  1.76s/it]progress:  20%|[34m        [0m| 10/50 [00:18<01:10,  1.75s/it]                                                         Episode 11	 reward: -6.23	 makespan: 617.25	 Mean_loss: 0.09889569,  training time: 1.75
progress:  20%|[34m        [0m| 10/50 [00:20<01:10,  1.75s/it]progress:  22%|[34m       [0m| 11/50 [00:20<01:08,  1.75s/it]                                                         Episode 12	 reward: -5.91	 makespan: 585.25	 Mean_loss: 0.11261803,  training time: 1.86
progress:  22%|[34m       [0m| 11/50 [00:22<01:08,  1.75s/it]progress:  24%|[34m       [0m| 12/50 [00:22<01:07,  1.79s/it]                                                         Episode 13	 reward: -6.07	 makespan: 600.50	 Mean_loss: 0.09833600,  training time: 1.86
progress:  24%|[34m       [0m| 12/50 [00:23<01:07,  1.79s/it]progress:  26%|[34m       [0m| 13/50 [00:23<01:06,  1.81s/it]                                                         Episode 14	 reward: -6.35	 makespan: 628.25	 Mean_loss: 0.08004363,  training time: 1.74
progress:  26%|[34m       [0m| 13/50 [00:25<01:06,  1.81s/it]progress:  28%|[34m       [0m| 14/50 [00:25<01:04,  1.79s/it]                                                         Episode 15	 reward: -6.24	 makespan: 618.00	 Mean_loss: 0.06187831,  training time: 1.75
progress:  28%|[34m       [0m| 14/50 [00:27<01:04,  1.79s/it]progress:  30%|[34m       [0m| 15/50 [00:27<01:02,  1.78s/it]                                                         Episode 16	 reward: -6.43	 makespan: 637.00	 Mean_loss: 0.06732475,  training time: 1.75
progress:  30%|[34m       [0m| 15/50 [00:29<01:02,  1.78s/it]progress:  32%|[34m      [0m| 16/50 [00:29<01:00,  1.77s/it]                                                         Episode 17	 reward: -6.36	 makespan: 629.75	 Mean_loss: 0.05380628,  training time: 1.72
progress:  32%|[34m      [0m| 16/50 [00:30<01:00,  1.77s/it]progress:  34%|[34m      [0m| 17/50 [00:30<00:57,  1.75s/it]                                                         Episode 18	 reward: -6.79	 makespan: 672.50	 Mean_loss: 0.05715684,  training time: 1.72
progress:  34%|[34m      [0m| 17/50 [00:32<00:57,  1.75s/it]progress:  36%|[34m      [0m| 18/50 [00:32<00:55,  1.74s/it]                                                         Episode 19	 reward: -6.50	 makespan: 643.50	 Mean_loss: 0.05843405,  training time: 1.71
progress:  36%|[34m      [0m| 18/50 [00:34<00:55,  1.74s/it]progress:  38%|[34m      [0m| 19/50 [00:34<00:53,  1.74s/it]                                                         Episode 20	 reward: -6.59	 makespan: 652.50	 Mean_loss: 0.07759884,  training time: 1.72
progress:  38%|[34m      [0m| 19/50 [00:36<00:53,  1.74s/it]progress:  40%|[34m      [0m| 20/50 [00:36<00:51,  1.73s/it]                                                         Episode 21	 reward: -7.01	 makespan: 694.00	 Mean_loss: 0.07667866,  training time: 1.72
progress:  40%|[34m      [0m| 20/50 [00:37<00:51,  1.73s/it]progress:  42%|[34m     [0m| 21/50 [00:37<00:50,  1.73s/it]                                                         Episode 22	 reward: -6.53	 makespan: 646.00	 Mean_loss: 0.05583583,  training time: 1.72
progress:  42%|[34m     [0m| 21/50 [00:39<00:50,  1.73s/it]progress:  44%|[34m     [0m| 22/50 [00:39<00:48,  1.73s/it]                                                         Episode 23	 reward: -6.57	 makespan: 650.75	 Mean_loss: 0.05018737,  training time: 1.72
progress:  44%|[34m     [0m| 22/50 [00:41<00:48,  1.73s/it]progress:  46%|[34m     [0m| 23/50 [00:41<00:46,  1.72s/it]                                                         Episode 24	 reward: -6.75	 makespan: 668.25	 Mean_loss: 0.06013624,  training time: 1.72
progress:  46%|[34m     [0m| 23/50 [00:42<00:46,  1.72s/it]progress:  48%|[34m     [0m| 24/50 [00:42<00:44,  1.72s/it]                                                         Episode 25	 reward: -6.36	 makespan: 629.75	 Mean_loss: 0.06996712,  training time: 1.72
progress:  48%|[34m     [0m| 24/50 [00:44<00:44,  1.72s/it]progress:  50%|[34m     [0m| 25/50 [00:44<00:43,  1.72s/it]                                                         Episode 26	 reward: -6.49	 makespan: 642.75	 Mean_loss: 0.05730253,  training time: 1.73
progress:  50%|[34m     [0m| 25/50 [00:46<00:43,  1.72s/it]progress:  52%|[34m    [0m| 26/50 [00:46<00:41,  1.72s/it]                                                         Episode 27	 reward: -6.70	 makespan: 663.75	 Mean_loss: 0.06197944,  training time: 1.71
progress:  52%|[34m    [0m| 26/50 [00:48<00:41,  1.72s/it]progress:  54%|[34m    [0m| 27/50 [00:48<00:39,  1.72s/it]                                                         Episode 28	 reward: -6.78	 makespan: 671.50	 Mean_loss: 0.04884273,  training time: 1.71
progress:  54%|[34m    [0m| 27/50 [00:49<00:39,  1.72s/it]progress:  56%|[34m    [0m| 28/50 [00:49<00:37,  1.72s/it]                                                         Episode 29	 reward: -6.77	 makespan: 670.00	 Mean_loss: 0.04060027,  training time: 1.72
progress:  56%|[34m    [0m| 28/50 [00:51<00:37,  1.72s/it]progress:  58%|[34m    [0m| 29/50 [00:51<00:36,  1.72s/it]                                                         Episode 30	 reward: -6.71	 makespan: 664.50	 Mean_loss: 0.06404554,  training time: 1.71
progress:  58%|[34m    [0m| 29/50 [00:53<00:36,  1.72s/it]progress:  60%|[34m    [0m| 30/50 [00:53<00:34,  1.71s/it]                                                         Episode 31	 reward: -6.95	 makespan: 688.00	 Mean_loss: 0.08158296,  training time: 1.72
progress:  60%|[34m    [0m| 30/50 [00:54<00:34,  1.71s/it]progress:  62%|[34m   [0m| 31/50 [00:54<00:32,  1.72s/it]                                                         Episode 32	 reward: -7.19	 makespan: 712.25	 Mean_loss: 0.06600053,  training time: 1.72
progress:  62%|[34m   [0m| 31/50 [00:56<00:32,  1.72s/it]progress:  64%|[34m   [0m| 32/50 [00:56<00:30,  1.72s/it]                                                         Episode 33	 reward: -7.44	 makespan: 736.25	 Mean_loss: 0.09475243,  training time: 1.72
progress:  64%|[34m   [0m| 32/50 [00:58<00:30,  1.72s/it]progress:  66%|[34m   [0m| 33/50 [00:58<00:29,  1.72s/it]                                                         Episode 34	 reward: -6.78	 makespan: 671.25	 Mean_loss: 0.06056128,  training time: 1.73
progress:  66%|[34m   [0m| 33/50 [01:00<00:29,  1.72s/it]progress:  68%|[34m   [0m| 34/50 [01:00<00:27,  1.72s/it]                                                         Episode 35	 reward: -6.95	 makespan: 688.00	 Mean_loss: 0.04238372,  training time: 1.71
progress:  68%|[34m   [0m| 34/50 [01:01<00:27,  1.72s/it]progress:  70%|[34m   [0m| 35/50 [01:01<00:25,  1.72s/it]                                                         Episode 36	 reward: -6.71	 makespan: 664.75	 Mean_loss: 0.05188818,  training time: 1.72
progress:  70%|[34m   [0m| 35/50 [01:03<00:25,  1.72s/it]progress:  72%|[34m  [0m| 36/50 [01:03<00:24,  1.72s/it]                                                         Episode 37	 reward: -6.97	 makespan: 690.00	 Mean_loss: 0.04432383,  training time: 1.71
progress:  72%|[34m  [0m| 36/50 [01:05<00:24,  1.72s/it]progress:  74%|[34m  [0m| 37/50 [01:05<00:22,  1.72s/it]                                                         Episode 38	 reward: -6.87	 makespan: 680.00	 Mean_loss: 0.05184469,  training time: 1.72
progress:  74%|[34m  [0m| 37/50 [01:06<00:22,  1.72s/it]progress:  76%|[34m  [0m| 38/50 [01:06<00:20,  1.72s/it]                                                         Episode 39	 reward: -6.52	 makespan: 645.00	 Mean_loss: 0.08256046,  training time: 1.70
progress:  76%|[34m  [0m| 38/50 [01:08<00:20,  1.72s/it]progress:  78%|[34m  [0m| 39/50 [01:08<00:18,  1.71s/it]                                                         Episode 40	 reward: -6.47	 makespan: 641.00	 Mean_loss: 0.04984736,  training time: 1.72
progress:  78%|[34m  [0m| 39/50 [01:10<00:18,  1.71s/it]progress:  80%|[34m  [0m| 40/50 [01:10<00:17,  1.72s/it]                                                         Episode 41	 reward: -6.61	 makespan: 654.00	 Mean_loss: 0.03960648,  training time: 1.72
progress:  80%|[34m  [0m| 40/50 [01:12<00:17,  1.72s/it]progress:  82%|[34m [0m| 41/50 [01:12<00:15,  1.72s/it]                                                         Episode 42	 reward: -6.32	 makespan: 625.75	 Mean_loss: 0.03039555,  training time: 1.71
progress:  82%|[34m [0m| 41/50 [01:13<00:15,  1.72s/it]progress:  84%|[34m [0m| 42/50 [01:13<00:13,  1.72s/it]                                                         Episode 43	 reward: -6.42	 makespan: 635.50	 Mean_loss: 0.05705476,  training time: 1.71
progress:  84%|[34m [0m| 42/50 [01:15<00:13,  1.72s/it]progress:  86%|[34m [0m| 43/50 [01:15<00:12,  1.72s/it]                                                         Episode 44	 reward: -6.39	 makespan: 632.75	 Mean_loss: 0.04486599,  training time: 1.70
progress:  86%|[34m [0m| 43/50 [01:17<00:12,  1.72s/it]progress:  88%|[34m [0m| 44/50 [01:17<00:10,  1.71s/it]                                                         Episode 45	 reward: -6.36	 makespan: 629.75	 Mean_loss: 0.03278740,  training time: 1.72
progress:  88%|[34m [0m| 44/50 [01:18<00:10,  1.71s/it]progress:  90%|[34m [0m| 45/50 [01:18<00:08,  1.71s/it]                                                         Episode 46	 reward: -6.07	 makespan: 601.25	 Mean_loss: 0.04444946,  training time: 1.72
progress:  90%|[34m [0m| 45/50 [01:20<00:08,  1.71s/it]progress:  92%|[34m[0m| 46/50 [01:20<00:06,  1.72s/it]                                                         Episode 47	 reward: -6.70	 makespan: 663.75	 Mean_loss: 0.03395266,  training time: 1.72
progress:  92%|[34m[0m| 46/50 [01:22<00:06,  1.72s/it]progress:  94%|[34m[0m| 47/50 [01:22<00:05,  1.72s/it]                                                         Episode 48	 reward: -6.36	 makespan: 629.75	 Mean_loss: 0.02942530,  training time: 1.72
progress:  94%|[34m[0m| 47/50 [01:24<00:05,  1.72s/it]progress:  96%|[34m[0m| 48/50 [01:24<00:03,  1.72s/it]                                                         Episode 49	 reward: -6.54	 makespan: 647.50	 Mean_loss: 0.02672624,  training time: 1.72
progress:  96%|[34m[0m| 48/50 [01:25<00:03,  1.72s/it]progress:  98%|[34m[0m| 49/50 [01:25<00:01,  1.72s/it]                                                         Episode 50	 reward: -7.23	 makespan: 715.50	 Mean_loss: 0.04921217,  training time: 1.72
progress:  98%|[34m[0m| 49/50 [01:27<00:01,  1.72s/it]progress: 100%|[34m[0m| 50/50 [01:27<00:00,  1.72s/it]progress: 100%|[34m[0m| 50/50 [01:27<00:00,  1.75s/it]
+ IFS=,
+ read n_j n_m
+ python train/DAN_finetuning.py --logdir ./runs/exp14/maml_finetuning/transfer_maml+exp14_1000_512_3/15x9 --model_suffix free --finetuning_model maml+exp14_1000_512_3 --max_updates 50 --n_j 15 --n_m 9 --num_envs 4 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x9+mix
save model name:  15x9+mix+free
./trained_network/SD2/maml+exp14_1000_512_3.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/maml+exp14_1000_512_3.pth
vali data :./data/data_train_vali/SD2/15x9+mix


progress:   0%|[34m          [0m| 0/50 [00:00<?, ?it/s]                                                Episode 1	 reward: -6.13	 makespan: 607.25	 Mean_loss: 4.41643763,  training time: 3.22
progress:   0%|[34m          [0m| 0/50 [00:03<?, ?it/s]progress:   2%|[34m         [0m| 1/50 [00:03<02:37,  3.22s/it]                                                        Episode 2	 reward: -5.95	 makespan: 588.75	 Mean_loss: 0.33865929,  training time: 2.14
progress:   2%|[34m         [0m| 1/50 [00:05<02:37,  3.22s/it]progress:   4%|[34m         [0m| 2/50 [00:05<02:04,  2.59s/it]                                                        Episode 3	 reward: -5.69	 makespan: 563.75	 Mean_loss: 0.45783472,  training time: 2.15
progress:   4%|[34m         [0m| 2/50 [00:07<02:04,  2.59s/it]progress:   6%|[34m         [0m| 3/50 [00:07<01:52,  2.39s/it]                                                        Episode 4	 reward: -5.84	 makespan: 578.25	 Mean_loss: 0.33313853,  training time: 2.11
progress:   6%|[34m         [0m| 3/50 [00:09<01:52,  2.39s/it]progress:   8%|[34m         [0m| 4/50 [00:09<01:44,  2.28s/it]                                                        Episode 5	 reward: -5.94	 makespan: 588.25	 Mean_loss: 0.30159873,  training time: 2.14
progress:   8%|[34m         [0m| 4/50 [00:11<01:44,  2.28s/it]progress:  10%|[34m         [0m| 5/50 [00:11<01:40,  2.23s/it]                                                        Episode 6	 reward: -5.72	 makespan: 566.50	 Mean_loss: 0.15332106,  training time: 2.26
progress:  10%|[34m         [0m| 5/50 [00:14<01:40,  2.23s/it]progress:  12%|[34m        [0m| 6/50 [00:14<01:38,  2.24s/it]                                                        Episode 7	 reward: -6.17	 makespan: 610.75	 Mean_loss: 0.17589208,  training time: 2.13
progress:  12%|[34m        [0m| 6/50 [00:16<01:38,  2.24s/it]progress:  14%|[34m        [0m| 7/50 [00:16<01:34,  2.20s/it]                                                        Episode 8	 reward: -6.19	 makespan: 613.25	 Mean_loss: 0.12494727,  training time: 2.13
progress:  14%|[34m        [0m| 7/50 [00:18<01:34,  2.20s/it]progress:  16%|[34m        [0m| 8/50 [00:18<01:31,  2.18s/it]                                                        Episode 9	 reward: -6.21	 makespan: 614.50	 Mean_loss: 0.09869238,  training time: 2.13
progress:  16%|[34m        [0m| 8/50 [00:20<01:31,  2.18s/it]progress:  18%|[34m        [0m| 9/50 [00:20<01:28,  2.16s/it]                                                        Episode 10	 reward: -6.09	 makespan: 602.50	 Mean_loss: 0.10216738,  training time: 2.13
progress:  18%|[34m        [0m| 9/50 [00:22<01:28,  2.16s/it]progress:  20%|[34m        [0m| 10/50 [00:22<01:26,  2.16s/it]                                                         Episode 11	 reward: -5.81	 makespan: 575.50	 Mean_loss: 0.08220251,  training time: 2.18
progress:  20%|[34m        [0m| 10/50 [00:24<01:26,  2.16s/it]progress:  22%|[34m       [0m| 11/50 [00:24<01:24,  2.16s/it]                                                         Episode 12	 reward: -6.25	 makespan: 618.75	 Mean_loss: 0.05794708,  training time: 2.14
progress:  22%|[34m       [0m| 11/50 [00:26<01:24,  2.16s/it]progress:  24%|[34m       [0m| 12/50 [00:26<01:21,  2.15s/it]                                                         Episode 13	 reward: -6.07	 makespan: 600.75	 Mean_loss: 0.07085916,  training time: 2.14
progress:  24%|[34m       [0m| 12/50 [00:29<01:21,  2.15s/it]progress:  26%|[34m       [0m| 13/50 [00:29<01:19,  2.15s/it]                                                         Episode 14	 reward: -6.22	 makespan: 615.50	 Mean_loss: 0.07307883,  training time: 2.14
progress:  26%|[34m       [0m| 13/50 [00:31<01:19,  2.15s/it]progress:  28%|[34m       [0m| 14/50 [00:31<01:17,  2.15s/it]                                                         Episode 15	 reward: -6.10	 makespan: 603.75	 Mean_loss: 0.04841381,  training time: 2.14
progress:  28%|[34m       [0m| 14/50 [00:33<01:17,  2.15s/it]progress:  30%|[34m       [0m| 15/50 [00:33<01:15,  2.15s/it]                                                         Episode 16	 reward: -6.02	 makespan: 595.75	 Mean_loss: 0.03338518,  training time: 2.15
progress:  30%|[34m       [0m| 15/50 [00:35<01:15,  2.15s/it]progress:  32%|[34m      [0m| 16/50 [00:35<01:12,  2.15s/it]                                                         Episode 17	 reward: -5.98	 makespan: 592.00	 Mean_loss: 0.06272025,  training time: 2.14
progress:  32%|[34m      [0m| 16/50 [00:37<01:12,  2.15s/it]progress:  34%|[34m      [0m| 17/50 [00:37<01:10,  2.15s/it]                                                         Episode 18	 reward: -6.39	 makespan: 632.50	 Mean_loss: 0.04779646,  training time: 2.15
progress:  34%|[34m      [0m| 17/50 [00:39<01:10,  2.15s/it]progress:  36%|[34m      [0m| 18/50 [00:39<01:08,  2.15s/it]                                                         Episode 19	 reward: -5.89	 makespan: 583.00	 Mean_loss: 0.04215682,  training time: 2.16
progress:  36%|[34m      [0m| 18/50 [00:41<01:08,  2.15s/it]progress:  38%|[34m      [0m| 19/50 [00:41<01:06,  2.15s/it]                                                         Episode 20	 reward: -5.83	 makespan: 577.25	 Mean_loss: 0.04240596,  training time: 2.14
progress:  38%|[34m      [0m| 19/50 [00:44<01:06,  2.15s/it]progress:  40%|[34m      [0m| 20/50 [00:44<01:04,  2.15s/it]                                                         Episode 21	 reward: -6.08	 makespan: 601.50	 Mean_loss: 0.03868792,  training time: 2.13
progress:  40%|[34m      [0m| 20/50 [00:46<01:04,  2.15s/it]progress:  42%|[34m     [0m| 21/50 [00:46<01:02,  2.14s/it]                                                         Episode 22	 reward: -6.38	 makespan: 631.50	 Mean_loss: 0.04064824,  training time: 2.12
progress:  42%|[34m     [0m| 21/50 [00:48<01:02,  2.14s/it]progress:  44%|[34m     [0m| 22/50 [00:48<00:59,  2.14s/it]                                                         Episode 23	 reward: -5.85	 makespan: 578.75	 Mean_loss: 0.04277314,  training time: 2.14
progress:  44%|[34m     [0m| 22/50 [00:50<00:59,  2.14s/it]progress:  46%|[34m     [0m| 23/50 [00:50<00:57,  2.14s/it]                                                         Episode 24	 reward: -5.78	 makespan: 572.50	 Mean_loss: 0.03344132,  training time: 2.15
progress:  46%|[34m     [0m| 23/50 [00:52<00:57,  2.14s/it]progress:  48%|[34m     [0m| 24/50 [00:52<00:55,  2.14s/it]                                                         Episode 25	 reward: -5.88	 makespan: 582.50	 Mean_loss: 0.03078709,  training time: 2.14
progress:  48%|[34m     [0m| 24/50 [00:54<00:55,  2.14s/it]progress:  50%|[34m     [0m| 25/50 [00:54<00:53,  2.14s/it]                                                         Episode 26	 reward: -5.75	 makespan: 569.50	 Mean_loss: 0.02908550,  training time: 2.15
progress:  50%|[34m     [0m| 25/50 [00:56<00:53,  2.14s/it]progress:  52%|[34m    [0m| 26/50 [00:56<00:51,  2.14s/it]                                                         Episode 27	 reward: -5.86	 makespan: 580.25	 Mean_loss: 0.03731626,  training time: 2.13
progress:  52%|[34m    [0m| 26/50 [00:58<00:51,  2.14s/it]progress:  54%|[34m    [0m| 27/50 [00:58<00:49,  2.14s/it]                                                         Episode 28	 reward: -6.11	 makespan: 605.25	 Mean_loss: 0.04935477,  training time: 2.13
progress:  54%|[34m    [0m| 27/50 [01:01<00:49,  2.14s/it]progress:  56%|[34m    [0m| 28/50 [01:01<00:47,  2.14s/it]                                                         Episode 29	 reward: -5.80	 makespan: 574.25	 Mean_loss: 0.02211009,  training time: 2.12
progress:  56%|[34m    [0m| 28/50 [01:03<00:47,  2.14s/it]progress:  58%|[34m    [0m| 29/50 [01:03<00:44,  2.13s/it]                                                         Episode 30	 reward: -5.87	 makespan: 581.00	 Mean_loss: 0.03925042,  training time: 2.13
progress:  58%|[34m    [0m| 29/50 [01:05<00:44,  2.13s/it]progress:  60%|[34m    [0m| 30/50 [01:05<00:42,  2.13s/it]                                                         Episode 31	 reward: -5.85	 makespan: 579.25	 Mean_loss: 0.02780823,  training time: 2.14
progress:  60%|[34m    [0m| 30/50 [01:07<00:42,  2.13s/it]progress:  62%|[34m   [0m| 31/50 [01:07<00:40,  2.14s/it]                                                         Episode 32	 reward: -5.85	 makespan: 579.25	 Mean_loss: 0.02952809,  training time: 2.15
progress:  62%|[34m   [0m| 31/50 [01:09<00:40,  2.14s/it]progress:  64%|[34m   [0m| 32/50 [01:09<00:38,  2.14s/it]                                                         Episode 33	 reward: -5.73	 makespan: 567.50	 Mean_loss: 0.04262735,  training time: 2.14
progress:  64%|[34m   [0m| 32/50 [01:11<00:38,  2.14s/it]progress:  66%|[34m   [0m| 33/50 [01:11<00:36,  2.14s/it]                                                         Episode 34	 reward: -6.20	 makespan: 613.50	 Mean_loss: 0.04022553,  training time: 2.21
progress:  66%|[34m   [0m| 33/50 [01:14<00:36,  2.14s/it]progress:  68%|[34m   [0m| 34/50 [01:14<00:34,  2.16s/it]                                                         Episode 35	 reward: -5.98	 makespan: 592.00	 Mean_loss: 0.03770100,  training time: 2.13
progress:  68%|[34m   [0m| 34/50 [01:16<00:34,  2.16s/it]progress:  70%|[34m   [0m| 35/50 [01:16<00:32,  2.15s/it]                                                         Episode 36	 reward: -6.11	 makespan: 604.75	 Mean_loss: 0.02670164,  training time: 2.13
progress:  70%|[34m   [0m| 35/50 [01:18<00:32,  2.15s/it]progress:  72%|[34m  [0m| 36/50 [01:18<00:30,  2.15s/it]                                                         Episode 37	 reward: -5.91	 makespan: 585.50	 Mean_loss: 0.03751290,  training time: 2.14
progress:  72%|[34m  [0m| 36/50 [01:20<00:30,  2.15s/it]progress:  74%|[34m  [0m| 37/50 [01:20<00:27,  2.14s/it]                                                         Episode 38	 reward: -5.92	 makespan: 586.25	 Mean_loss: 0.03749100,  training time: 2.15
progress:  74%|[34m  [0m| 37/50 [01:22<00:27,  2.14s/it]progress:  76%|[34m  [0m| 38/50 [01:22<00:25,  2.15s/it]                                                         Episode 39	 reward: -5.89	 makespan: 583.50	 Mean_loss: 0.03373734,  training time: 2.16
progress:  76%|[34m  [0m| 38/50 [01:24<00:25,  2.15s/it]progress:  78%|[34m  [0m| 39/50 [01:24<00:23,  2.15s/it]                                                         Episode 40	 reward: -5.83	 makespan: 576.75	 Mean_loss: 0.03516370,  training time: 2.15
progress:  78%|[34m  [0m| 39/50 [01:26<00:23,  2.15s/it]progress:  80%|[34m  [0m| 40/50 [01:26<00:21,  2.15s/it]                                                         Episode 41	 reward: -6.42	 makespan: 635.75	 Mean_loss: 0.06590845,  training time: 2.14
progress:  80%|[34m  [0m| 40/50 [01:29<00:21,  2.15s/it]progress:  82%|[34m [0m| 41/50 [01:29<00:19,  2.15s/it]                                                         Episode 42	 reward: -5.74	 makespan: 568.25	 Mean_loss: 0.04073340,  training time: 2.16
progress:  82%|[34m [0m| 41/50 [01:31<00:19,  2.15s/it]progress:  84%|[34m [0m| 42/50 [01:31<00:17,  2.15s/it]                                                         Episode 43	 reward: -5.76	 makespan: 569.75	 Mean_loss: 0.04413792,  training time: 2.16
progress:  84%|[34m [0m| 42/50 [01:33<00:17,  2.15s/it]progress:  86%|[34m [0m| 43/50 [01:33<00:15,  2.16s/it]                                                         Episode 44	 reward: -5.92	 makespan: 586.00	 Mean_loss: 0.02626760,  training time: 2.15
progress:  86%|[34m [0m| 43/50 [01:35<00:15,  2.16s/it]progress:  88%|[34m [0m| 44/50 [01:35<00:12,  2.15s/it]                                                         Episode 45	 reward: -6.10	 makespan: 603.50	 Mean_loss: 0.02562744,  training time: 2.14
progress:  88%|[34m [0m| 44/50 [01:37<00:12,  2.15s/it]progress:  90%|[34m [0m| 45/50 [01:37<00:10,  2.15s/it]                                                         Episode 46	 reward: -5.64	 makespan: 558.75	 Mean_loss: 0.04158508,  training time: 2.17
progress:  90%|[34m [0m| 45/50 [01:39<00:10,  2.15s/it]progress:  92%|[34m[0m| 46/50 [01:39<00:08,  2.15s/it]                                                         Episode 47	 reward: -5.85	 makespan: 579.25	 Mean_loss: 0.03616943,  training time: 2.14
progress:  92%|[34m[0m| 46/50 [01:41<00:08,  2.15s/it]progress:  94%|[34m[0m| 47/50 [01:41<00:06,  2.15s/it]                                                         Episode 48	 reward: -5.93	 makespan: 587.50	 Mean_loss: 0.05794263,  training time: 2.15
progress:  94%|[34m[0m| 47/50 [01:44<00:06,  2.15s/it]progress:  96%|[34m[0m| 48/50 [01:44<00:04,  2.15s/it]                                                         Episode 49	 reward: -5.97	 makespan: 591.50	 Mean_loss: 0.03567388,  training time: 2.15
progress:  96%|[34m[0m| 48/50 [01:46<00:04,  2.15s/it]progress:  98%|[34m[0m| 49/50 [01:46<00:02,  2.15s/it]                                                         Episode 50	 reward: -5.93	 makespan: 587.25	 Mean_loss: 0.02212925,  training time: 2.15
progress:  98%|[34m[0m| 49/50 [01:48<00:02,  2.15s/it]progress: 100%|[34m[0m| 50/50 [01:48<00:00,  2.15s/it]progress: 100%|[34m[0m| 50/50 [01:48<00:00,  2.17s/it]
+ IFS=,
+ read n_j n_m
+ python train/DAN_finetuning.py --logdir ./runs/exp14/maml_finetuning/transfer_maml+exp14_1000_512_3/15x10 --model_suffix free --finetuning_model maml+exp14_1000_512_3 --max_updates 50 --n_j 15 --n_m 10 --num_envs 4 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x10+mix
save model name:  15x10+mix+free
./trained_network/SD2/maml+exp14_1000_512_3.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/maml+exp14_1000_512_3.pth
vali data :./data/data_train_vali/SD2/15x10+mix


progress:   0%|[34m          [0m| 0/50 [00:00<?, ?it/s]                                                Episode 1	 reward: -6.93	 makespan: 685.75	 Mean_loss: 4.10174179,  training time: 3.49
progress:   0%|[34m          [0m| 0/50 [00:03<?, ?it/s]progress:   2%|[34m         [0m| 1/50 [00:03<02:51,  3.49s/it]                                                        Episode 2	 reward: -6.57	 makespan: 650.75	 Mean_loss: 0.29355687,  training time: 2.33
progress:   2%|[34m         [0m| 1/50 [00:05<02:51,  3.49s/it]progress:   4%|[34m         [0m| 2/50 [00:05<02:14,  2.81s/it]                                                        Episode 3	 reward: -6.53	 makespan: 646.50	 Mean_loss: 0.34148628,  training time: 2.38
progress:   4%|[34m         [0m| 2/50 [00:08<02:14,  2.81s/it]progress:   6%|[34m         [0m| 3/50 [00:08<02:02,  2.61s/it]                                                        Episode 4	 reward: -6.74	 makespan: 667.50	 Mean_loss: 0.31131333,  training time: 2.39
progress:   6%|[34m         [0m| 3/50 [00:10<02:02,  2.61s/it]progress:   8%|[34m         [0m| 4/50 [00:10<01:56,  2.53s/it]                                                        Episode 5	 reward: -6.73	 makespan: 666.75	 Mean_loss: 0.30320862,  training time: 2.35
progress:   8%|[34m         [0m| 4/50 [00:12<01:56,  2.53s/it]progress:  10%|[34m         [0m| 5/50 [00:12<01:50,  2.46s/it]                                                        Episode 6	 reward: -6.54	 makespan: 647.50	 Mean_loss: 0.23148516,  training time: 2.49
progress:  10%|[34m         [0m| 5/50 [00:15<01:50,  2.46s/it]progress:  12%|[34m        [0m| 6/50 [00:15<01:48,  2.47s/it]                                                        Episode 7	 reward: -6.53	 makespan: 646.50	 Mean_loss: 0.16695693,  training time: 2.40
progress:  12%|[34m        [0m| 6/50 [00:17<01:48,  2.47s/it]progress:  14%|[34m        [0m| 7/50 [00:17<01:45,  2.45s/it]                                                        Episode 8	 reward: -6.35	 makespan: 629.00	 Mean_loss: 0.13079616,  training time: 2.34
progress:  14%|[34m        [0m| 7/50 [00:20<01:45,  2.45s/it]progress:  16%|[34m        [0m| 8/50 [00:20<01:41,  2.41s/it]                                                        Episode 9	 reward: -6.63	 makespan: 656.50	 Mean_loss: 0.14679107,  training time: 2.36
progress:  16%|[34m        [0m| 8/50 [00:22<01:41,  2.41s/it]progress:  18%|[34m        [0m| 9/50 [00:22<01:38,  2.40s/it]                                                        Episode 10	 reward: -6.38	 makespan: 631.25	 Mean_loss: 0.09826833,  training time: 2.36
progress:  18%|[34m        [0m| 9/50 [00:24<01:38,  2.40s/it]progress:  20%|[34m        [0m| 10/50 [00:24<01:35,  2.39s/it]                                                         Episode 11	 reward: -6.48	 makespan: 641.75	 Mean_loss: 0.10945325,  training time: 2.37
progress:  20%|[34m        [0m| 10/50 [00:27<01:35,  2.39s/it]progress:  22%|[34m       [0m| 11/50 [00:27<01:32,  2.38s/it]                                                         Episode 12	 reward: -6.68	 makespan: 661.50	 Mean_loss: 0.06429991,  training time: 2.33
progress:  22%|[34m       [0m| 11/50 [00:29<01:32,  2.38s/it]progress:  24%|[34m       [0m| 12/50 [00:29<01:29,  2.37s/it]                                                         Episode 13	 reward: -6.61	 makespan: 654.50	 Mean_loss: 0.05960527,  training time: 2.32
progress:  24%|[34m       [0m| 12/50 [00:31<01:29,  2.37s/it]progress:  26%|[34m       [0m| 13/50 [00:31<01:27,  2.35s/it]                                                         Episode 14	 reward: -6.57	 makespan: 650.75	 Mean_loss: 0.05251980,  training time: 2.34
progress:  26%|[34m       [0m| 13/50 [00:34<01:27,  2.35s/it]progress:  28%|[34m       [0m| 14/50 [00:34<01:24,  2.35s/it]                                                         Episode 15	 reward: -6.80	 makespan: 673.50	 Mean_loss: 0.06805661,  training time: 2.37
progress:  28%|[34m       [0m| 14/50 [00:36<01:24,  2.35s/it]progress:  30%|[34m       [0m| 15/50 [00:36<01:22,  2.35s/it]                                                         Episode 16	 reward: -6.67	 makespan: 660.75	 Mean_loss: 0.05642334,  training time: 2.32
progress:  30%|[34m       [0m| 15/50 [00:38<01:22,  2.35s/it]progress:  32%|[34m      [0m| 16/50 [00:38<01:19,  2.35s/it]                                                         Episode 17	 reward: -7.06	 makespan: 698.75	 Mean_loss: 0.12304159,  training time: 2.36
progress:  32%|[34m      [0m| 16/50 [00:41<01:19,  2.35s/it]progress:  34%|[34m      [0m| 17/50 [00:41<01:17,  2.35s/it]                                                         Episode 18	 reward: -7.51	 makespan: 743.75	 Mean_loss: 0.09035502,  training time: 2.32
progress:  34%|[34m      [0m| 17/50 [00:43<01:17,  2.35s/it]progress:  36%|[34m      [0m| 18/50 [00:43<01:14,  2.34s/it]                                                         Episode 19	 reward: -7.12	 makespan: 705.00	 Mean_loss: 0.08442102,  training time: 2.34
progress:  36%|[34m      [0m| 18/50 [00:45<01:14,  2.34s/it]progress:  38%|[34m      [0m| 19/50 [00:45<01:12,  2.34s/it]                                                         Episode 20	 reward: -6.87	 makespan: 680.50	 Mean_loss: 0.09287403,  training time: 2.35
progress:  38%|[34m      [0m| 19/50 [00:48<01:12,  2.34s/it]progress:  40%|[34m      [0m| 20/50 [00:48<01:10,  2.35s/it]                                                         Episode 21	 reward: -6.82	 makespan: 675.00	 Mean_loss: 0.07322911,  training time: 2.38
progress:  40%|[34m      [0m| 20/50 [00:50<01:10,  2.35s/it]progress:  42%|[34m     [0m| 21/50 [00:50<01:08,  2.36s/it]                                                         Episode 22	 reward: -6.69	 makespan: 662.25	 Mean_loss: 0.05194087,  training time: 2.34
progress:  42%|[34m     [0m| 21/50 [00:53<01:08,  2.36s/it]progress:  44%|[34m     [0m| 22/50 [00:53<01:05,  2.35s/it]                                                         Episode 23	 reward: -6.49	 makespan: 642.75	 Mean_loss: 0.07871455,  training time: 2.35
progress:  44%|[34m     [0m| 22/50 [00:55<01:05,  2.35s/it]progress:  46%|[34m     [0m| 23/50 [00:55<01:03,  2.35s/it]                                                         Episode 24	 reward: -6.74	 makespan: 667.50	 Mean_loss: 0.06387316,  training time: 2.36
progress:  46%|[34m     [0m| 23/50 [00:57<01:03,  2.35s/it]progress:  48%|[34m     [0m| 24/50 [00:57<01:01,  2.36s/it]                                                         Episode 25	 reward: -6.53	 makespan: 646.00	 Mean_loss: 0.05729391,  training time: 2.36
progress:  48%|[34m     [0m| 24/50 [01:00<01:01,  2.36s/it]progress:  50%|[34m     [0m| 25/50 [01:00<00:58,  2.36s/it]                                                         Episode 26	 reward: -6.34	 makespan: 627.75	 Mean_loss: 0.05616744,  training time: 2.31
progress:  50%|[34m     [0m| 25/50 [01:02<00:58,  2.36s/it]progress:  52%|[34m    [0m| 26/50 [01:02<00:56,  2.34s/it]                                                         Episode 27	 reward: -6.51	 makespan: 644.00	 Mean_loss: 0.03654174,  training time: 2.33
progress:  52%|[34m    [0m| 26/50 [01:04<00:56,  2.34s/it]progress:  54%|[34m    [0m| 27/50 [01:04<00:53,  2.34s/it]                                                         Episode 28	 reward: -6.45	 makespan: 638.25	 Mean_loss: 0.04055236,  training time: 2.32
progress:  54%|[34m    [0m| 27/50 [01:07<00:53,  2.34s/it]progress:  56%|[34m    [0m| 28/50 [01:07<00:51,  2.33s/it]                                                         Episode 29	 reward: -6.41	 makespan: 634.50	 Mean_loss: 0.03947021,  training time: 2.35
progress:  56%|[34m    [0m| 28/50 [01:09<00:51,  2.33s/it]progress:  58%|[34m    [0m| 29/50 [01:09<00:49,  2.34s/it]                                                         Episode 30	 reward: -6.42	 makespan: 635.25	 Mean_loss: 0.03239702,  training time: 2.33
progress:  58%|[34m    [0m| 29/50 [01:11<00:49,  2.34s/it]progress:  60%|[34m    [0m| 30/50 [01:11<00:46,  2.34s/it]                                                         Episode 31	 reward: -6.66	 makespan: 659.00	 Mean_loss: 0.03976836,  training time: 2.30
progress:  60%|[34m    [0m| 30/50 [01:14<00:46,  2.34s/it]progress:  62%|[34m   [0m| 31/50 [01:14<00:44,  2.33s/it]                                                         Episode 32	 reward: -6.88	 makespan: 680.75	 Mean_loss: 0.05488175,  training time: 2.41
progress:  62%|[34m   [0m| 31/50 [01:16<00:44,  2.33s/it]progress:  64%|[34m   [0m| 32/50 [01:16<00:42,  2.35s/it]                                                         Episode 33	 reward: -6.69	 makespan: 662.75	 Mean_loss: 0.05118104,  training time: 2.34
progress:  64%|[34m   [0m| 32/50 [01:18<00:42,  2.35s/it]progress:  66%|[34m   [0m| 33/50 [01:18<00:39,  2.35s/it]                                                         Episode 34	 reward: -6.42	 makespan: 635.50	 Mean_loss: 0.03802728,  training time: 2.35
progress:  66%|[34m   [0m| 33/50 [01:21<00:39,  2.35s/it]progress:  68%|[34m   [0m| 34/50 [01:21<00:37,  2.35s/it]                                                         Episode 35	 reward: -6.58	 makespan: 651.25	 Mean_loss: 0.03729313,  training time: 2.40
progress:  68%|[34m   [0m| 34/50 [01:23<00:37,  2.35s/it]progress:  70%|[34m   [0m| 35/50 [01:23<00:35,  2.37s/it]                                                         Episode 36	 reward: -6.51	 makespan: 644.25	 Mean_loss: 0.03016660,  training time: 2.36
progress:  70%|[34m   [0m| 35/50 [01:25<00:35,  2.37s/it]progress:  72%|[34m  [0m| 36/50 [01:25<00:33,  2.36s/it]                                                         Episode 37	 reward: -6.57	 makespan: 650.50	 Mean_loss: 0.02414797,  training time: 2.33
progress:  72%|[34m  [0m| 36/50 [01:28<00:33,  2.36s/it]progress:  74%|[34m  [0m| 37/50 [01:28<00:30,  2.36s/it]                                                         Episode 38	 reward: -6.62	 makespan: 655.25	 Mean_loss: 0.03735563,  training time: 2.36
progress:  74%|[34m  [0m| 37/50 [01:30<00:30,  2.36s/it]progress:  76%|[34m  [0m| 38/50 [01:30<00:28,  2.36s/it]                                                         Episode 39	 reward: -6.86	 makespan: 679.25	 Mean_loss: 0.04634988,  training time: 2.36
progress:  76%|[34m  [0m| 38/50 [01:33<00:28,  2.36s/it]progress:  78%|[34m  [0m| 39/50 [01:33<00:25,  2.36s/it]                                                         Episode 40	 reward: -6.58	 makespan: 651.75	 Mean_loss: 0.04366240,  training time: 2.32
progress:  78%|[34m  [0m| 39/50 [01:35<00:25,  2.36s/it]progress:  80%|[34m  [0m| 40/50 [01:35<00:23,  2.35s/it]                                                         Episode 41	 reward: -6.85	 makespan: 678.25	 Mean_loss: 0.04501265,  training time: 2.34
progress:  80%|[34m  [0m| 40/50 [01:37<00:23,  2.35s/it]progress:  82%|[34m [0m| 41/50 [01:37<00:21,  2.35s/it]                                                         Episode 42	 reward: -6.83	 makespan: 676.50	 Mean_loss: 0.03178826,  training time: 2.35
progress:  82%|[34m [0m| 41/50 [01:40<00:21,  2.35s/it]progress:  84%|[34m [0m| 42/50 [01:40<00:18,  2.35s/it]                                                         Episode 43	 reward: -6.90	 makespan: 683.00	 Mean_loss: 0.04035423,  training time: 2.34
progress:  84%|[34m [0m| 42/50 [01:42<00:18,  2.35s/it]progress:  86%|[34m [0m| 43/50 [01:42<00:16,  2.35s/it]                                                         Episode 44	 reward: -6.91	 makespan: 683.75	 Mean_loss: 0.03388410,  training time: 2.31
progress:  86%|[34m [0m| 43/50 [01:44<00:16,  2.35s/it]progress:  88%|[34m [0m| 44/50 [01:44<00:14,  2.34s/it]                                                         Episode 45	 reward: -6.69	 makespan: 662.00	 Mean_loss: 0.05509268,  training time: 2.31
progress:  88%|[34m [0m| 44/50 [01:46<00:14,  2.34s/it]progress:  90%|[34m [0m| 45/50 [01:47<00:11,  2.33s/it]                                                         Episode 46	 reward: -6.46	 makespan: 639.50	 Mean_loss: 0.03828942,  training time: 2.32
progress:  90%|[34m [0m| 45/50 [01:49<00:11,  2.33s/it]progress:  92%|[34m[0m| 46/50 [01:49<00:09,  2.33s/it]                                                         Episode 47	 reward: -7.08	 makespan: 700.75	 Mean_loss: 0.04443213,  training time: 2.35
progress:  92%|[34m[0m| 46/50 [01:51<00:09,  2.33s/it]progress:  94%|[34m[0m| 47/50 [01:51<00:07,  2.33s/it]                                                         Episode 48	 reward: -6.74	 makespan: 667.00	 Mean_loss: 0.03272767,  training time: 2.35
progress:  94%|[34m[0m| 47/50 [01:54<00:07,  2.33s/it]progress:  96%|[34m[0m| 48/50 [01:54<00:04,  2.34s/it]                                                         Episode 49	 reward: -6.82	 makespan: 674.75	 Mean_loss: 0.04633806,  training time: 2.35
progress:  96%|[34m[0m| 48/50 [01:56<00:04,  2.34s/it]progress:  98%|[34m[0m| 49/50 [01:56<00:02,  2.34s/it]                                                         Episode 50	 reward: -7.29	 makespan: 721.50	 Mean_loss: 0.06432173,  training time: 2.35
progress:  98%|[34m[0m| 49/50 [01:58<00:02,  2.34s/it]progress: 100%|[34m[0m| 50/50 [01:58<00:00,  2.35s/it]progress: 100%|[34m[0m| 50/50 [01:58<00:00,  2.37s/it]
+ IFS=,
+ read n_j n_m

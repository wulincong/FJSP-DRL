+ source /work/home/lxx_hzau/.bashrc
++ export CUDA_HOME=/usr/local/cuda-11.1/
++ CUDA_HOME=/usr/local/cuda-11.1/
++ export PATH=/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
++ PATH=/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
++ '[' -f /etc/bashrc ']'
++ . /etc/bashrc
+++ '[' '' ']'
+++ shopt -q login_shell
+++ '[' 5235 -gt 199 ']'
++++ /usr/bin/id -gn
++++ /usr/bin/id -un
+++ '[' ac6owqc808 = lxx_hzau ']'
+++ umask 022
+++ SHELL=/bin/bash
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/256term.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/256term.sh
++++ local256=truecolor
++++ '[' -n truecolor ']'
++++ case "$TERM" in
++++ export TERM
++++ '[' -n '' ']'
++++ unset local256
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/abrt-console-notification.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/abrt-console-notification.sh
++++ tty -s
++++ return 0
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/bash_completion.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/bash_completion.sh
++++ '[' -z '4.2.46(2)-release' -o -z '' -o -n '' ']'
++++ return
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/check.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/check.sh
++++ export CHECK_HOME=/opt/hpc/setfreq/
++++ CHECK_HOME=/opt/hpc/setfreq/
++++ export PATH=/opt/hpc/setfreq/:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
++++ PATH=/opt/hpc/setfreq/:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/clusconf-env.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/clusconf-env.sh
++++ export CLUSCONF_HOME=/opt/clusconf
++++ CLUSCONF_HOME=/opt/clusconf
++++ export PATH=/opt/clusconf/bin:/opt/hpc/setfreq/:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
++++ PATH=/opt/clusconf/bin:/opt/hpc/setfreq/:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
++++ export PATH=/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq/:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
++++ PATH=/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq/:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
++++ export NFSCONF=/opt/clusconf/etc/nfs.cfg
++++ NFSCONF=/opt/clusconf/etc/nfs.cfg
++++ export IPMICONF=/opt/clusconf/etc/ipmi.cfg
++++ IPMICONF=/opt/clusconf/etc/ipmi.cfg
++++ export AUTOCLUSCONF=/opt/clusconf/etc/autoconf.cfg
++++ AUTOCLUSCONF=/opt/clusconf/etc/autoconf.cfg
++++ export STARTWAITTIME=300
++++ STARTWAITTIME=300
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/colorgrep.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/colorgrep.sh
++++ /usr/libexec/grepconf.sh -c
++++ alias 'grep=grep --color=auto'
++++ alias 'egrep=egrep --color=auto'
++++ alias 'fgrep=fgrep --color=auto'
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/colorls.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/colorls.sh
++++ '[' '!' -t 0 ']'
++++ return
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/dawning.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/dawning.sh
++++ export GRIDVIEW_HOME=/opt/gridview
++++ GRIDVIEW_HOME=/opt/gridview
++++ export MUNGE_HOME=/opt/gridview/munge
++++ MUNGE_HOME=/opt/gridview/munge
++++ export PATH=/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq/:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
++++ PATH=/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq/:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
++++ export LD_LIBRARY_PATH=/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/sharp/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/lib:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:
++++ LD_LIBRARY_PATH=/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/sharp/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/lib:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:
++++ export MANPATH=/opt/gridview/munge/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/share/man:/opt/rh/devtoolset-7/root/usr/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:
++++ MANPATH=/opt/gridview/munge/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/share/man:/opt/rh/devtoolset-7/root/usr/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:
++++ export C_INCLUDE_PATH=/opt/gridview/munge/include:/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/rh/devtoolset-7/root/usr/include/c++/7:/opt/rh/devtoolset-7/root/usr/include/c++/7/x86_64-redhat-linux:/opt/gridview/slurm/include:/opt/gridview/munge/include:
++++ C_INCLUDE_PATH=/opt/gridview/munge/include:/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/rh/devtoolset-7/root/usr/include/c++/7:/opt/rh/devtoolset-7/root/usr/include/c++/7/x86_64-redhat-linux:/opt/gridview/slurm/include:/opt/gridview/munge/include:
++++ export SLURM_HOME=/opt/gridview/slurm
++++ SLURM_HOME=/opt/gridview/slurm
++++ export PATH=/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq/:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
++++ PATH=/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq/:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
++++ export LD_LIBRARY_PATH=/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/sharp/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/lib:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:
++++ LD_LIBRARY_PATH=/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/sharp/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/lib:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:
++++ export MANPATH=/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/share/man:/opt/rh/devtoolset-7/root/usr/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:
++++ MANPATH=/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/share/man:/opt/rh/devtoolset-7/root/usr/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:
++++ export C_INCLUDE_PATH=/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/rh/devtoolset-7/root/usr/include/c++/7:/opt/rh/devtoolset-7/root/usr/include/c++/7/x86_64-redhat-linux:/opt/gridview/slurm/include:/opt/gridview/munge/include:
++++ C_INCLUDE_PATH=/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/rh/devtoolset-7/root/usr/include/c++/7:/opt/rh/devtoolset-7/root/usr/include/c++/7/x86_64-redhat-linux:/opt/gridview/slurm/include:/opt/gridview/munge/include:
++++ export SLURM_PMIX_DIRECT_CONN=true
++++ SLURM_PMIX_DIRECT_CONN=true
++++ export SLURM_PMIX_DIRECT_CONN_UCX=true
++++ SLURM_PMIX_DIRECT_CONN_UCX=true
++++ export SLURM_PMIX_DIRECT_CONN_EARLY=true
++++ SLURM_PMIX_DIRECT_CONN_EARLY=true
++++ export SLURM_PMIX_TIMEOUT=3000
++++ SLURM_PMIX_TIMEOUT=3000
++++ '[' lxx_hzau '!=' root ']'
++++ export SQUEUE_USERS=lxx_hzau
++++ SQUEUE_USERS=lxx_hzau
++++ export SCONTROL_JOB_USERS=lxx_hzau
++++ SCONTROL_JOB_USERS=lxx_hzau
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/flatpak.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/flatpak.sh
++++ '[' /exports/share:/var/lib/flatpak/exports/share:/usr/local/share:/usr/share = /work/home/lxx_hzau/.local/share/flatpak/exports/share:/var/lib/flatpak/exports/share:/usr/local/share:/usr/share ']'
++++ export XDG_DATA_DIRS
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/kde.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/kde.sh
++++ '[' -z /usr ']'
++++ '[' -z '' ']'
++++ grep --color=auto -qs '^PRELINKING=yes' /etc/sysconfig/prelink
++++ for libdir in /usr/lib64 /usr/lib
++++ '[' -n /usr/lib64/kde4/plugins:/usr/lib/kde4/plugins ']'
++++ echo /usr/lib64/kde4/plugins:/usr/lib/kde4/plugins
++++ /bin/grep -q /usr/lib64/kde4/plugins
++++ for libdir in /usr/lib64 /usr/lib
++++ '[' -n /usr/lib64/kde4/plugins:/usr/lib/kde4/plugins ']'
++++ echo /usr/lib64/kde4/plugins:/usr/lib/kde4/plugins
++++ /bin/grep -q /usr/lib/kde4/plugins
++++ '[' '!' -d /work/home/lxx_hzau/.local/share ']'
++++ unset libdir
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/lang.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/lang.sh
++++ sourced=0
++++ '[' -n en_US.UTF-8 ']'
++++ saved_lang=en_US.UTF-8
++++ '[' -f /work/home/lxx_hzau/.i18n ']'
++++ LANG=en_US.UTF-8
++++ unset saved_lang
++++ '[' 0 = 1 ']'
++++ unset sourced
++++ unset langfile
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/less.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/less.sh
++++ '[' -x /usr/bin/lesspipe.sh ']'
++++ export 'LESSOPEN=||/usr/bin/lesspipe.sh %s'
++++ LESSOPEN='||/usr/bin/lesspipe.sh %s'
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/modules.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/modules.sh
++++++ /bin/ps -p 14117 -ocomm=
+++++ /bin/basename slurm_script
++++ shell=slurm_script
++++ '[' -f /usr/share/Modules/init/slurm_script ']'
++++ . /usr/share/Modules/init/sh
+++++ MODULESHOME=/usr/share/Modules
+++++ export MODULESHOME
+++++ '[' compiler/devtoolset/7.3.1:mpi/hpcx/gcc-7.3.1 = '' ']'
+++++ '[' /public/software/modules/base:/public/software/modules/apps = '' ']'
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/mpi-selector.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/mpi-selector.sh
++++ mpi_selector_dir=/var/mpi-selector/data
++++ mpi_selector_homefile=/work/home/lxx_hzau/.mpi-selector
++++ mpi_selector_sysfile=/etc/sysconfig/mpi-selector
++++ mpi_selection=
++++ test -f /work/home/lxx_hzau/.mpi-selector
++++ test -f /etc/sysconfig/mpi-selector
++++ test '' '!=' '' -a -f /var/mpi-selector/data/.sh
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/PackageKit.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/PackageKit.sh
++++ [[ -n '' ]]
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/perl-homedir.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/perl-homedir.sh
++++ PERL_HOMEDIR=1
++++ '[' -f /etc/sysconfig/perl-homedir ']'
++++ '[' -f /work/home/lxx_hzau/.perl-homedir ']'
++++ alias 'perlll=eval `perl -Mlocal::lib`'
++++ '[' x1 = x1 ']'
+++++ perl -Mlocal::lib
++++ eval
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/qt-graphicssystem.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/qt-graphicssystem.sh
++++ '[' -z 1 -a -z '' ']'
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/qt.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/qt.sh
++++ '[' -z /usr/lib64/qt-3.3 ']'
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/rocm-gcc-mpi-default.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/rocm-gcc-mpi-default.sh
++++ [[ lxx_hzau =~ root|xhadmin ]]
++++ module use /public/software/modules/apps
+++++ /usr/bin/modulecmd sh use /public/software/modules/apps
++++ eval
++++ module use /public/software/modules/base
+++++ /usr/bin/modulecmd sh use /public/software/modules/base
++++ eval
++++ module unuse /opt/hpc/software/modules
+++++ /usr/bin/modulecmd sh unuse /opt/hpc/software/modules
++++ eval
++++ module load compiler/devtoolset/7.3.1
+++++ /usr/bin/modulecmd sh load compiler/devtoolset/7.3.1
++++ eval
++++ module load mpi/hpcx/gcc-7.3.1
+++++ /usr/bin/modulecmd sh load mpi/hpcx/gcc-7.3.1
++++ eval
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/vim.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/vim.sh
++++ '[' -n '4.2.46(2)-release' -o -n '' -o -n '' ']'
++++ '[' -x /usr/bin/id ']'
+++++ /usr/bin/id -u
++++ ID=5235
++++ '[' -n 5235 -a 5235 -le 200 ']'
++++ alias vi
++++ alias vi=vim
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/vte.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/vte.sh
++++ '[' -n '4.2.46(2)-release' -o -n '' ']'
++++ [[ hxB == *i* ]]
++++ return 0
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/which2.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/which2.sh
++++ alias 'which=alias | /usr/bin/which --tty-only --read-alias --show-dot --show-tilde'
+++ unset i
+++ unset -f pathmunge
+++ /work/home/lxx_hzau/miniconda3/bin/conda shell.bash hook
++ __conda_setup='export CONDA_EXE='\''/work/home/lxx_hzau/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/work/home/lxx_hzau/miniconda3/bin/python'\''

# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
__conda_exe() (
    "$CONDA_EXE" $_CE_M $_CE_CONDA "$@"
)

__conda_hashr() {
    if [ -n "${ZSH_VERSION:+x}" ]; then
        \rehash
    elif [ -n "${POSH_VERSION:+x}" ]; then
        :  # pass
    else
        \hash -r
    fi
}

__conda_activate() {
    if [ -n "${CONDA_PS1_BACKUP:+x}" ]; then
        # Handle transition from shell activated with conda <= 4.3 to a subsequent activation
        # after conda updated to >= 4.4. See issue #6173.
        PS1="$CONDA_PS1_BACKUP"
        \unset CONDA_PS1_BACKUP
    fi
    \local ask_conda
    ask_conda="$(PS1="${PS1:-}" __conda_exe shell.posix "$@")" || \return
    \eval "$ask_conda"
    __conda_hashr
}

__conda_reactivate() {
    \local ask_conda
    ask_conda="$(PS1="${PS1:-}" __conda_exe shell.posix reactivate)" || \return
    \eval "$ask_conda"
    __conda_hashr
}

conda() {
    \local cmd="${1-__missing__}"
    case "$cmd" in
        activate|deactivate)
            __conda_activate "$@"
            ;;
        install|update|upgrade|remove|uninstall)
            __conda_exe "$@" || \return
            __conda_reactivate
            ;;
        *)
            __conda_exe "$@"
            ;;
    esac
}

if [ -z "${CONDA_SHLVL+x}" ]; then
    \export CONDA_SHLVL=0
    # In dev-mode CONDA_EXE is python.exe and on Windows
    # it is in a different relative location to condabin.
    if [ -n "${_CE_CONDA:+x}" ] && [ -n "${WINDIR+x}" ]; then
        PATH="$(\dirname "$CONDA_EXE")/condabin${PATH:+":${PATH}"}"
    else
        PATH="$(\dirname "$(\dirname "$CONDA_EXE")")/condabin${PATH:+":${PATH}"}"
    fi
    \export PATH

    # We'\''re not allowing PS1 to be unbound. It must at least be set.
    # However, we'\''re not exporting it, which can cause problems when starting a second shell
    # via a first shell (i.e. starting zsh from bash).
    if [ -z "${PS1+x}" ]; then
        PS1=
    fi
fi

conda activate base'
++ '[' 0 -eq 0 ']'
++ eval 'export CONDA_EXE='\''/work/home/lxx_hzau/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/work/home/lxx_hzau/miniconda3/bin/python'\''

# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
__conda_exe() (
    "$CONDA_EXE" $_CE_M $_CE_CONDA "$@"
)

__conda_hashr() {
    if [ -n "${ZSH_VERSION:+x}" ]; then
        \rehash
    elif [ -n "${POSH_VERSION:+x}" ]; then
        :  # pass
    else
        \hash -r
    fi
}

__conda_activate() {
    if [ -n "${CONDA_PS1_BACKUP:+x}" ]; then
        # Handle transition from shell activated with conda <= 4.3 to a subsequent activation
        # after conda updated to >= 4.4. See issue #6173.
        PS1="$CONDA_PS1_BACKUP"
        \unset CONDA_PS1_BACKUP
    fi
    \local ask_conda
    ask_conda="$(PS1="${PS1:-}" __conda_exe shell.posix "$@")" || \return
    \eval "$ask_conda"
    __conda_hashr
}

__conda_reactivate() {
    \local ask_conda
    ask_conda="$(PS1="${PS1:-}" __conda_exe shell.posix reactivate)" || \return
    \eval "$ask_conda"
    __conda_hashr
}

conda() {
    \local cmd="${1-__missing__}"
    case "$cmd" in
        activate|deactivate)
            __conda_activate "$@"
            ;;
        install|update|upgrade|remove|uninstall)
            __conda_exe "$@" || \return
            __conda_reactivate
            ;;
        *)
            __conda_exe "$@"
            ;;
    esac
}

if [ -z "${CONDA_SHLVL+x}" ]; then
    \export CONDA_SHLVL=0
    # In dev-mode CONDA_EXE is python.exe and on Windows
    # it is in a different relative location to condabin.
    if [ -n "${_CE_CONDA:+x}" ] && [ -n "${WINDIR+x}" ]; then
        PATH="$(\dirname "$CONDA_EXE")/condabin${PATH:+":${PATH}"}"
    else
        PATH="$(\dirname "$(\dirname "$CONDA_EXE")")/condabin${PATH:+":${PATH}"}"
    fi
    \export PATH

    # We'\''re not allowing PS1 to be unbound. It must at least be set.
    # However, we'\''re not exporting it, which can cause problems when starting a second shell
    # via a first shell (i.e. starting zsh from bash).
    if [ -z "${PS1+x}" ]; then
        PS1=
    fi
fi

conda activate base'
+++ export CONDA_EXE=/work/home/lxx_hzau/miniconda3/bin/conda
+++ CONDA_EXE=/work/home/lxx_hzau/miniconda3/bin/conda
+++ export _CE_M=
+++ _CE_M=
+++ export _CE_CONDA=
+++ _CE_CONDA=
+++ export CONDA_PYTHON_EXE=/work/home/lxx_hzau/miniconda3/bin/python
+++ CONDA_PYTHON_EXE=/work/home/lxx_hzau/miniconda3/bin/python
+++ '[' -z x ']'
+++ conda activate base
+++ local cmd=activate
+++ case "$cmd" in
+++ __conda_activate activate base
+++ '[' -n '' ']'
+++ local ask_conda
++++ PS1=
++++ __conda_exe shell.posix activate base
++++ /work/home/lxx_hzau/miniconda3/bin/conda shell.posix activate base
+++ ask_conda='PS1='\''(base) '\''
export PATH='\''/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin'\''
export CONDA_SHLVL='\''1'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export PYTHONPATH='\''/work/home/lxx_hzau/project/FJSP-DRL-main'\'''
+++ eval 'PS1='\''(base) '\''
export PATH='\''/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin'\''
export CONDA_SHLVL='\''1'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export PYTHONPATH='\''/work/home/lxx_hzau/project/FJSP-DRL-main'\'''
++++ PS1='(base) '
++++ export PATH=/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin
++++ PATH=/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin
++++ export CONDA_SHLVL=1
++++ CONDA_SHLVL=1
++++ export 'CONDA_PROMPT_MODIFIER=(base) '
++++ CONDA_PROMPT_MODIFIER='(base) '
++++ export PYTHONPATH=/work/home/lxx_hzau/project/FJSP-DRL-main
++++ PYTHONPATH=/work/home/lxx_hzau/project/FJSP-DRL-main
+++ __conda_hashr
+++ '[' -n '' ']'
+++ '[' -n '' ']'
+++ hash -r
++ unset __conda_setup
++ export PATH=/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin
++ PATH=/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin
+ module load nvidia/cuda/11.6
++ /usr/bin/modulecmd sh load nvidia/cuda/11.6
+ eval CUDA_HOME=/public/software/compiler/nvidia/cuda/11.6.0:/usr/local/cuda-11.1/ ';export' 'CUDA_HOME;CUDA_ROOT=/public/software/compiler/nvidia/cuda/11.6.0' ';export' 'CUDA_ROOT;INCLUDE=/public/software/compiler/nvidia/cuda/11.6.0/include:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/include:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/include:/opt/hpc/software/mpi/hpcx/v2.11.0/sharp/include:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/include' ';export' 'INCLUDE;LD_LIBRARY_PATH=/public/software/compiler/nvidia/cuda/11.6.0/lib64:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/sharp/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/lib:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:' ';export' 'LD_LIBRARY_PATH;LOADEDMODULES=compiler/devtoolset/7.3.1:mpi/hpcx/gcc-7.3.1:nvidia/cuda/11.6' ';export' 'LOADEDMODULES;PATH=/public/software/compiler/nvidia/cuda/11.6.0/bin:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin' ';export' 'PATH;_LMFILES_=/public/software/modules/base/compiler/devtoolset/7.3.1:/public/software/modules/base/mpi/hpcx/gcc-7.3.1:/public/software/modules/base/nvidia/cuda/11.6' ';export' '_LMFILES_;'
++ CUDA_HOME=/public/software/compiler/nvidia/cuda/11.6.0:/usr/local/cuda-11.1/
++ export CUDA_HOME
++ CUDA_ROOT=/public/software/compiler/nvidia/cuda/11.6.0
++ export CUDA_ROOT
++ INCLUDE=/public/software/compiler/nvidia/cuda/11.6.0/include:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/include:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/include:/opt/hpc/software/mpi/hpcx/v2.11.0/sharp/include:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/include
++ export INCLUDE
++ LD_LIBRARY_PATH=/public/software/compiler/nvidia/cuda/11.6.0/lib64:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/sharp/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/lib:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:
++ export LD_LIBRARY_PATH
++ LOADEDMODULES=compiler/devtoolset/7.3.1:mpi/hpcx/gcc-7.3.1:nvidia/cuda/11.6
++ export LOADEDMODULES
++ PATH=/public/software/compiler/nvidia/cuda/11.6.0/bin:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin
++ export PATH
++ _LMFILES_=/public/software/modules/base/compiler/devtoolset/7.3.1:/public/software/modules/base/mpi/hpcx/gcc-7.3.1:/public/software/modules/base/nvidia/cuda/11.6
++ export _LMFILES_
+ conda activate RL-torch
+ local cmd=activate
+ case "$cmd" in
+ __conda_activate activate RL-torch
+ '[' -n '' ']'
+ local ask_conda
++ PS1='(base) '
++ __conda_exe shell.posix activate RL-torch
++ /work/home/lxx_hzau/miniconda3/bin/conda shell.posix activate RL-torch
+ ask_conda='PS1='\''(RL-torch) '\''
export PATH='\''/public/software/compiler/nvidia/cuda/11.6.0/bin:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/envs/RL-torch/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin'\''
export CONDA_PREFIX='\''/work/home/lxx_hzau/miniconda3/envs/RL-torch'\''
export CONDA_SHLVL='\''2'\''
export CONDA_DEFAULT_ENV='\''RL-torch'\''
export CONDA_PROMPT_MODIFIER='\''(RL-torch) '\''
export PYTHONPATH='\''/work/home/lxx_hzau/project/FJSP-DRL-main'\''
export CONDA_PREFIX_1='\''/work/home/lxx_hzau/miniconda3'\''
export CONDA_EXE='\''/work/home/lxx_hzau/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/work/home/lxx_hzau/miniconda3/bin/python'\'''
+ eval 'PS1='\''(RL-torch) '\''
export PATH='\''/public/software/compiler/nvidia/cuda/11.6.0/bin:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/envs/RL-torch/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin'\''
export CONDA_PREFIX='\''/work/home/lxx_hzau/miniconda3/envs/RL-torch'\''
export CONDA_SHLVL='\''2'\''
export CONDA_DEFAULT_ENV='\''RL-torch'\''
export CONDA_PROMPT_MODIFIER='\''(RL-torch) '\''
export PYTHONPATH='\''/work/home/lxx_hzau/project/FJSP-DRL-main'\''
export CONDA_PREFIX_1='\''/work/home/lxx_hzau/miniconda3'\''
export CONDA_EXE='\''/work/home/lxx_hzau/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/work/home/lxx_hzau/miniconda3/bin/python'\'''
++ PS1='(RL-torch) '
++ export PATH=/public/software/compiler/nvidia/cuda/11.6.0/bin:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/envs/RL-torch/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin
++ PATH=/public/software/compiler/nvidia/cuda/11.6.0/bin:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/opt/hpc/setfreq:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/envs/RL-torch/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin
++ export CONDA_PREFIX=/work/home/lxx_hzau/miniconda3/envs/RL-torch
++ CONDA_PREFIX=/work/home/lxx_hzau/miniconda3/envs/RL-torch
++ export CONDA_SHLVL=2
++ CONDA_SHLVL=2
++ export CONDA_DEFAULT_ENV=RL-torch
++ CONDA_DEFAULT_ENV=RL-torch
++ export 'CONDA_PROMPT_MODIFIER=(RL-torch) '
++ CONDA_PROMPT_MODIFIER='(RL-torch) '
++ export PYTHONPATH=/work/home/lxx_hzau/project/FJSP-DRL-main
++ PYTHONPATH=/work/home/lxx_hzau/project/FJSP-DRL-main
++ export CONDA_PREFIX_1=/work/home/lxx_hzau/miniconda3
++ CONDA_PREFIX_1=/work/home/lxx_hzau/miniconda3
++ export CONDA_EXE=/work/home/lxx_hzau/miniconda3/bin/conda
++ CONDA_EXE=/work/home/lxx_hzau/miniconda3/bin/conda
++ export _CE_M=
++ _CE_M=
++ export _CE_CONDA=
++ _CE_CONDA=
++ export CONDA_PYTHON_EXE=/work/home/lxx_hzau/miniconda3/bin/python
++ CONDA_PYTHON_EXE=/work/home/lxx_hzau/miniconda3/bin/python
+ __conda_hashr
+ '[' -n '' ']'
+ '[' -n '' ']'
+ hash -r
+ t=train
+ exp=exp17
+ echo exp17
exp17
+ cat

->  op_per_job ->
+ n_j_options='15 15 15 15'
+ n_m_options='13 10 7 5'
+ op_per_job_options='4 7 10 12'
+ logdir=./runs/exp17
+ hidden_dim_actor=512
+ hidden_dim_critic=512
+ num_mlp_layers_actor=3
+ num_mlp_layers_critic=3
+ num_envs=4
+ meta_iterations=500
+ max_updates_maml=500
+ num_tasks=4
+ max_updates_finetune=20
+ lr=0.003
+ data='13,12 10,10 7,7 5,4'
+ data_m_operjob='13,12 10,10 7,7 5,4'
+ logdir_dan=./runs/exp17/DAN
+ n_j=15
+ for model in 15x13+mix+SD2 15x5+mix+SD2
+ for n_m in '$n_m_options'
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp17/DAN/finetuning/15x13+mix+SD2/15x13_4 --model_suffix free --finetuning_model 15x13+mix+SD2 --max_updates 20 --n_j 15 --n_m 13 --op_per_job 4
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x13+mix
save model name:  15x13+mix+free
./trained_network/SD2/15x13+mix+SD2.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/15x13+mix+SD2.pth
vali data :./data/data_train_vali/SD2/15x13+mix


progress:   0%|[34m          [0m| 0/20 [00:00<?, ?it/s]                                                Episode 1	 reward: -2.55	 makespan: 252.25	 Mean_loss: 0.04802271,  training time: 4.11
progress:   0%|[34m          [0m| 0/20 [00:04<?, ?it/s]progress:   5%|[34m         [0m| 1/20 [00:04<01:18,  4.12s/it]                                                        Episode 2	 reward: -2.49	 makespan: 246.75	 Mean_loss: 0.01911166,  training time: 1.28
progress:   5%|[34m         [0m| 1/20 [00:05<01:18,  4.12s/it]progress:  10%|[34m         [0m| 2/20 [00:05<00:44,  2.45s/it]                                                        Episode 3	 reward: -2.56	 makespan: 253.90	 Mean_loss: 0.02482370,  training time: 1.30
progress:  10%|[34m         [0m| 2/20 [00:06<00:44,  2.45s/it]progress:  15%|[34m        [0m| 3/20 [00:06<00:32,  1.93s/it]                                                        Episode 4	 reward: -2.57	 makespan: 254.25	 Mean_loss: 0.04764971,  training time: 1.33
progress:  15%|[34m        [0m| 3/20 [00:08<00:32,  1.93s/it]progress:  20%|[34m        [0m| 4/20 [00:08<00:27,  1.70s/it]                                                        Episode 5	 reward: -2.62	 makespan: 259.50	 Mean_loss: 0.01720164,  training time: 1.38
progress:  20%|[34m        [0m| 4/20 [00:09<00:27,  1.70s/it]progress:  25%|[34m       [0m| 5/20 [00:09<00:23,  1.58s/it]                                                        Episode 6	 reward: -2.55	 makespan: 252.00	 Mean_loss: 0.03714592,  training time: 1.32
progress:  25%|[34m       [0m| 5/20 [00:10<00:23,  1.58s/it]progress:  30%|[34m       [0m| 6/20 [00:10<00:20,  1.50s/it]                                                        Episode 7	 reward: -2.50	 makespan: 247.05	 Mean_loss: 0.04517011,  training time: 1.30
progress:  30%|[34m       [0m| 6/20 [00:12<00:20,  1.50s/it]progress:  35%|[34m      [0m| 7/20 [00:12<00:18,  1.43s/it]                                                        Episode 8	 reward: -2.49	 makespan: 246.30	 Mean_loss: 0.04030872,  training time: 1.30
progress:  35%|[34m      [0m| 7/20 [00:13<00:18,  1.43s/it]progress:  40%|[34m      [0m| 8/20 [00:13<00:16,  1.39s/it]                                                        Episode 9	 reward: -2.57	 makespan: 254.05	 Mean_loss: 0.01571983,  training time: 1.31
progress:  40%|[34m      [0m| 8/20 [00:14<00:16,  1.39s/it]progress:  45%|[34m     [0m| 9/20 [00:14<00:15,  1.37s/it]                                                        Episode 10	 reward: -2.55	 makespan: 252.20	 Mean_loss: 0.05037077,  training time: 1.30
progress:  45%|[34m     [0m| 9/20 [00:15<00:15,  1.37s/it]progress:  50%|[34m     [0m| 10/20 [00:15<00:13,  1.35s/it]                                                         Episode 11	 reward: -2.50	 makespan: 247.75	 Mean_loss: 0.03195381,  training time: 1.33
progress:  50%|[34m     [0m| 10/20 [00:17<00:13,  1.35s/it]progress:  55%|[34m    [0m| 11/20 [00:17<00:12,  1.35s/it]                                                         Episode 12	 reward: -2.54	 makespan: 251.70	 Mean_loss: 0.04591164,  training time: 1.39
progress:  55%|[34m    [0m| 11/20 [00:18<00:12,  1.35s/it]progress:  60%|[34m    [0m| 12/20 [00:18<00:10,  1.36s/it]                                                         Episode 13	 reward: -2.50	 makespan: 247.60	 Mean_loss: 0.03245991,  training time: 1.30
progress:  60%|[34m    [0m| 12/20 [00:20<00:10,  1.36s/it]progress:  65%|[34m   [0m| 13/20 [00:20<00:09,  1.35s/it]                                                         Episode 14	 reward: -2.64	 makespan: 261.35	 Mean_loss: 0.03524295,  training time: 1.31
progress:  65%|[34m   [0m| 13/20 [00:21<00:09,  1.35s/it]progress:  70%|[34m   [0m| 14/20 [00:21<00:08,  1.34s/it]                                                         Episode 15	 reward: -2.53	 makespan: 250.05	 Mean_loss: 0.01921413,  training time: 1.34
progress:  70%|[34m   [0m| 14/20 [00:22<00:08,  1.34s/it]progress:  75%|[34m  [0m| 15/20 [00:22<00:06,  1.34s/it]                                                         Episode 16	 reward: -2.56	 makespan: 253.75	 Mean_loss: 0.04137364,  training time: 1.33
progress:  75%|[34m  [0m| 15/20 [00:24<00:06,  1.34s/it]progress:  80%|[34m  [0m| 16/20 [00:24<00:05,  1.34s/it]                                                         Episode 17	 reward: -2.50	 makespan: 247.60	 Mean_loss: 0.02681939,  training time: 1.30
progress:  80%|[34m  [0m| 16/20 [00:25<00:05,  1.34s/it]progress:  85%|[34m [0m| 17/20 [00:25<00:03,  1.33s/it]                                                         Episode 18	 reward: -2.64	 makespan: 261.45	 Mean_loss: 0.02261534,  training time: 1.30
progress:  85%|[34m [0m| 17/20 [00:26<00:03,  1.33s/it]progress:  90%|[34m [0m| 18/20 [00:26<00:02,  1.32s/it]                                                         Episode 19	 reward: -2.49	 makespan: 246.70	 Mean_loss: 0.02204033,  training time: 1.32
progress:  90%|[34m [0m| 18/20 [00:27<00:02,  1.32s/it]progress:  95%|[34m[0m| 19/20 [00:27<00:01,  1.32s/it]                                                         Episode 20	 reward: -2.56	 makespan: 253.70	 Mean_loss: 0.06330651,  training time: 1.34
progress:  95%|[34m[0m| 19/20 [00:29<00:01,  1.32s/it]progress: 100%|[34m[0m| 20/20 [00:29<00:00,  1.33s/it]progress: 100%|[34m[0m| 20/20 [00:29<00:00,  1.47s/it]
+ --num_envs 4 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp17/DAN/finetuning/15x13+mix+SD2/15x13_7 --model_suffix free --finetuning_model 15x13+mix+SD2 --max_updates 20 --n_j 15 --n_m 13 --op_per_job 7
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x13+mix
save model name:  15x13+mix+free
./trained_network/SD2/15x13+mix+SD2.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/15x13+mix+SD2.pth
vali data :./data/data_train_vali/SD2/15x13+mix


progress:   0%|[34m          [0m| 0/20 [00:00<?, ?it/s]                                                Episode 1	 reward: -4.38	 makespan: 433.20	 Mean_loss: -0.58633858,  training time: 3.44
progress:   0%|[34m          [0m| 0/20 [00:03<?, ?it/s]progress:   5%|[34m         [0m| 1/20 [00:03<01:05,  3.45s/it]                                                        Episode 2	 reward: -4.28	 makespan: 423.40	 Mean_loss: -0.70729911,  training time: 2.42
progress:   5%|[34m         [0m| 1/20 [00:05<01:05,  3.45s/it]progress:  10%|[34m         [0m| 2/20 [00:05<00:51,  2.85s/it]                                                        Episode 3	 reward: -4.33	 makespan: 428.20	 Mean_loss: -0.58969337,  training time: 2.36
progress:  10%|[34m         [0m| 2/20 [00:08<00:51,  2.85s/it]progress:  15%|[34m        [0m| 3/20 [00:08<00:44,  2.63s/it]                                                        Episode 4	 reward: -4.48	 makespan: 443.70	 Mean_loss: -0.56210220,  training time: 2.50
progress:  15%|[34m        [0m| 3/20 [00:10<00:44,  2.63s/it]progress:  20%|[34m        [0m| 4/20 [00:10<00:41,  2.58s/it]                                                        Episode 5	 reward: -4.27	 makespan: 422.70	 Mean_loss: -0.65131617,  training time: 2.38
progress:  20%|[34m        [0m| 4/20 [00:13<00:41,  2.58s/it]progress:  25%|[34m       [0m| 5/20 [00:13<00:37,  2.51s/it]                                                        Episode 6	 reward: -4.28	 makespan: 423.95	 Mean_loss: -0.08840496,  training time: 2.36
progress:  25%|[34m       [0m| 5/20 [00:15<00:37,  2.51s/it]progress:  30%|[34m       [0m| 6/20 [00:15<00:34,  2.46s/it]                                                        Episode 7	 reward: -4.21	 makespan: 416.35	 Mean_loss: -0.47396982,  training time: 2.34
progress:  30%|[34m       [0m| 6/20 [00:17<00:34,  2.46s/it]progress:  35%|[34m      [0m| 7/20 [00:17<00:31,  2.42s/it]                                                        Episode 8	 reward: -4.34	 makespan: 429.50	 Mean_loss: -0.65929615,  training time: 2.35
progress:  35%|[34m      [0m| 7/20 [00:20<00:31,  2.42s/it]progress:  40%|[34m      [0m| 8/20 [00:20<00:28,  2.40s/it]                                                        Episode 9	 reward: -4.28	 makespan: 424.10	 Mean_loss: -0.60254645,  training time: 2.49
progress:  40%|[34m      [0m| 8/20 [00:22<00:28,  2.40s/it]progress:  45%|[34m     [0m| 9/20 [00:22<00:26,  2.43s/it]                                                        Episode 10	 reward: -4.25	 makespan: 421.00	 Mean_loss: -0.58521307,  training time: 2.39
progress:  45%|[34m     [0m| 9/20 [00:25<00:26,  2.43s/it]progress:  50%|[34m     [0m| 10/20 [00:25<00:24,  2.42s/it]                                                         Episode 11	 reward: -4.26	 makespan: 421.85	 Mean_loss: -0.36668646,  training time: 2.36
progress:  50%|[34m     [0m| 10/20 [00:27<00:24,  2.42s/it]progress:  55%|[34m    [0m| 11/20 [00:27<00:21,  2.40s/it]                                                         Episode 12	 reward: -4.23	 makespan: 419.10	 Mean_loss: 0.10628615,  training time: 2.35
progress:  55%|[34m    [0m| 11/20 [00:29<00:21,  2.40s/it]progress:  60%|[34m    [0m| 12/20 [00:29<00:19,  2.39s/it]                                                         Episode 13	 reward: -4.32	 makespan: 427.25	 Mean_loss: -0.05396404,  training time: 2.46
progress:  60%|[34m    [0m| 12/20 [00:32<00:19,  2.39s/it]progress:  65%|[34m   [0m| 13/20 [00:32<00:16,  2.41s/it]                                                         Episode 14	 reward: -4.28	 makespan: 424.05	 Mean_loss: -0.40816259,  training time: 2.36
progress:  65%|[34m   [0m| 13/20 [00:34<00:16,  2.41s/it]progress:  70%|[34m   [0m| 14/20 [00:34<00:14,  2.40s/it]                                                         Episode 15	 reward: -4.18	 makespan: 413.90	 Mean_loss: -0.17049816,  training time: 2.36
progress:  70%|[34m   [0m| 14/20 [00:37<00:14,  2.40s/it]progress:  75%|[34m  [0m| 15/20 [00:37<00:11,  2.39s/it]                                                         Episode 16	 reward: -4.17	 makespan: 413.05	 Mean_loss: -0.25866106,  training time: 2.37
progress:  75%|[34m  [0m| 15/20 [00:39<00:11,  2.39s/it]progress:  80%|[34m  [0m| 16/20 [00:39<00:09,  2.39s/it]                                                         Episode 17	 reward: -4.24	 makespan: 420.10	 Mean_loss: -0.29832590,  training time: 2.35
progress:  80%|[34m  [0m| 16/20 [00:41<00:09,  2.39s/it]progress:  85%|[34m [0m| 17/20 [00:41<00:07,  2.38s/it]                                                         Episode 18	 reward: -4.29	 makespan: 424.35	 Mean_loss: -0.62695497,  training time: 2.35
progress:  85%|[34m [0m| 17/20 [00:44<00:07,  2.38s/it]progress:  90%|[34m [0m| 18/20 [00:44<00:04,  2.37s/it]                                                         Episode 19	 reward: -4.18	 makespan: 414.30	 Mean_loss: -0.17236257,  training time: 2.35
progress:  90%|[34m [0m| 18/20 [00:46<00:04,  2.37s/it]progress:  95%|[34m[0m| 19/20 [00:46<00:02,  2.36s/it]                                                         Episode 20	 reward: -4.26	 makespan: 421.90	 Mean_loss: 0.11587168,  training time: 2.35
progress:  95%|[34m[0m| 19/20 [00:48<00:02,  2.36s/it]progress: 100%|[34m[0m| 20/20 [00:48<00:00,  2.36s/it]progress: 100%|[34m[0m| 20/20 [00:48<00:00,  2.44s/it]
+ --num_envs 4 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp17/DAN/finetuning/15x13+mix+SD2/15x13_10 --model_suffix free --finetuning_model 15x13+mix+SD2 --max_updates 20 --n_j 15 --n_m 13 --op_per_job 10
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x13+mix
save model name:  15x13+mix+free
./trained_network/SD2/15x13+mix+SD2.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/15x13+mix+SD2.pth
vali data :./data/data_train_vali/SD2/15x13+mix


progress:   0%|[34m          [0m| 0/20 [00:00<?, ?it/s]                                                Episode 1	 reward: -5.88	 makespan: 582.05	 Mean_loss: 0.21534088,  training time: 4.53
progress:   0%|[34m          [0m| 0/20 [00:04<?, ?it/s]progress:   5%|[34m         [0m| 1/20 [00:04<01:26,  4.54s/it]                                                        Episode 2	 reward: -5.99	 makespan: 593.00	 Mean_loss: 0.32450876,  training time: 3.41
progress:   5%|[34m         [0m| 1/20 [00:07<01:26,  4.54s/it]progress:  10%|[34m         [0m| 2/20 [00:07<01:09,  3.88s/it]                                                        Episode 3	 reward: -5.85	 makespan: 579.40	 Mean_loss: 0.31767192,  training time: 3.38
progress:  10%|[34m         [0m| 2/20 [00:11<01:09,  3.88s/it]progress:  15%|[34m        [0m| 3/20 [00:11<01:02,  3.65s/it]                                                        Episode 4	 reward: -5.87	 makespan: 581.60	 Mean_loss: 0.30891946,  training time: 3.35
progress:  15%|[34m        [0m| 3/20 [00:14<01:02,  3.65s/it]progress:  20%|[34m        [0m| 4/20 [00:14<00:56,  3.54s/it]                                                        Episode 5	 reward: -5.94	 makespan: 587.95	 Mean_loss: 0.31496394,  training time: 3.38
progress:  20%|[34m        [0m| 4/20 [00:18<00:56,  3.54s/it]progress:  25%|[34m       [0m| 5/20 [00:18<00:52,  3.48s/it]                                                        Episode 6	 reward: -5.85	 makespan: 579.40	 Mean_loss: 0.27559960,  training time: 3.46
progress:  25%|[34m       [0m| 5/20 [00:21<00:52,  3.48s/it]progress:  30%|[34m       [0m| 6/20 [00:21<00:48,  3.48s/it]                                                        Episode 7	 reward: -5.82	 makespan: 576.60	 Mean_loss: 0.23104583,  training time: 3.34
progress:  30%|[34m       [0m| 6/20 [00:24<00:48,  3.48s/it]progress:  35%|[34m      [0m| 7/20 [00:24<00:44,  3.43s/it]                                                        Episode 8	 reward: -5.90	 makespan: 583.70	 Mean_loss: 0.22250706,  training time: 3.36
progress:  35%|[34m      [0m| 7/20 [00:28<00:44,  3.43s/it]progress:  40%|[34m      [0m| 8/20 [00:28<00:40,  3.41s/it]                                                        Episode 9	 reward: -5.94	 makespan: 588.50	 Mean_loss: 0.18134911,  training time: 3.32
progress:  40%|[34m      [0m| 8/20 [00:31<00:40,  3.41s/it]progress:  45%|[34m     [0m| 9/20 [00:31<00:37,  3.39s/it]                                                        Episode 10	 reward: -5.98	 makespan: 592.40	 Mean_loss: 0.17223792,  training time: 3.39
progress:  45%|[34m     [0m| 9/20 [00:34<00:37,  3.39s/it]progress:  50%|[34m     [0m| 10/20 [00:34<00:33,  3.39s/it]                                                         Episode 11	 reward: -6.04	 makespan: 597.65	 Mean_loss: 0.12738301,  training time: 3.35
progress:  50%|[34m     [0m| 10/20 [00:38<00:33,  3.39s/it]progress:  55%|[34m    [0m| 11/20 [00:38<00:30,  3.38s/it]                                                         Episode 12	 reward: -5.96	 makespan: 590.40	 Mean_loss: 0.13896297,  training time: 3.38
progress:  55%|[34m    [0m| 11/20 [00:41<00:30,  3.38s/it]progress:  60%|[34m    [0m| 12/20 [00:41<00:27,  3.38s/it]                                                         Episode 13	 reward: -5.98	 makespan: 592.30	 Mean_loss: 0.14458916,  training time: 3.34
progress:  60%|[34m    [0m| 12/20 [00:45<00:27,  3.38s/it]progress:  65%|[34m   [0m| 13/20 [00:45<00:23,  3.37s/it]                                                         Episode 14	 reward: -5.84	 makespan: 577.75	 Mean_loss: 0.12599321,  training time: 3.34
progress:  65%|[34m   [0m| 13/20 [00:48<00:23,  3.37s/it]progress:  70%|[34m   [0m| 14/20 [00:48<00:20,  3.37s/it]                                                         Episode 15	 reward: -6.04	 makespan: 597.60	 Mean_loss: 0.14655125,  training time: 3.40
progress:  70%|[34m   [0m| 14/20 [00:51<00:20,  3.37s/it]progress:  75%|[34m  [0m| 15/20 [00:51<00:16,  3.38s/it]                                                         Episode 16	 reward: -6.01	 makespan: 595.15	 Mean_loss: 0.15493320,  training time: 3.34
progress:  75%|[34m  [0m| 15/20 [00:55<00:16,  3.38s/it]progress:  80%|[34m  [0m| 16/20 [00:55<00:13,  3.37s/it]                                                         Episode 17	 reward: -6.00	 makespan: 593.55	 Mean_loss: 0.12094950,  training time: 3.40
progress:  80%|[34m  [0m| 16/20 [00:58<00:13,  3.37s/it]progress:  85%|[34m [0m| 17/20 [00:58<00:10,  3.38s/it]                                                         Episode 18	 reward: -5.94	 makespan: 588.50	 Mean_loss: 0.12362850,  training time: 3.31
progress:  85%|[34m [0m| 17/20 [01:01<00:10,  3.38s/it]progress:  90%|[34m [0m| 18/20 [01:01<00:06,  3.36s/it]                                                         Episode 19	 reward: -5.87	 makespan: 581.60	 Mean_loss: 0.12867714,  training time: 3.34
progress:  90%|[34m [0m| 18/20 [01:05<00:06,  3.36s/it]progress:  95%|[34m[0m| 19/20 [01:05<00:03,  3.36s/it]                                                         Episode 20	 reward: -5.92	 makespan: 586.50	 Mean_loss: 0.11082306,  training time: 3.36
progress:  95%|[34m[0m| 19/20 [01:08<00:03,  3.36s/it]progress: 100%|[34m[0m| 20/20 [01:08<00:00,  3.36s/it]progress: 100%|[34m[0m| 20/20 [01:08<00:00,  3.43s/it]
+ --num_envs 4 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp17/DAN/finetuning/15x13+mix+SD2/15x13_12 --model_suffix free --finetuning_model 15x13+mix+SD2 --max_updates 20 --n_j 15 --n_m 13 --op_per_job 12
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x13+mix
save model name:  15x13+mix+free
./trained_network/SD2/15x13+mix+SD2.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/15x13+mix+SD2.pth
vali data :./data/data_train_vali/SD2/15x13+mix


progress:   0%|[34m          [0m| 0/20 [00:00<?, ?it/s]                                                Episode 1	 reward: -6.77	 makespan: 670.10	 Mean_loss: 0.37282833,  training time: 5.32
progress:   0%|[34m          [0m| 0/20 [00:05<?, ?it/s]progress:   5%|[34m         [0m| 1/20 [00:05<01:41,  5.33s/it]                                                        Episode 2	 reward: -6.90	 makespan: 683.50	 Mean_loss: 0.52983236,  training time: 4.19
progress:   5%|[34m         [0m| 1/20 [00:09<01:41,  5.33s/it]progress:  10%|[34m         [0m| 2/20 [00:09<01:23,  4.66s/it]                                                        Episode 3	 reward: -6.90	 makespan: 683.20	 Mean_loss: 0.56980050,  training time: 4.51
progress:  10%|[34m         [0m| 2/20 [00:14<01:23,  4.66s/it]progress:  15%|[34m        [0m| 3/20 [00:14<01:18,  4.60s/it]                                                        Episode 4	 reward: -6.80	 makespan: 673.10	 Mean_loss: 0.51061094,  training time: 4.34
progress:  15%|[34m        [0m| 3/20 [00:18<01:18,  4.60s/it]progress:  20%|[34m        [0m| 4/20 [00:18<01:12,  4.50s/it]                                                        Episode 5	 reward: -6.90	 makespan: 683.20	 Mean_loss: 0.49332994,  training time: 4.49
progress:  20%|[34m        [0m| 4/20 [00:22<01:12,  4.50s/it]progress:  25%|[34m       [0m| 5/20 [00:22<01:07,  4.50s/it]                                                        Episode 6	 reward: -6.86	 makespan: 679.45	 Mean_loss: 0.47362706,  training time: 4.33
progress:  25%|[34m       [0m| 5/20 [00:27<01:07,  4.50s/it]progress:  30%|[34m       [0m| 6/20 [00:27<01:02,  4.44s/it]                                                        Episode 7	 reward: -6.78	 makespan: 671.65	 Mean_loss: 0.46178904,  training time: 4.26
progress:  30%|[34m       [0m| 6/20 [00:31<01:02,  4.44s/it]progress:  35%|[34m      [0m| 7/20 [00:31<00:57,  4.39s/it]                                                        Episode 8	 reward: -6.76	 makespan: 669.05	 Mean_loss: 0.35169205,  training time: 4.42
progress:  35%|[34m      [0m| 7/20 [00:35<00:57,  4.39s/it]progress:  40%|[34m      [0m| 8/20 [00:35<00:52,  4.40s/it]                                                        Episode 9	 reward: -6.82	 makespan: 675.00	 Mean_loss: 0.31580845,  training time: 4.27
progress:  40%|[34m      [0m| 8/20 [00:40<00:52,  4.40s/it]progress:  45%|[34m     [0m| 9/20 [00:40<00:47,  4.36s/it]                                                        Episode 10	 reward: -6.90	 makespan: 683.25	 Mean_loss: 0.33115333,  training time: 4.17
progress:  45%|[34m     [0m| 9/20 [00:44<00:47,  4.36s/it]progress:  50%|[34m     [0m| 10/20 [00:44<00:43,  4.30s/it]                                                         Episode 11	 reward: -6.88	 makespan: 681.15	 Mean_loss: 0.32286948,  training time: 4.17
progress:  50%|[34m     [0m| 10/20 [00:48<00:43,  4.30s/it]progress:  55%|[34m    [0m| 11/20 [00:48<00:38,  4.27s/it]                                                         Episode 12	 reward: -6.77	 makespan: 669.85	 Mean_loss: 0.24507956,  training time: 4.19
progress:  55%|[34m    [0m| 11/20 [00:52<00:38,  4.27s/it]progress:  60%|[34m    [0m| 12/20 [00:52<00:33,  4.24s/it]                                                         Episode 13	 reward: -6.82	 makespan: 675.60	 Mean_loss: 0.23869921,  training time: 4.22
progress:  60%|[34m    [0m| 12/20 [00:56<00:33,  4.24s/it]progress:  65%|[34m   [0m| 13/20 [00:56<00:29,  4.24s/it]                                                         Episode 14	 reward: -6.95	 makespan: 687.60	 Mean_loss: 0.25516450,  training time: 4.30
progress:  65%|[34m   [0m| 13/20 [01:01<00:29,  4.24s/it]progress:  70%|[34m   [0m| 14/20 [01:01<00:25,  4.26s/it]                                                         Episode 15	 reward: -6.80	 makespan: 673.65	 Mean_loss: 0.25641644,  training time: 4.26
progress:  70%|[34m   [0m| 14/20 [01:05<00:25,  4.26s/it]progress:  75%|[34m  [0m| 15/20 [01:05<00:21,  4.26s/it]                                                         Episode 16	 reward: -6.90	 makespan: 682.65	 Mean_loss: 0.24514847,  training time: 4.18
progress:  75%|[34m  [0m| 15/20 [01:09<00:21,  4.26s/it]progress:  80%|[34m  [0m| 16/20 [01:09<00:16,  4.24s/it]                                                         Episode 17	 reward: -6.95	 makespan: 688.40	 Mean_loss: 0.23974857,  training time: 4.17
progress:  80%|[34m  [0m| 16/20 [01:13<00:16,  4.24s/it]progress:  85%|[34m [0m| 17/20 [01:13<00:12,  4.22s/it]                                                         Episode 18	 reward: -6.85	 makespan: 677.90	 Mean_loss: 0.21570277,  training time: 4.36
progress:  85%|[34m [0m| 17/20 [01:18<00:12,  4.22s/it]progress:  90%|[34m [0m| 18/20 [01:18<00:08,  4.27s/it]                                                         Episode 19	 reward: -6.81	 makespan: 674.45	 Mean_loss: 0.21139649,  training time: 4.19
progress:  90%|[34m [0m| 18/20 [01:22<00:08,  4.27s/it]progress:  95%|[34m[0m| 19/20 [01:22<00:04,  4.24s/it]                                                         Episode 20	 reward: -6.89	 makespan: 681.85	 Mean_loss: 0.19663866,  training time: 4.20
progress:  95%|[34m[0m| 19/20 [01:26<00:04,  4.24s/it]progress: 100%|[34m[0m| 20/20 [01:26<00:00,  4.23s/it]progress: 100%|[34m[0m| 20/20 [01:26<00:00,  4.33s/it]
+ --num_envs 4 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for n_m in '$n_m_options'
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp17/DAN/finetuning/15x13+mix+SD2/15x10_4 --model_suffix free --finetuning_model 15x13+mix+SD2 --max_updates 20 --n_j 15 --n_m 10 --op_per_job 4
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x10+mix
save model name:  15x10+mix+free
./trained_network/SD2/15x13+mix+SD2.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/15x13+mix+SD2.pth
vali data :./data/data_train_vali/SD2/15x10+mix


progress:   0%|[34m          [0m| 0/20 [00:00<?, ?it/s]                                                Episode 1	 reward: -2.81	 makespan: 278.15	 Mean_loss: 0.01834681,  training time: 2.39
progress:   0%|[34m          [0m| 0/20 [00:02<?, ?it/s]progress:   5%|[34m         [0m| 1/20 [00:02<00:45,  2.40s/it]                                                        Episode 2	 reward: -2.75	 makespan: 271.80	 Mean_loss: 0.03510046,  training time: 1.32
progress:   5%|[34m         [0m| 1/20 [00:03<00:45,  2.40s/it]progress:  10%|[34m         [0m| 2/20 [00:03<00:31,  1.77s/it]                                                        Episode 3	 reward: -2.78	 makespan: 274.95	 Mean_loss: 0.01807613,  training time: 1.33
progress:  10%|[34m         [0m| 2/20 [00:05<00:31,  1.77s/it]progress:  15%|[34m        [0m| 3/20 [00:05<00:26,  1.57s/it]                                                        Episode 4	 reward: -2.85	 makespan: 282.25	 Mean_loss: 0.02466842,  training time: 1.35
progress:  15%|[34m        [0m| 3/20 [00:06<00:26,  1.57s/it]progress:  20%|[34m        [0m| 4/20 [00:06<00:23,  1.48s/it]                                                        Episode 5	 reward: -2.81	 makespan: 278.20	 Mean_loss: 0.00490350,  training time: 1.39
progress:  20%|[34m        [0m| 4/20 [00:07<00:23,  1.48s/it]progress:  25%|[34m       [0m| 5/20 [00:07<00:21,  1.45s/it]                                                        Episode 6	 reward: -2.77	 makespan: 274.55	 Mean_loss: 0.03973132,  training time: 1.29
progress:  25%|[34m       [0m| 5/20 [00:09<00:21,  1.45s/it]progress:  30%|[34m       [0m| 6/20 [00:09<00:19,  1.40s/it]                                                        Episode 7	 reward: -2.81	 makespan: 278.50	 Mean_loss: 0.01080048,  training time: 1.40
progress:  30%|[34m       [0m| 6/20 [00:10<00:19,  1.40s/it]progress:  35%|[34m      [0m| 7/20 [00:10<00:18,  1.40s/it]                                                        Episode 8	 reward: -2.85	 makespan: 282.00	 Mean_loss: 0.01976985,  training time: 1.31
progress:  35%|[34m      [0m| 7/20 [00:11<00:18,  1.40s/it]progress:  40%|[34m      [0m| 8/20 [00:11<00:16,  1.37s/it]                                                        Episode 9	 reward: -2.78	 makespan: 274.95	 Mean_loss: 0.03122754,  training time: 1.34
progress:  40%|[34m      [0m| 8/20 [00:13<00:16,  1.37s/it]progress:  45%|[34m     [0m| 9/20 [00:13<00:15,  1.36s/it]                                                        Episode 10	 reward: -2.80	 makespan: 276.90	 Mean_loss: 0.02993804,  training time: 1.32
progress:  45%|[34m     [0m| 9/20 [00:14<00:15,  1.36s/it]progress:  50%|[34m     [0m| 10/20 [00:14<00:13,  1.35s/it]                                                         Episode 11	 reward: -2.91	 makespan: 287.65	 Mean_loss: 0.03137673,  training time: 1.31
progress:  50%|[34m     [0m| 10/20 [00:15<00:13,  1.35s/it]progress:  55%|[34m    [0m| 11/20 [00:15<00:12,  1.34s/it]                                                         Episode 12	 reward: -2.89	 makespan: 285.70	 Mean_loss: 0.06888214,  training time: 1.39
progress:  55%|[34m    [0m| 11/20 [00:17<00:12,  1.34s/it]progress:  60%|[34m    [0m| 12/20 [00:17<00:10,  1.36s/it]                                                         Episode 13	 reward: -2.71	 makespan: 268.10	 Mean_loss: 0.03046816,  training time: 1.33
progress:  60%|[34m    [0m| 12/20 [00:18<00:10,  1.36s/it]progress:  65%|[34m   [0m| 13/20 [00:18<00:09,  1.35s/it]                                                         Episode 14	 reward: -2.80	 makespan: 277.00	 Mean_loss: 0.04074863,  training time: 1.40
progress:  65%|[34m   [0m| 13/20 [00:19<00:09,  1.35s/it]progress:  70%|[34m   [0m| 14/20 [00:19<00:08,  1.37s/it]                                                         Episode 15	 reward: -2.91	 makespan: 288.45	 Mean_loss: 0.06232695,  training time: 1.36
progress:  70%|[34m   [0m| 14/20 [00:21<00:08,  1.37s/it]progress:  75%|[34m  [0m| 15/20 [00:21<00:06,  1.36s/it]                                                         Episode 16	 reward: -2.72	 makespan: 269.40	 Mean_loss: 0.01408523,  training time: 1.31
progress:  75%|[34m  [0m| 15/20 [00:22<00:06,  1.36s/it]progress:  80%|[34m  [0m| 16/20 [00:22<00:05,  1.35s/it]                                                         Episode 17	 reward: -2.87	 makespan: 283.90	 Mean_loss: 0.02141165,  training time: 1.31
progress:  80%|[34m  [0m| 16/20 [00:23<00:05,  1.35s/it]progress:  85%|[34m [0m| 17/20 [00:23<00:04,  1.34s/it]                                                         Episode 18	 reward: -2.86	 makespan: 282.65	 Mean_loss: 0.02927154,  training time: 1.34
progress:  85%|[34m [0m| 17/20 [00:25<00:04,  1.34s/it]progress:  90%|[34m [0m| 18/20 [00:25<00:02,  1.34s/it]                                                         Episode 19	 reward: -2.83	 makespan: 280.65	 Mean_loss: 0.03662072,  training time: 1.29
progress:  90%|[34m [0m| 18/20 [00:26<00:02,  1.34s/it]progress:  95%|[34m[0m| 19/20 [00:26<00:01,  1.33s/it]                                                         Episode 20	 reward: -2.82	 makespan: 279.05	 Mean_loss: 0.01666279,  training time: 1.30
progress:  95%|[34m[0m| 19/20 [00:27<00:01,  1.33s/it]progress: 100%|[34m[0m| 20/20 [00:27<00:00,  1.32s/it]progress: 100%|[34m[0m| 20/20 [00:27<00:00,  1.39s/it]
+ --num_envs 4 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp17/DAN/finetuning/15x13+mix+SD2/15x10_7 --model_suffix free --finetuning_model 15x13+mix+SD2 --max_updates 20 --n_j 15 --n_m 10 --op_per_job 7
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x10+mix
save model name:  15x10+mix+free
./trained_network/SD2/15x13+mix+SD2.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/15x13+mix+SD2.pth
vali data :./data/data_train_vali/SD2/15x10+mix


progress:   0%|[34m          [0m| 0/20 [00:00<?, ?it/s]                                                Episode 1	 reward: -4.62	 makespan: 457.80	 Mean_loss: -0.37938875,  training time: 3.51
progress:   0%|[34m          [0m| 0/20 [00:03<?, ?it/s]progress:   5%|[34m         [0m| 1/20 [00:03<01:06,  3.52s/it]                                                        Episode 2	 reward: -4.70	 makespan: 465.45	 Mean_loss: -0.61300707,  training time: 2.31
progress:   5%|[34m         [0m| 1/20 [00:05<01:06,  3.52s/it]progress:  10%|[34m         [0m| 2/20 [00:05<00:50,  2.81s/it]                                                        Episode 3	 reward: -4.74	 makespan: 469.40	 Mean_loss: -0.50044316,  training time: 2.29
progress:  10%|[34m         [0m| 2/20 [00:08<00:50,  2.81s/it]progress:  15%|[34m        [0m| 3/20 [00:08<00:43,  2.58s/it]                                                        Episode 4	 reward: -4.65	 makespan: 460.20	 Mean_loss: 0.17082408,  training time: 2.27
progress:  15%|[34m        [0m| 3/20 [00:10<00:43,  2.58s/it]progress:  20%|[34m        [0m| 4/20 [00:10<00:39,  2.46s/it]                                                        Episode 5	 reward: -4.73	 makespan: 467.95	 Mean_loss: -0.34092593,  training time: 2.26
progress:  20%|[34m        [0m| 4/20 [00:12<00:39,  2.46s/it]progress:  25%|[34m       [0m| 5/20 [00:12<00:35,  2.39s/it]                                                        Episode 6	 reward: -4.47	 makespan: 442.40	 Mean_loss: -0.16690284,  training time: 2.33
progress:  25%|[34m       [0m| 5/20 [00:15<00:35,  2.39s/it]progress:  30%|[34m       [0m| 6/20 [00:15<00:33,  2.37s/it]                                                        Episode 7	 reward: -4.64	 makespan: 459.40	 Mean_loss: -0.16450530,  training time: 2.27
progress:  30%|[34m       [0m| 6/20 [00:17<00:33,  2.37s/it]progress:  35%|[34m      [0m| 7/20 [00:17<00:30,  2.34s/it]                                                        Episode 8	 reward: -4.70	 makespan: 465.35	 Mean_loss: -0.19790965,  training time: 2.26
progress:  35%|[34m      [0m| 7/20 [00:19<00:30,  2.34s/it]progress:  40%|[34m      [0m| 8/20 [00:19<00:27,  2.32s/it]                                                        Episode 9	 reward: -4.57	 makespan: 452.90	 Mean_loss: -0.55743802,  training time: 2.27
progress:  40%|[34m      [0m| 8/20 [00:21<00:27,  2.32s/it]progress:  45%|[34m     [0m| 9/20 [00:21<00:25,  2.30s/it]                                                        Episode 10	 reward: -4.63	 makespan: 458.45	 Mean_loss: 0.34925187,  training time: 2.27
progress:  45%|[34m     [0m| 9/20 [00:24<00:25,  2.30s/it]progress:  50%|[34m     [0m| 10/20 [00:24<00:22,  2.30s/it]                                                         Episode 11	 reward: -4.65	 makespan: 459.90	 Mean_loss: -0.04439340,  training time: 2.34
progress:  50%|[34m     [0m| 10/20 [00:26<00:22,  2.30s/it]progress:  55%|[34m    [0m| 11/20 [00:26<00:20,  2.31s/it]                                                         Episode 12	 reward: -4.72	 makespan: 467.50	 Mean_loss: 0.10709781,  training time: 2.27
progress:  55%|[34m    [0m| 11/20 [00:28<00:20,  2.31s/it]progress:  60%|[34m    [0m| 12/20 [00:28<00:18,  2.30s/it]                                                         Episode 13	 reward: -4.70	 makespan: 465.45	 Mean_loss: -0.03347017,  training time: 2.38
progress:  60%|[34m    [0m| 12/20 [00:31<00:18,  2.30s/it]progress:  65%|[34m   [0m| 13/20 [00:31<00:16,  2.33s/it]                                                         Episode 14	 reward: -4.71	 makespan: 466.40	 Mean_loss: -0.31989697,  training time: 2.27
progress:  65%|[34m   [0m| 13/20 [00:33<00:16,  2.33s/it]progress:  70%|[34m   [0m| 14/20 [00:33<00:13,  2.31s/it]                                                         Episode 15	 reward: -4.70	 makespan: 465.05	 Mean_loss: -0.03829184,  training time: 2.36
progress:  70%|[34m   [0m| 14/20 [00:35<00:13,  2.31s/it]progress:  75%|[34m  [0m| 15/20 [00:35<00:11,  2.33s/it]                                                         Episode 16	 reward: -4.62	 makespan: 457.20	 Mean_loss: 0.06712217,  training time: 2.27
progress:  75%|[34m  [0m| 15/20 [00:38<00:11,  2.33s/it]progress:  80%|[34m  [0m| 16/20 [00:38<00:09,  2.31s/it]                                                         Episode 17	 reward: -4.67	 makespan: 462.75	 Mean_loss: 0.32404816,  training time: 2.28
progress:  80%|[34m  [0m| 16/20 [00:40<00:09,  2.31s/it]progress:  85%|[34m [0m| 17/20 [00:40<00:06,  2.30s/it]                                                         Episode 18	 reward: -4.62	 makespan: 457.55	 Mean_loss: 0.20893675,  training time: 2.33
progress:  85%|[34m [0m| 17/20 [00:42<00:06,  2.30s/it]progress:  90%|[34m [0m| 18/20 [00:42<00:04,  2.31s/it]                                                         Episode 19	 reward: -4.71	 makespan: 465.80	 Mean_loss: -0.01503127,  training time: 2.25
progress:  90%|[34m [0m| 18/20 [00:44<00:04,  2.31s/it]progress:  95%|[34m[0m| 19/20 [00:44<00:02,  2.30s/it]                                                         Episode 20	 reward: -4.54	 makespan: 449.85	 Mean_loss: 0.06445410,  training time: 2.25
progress:  95%|[34m[0m| 19/20 [00:47<00:02,  2.30s/it]progress: 100%|[34m[0m| 20/20 [00:47<00:00,  2.29s/it]progress: 100%|[34m[0m| 20/20 [00:47<00:00,  2.36s/it]
+ --num_envs 4 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp17/DAN/finetuning/15x13+mix+SD2/15x10_10 --model_suffix free --finetuning_model 15x13+mix+SD2 --max_updates 20 --n_j 15 --n_m 10 --op_per_job 10
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x10+mix
save model name:  15x10+mix+free
./trained_network/SD2/15x13+mix+SD2.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/15x13+mix+SD2.pth
vali data :./data/data_train_vali/SD2/15x10+mix


progress:   0%|[34m          [0m| 0/20 [00:00<?, ?it/s]                                                Episode 1	 reward: -6.36	 makespan: 629.50	 Mean_loss: 0.25879267,  training time: 4.33
progress:   0%|[34m          [0m| 0/20 [00:04<?, ?it/s]progress:   5%|[34m         [0m| 1/20 [00:04<01:22,  4.34s/it]                                                        Episode 2	 reward: -6.30	 makespan: 624.05	 Mean_loss: 0.30412003,  training time: 3.17
progress:   5%|[34m         [0m| 1/20 [00:07<01:22,  4.34s/it]progress:  10%|[34m         [0m| 2/20 [00:07<01:05,  3.66s/it]                                                        Episode 3	 reward: -6.24	 makespan: 617.75	 Mean_loss: 0.35063019,  training time: 3.16
progress:  10%|[34m         [0m| 2/20 [00:10<01:05,  3.66s/it]progress:  15%|[34m        [0m| 3/20 [00:10<00:58,  3.43s/it]                                                        Episode 4	 reward: -6.24	 makespan: 618.25	 Mean_loss: 0.31076011,  training time: 3.16
progress:  15%|[34m        [0m| 3/20 [00:13<00:58,  3.43s/it]progress:  20%|[34m        [0m| 4/20 [00:13<00:53,  3.33s/it]                                                        Episode 5	 reward: -6.26	 makespan: 620.10	 Mean_loss: 0.26528326,  training time: 3.18
progress:  20%|[34m        [0m| 4/20 [00:17<00:53,  3.33s/it]progress:  25%|[34m       [0m| 5/20 [00:17<00:49,  3.28s/it]                                                        Episode 6	 reward: -6.29	 makespan: 623.05	 Mean_loss: 0.25791767,  training time: 3.34
progress:  25%|[34m       [0m| 5/20 [00:20<00:49,  3.28s/it]progress:  30%|[34m       [0m| 6/20 [00:20<00:46,  3.30s/it]                                                        Episode 7	 reward: -6.15	 makespan: 608.45	 Mean_loss: 0.26625615,  training time: 3.29
progress:  30%|[34m       [0m| 6/20 [00:23<00:46,  3.30s/it]progress:  35%|[34m      [0m| 7/20 [00:23<00:42,  3.30s/it]                                                        Episode 8	 reward: -6.39	 makespan: 632.75	 Mean_loss: 0.21222781,  training time: 3.15
progress:  35%|[34m      [0m| 7/20 [00:26<00:42,  3.30s/it]progress:  40%|[34m      [0m| 8/20 [00:26<00:39,  3.25s/it]                                                        Episode 9	 reward: -6.33	 makespan: 626.80	 Mean_loss: 0.17858826,  training time: 3.15
progress:  40%|[34m      [0m| 8/20 [00:29<00:39,  3.25s/it]progress:  45%|[34m     [0m| 9/20 [00:29<00:35,  3.22s/it]                                                        Episode 10	 reward: -6.37	 makespan: 631.00	 Mean_loss: 0.20742081,  training time: 3.19
progress:  45%|[34m     [0m| 9/20 [00:33<00:35,  3.22s/it]progress:  50%|[34m     [0m| 10/20 [00:33<00:32,  3.22s/it]                                                         Episode 11	 reward: -6.30	 makespan: 623.30	 Mean_loss: 0.18430252,  training time: 3.17
progress:  50%|[34m     [0m| 10/20 [00:36<00:32,  3.22s/it]progress:  55%|[34m    [0m| 11/20 [00:36<00:28,  3.20s/it]                                                         Episode 12	 reward: -6.37	 makespan: 631.05	 Mean_loss: 0.15454441,  training time: 3.15
progress:  55%|[34m    [0m| 11/20 [00:39<00:28,  3.20s/it]progress:  60%|[34m    [0m| 12/20 [00:39<00:25,  3.19s/it]                                                         Episode 13	 reward: -6.25	 makespan: 618.30	 Mean_loss: 0.15936975,  training time: 3.24
progress:  60%|[34m    [0m| 12/20 [00:42<00:25,  3.19s/it]progress:  65%|[34m   [0m| 13/20 [00:42<00:22,  3.21s/it]                                                         Episode 14	 reward: -6.25	 makespan: 618.55	 Mean_loss: 0.16532047,  training time: 3.20
progress:  65%|[34m   [0m| 13/20 [00:45<00:22,  3.21s/it]progress:  70%|[34m   [0m| 14/20 [00:45<00:19,  3.21s/it]                                                         Episode 15	 reward: -6.15	 makespan: 609.30	 Mean_loss: 0.15258273,  training time: 3.21
progress:  70%|[34m   [0m| 14/20 [00:49<00:19,  3.21s/it]progress:  75%|[34m  [0m| 15/20 [00:49<00:16,  3.21s/it]                                                         Episode 16	 reward: -6.29	 makespan: 622.70	 Mean_loss: 0.13092662,  training time: 3.18
progress:  75%|[34m  [0m| 15/20 [00:52<00:16,  3.21s/it]progress:  80%|[34m  [0m| 16/20 [00:52<00:12,  3.20s/it]                                                         Episode 17	 reward: -6.36	 makespan: 629.95	 Mean_loss: 0.14015992,  training time: 3.21
progress:  80%|[34m  [0m| 16/20 [00:55<00:12,  3.20s/it]progress:  85%|[34m [0m| 17/20 [00:55<00:09,  3.21s/it]                                                         Episode 18	 reward: -6.24	 makespan: 617.35	 Mean_loss: 0.14152694,  training time: 3.21
progress:  85%|[34m [0m| 17/20 [00:58<00:09,  3.21s/it]progress:  90%|[34m [0m| 18/20 [00:58<00:06,  3.21s/it]                                                         Episode 19	 reward: -6.27	 makespan: 620.40	 Mean_loss: 0.11426654,  training time: 3.33
progress:  90%|[34m [0m| 18/20 [01:02<00:06,  3.21s/it]progress:  95%|[34m[0m| 19/20 [01:02<00:03,  3.25s/it]                                                         Episode 20	 reward: -6.18	 makespan: 611.90	 Mean_loss: 0.11065929,  training time: 3.14
progress:  95%|[34m[0m| 19/20 [01:05<00:03,  3.25s/it]progress: 100%|[34m[0m| 20/20 [01:05<00:00,  3.22s/it]progress: 100%|[34m[0m| 20/20 [01:05<00:00,  3.27s/it]
+ --num_envs 4 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp17/DAN/finetuning/15x13+mix+SD2/15x10_12 --model_suffix free --finetuning_model 15x13+mix+SD2 --max_updates 20 --n_j 15 --n_m 10 --op_per_job 12
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x10+mix
save model name:  15x10+mix+free
./trained_network/SD2/15x13+mix+SD2.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/15x13+mix+SD2.pth
vali data :./data/data_train_vali/SD2/15x10+mix


progress:   0%|[34m          [0m| 0/20 [00:00<?, ?it/s]                                                Episode 1	 reward: -7.43	 makespan: 735.75	 Mean_loss: 0.34978986,  training time: 5.12
progress:   0%|[34m          [0m| 0/20 [00:05<?, ?it/s]progress:   5%|[34m         [0m| 1/20 [00:05<01:37,  5.13s/it]                                                        Episode 2	 reward: -7.46	 makespan: 738.80	 Mean_loss: 0.45727527,  training time: 4.01
progress:   5%|[34m         [0m| 1/20 [00:09<01:37,  5.13s/it]progress:  10%|[34m         [0m| 2/20 [00:09<01:20,  4.48s/it]                                                        Episode 3	 reward: -7.65	 makespan: 757.05	 Mean_loss: 0.55145729,  training time: 4.02
progress:  10%|[34m         [0m| 2/20 [00:13<01:20,  4.48s/it]progress:  15%|[34m        [0m| 3/20 [00:13<01:12,  4.27s/it]                                                        Episode 4	 reward: -7.59	 makespan: 751.15	 Mean_loss: 0.54924142,  training time: 4.10
progress:  15%|[34m        [0m| 3/20 [00:17<01:12,  4.27s/it]progress:  20%|[34m        [0m| 4/20 [00:17<01:07,  4.21s/it]                                                        Episode 5	 reward: -7.63	 makespan: 755.85	 Mean_loss: 0.46891710,  training time: 4.11
progress:  20%|[34m        [0m| 4/20 [00:21<01:07,  4.21s/it]progress:  25%|[34m       [0m| 5/20 [00:21<01:02,  4.17s/it]                                                        Episode 6	 reward: -7.57	 makespan: 749.05	 Mean_loss: 0.50179869,  training time: 4.05
progress:  25%|[34m       [0m| 5/20 [00:25<01:02,  4.17s/it]progress:  30%|[34m       [0m| 6/20 [00:25<00:57,  4.13s/it]                                                        Episode 7	 reward: -7.69	 makespan: 761.05	 Mean_loss: 0.42526793,  training time: 4.02
progress:  30%|[34m       [0m| 6/20 [00:29<00:57,  4.13s/it]progress:  35%|[34m      [0m| 7/20 [00:29<00:53,  4.10s/it]                                                        Episode 8	 reward: -7.42	 makespan: 734.95	 Mean_loss: 0.41664916,  training time: 4.00
progress:  35%|[34m      [0m| 7/20 [00:33<00:53,  4.10s/it]progress:  40%|[34m      [0m| 8/20 [00:33<00:48,  4.07s/it]                                                        Episode 9	 reward: -7.52	 makespan: 744.60	 Mean_loss: 0.34457737,  training time: 4.00
progress:  40%|[34m      [0m| 8/20 [00:37<00:48,  4.07s/it]progress:  45%|[34m     [0m| 9/20 [00:37<00:44,  4.05s/it]                                                        Episode 10	 reward: -7.54	 makespan: 746.05	 Mean_loss: 0.35616821,  training time: 3.99
progress:  45%|[34m     [0m| 9/20 [00:41<00:44,  4.05s/it]progress:  50%|[34m     [0m| 10/20 [00:41<00:40,  4.03s/it]                                                         Episode 11	 reward: -7.50	 makespan: 742.35	 Mean_loss: 0.26983300,  training time: 4.10
progress:  50%|[34m     [0m| 10/20 [00:45<00:40,  4.03s/it]progress:  55%|[34m    [0m| 11/20 [00:45<00:36,  4.05s/it]                                                         Episode 12	 reward: -7.60	 makespan: 752.70	 Mean_loss: 0.29181361,  training time: 3.99
progress:  55%|[34m    [0m| 11/20 [00:49<00:36,  4.05s/it]progress:  60%|[34m    [0m| 12/20 [00:49<00:32,  4.04s/it]                                                         Episode 13	 reward: -7.64	 makespan: 756.45	 Mean_loss: 0.23385963,  training time: 4.04
progress:  60%|[34m    [0m| 12/20 [00:53<00:32,  4.04s/it]progress:  65%|[34m   [0m| 13/20 [00:53<00:28,  4.04s/it]                                                         Episode 14	 reward: -7.46	 makespan: 738.40	 Mean_loss: 0.24016105,  training time: 3.99
progress:  65%|[34m   [0m| 13/20 [00:57<00:28,  4.04s/it]progress:  70%|[34m   [0m| 14/20 [00:57<00:24,  4.03s/it]                                                         Episode 15	 reward: -7.59	 makespan: 751.60	 Mean_loss: 0.22477615,  training time: 3.99
progress:  70%|[34m   [0m| 14/20 [01:01<00:24,  4.03s/it]progress:  75%|[34m  [0m| 15/20 [01:01<00:20,  4.02s/it]                                                         Episode 16	 reward: -7.55	 makespan: 747.15	 Mean_loss: 0.24033479,  training time: 4.01
progress:  75%|[34m  [0m| 15/20 [01:05<00:20,  4.02s/it]progress:  80%|[34m  [0m| 16/20 [01:05<00:16,  4.02s/it]                                                         Episode 17	 reward: -7.45	 makespan: 737.15	 Mean_loss: 0.26092291,  training time: 4.03
progress:  80%|[34m  [0m| 16/20 [01:09<00:16,  4.02s/it]progress:  85%|[34m [0m| 17/20 [01:09<00:12,  4.03s/it]                                                         Episode 18	 reward: -7.53	 makespan: 745.40	 Mean_loss: 0.26058447,  training time: 4.02
progress:  85%|[34m [0m| 17/20 [01:13<00:12,  4.03s/it]progress:  90%|[34m [0m| 18/20 [01:13<00:08,  4.03s/it]                                                         Episode 19	 reward: -7.51	 makespan: 743.20	 Mean_loss: 0.18975170,  training time: 4.02
progress:  90%|[34m [0m| 18/20 [01:17<00:08,  4.03s/it]progress:  95%|[34m[0m| 19/20 [01:17<00:04,  4.03s/it]                                                         Episode 20	 reward: -7.43	 makespan: 735.95	 Mean_loss: 0.20139946,  training time: 4.02
progress:  95%|[34m[0m| 19/20 [01:21<00:04,  4.03s/it]progress: 100%|[34m[0m| 20/20 [01:21<00:00,  4.03s/it]progress: 100%|[34m[0m| 20/20 [01:21<00:00,  4.09s/it]
+ --num_envs 4 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for n_m in '$n_m_options'
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp17/DAN/finetuning/15x13+mix+SD2/15x7_4 --model_suffix free --finetuning_model 15x13+mix+SD2 --max_updates 20 --n_j 15 --n_m 7 --op_per_job 4
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x7+mix
save model name:  15x7+mix+free
./trained_network/SD2/15x13+mix+SD2.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/15x13+mix+SD2.pth
vali data :./data/data_train_vali/SD2/15x7+mix


progress:   0%|[34m          [0m| 0/20 [00:00<?, ?it/s]                                                Episode 1	 reward: -3.68	 makespan: 364.50	 Mean_loss: 0.03118320,  training time: 2.37
progress:   0%|[34m          [0m| 0/20 [00:02<?, ?it/s]progress:   5%|[34m         [0m| 1/20 [00:02<00:45,  2.38s/it]                                                        Episode 2	 reward: -3.63	 makespan: 359.40	 Mean_loss: 0.02117874,  training time: 1.33
progress:   5%|[34m         [0m| 1/20 [00:03<00:45,  2.38s/it]progress:  10%|[34m         [0m| 2/20 [00:03<00:31,  1.76s/it]                                                        Episode 3	 reward: -3.67	 makespan: 363.40	 Mean_loss: 0.00767199,  training time: 1.29
progress:  10%|[34m         [0m| 2/20 [00:04<00:31,  1.76s/it]progress:  15%|[34m        [0m| 3/20 [00:05<00:26,  1.55s/it]                                                        Episode 4	 reward: -3.70	 makespan: 366.65	 Mean_loss: 0.02339020,  training time: 1.26
progress:  15%|[34m        [0m| 3/20 [00:06<00:26,  1.55s/it]progress:  20%|[34m        [0m| 4/20 [00:06<00:22,  1.44s/it]                                                        Episode 5	 reward: -3.63	 makespan: 359.60	 Mean_loss: 0.03032954,  training time: 1.24
progress:  20%|[34m        [0m| 4/20 [00:07<00:22,  1.44s/it]progress:  25%|[34m       [0m| 5/20 [00:07<00:20,  1.37s/it]                                                        Episode 6	 reward: -3.67	 makespan: 363.75	 Mean_loss: 0.05437132,  training time: 1.22
progress:  25%|[34m       [0m| 5/20 [00:08<00:20,  1.37s/it]progress:  30%|[34m       [0m| 6/20 [00:08<00:18,  1.32s/it]                                                        Episode 7	 reward: -3.59	 makespan: 355.30	 Mean_loss: 0.04193198,  training time: 1.34
progress:  30%|[34m       [0m| 6/20 [00:10<00:18,  1.32s/it]progress:  35%|[34m      [0m| 7/20 [00:10<00:17,  1.33s/it]                                                        Episode 8	 reward: -3.63	 makespan: 358.95	 Mean_loss: 0.01221556,  training time: 1.25
progress:  35%|[34m      [0m| 7/20 [00:11<00:17,  1.33s/it]progress:  40%|[34m      [0m| 8/20 [00:11<00:15,  1.30s/it]                                                        Episode 9	 reward: -3.60	 makespan: 356.80	 Mean_loss: 0.04648841,  training time: 1.22
progress:  40%|[34m      [0m| 8/20 [00:12<00:15,  1.30s/it]progress:  45%|[34m     [0m| 9/20 [00:12<00:14,  1.28s/it]                                                        Episode 10	 reward: -3.63	 makespan: 359.85	 Mean_loss: 0.03427125,  training time: 1.22
progress:  45%|[34m     [0m| 9/20 [00:13<00:14,  1.28s/it]progress:  50%|[34m     [0m| 10/20 [00:13<00:12,  1.26s/it]                                                         Episode 11	 reward: -3.64	 makespan: 359.90	 Mean_loss: 0.02583418,  training time: 1.28
progress:  50%|[34m     [0m| 10/20 [00:15<00:12,  1.26s/it]progress:  55%|[34m    [0m| 11/20 [00:15<00:11,  1.27s/it]                                                         Episode 12	 reward: -3.54	 makespan: 350.75	 Mean_loss: 0.01737199,  training time: 1.35
progress:  55%|[34m    [0m| 11/20 [00:16<00:11,  1.27s/it]progress:  60%|[34m    [0m| 12/20 [00:16<00:10,  1.30s/it]                                                         Episode 13	 reward: -3.56	 makespan: 352.65	 Mean_loss: 0.01681970,  training time: 1.24
progress:  60%|[34m    [0m| 12/20 [00:17<00:10,  1.30s/it]progress:  65%|[34m   [0m| 13/20 [00:17<00:08,  1.28s/it]                                                         Episode 14	 reward: -3.46	 makespan: 342.90	 Mean_loss: 0.04065043,  training time: 1.20
progress:  65%|[34m   [0m| 13/20 [00:18<00:08,  1.28s/it]progress:  70%|[34m   [0m| 14/20 [00:18<00:07,  1.26s/it]                                                         Episode 15	 reward: -3.58	 makespan: 354.30	 Mean_loss: 0.03766981,  training time: 1.28
progress:  70%|[34m   [0m| 14/20 [00:20<00:07,  1.26s/it]progress:  75%|[34m  [0m| 15/20 [00:20<00:06,  1.27s/it]                                                         Episode 16	 reward: -3.63	 makespan: 359.55	 Mean_loss: 0.03732596,  training time: 1.27
progress:  75%|[34m  [0m| 15/20 [00:21<00:06,  1.27s/it]progress:  80%|[34m  [0m| 16/20 [00:21<00:05,  1.27s/it]                                                         Episode 17	 reward: -3.54	 makespan: 350.40	 Mean_loss: 0.04858050,  training time: 1.25
progress:  80%|[34m  [0m| 16/20 [00:22<00:05,  1.27s/it]progress:  85%|[34m [0m| 17/20 [00:22<00:03,  1.26s/it]                                                         Episode 18	 reward: -3.52	 makespan: 348.50	 Mean_loss: 0.03923146,  training time: 1.21
progress:  85%|[34m [0m| 17/20 [00:23<00:03,  1.26s/it]progress:  90%|[34m [0m| 18/20 [00:23<00:02,  1.25s/it]                                                         Episode 19	 reward: -3.57	 makespan: 353.75	 Mean_loss: 0.03572805,  training time: 1.35
progress:  90%|[34m [0m| 18/20 [00:25<00:02,  1.25s/it]progress:  95%|[34m[0m| 19/20 [00:25<00:01,  1.28s/it]                                                         Episode 20	 reward: -3.53	 makespan: 349.05	 Mean_loss: 0.02542216,  training time: 1.28
progress:  95%|[34m[0m| 19/20 [00:26<00:01,  1.28s/it]progress: 100%|[34m[0m| 20/20 [00:26<00:00,  1.28s/it]progress: 100%|[34m[0m| 20/20 [00:26<00:00,  1.33s/it]
+ --num_envs 4 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp17/DAN/finetuning/15x13+mix+SD2/15x7_7 --model_suffix free --finetuning_model 15x13+mix+SD2 --max_updates 20 --n_j 15 --n_m 7 --op_per_job 7
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x7+mix
save model name:  15x7+mix+free
./trained_network/SD2/15x13+mix+SD2.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/15x13+mix+SD2.pth
vali data :./data/data_train_vali/SD2/15x7+mix


progress:   0%|[34m          [0m| 0/20 [00:00<?, ?it/s]                                                Episode 1	 reward: -5.64	 makespan: 558.00	 Mean_loss: -0.57244158,  training time: 3.38
progress:   0%|[34m          [0m| 0/20 [00:03<?, ?it/s]progress:   5%|[34m         [0m| 1/20 [00:03<01:04,  3.39s/it]                                                        Episode 2	 reward: -5.74	 makespan: 568.10	 Mean_loss: -0.59450638,  training time: 2.25
progress:   5%|[34m         [0m| 1/20 [00:05<01:04,  3.39s/it]progress:  10%|[34m         [0m| 2/20 [00:05<00:49,  2.73s/it]                                                        Episode 3	 reward: -5.73	 makespan: 567.70	 Mean_loss: -0.41618073,  training time: 2.19
progress:  10%|[34m         [0m| 2/20 [00:07<00:49,  2.73s/it]progress:  15%|[34m        [0m| 3/20 [00:07<00:42,  2.48s/it]                                                        Episode 4	 reward: -5.78	 makespan: 572.60	 Mean_loss: -0.38342947,  training time: 2.18
progress:  15%|[34m        [0m| 3/20 [00:10<00:42,  2.48s/it]progress:  20%|[34m        [0m| 4/20 [00:10<00:37,  2.37s/it]                                                        Episode 5	 reward: -5.70	 makespan: 564.55	 Mean_loss: 0.25835127,  training time: 2.22
progress:  20%|[34m        [0m| 4/20 [00:12<00:37,  2.37s/it]progress:  25%|[34m       [0m| 5/20 [00:12<00:34,  2.32s/it]                                                        Episode 6	 reward: -5.81	 makespan: 575.10	 Mean_loss: 0.33251435,  training time: 2.20
progress:  25%|[34m       [0m| 5/20 [00:14<00:34,  2.32s/it]progress:  30%|[34m       [0m| 6/20 [00:14<00:31,  2.28s/it]                                                        Episode 7	 reward: -5.63	 makespan: 557.85	 Mean_loss: -0.15948436,  training time: 2.21
progress:  30%|[34m       [0m| 6/20 [00:16<00:31,  2.28s/it]progress:  35%|[34m      [0m| 7/20 [00:16<00:29,  2.26s/it]                                                        Episode 8	 reward: -5.58	 makespan: 552.85	 Mean_loss: 0.07101852,  training time: 2.19
progress:  35%|[34m      [0m| 7/20 [00:18<00:29,  2.26s/it]progress:  40%|[34m      [0m| 8/20 [00:18<00:26,  2.24s/it]                                                        Episode 9	 reward: -5.78	 makespan: 571.90	 Mean_loss: 0.08912201,  training time: 2.17
progress:  40%|[34m      [0m| 8/20 [00:21<00:26,  2.24s/it]progress:  45%|[34m     [0m| 9/20 [00:21<00:24,  2.22s/it]                                                        Episode 10	 reward: -5.64	 makespan: 558.45	 Mean_loss: 0.76164973,  training time: 2.19
progress:  45%|[34m     [0m| 9/20 [00:23<00:24,  2.22s/it]progress:  50%|[34m     [0m| 10/20 [00:23<00:22,  2.21s/it]                                                         Episode 11	 reward: -5.78	 makespan: 571.90	 Mean_loss: -0.50063157,  training time: 2.19
progress:  50%|[34m     [0m| 10/20 [00:25<00:22,  2.21s/it]progress:  55%|[34m    [0m| 11/20 [00:25<00:19,  2.21s/it]                                                         Episode 12	 reward: -5.80	 makespan: 574.45	 Mean_loss: 0.41513759,  training time: 2.21
progress:  55%|[34m    [0m| 11/20 [00:27<00:19,  2.21s/it]progress:  60%|[34m    [0m| 12/20 [00:27<00:17,  2.21s/it]                                                         Episode 13	 reward: -5.75	 makespan: 569.00	 Mean_loss: 0.65043890,  training time: 2.31
progress:  60%|[34m    [0m| 12/20 [00:29<00:17,  2.21s/it]progress:  65%|[34m   [0m| 13/20 [00:29<00:15,  2.24s/it]                                                         Episode 14	 reward: -5.81	 makespan: 575.00	 Mean_loss: 0.19096653,  training time: 2.14
progress:  65%|[34m   [0m| 13/20 [00:32<00:15,  2.24s/it]progress:  70%|[34m   [0m| 14/20 [00:32<00:13,  2.21s/it]                                                         Episode 15	 reward: -5.79	 makespan: 573.30	 Mean_loss: 0.08956084,  training time: 2.21
progress:  70%|[34m   [0m| 14/20 [00:34<00:13,  2.21s/it]progress:  75%|[34m  [0m| 15/20 [00:34<00:11,  2.21s/it]                                                         Episode 16	 reward: -5.84	 makespan: 578.20	 Mean_loss: 0.78665358,  training time: 2.20
progress:  75%|[34m  [0m| 15/20 [00:36<00:11,  2.21s/it]progress:  80%|[34m  [0m| 16/20 [00:36<00:08,  2.21s/it]                                                         Episode 17	 reward: -5.75	 makespan: 569.70	 Mean_loss: -0.04250516,  training time: 2.19
progress:  80%|[34m  [0m| 16/20 [00:38<00:08,  2.21s/it]progress:  85%|[34m [0m| 17/20 [00:38<00:06,  2.21s/it]                                                         Episode 18	 reward: -5.80	 makespan: 574.65	 Mean_loss: -0.44057614,  training time: 2.19
progress:  85%|[34m [0m| 17/20 [00:40<00:06,  2.21s/it]progress:  90%|[34m [0m| 18/20 [00:40<00:04,  2.21s/it]                                                         Episode 19	 reward: -5.84	 makespan: 578.10	 Mean_loss: 0.51037562,  training time: 2.19
progress:  90%|[34m [0m| 18/20 [00:43<00:04,  2.21s/it]progress:  95%|[34m[0m| 19/20 [00:43<00:02,  2.20s/it]                                                         Episode 20	 reward: -5.83	 makespan: 577.50	 Mean_loss: 0.73213208,  training time: 2.26
progress:  95%|[34m[0m| 19/20 [00:45<00:02,  2.20s/it]progress: 100%|[34m[0m| 20/20 [00:45<00:00,  2.22s/it]progress: 100%|[34m[0m| 20/20 [00:45<00:00,  2.27s/it]
+ --num_envs 4 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp17/DAN/finetuning/15x13+mix+SD2/15x7_10 --model_suffix free --finetuning_model 15x13+mix+SD2 --max_updates 20 --n_j 15 --n_m 7 --op_per_job 10
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x7+mix
save model name:  15x7+mix+free
./trained_network/SD2/15x13+mix+SD2.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/15x13+mix+SD2.pth
vali data :./data/data_train_vali/SD2/15x7+mix


progress:   0%|[34m          [0m| 0/20 [00:00<?, ?it/s]                                                Episode 1	 reward: -7.97	 makespan: 788.85	 Mean_loss: 0.62154770,  training time: 4.37
progress:   0%|[34m          [0m| 0/20 [00:04<?, ?it/s]progress:   5%|[34m         [0m| 1/20 [00:04<01:23,  4.38s/it]                                                        Episode 2	 reward: -7.88	 makespan: 780.45	 Mean_loss: 0.59296131,  training time: 3.16
progress:   5%|[34m         [0m| 1/20 [00:07<01:23,  4.38s/it]progress:  10%|[34m         [0m| 2/20 [00:07<01:05,  3.67s/it]                                                        Episode 3	 reward: -7.91	 makespan: 783.50	 Mean_loss: 0.59922200,  training time: 3.12
progress:  10%|[34m         [0m| 2/20 [00:10<01:05,  3.67s/it]progress:  15%|[34m        [0m| 3/20 [00:10<00:58,  3.42s/it]                                                        Episode 4	 reward: -7.96	 makespan: 788.30	 Mean_loss: 0.74542832,  training time: 3.10
progress:  15%|[34m        [0m| 3/20 [00:13<00:58,  3.42s/it]progress:  20%|[34m        [0m| 4/20 [00:13<00:52,  3.30s/it]                                                        Episode 5	 reward: -7.94	 makespan: 785.70	 Mean_loss: 0.66639161,  training time: 3.08
progress:  20%|[34m        [0m| 4/20 [00:16<00:52,  3.30s/it]progress:  25%|[34m       [0m| 5/20 [00:16<00:48,  3.22s/it]                                                        Episode 6	 reward: -8.06	 makespan: 797.90	 Mean_loss: 0.63521171,  training time: 3.19
progress:  25%|[34m       [0m| 5/20 [00:20<00:48,  3.22s/it]progress:  30%|[34m       [0m| 6/20 [00:20<00:45,  3.21s/it]                                                        Episode 7	 reward: -7.94	 makespan: 786.05	 Mean_loss: 0.51354551,  training time: 3.06
progress:  30%|[34m       [0m| 6/20 [00:23<00:45,  3.21s/it]progress:  35%|[34m      [0m| 7/20 [00:23<00:41,  3.17s/it]                                                        Episode 8	 reward: -7.94	 makespan: 785.80	 Mean_loss: 0.47978657,  training time: 3.05
progress:  35%|[34m      [0m| 7/20 [00:26<00:41,  3.17s/it]progress:  40%|[34m      [0m| 8/20 [00:26<00:37,  3.13s/it]                                                        Episode 9	 reward: -8.04	 makespan: 795.95	 Mean_loss: 0.49792001,  training time: 3.05
progress:  40%|[34m      [0m| 8/20 [00:29<00:37,  3.13s/it]progress:  45%|[34m     [0m| 9/20 [00:29<00:34,  3.11s/it]                                                        Episode 10	 reward: -7.98	 makespan: 790.05	 Mean_loss: 0.41877824,  training time: 3.05
progress:  45%|[34m     [0m| 9/20 [00:32<00:34,  3.11s/it]progress:  50%|[34m     [0m| 10/20 [00:32<00:30,  3.09s/it]                                                         Episode 11	 reward: -8.03	 makespan: 794.85	 Mean_loss: 0.36566469,  training time: 3.04
progress:  50%|[34m     [0m| 10/20 [00:35<00:30,  3.09s/it]progress:  55%|[34m    [0m| 11/20 [00:35<00:27,  3.08s/it]                                                         Episode 12	 reward: -7.91	 makespan: 782.95	 Mean_loss: 0.36604393,  training time: 3.03
progress:  55%|[34m    [0m| 11/20 [00:38<00:27,  3.08s/it]progress:  60%|[34m    [0m| 12/20 [00:38<00:24,  3.07s/it]                                                         Episode 13	 reward: -7.86	 makespan: 778.50	 Mean_loss: 0.30617398,  training time: 3.04
progress:  60%|[34m    [0m| 12/20 [00:41<00:24,  3.07s/it]progress:  65%|[34m   [0m| 13/20 [00:41<00:21,  3.06s/it]                                                         Episode 14	 reward: -8.15	 makespan: 806.50	 Mean_loss: 0.28231153,  training time: 3.05
progress:  65%|[34m   [0m| 13/20 [00:44<00:21,  3.06s/it]progress:  70%|[34m   [0m| 14/20 [00:44<00:18,  3.06s/it]                                                         Episode 15	 reward: -7.98	 makespan: 790.20	 Mean_loss: 0.28164434,  training time: 3.05
progress:  70%|[34m   [0m| 14/20 [00:47<00:18,  3.06s/it]progress:  75%|[34m  [0m| 15/20 [00:47<00:15,  3.06s/it]                                                         Episode 16	 reward: -7.89	 makespan: 781.15	 Mean_loss: 0.25395516,  training time: 3.08
progress:  75%|[34m  [0m| 15/20 [00:50<00:15,  3.06s/it]progress:  80%|[34m  [0m| 16/20 [00:50<00:12,  3.07s/it]                                                         Episode 17	 reward: -8.06	 makespan: 798.05	 Mean_loss: 0.24520126,  training time: 3.01
progress:  80%|[34m  [0m| 16/20 [00:53<00:12,  3.07s/it]progress:  85%|[34m [0m| 17/20 [00:53<00:09,  3.05s/it]                                                         Episode 18	 reward: -8.06	 makespan: 797.50	 Mean_loss: 0.24124241,  training time: 3.02
progress:  85%|[34m [0m| 17/20 [00:56<00:09,  3.05s/it]progress:  90%|[34m [0m| 18/20 [00:56<00:06,  3.04s/it]                                                         Episode 19	 reward: -8.26	 makespan: 818.10	 Mean_loss: 0.28368637,  training time: 3.05
progress:  90%|[34m [0m| 18/20 [00:59<00:06,  3.04s/it]progress:  95%|[34m[0m| 19/20 [00:59<00:03,  3.05s/it]                                                         Episode 20	 reward: -8.09	 makespan: 800.60	 Mean_loss: 0.26584932,  training time: 3.02
progress:  95%|[34m[0m| 19/20 [01:02<00:03,  3.05s/it]progress: 100%|[34m[0m| 20/20 [01:02<00:00,  3.04s/it]progress: 100%|[34m[0m| 20/20 [01:02<00:00,  3.14s/it]
+ --num_envs 4 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp17/DAN/finetuning/15x13+mix+SD2/15x7_12 --model_suffix free --finetuning_model 15x13+mix+SD2 --max_updates 20 --n_j 15 --n_m 7 --op_per_job 12
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x7+mix
save model name:  15x7+mix+free
./trained_network/SD2/15x13+mix+SD2.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/15x13+mix+SD2.pth
vali data :./data/data_train_vali/SD2/15x7+mix


progress:   0%|[34m          [0m| 0/20 [00:00<?, ?it/s]                                                Episode 1	 reward: -9.49	 makespan: 939.60	 Mean_loss: 0.82228827,  training time: 5.03
progress:   0%|[34m          [0m| 0/20 [00:05<?, ?it/s]progress:   5%|[34m         [0m| 1/20 [00:05<01:35,  5.04s/it]                                                        Episode 2	 reward: -9.42	 makespan: 933.00	 Mean_loss: 0.94266105,  training time: 3.87
progress:   5%|[34m         [0m| 1/20 [00:08<01:35,  5.04s/it]progress:  10%|[34m         [0m| 2/20 [00:08<01:18,  4.38s/it]                                                        Episode 3	 reward: -9.35	 makespan: 926.10	 Mean_loss: 1.07313240,  training time: 3.88
progress:  10%|[34m         [0m| 2/20 [00:12<01:18,  4.38s/it]progress:  15%|[34m        [0m| 3/20 [00:12<01:10,  4.15s/it]                                                        Episode 4	 reward: -9.52	 makespan: 942.00	 Mean_loss: 1.15241146,  training time: 3.93
progress:  15%|[34m        [0m| 3/20 [00:16<01:10,  4.15s/it]progress:  20%|[34m        [0m| 4/20 [00:16<01:05,  4.07s/it]                                                        Episode 5	 reward: -9.54	 makespan: 944.60	 Mean_loss: 1.09092164,  training time: 4.22
progress:  20%|[34m        [0m| 4/20 [00:20<01:05,  4.07s/it]progress:  25%|[34m       [0m| 5/20 [00:21<01:02,  4.16s/it]                                                        Episode 6	 reward: -9.58	 makespan: 948.60	 Mean_loss: 0.94010389,  training time: 3.94
progress:  25%|[34m       [0m| 5/20 [00:25<01:02,  4.16s/it]progress:  30%|[34m       [0m| 6/20 [00:25<00:57,  4.09s/it]                                                        Episode 7	 reward: -9.59	 makespan: 949.15	 Mean_loss: 0.84844190,  training time: 4.25
progress:  30%|[34m       [0m| 6/20 [00:29<00:57,  4.09s/it]progress:  35%|[34m      [0m| 7/20 [00:29<00:53,  4.14s/it]                                                        Episode 8	 reward: -9.69	 makespan: 959.15	 Mean_loss: 0.84791493,  training time: 3.95
progress:  35%|[34m      [0m| 7/20 [00:33<00:53,  4.14s/it]progress:  40%|[34m      [0m| 8/20 [00:33<00:48,  4.08s/it]                                                        Episode 9	 reward: -9.69	 makespan: 959.15	 Mean_loss: 0.80528635,  training time: 4.17
progress:  40%|[34m      [0m| 8/20 [00:37<00:48,  4.08s/it]progress:  45%|[34m     [0m| 9/20 [00:37<00:45,  4.11s/it]                                                        Episode 10	 reward: -9.66	 makespan: 956.25	 Mean_loss: 0.82493740,  training time: 3.99
progress:  45%|[34m     [0m| 9/20 [00:41<00:45,  4.11s/it]progress:  50%|[34m     [0m| 10/20 [00:41<00:40,  4.08s/it]                                                         Episode 11	 reward: -9.63	 makespan: 952.90	 Mean_loss: 0.73937583,  training time: 3.96
progress:  50%|[34m     [0m| 10/20 [00:45<00:40,  4.08s/it]progress:  55%|[34m    [0m| 11/20 [00:45<00:36,  4.05s/it]                                                         Episode 12	 reward: -9.59	 makespan: 949.05	 Mean_loss: 0.64022303,  training time: 4.00
progress:  55%|[34m    [0m| 11/20 [00:49<00:36,  4.05s/it]progress:  60%|[34m    [0m| 12/20 [00:49<00:32,  4.04s/it]                                                         Episode 13	 reward: -9.69	 makespan: 959.70	 Mean_loss: 0.45369911,  training time: 3.84
progress:  60%|[34m    [0m| 12/20 [00:53<00:32,  4.04s/it]progress:  65%|[34m   [0m| 13/20 [00:53<00:27,  3.98s/it]                                                         Episode 14	 reward: -9.77	 makespan: 966.90	 Mean_loss: 0.55273515,  training time: 3.93
progress:  65%|[34m   [0m| 13/20 [00:57<00:27,  3.98s/it]progress:  70%|[34m   [0m| 14/20 [00:57<00:23,  3.97s/it]                                                         Episode 15	 reward: -9.53	 makespan: 943.45	 Mean_loss: 0.46351290,  training time: 3.83
progress:  70%|[34m   [0m| 14/20 [01:01<00:23,  3.97s/it]progress:  75%|[34m  [0m| 15/20 [01:01<00:19,  3.93s/it]                                                         Episode 16	 reward: -9.63	 makespan: 953.65	 Mean_loss: 0.43979967,  training time: 4.16
progress:  75%|[34m  [0m| 15/20 [01:05<00:19,  3.93s/it]progress:  80%|[34m  [0m| 16/20 [01:05<00:16,  4.00s/it]                                                         Episode 17	 reward: -9.66	 makespan: 955.90	 Mean_loss: 0.42617995,  training time: 3.94
progress:  80%|[34m  [0m| 16/20 [01:09<00:16,  4.00s/it]progress:  85%|[34m [0m| 17/20 [01:09<00:11,  3.99s/it]                                                         Episode 18	 reward: -9.70	 makespan: 960.10	 Mean_loss: 0.49298456,  training time: 3.88
progress:  85%|[34m [0m| 17/20 [01:13<00:11,  3.99s/it]progress:  90%|[34m [0m| 18/20 [01:13<00:07,  3.96s/it]                                                         Episode 19	 reward: -9.92	 makespan: 982.45	 Mean_loss: 0.41898620,  training time: 3.83
progress:  90%|[34m [0m| 18/20 [01:16<00:07,  3.96s/it]progress:  95%|[34m[0m| 19/20 [01:16<00:03,  3.92s/it]                                                         Episode 20	 reward: -9.52	 makespan: 942.40	 Mean_loss: 0.39486626,  training time: 4.01
progress:  95%|[34m[0m| 19/20 [01:20<00:03,  3.92s/it]progress: 100%|[34m[0m| 20/20 [01:20<00:00,  3.95s/it]progress: 100%|[34m[0m| 20/20 [01:20<00:00,  4.05s/it]
+ --num_envs 4 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for n_m in '$n_m_options'
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp17/DAN/finetuning/15x13+mix+SD2/15x5_4 --model_suffix free --finetuning_model 15x13+mix+SD2 --max_updates 20 --n_j 15 --n_m 5 --op_per_job 4
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x5+mix
save model name:  15x5+mix+free
./trained_network/SD2/15x13+mix+SD2.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/15x13+mix+SD2.pth
vali data :./data/data_train_vali/SD2/15x5+mix


progress:   0%|[34m          [0m| 0/20 [00:00<?, ?it/s]                                                Episode 1	 reward: -4.82	 makespan: 476.70	 Mean_loss: 0.44424322,  training time: 2.37
progress:   0%|[34m          [0m| 0/20 [00:02<?, ?it/s]progress:   5%|[34m         [0m| 1/20 [00:02<00:45,  2.38s/it]                                                        Episode 2	 reward: -4.89	 makespan: 484.15	 Mean_loss: 0.39754421,  training time: 1.22
progress:   5%|[34m         [0m| 1/20 [00:03<00:45,  2.38s/it]progress:  10%|[34m         [0m| 2/20 [00:03<00:30,  1.70s/it]                                                        Episode 3	 reward: -4.82	 makespan: 477.65	 Mean_loss: 0.20860566,  training time: 1.21
progress:  10%|[34m         [0m| 2/20 [00:04<00:30,  1.70s/it]progress:  15%|[34m        [0m| 3/20 [00:04<00:25,  1.48s/it]                                                        Episode 4	 reward: -4.89	 makespan: 484.45	 Mean_loss: 0.30854288,  training time: 1.26
progress:  15%|[34m        [0m| 3/20 [00:06<00:25,  1.48s/it]progress:  20%|[34m        [0m| 4/20 [00:06<00:22,  1.40s/it]                                                        Episode 5	 reward: -4.84	 makespan: 478.75	 Mean_loss: 0.25357294,  training time: 1.39
progress:  20%|[34m        [0m| 4/20 [00:07<00:22,  1.40s/it]progress:  25%|[34m       [0m| 5/20 [00:07<00:20,  1.40s/it]                                                        Episode 6	 reward: -4.88	 makespan: 483.45	 Mean_loss: 0.28913671,  training time: 1.26
progress:  25%|[34m       [0m| 5/20 [00:08<00:20,  1.40s/it]progress:  30%|[34m       [0m| 6/20 [00:08<00:18,  1.35s/it]                                                        Episode 7	 reward: -4.82	 makespan: 476.70	 Mean_loss: 0.16356599,  training time: 1.29
progress:  30%|[34m       [0m| 6/20 [00:10<00:18,  1.35s/it]progress:  35%|[34m      [0m| 7/20 [00:10<00:17,  1.33s/it]                                                        Episode 8	 reward: -4.82	 makespan: 477.65	 Mean_loss: 0.16827190,  training time: 1.25
progress:  35%|[34m      [0m| 7/20 [00:11<00:17,  1.33s/it]progress:  40%|[34m      [0m| 8/20 [00:11<00:15,  1.31s/it]                                                        Episode 9	 reward: -4.94	 makespan: 489.05	 Mean_loss: 0.17428799,  training time: 1.30
progress:  40%|[34m      [0m| 8/20 [00:12<00:15,  1.31s/it]progress:  45%|[34m     [0m| 9/20 [00:12<00:14,  1.31s/it]                                                        Episode 10	 reward: -4.93	 makespan: 487.60	 Mean_loss: 0.16951165,  training time: 1.26
progress:  45%|[34m     [0m| 9/20 [00:13<00:14,  1.31s/it]progress:  50%|[34m     [0m| 10/20 [00:13<00:12,  1.29s/it]                                                         Episode 11	 reward: -4.85	 makespan: 479.80	 Mean_loss: 0.15161294,  training time: 1.29
progress:  50%|[34m     [0m| 10/20 [00:15<00:12,  1.29s/it]progress:  55%|[34m    [0m| 11/20 [00:15<00:11,  1.29s/it]                                                         Episode 12	 reward: -4.89	 makespan: 484.20	 Mean_loss: 0.29263508,  training time: 1.38
progress:  55%|[34m    [0m| 11/20 [00:16<00:11,  1.29s/it]progress:  60%|[34m    [0m| 12/20 [00:16<00:10,  1.32s/it]                                                         Episode 13	 reward: -4.80	 makespan: 475.55	 Mean_loss: 0.14506148,  training time: 1.37
progress:  60%|[34m    [0m| 12/20 [00:17<00:10,  1.32s/it]progress:  65%|[34m   [0m| 13/20 [00:17<00:09,  1.34s/it]                                                         Episode 14	 reward: -4.87	 makespan: 481.75	 Mean_loss: 0.31409022,  training time: 1.33
progress:  65%|[34m   [0m| 13/20 [00:19<00:09,  1.34s/it]progress:  70%|[34m   [0m| 14/20 [00:19<00:08,  1.34s/it]                                                         Episode 15	 reward: -4.88	 makespan: 483.15	 Mean_loss: 0.22292811,  training time: 1.29
progress:  70%|[34m   [0m| 14/20 [00:20<00:08,  1.34s/it]progress:  75%|[34m  [0m| 15/20 [00:20<00:06,  1.32s/it]                                                         Episode 16	 reward: -4.70	 makespan: 465.50	 Mean_loss: 0.18412325,  training time: 1.34
progress:  75%|[34m  [0m| 15/20 [00:21<00:06,  1.32s/it]progress:  80%|[34m  [0m| 16/20 [00:21<00:05,  1.33s/it]                                                         Episode 17	 reward: -4.87	 makespan: 482.45	 Mean_loss: 0.15913892,  training time: 1.29
progress:  80%|[34m  [0m| 16/20 [00:23<00:05,  1.33s/it]progress:  85%|[34m [0m| 17/20 [00:23<00:03,  1.32s/it]                                                         Episode 18	 reward: -4.66	 makespan: 461.45	 Mean_loss: 0.13340873,  training time: 1.29
progress:  85%|[34m [0m| 17/20 [00:24<00:03,  1.32s/it]progress:  90%|[34m [0m| 18/20 [00:24<00:02,  1.31s/it]                                                         Episode 19	 reward: -4.76	 makespan: 471.70	 Mean_loss: 0.07518232,  training time: 1.30
progress:  90%|[34m [0m| 18/20 [00:25<00:02,  1.31s/it]progress:  95%|[34m[0m| 19/20 [00:25<00:01,  1.31s/it]                                                         Episode 20	 reward: -4.75	 makespan: 470.35	 Mean_loss: 0.14527661,  training time: 1.22
progress:  95%|[34m[0m| 19/20 [00:27<00:01,  1.31s/it]progress: 100%|[34m[0m| 20/20 [00:27<00:00,  1.28s/it]progress: 100%|[34m[0m| 20/20 [00:27<00:00,  1.35s/it]
+ --num_envs 4 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp17/DAN/finetuning/15x13+mix+SD2/15x5_7 --model_suffix free --finetuning_model 15x13+mix+SD2 --max_updates 20 --n_j 15 --n_m 5 --op_per_job 7
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x5+mix
save model name:  15x5+mix+free
./trained_network/SD2/15x13+mix+SD2.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/15x13+mix+SD2.pth
vali data :./data/data_train_vali/SD2/15x5+mix


progress:   0%|[34m          [0m| 0/20 [00:00<?, ?it/s]                                                Episode 1	 reward: -7.83	 makespan: 775.60	 Mean_loss: 0.28279918,  training time: 3.32
progress:   0%|[34m          [0m| 0/20 [00:03<?, ?it/s]progress:   5%|[34m         [0m| 1/20 [00:03<01:03,  3.33s/it]                                                        Episode 2	 reward: -7.91	 makespan: 782.75	 Mean_loss: -0.04218380,  training time: 2.16
progress:   5%|[34m         [0m| 1/20 [00:05<01:03,  3.33s/it]progress:  10%|[34m         [0m| 2/20 [00:05<00:47,  2.65s/it]                                                        Episode 3	 reward: -7.81	 makespan: 773.35	 Mean_loss: 0.07116172,  training time: 2.14
progress:  10%|[34m         [0m| 2/20 [00:07<00:47,  2.65s/it]progress:  15%|[34m        [0m| 3/20 [00:07<00:41,  2.42s/it]                                                        Episode 4	 reward: -7.88	 makespan: 779.70	 Mean_loss: 0.13089076,  training time: 2.23
progress:  15%|[34m        [0m| 3/20 [00:09<00:41,  2.42s/it]progress:  20%|[34m        [0m| 4/20 [00:09<00:37,  2.35s/it]                                                        Episode 5	 reward: -7.93	 makespan: 784.95	 Mean_loss: -0.07116228,  training time: 2.17
progress:  20%|[34m        [0m| 4/20 [00:12<00:37,  2.35s/it]progress:  25%|[34m       [0m| 5/20 [00:12<00:34,  2.29s/it]                                                        Episode 6	 reward: -8.03	 makespan: 794.90	 Mean_loss: -0.22727813,  training time: 2.16
progress:  25%|[34m       [0m| 5/20 [00:14<00:34,  2.29s/it]progress:  30%|[34m       [0m| 6/20 [00:14<00:31,  2.24s/it]                                                        Episode 7	 reward: -7.96	 makespan: 787.60	 Mean_loss: -0.28022367,  training time: 2.13
progress:  30%|[34m       [0m| 6/20 [00:16<00:31,  2.24s/it]progress:  35%|[34m      [0m| 7/20 [00:16<00:28,  2.21s/it]                                                        Episode 8	 reward: -7.88	 makespan: 780.05	 Mean_loss: -0.33059543,  training time: 2.12
progress:  35%|[34m      [0m| 7/20 [00:18<00:28,  2.21s/it]progress:  40%|[34m      [0m| 8/20 [00:18<00:26,  2.18s/it]                                                        Episode 9	 reward: -7.97	 makespan: 788.90	 Mean_loss: -0.27086669,  training time: 2.13
progress:  40%|[34m      [0m| 8/20 [00:20<00:26,  2.18s/it]progress:  45%|[34m     [0m| 9/20 [00:20<00:23,  2.17s/it]                                                        Episode 10	 reward: -8.02	 makespan: 793.80	 Mean_loss: -0.30246866,  training time: 2.13
progress:  45%|[34m     [0m| 9/20 [00:22<00:23,  2.17s/it]progress:  50%|[34m     [0m| 10/20 [00:22<00:21,  2.16s/it]                                                         Episode 11	 reward: -7.97	 makespan: 788.65	 Mean_loss: -0.28877777,  training time: 2.16
progress:  50%|[34m     [0m| 10/20 [00:24<00:21,  2.16s/it]progress:  55%|[34m    [0m| 11/20 [00:24<00:19,  2.16s/it]                                                         Episode 12	 reward: -7.94	 makespan: 785.90	 Mean_loss: -0.14857849,  training time: 2.15
progress:  55%|[34m    [0m| 11/20 [00:27<00:19,  2.16s/it]progress:  60%|[34m    [0m| 12/20 [00:27<00:17,  2.16s/it]                                                         Episode 13	 reward: -8.10	 makespan: 801.60	 Mean_loss: -0.13766456,  training time: 2.21
progress:  60%|[34m    [0m| 12/20 [00:29<00:17,  2.16s/it]progress:  65%|[34m   [0m| 13/20 [00:29<00:15,  2.18s/it]                                                         Episode 14	 reward: -8.09	 makespan: 800.80	 Mean_loss: -0.08649075,  training time: 2.13
progress:  65%|[34m   [0m| 13/20 [00:31<00:15,  2.18s/it]progress:  70%|[34m   [0m| 14/20 [00:31<00:12,  2.16s/it]                                                         Episode 15	 reward: -7.97	 makespan: 789.15	 Mean_loss: -0.34546709,  training time: 2.18
progress:  70%|[34m   [0m| 14/20 [00:33<00:12,  2.16s/it]progress:  75%|[34m  [0m| 15/20 [00:33<00:10,  2.17s/it]                                                         Episode 16	 reward: -8.03	 makespan: 794.80	 Mean_loss: -0.08736236,  training time: 2.16
progress:  75%|[34m  [0m| 15/20 [00:35<00:10,  2.17s/it]progress:  80%|[34m  [0m| 16/20 [00:35<00:08,  2.17s/it]                                                         Episode 17	 reward: -7.97	 makespan: 788.95	 Mean_loss: 0.09582888,  training time: 2.13
progress:  80%|[34m  [0m| 16/20 [00:37<00:08,  2.17s/it]progress:  85%|[34m [0m| 17/20 [00:37<00:06,  2.16s/it]                                                         Episode 18	 reward: -7.97	 makespan: 789.35	 Mean_loss: -0.10613737,  training time: 2.23
progress:  85%|[34m [0m| 17/20 [00:40<00:06,  2.16s/it]progress:  90%|[34m [0m| 18/20 [00:40<00:04,  2.18s/it]                                                         Episode 19	 reward: -8.05	 makespan: 796.95	 Mean_loss: 0.19440746,  training time: 2.11
progress:  90%|[34m [0m| 18/20 [00:42<00:04,  2.18s/it]progress:  95%|[34m[0m| 19/20 [00:42<00:02,  2.16s/it]                                                         Episode 20	 reward: -7.82	 makespan: 773.90	 Mean_loss: -0.24630734,  training time: 2.15
progress:  95%|[34m[0m| 19/20 [00:44<00:02,  2.16s/it]progress: 100%|[34m[0m| 20/20 [00:44<00:00,  2.16s/it]progress: 100%|[34m[0m| 20/20 [00:44<00:00,  2.22s/it]
+ --num_envs 4 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp17/DAN/finetuning/15x13+mix+SD2/15x5_10 --model_suffix free --finetuning_model 15x13+mix+SD2 --max_updates 20 --n_j 15 --n_m 5 --op_per_job 10
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x5+mix
save model name:  15x5+mix+free
./trained_network/SD2/15x13+mix+SD2.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/15x13+mix+SD2.pth
vali data :./data/data_train_vali/SD2/15x5+mix


progress:   0%|[34m          [0m| 0/20 [00:00<?, ?it/s]                                                Episode 1	 reward: -10.81	 makespan: 1070.55	 Mean_loss: 2.09849906,  training time: 4.13
progress:   0%|[34m          [0m| 0/20 [00:04<?, ?it/s]progress:   5%|[34m         [0m| 1/20 [00:04<01:18,  4.14s/it]                                                        Episode 2	 reward: -10.89	 makespan: 1078.35	 Mean_loss: 1.84091282,  training time: 3.03
progress:   5%|[34m         [0m| 1/20 [00:07<01:18,  4.14s/it]progress:  10%|[34m         [0m| 2/20 [00:07<01:02,  3.49s/it]                                                        Episode 3	 reward: -10.90	 makespan: 1078.90	 Mean_loss: 1.75222611,  training time: 3.02
progress:  10%|[34m         [0m| 2/20 [00:10<01:02,  3.49s/it]progress:  15%|[34m        [0m| 3/20 [00:10<00:55,  3.28s/it]                                                        Episode 4	 reward: -10.83	 makespan: 1071.95	 Mean_loss: 1.95560479,  training time: 3.01
progress:  15%|[34m        [0m| 3/20 [00:13<00:55,  3.28s/it]progress:  20%|[34m        [0m| 4/20 [00:13<00:50,  3.18s/it]                                                        Episode 5	 reward: -10.88	 makespan: 1076.75	 Mean_loss: 2.30564857,  training time: 3.11
progress:  20%|[34m        [0m| 4/20 [00:16<00:50,  3.18s/it]progress:  25%|[34m       [0m| 5/20 [00:16<00:47,  3.16s/it]                                                        Episode 6	 reward: -11.04	 makespan: 1093.10	 Mean_loss: 2.37396288,  training time: 3.17
progress:  25%|[34m       [0m| 5/20 [00:19<00:47,  3.16s/it]progress:  30%|[34m       [0m| 6/20 [00:19<00:44,  3.16s/it]                                                        Episode 7	 reward: -11.03	 makespan: 1091.80	 Mean_loss: 2.34398985,  training time: 3.05
progress:  30%|[34m       [0m| 6/20 [00:22<00:44,  3.16s/it]progress:  35%|[34m      [0m| 7/20 [00:22<00:40,  3.13s/it]                                                        Episode 8	 reward: -11.13	 makespan: 1101.75	 Mean_loss: 2.01008415,  training time: 3.03
progress:  35%|[34m      [0m| 7/20 [00:25<00:40,  3.13s/it]progress:  40%|[34m      [0m| 8/20 [00:25<00:37,  3.10s/it]                                                        Episode 9	 reward: -10.98	 makespan: 1087.00	 Mean_loss: 1.54233468,  training time: 3.03
progress:  40%|[34m      [0m| 8/20 [00:28<00:37,  3.10s/it]progress:  45%|[34m     [0m| 9/20 [00:28<00:33,  3.08s/it]                                                        Episode 10	 reward: -10.92	 makespan: 1080.60	 Mean_loss: 1.34911323,  training time: 3.01
progress:  45%|[34m     [0m| 9/20 [00:31<00:33,  3.08s/it]progress:  50%|[34m     [0m| 10/20 [00:31<00:30,  3.06s/it]                                                         Episode 11	 reward: -11.02	 makespan: 1090.60	 Mean_loss: 1.29146302,  training time: 3.03
progress:  50%|[34m     [0m| 10/20 [00:34<00:30,  3.06s/it]progress:  55%|[34m    [0m| 11/20 [00:34<00:27,  3.05s/it]                                                         Episode 12	 reward: -11.07	 makespan: 1095.80	 Mean_loss: 1.23564029,  training time: 3.00
progress:  55%|[34m    [0m| 11/20 [00:37<00:27,  3.05s/it]progress:  60%|[34m    [0m| 12/20 [00:37<00:24,  3.04s/it]                                                         Episode 13	 reward: -10.96	 makespan: 1085.45	 Mean_loss: 1.20033109,  training time: 3.01
progress:  60%|[34m    [0m| 12/20 [00:40<00:24,  3.04s/it]progress:  65%|[34m   [0m| 13/20 [00:40<00:21,  3.03s/it]                                                         Episode 14	 reward: -11.19	 makespan: 1107.80	 Mean_loss: 1.08732998,  training time: 3.02
progress:  65%|[34m   [0m| 13/20 [00:43<00:21,  3.03s/it]progress:  70%|[34m   [0m| 14/20 [00:43<00:18,  3.03s/it]                                                         Episode 15	 reward: -11.16	 makespan: 1104.35	 Mean_loss: 0.95641452,  training time: 3.00
progress:  70%|[34m   [0m| 14/20 [00:46<00:18,  3.03s/it]progress:  75%|[34m  [0m| 15/20 [00:46<00:15,  3.02s/it]                                                         Episode 16	 reward: -11.06	 makespan: 1094.80	 Mean_loss: 0.87416178,  training time: 3.02
progress:  75%|[34m  [0m| 15/20 [00:49<00:15,  3.02s/it]progress:  80%|[34m  [0m| 16/20 [00:49<00:12,  3.03s/it]                                                         Episode 17	 reward: -11.04	 makespan: 1092.85	 Mean_loss: 0.91598612,  training time: 3.01
progress:  80%|[34m  [0m| 16/20 [00:52<00:12,  3.03s/it]progress:  85%|[34m [0m| 17/20 [00:52<00:09,  3.02s/it]                                                         Episode 18	 reward: -11.10	 makespan: 1098.75	 Mean_loss: 0.87610871,  training time: 3.00
progress:  85%|[34m [0m| 17/20 [00:55<00:09,  3.02s/it]progress:  90%|[34m [0m| 18/20 [00:55<00:06,  3.02s/it]                                                         Episode 19	 reward: -11.02	 makespan: 1090.85	 Mean_loss: 0.76943249,  training time: 2.99
progress:  90%|[34m [0m| 18/20 [00:58<00:06,  3.02s/it]progress:  95%|[34m[0m| 19/20 [00:58<00:03,  3.01s/it]                                                         Episode 20	 reward: -10.89	 makespan: 1077.70	 Mean_loss: 0.70652473,  training time: 3.01
progress:  95%|[34m[0m| 19/20 [01:01<00:03,  3.01s/it]progress: 100%|[34m[0m| 20/20 [01:01<00:00,  3.01s/it]progress: 100%|[34m[0m| 20/20 [01:01<00:00,  3.09s/it]
+ --num_envs 4 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp17/DAN/finetuning/15x13+mix+SD2/15x5_12 --model_suffix free --finetuning_model 15x13+mix+SD2 --max_updates 20 --n_j 15 --n_m 5 --op_per_job 12
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x5+mix
save model name:  15x5+mix+free
./trained_network/SD2/15x13+mix+SD2.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/15x13+mix+SD2.pth
vali data :./data/data_train_vali/SD2/15x5+mix


progress:   0%|[34m          [0m| 0/20 [00:00<?, ?it/s]                                                Episode 1	 reward: -13.27	 makespan: 1313.30	 Mean_loss: 3.31574678,  training time: 5.09
progress:   0%|[34m          [0m| 0/20 [00:05<?, ?it/s]progress:   5%|[34m         [0m| 1/20 [00:05<01:36,  5.10s/it]                                                        Episode 2	 reward: -13.09	 makespan: 1295.55	 Mean_loss: 2.60952592,  training time: 3.88
progress:   5%|[34m         [0m| 1/20 [00:08<01:36,  5.10s/it]progress:  10%|[34m         [0m| 2/20 [00:08<01:18,  4.39s/it]                                                        Episode 3	 reward: -13.08	 makespan: 1294.90	 Mean_loss: 2.71277881,  training time: 3.72
progress:  10%|[34m         [0m| 2/20 [00:12<01:18,  4.39s/it]progress:  15%|[34m        [0m| 3/20 [00:12<01:09,  4.09s/it]                                                        Episode 4	 reward: -13.22	 makespan: 1308.30	 Mean_loss: 3.43762183,  training time: 3.80
progress:  15%|[34m        [0m| 3/20 [00:16<01:09,  4.09s/it]progress:  20%|[34m        [0m| 4/20 [00:16<01:03,  3.98s/it]                                                        Episode 5	 reward: -13.07	 makespan: 1293.70	 Mean_loss: 3.90839100,  training time: 3.89
progress:  20%|[34m        [0m| 4/20 [00:20<01:03,  3.98s/it]progress:  25%|[34m       [0m| 5/20 [00:20<00:59,  3.95s/it]                                                        Episode 6	 reward: -13.26	 makespan: 1312.50	 Mean_loss: 3.73418832,  training time: 3.73
progress:  25%|[34m       [0m| 5/20 [00:24<00:59,  3.95s/it]progress:  30%|[34m       [0m| 6/20 [00:24<00:54,  3.88s/it]                                                        Episode 7	 reward: -13.19	 makespan: 1306.25	 Mean_loss: 3.33212757,  training time: 3.73
progress:  30%|[34m       [0m| 6/20 [00:27<00:54,  3.88s/it]progress:  35%|[34m      [0m| 7/20 [00:27<00:49,  3.83s/it]                                                        Episode 8	 reward: -13.39	 makespan: 1325.25	 Mean_loss: 2.89532614,  training time: 3.74
progress:  35%|[34m      [0m| 7/20 [00:31<00:49,  3.83s/it]progress:  40%|[34m      [0m| 8/20 [00:31<00:45,  3.80s/it]                                                        Episode 9	 reward: -13.28	 makespan: 1314.75	 Mean_loss: 2.72791362,  training time: 3.91
progress:  40%|[34m      [0m| 8/20 [00:35<00:45,  3.80s/it]progress:  45%|[34m     [0m| 9/20 [00:35<00:42,  3.84s/it]                                                        Episode 10	 reward: -13.33	 makespan: 1319.90	 Mean_loss: 2.85050917,  training time: 3.75
progress:  45%|[34m     [0m| 9/20 [00:39<00:42,  3.84s/it]progress:  50%|[34m     [0m| 10/20 [00:39<00:38,  3.81s/it]                                                         Episode 11	 reward: -13.52	 makespan: 1338.10	 Mean_loss: 2.60519624,  training time: 3.73
progress:  50%|[34m     [0m| 10/20 [00:43<00:38,  3.81s/it]progress:  55%|[34m    [0m| 11/20 [00:43<00:34,  3.79s/it]                                                         Episode 12	 reward: -13.29	 makespan: 1315.60	 Mean_loss: 2.39916110,  training time: 3.77
progress:  55%|[34m    [0m| 11/20 [00:46<00:34,  3.79s/it]progress:  60%|[34m    [0m| 12/20 [00:46<00:30,  3.78s/it]                                                         Episode 13	 reward: -13.31	 makespan: 1317.90	 Mean_loss: 2.22198415,  training time: 3.74
progress:  60%|[34m    [0m| 12/20 [00:50<00:30,  3.78s/it]progress:  65%|[34m   [0m| 13/20 [00:50<00:26,  3.77s/it]                                                         Episode 14	 reward: -13.34	 makespan: 1320.55	 Mean_loss: 1.94027615,  training time: 3.73
progress:  65%|[34m   [0m| 13/20 [00:54<00:26,  3.77s/it]progress:  70%|[34m   [0m| 14/20 [00:54<00:22,  3.76s/it]                                                         Episode 15	 reward: -13.28	 makespan: 1314.60	 Mean_loss: 1.81855893,  training time: 3.77
progress:  70%|[34m   [0m| 14/20 [00:58<00:22,  3.76s/it]progress:  75%|[34m  [0m| 15/20 [00:58<00:18,  3.77s/it]                                                         Episode 16	 reward: -13.05	 makespan: 1291.95	 Mean_loss: 1.65252709,  training time: 3.73
progress:  75%|[34m  [0m| 15/20 [01:01<00:18,  3.77s/it]progress:  80%|[34m  [0m| 16/20 [01:01<00:15,  3.76s/it]                                                         Episode 17	 reward: -13.36	 makespan: 1322.90	 Mean_loss: 1.67982888,  training time: 3.72
progress:  80%|[34m  [0m| 16/20 [01:05<00:15,  3.76s/it]progress:  85%|[34m [0m| 17/20 [01:05<00:11,  3.75s/it]                                                         Episode 18	 reward: -13.24	 makespan: 1310.80	 Mean_loss: 1.48846281,  training time: 3.71
progress:  85%|[34m [0m| 17/20 [01:09<00:11,  3.75s/it]progress:  90%|[34m [0m| 18/20 [01:09<00:07,  3.74s/it]                                                         Episode 19	 reward: -13.44	 makespan: 1330.80	 Mean_loss: 1.42276049,  training time: 3.73
progress:  90%|[34m [0m| 18/20 [01:12<00:07,  3.74s/it]progress:  95%|[34m[0m| 19/20 [01:13<00:03,  3.74s/it]                                                         Episode 20	 reward: -13.32	 makespan: 1318.50	 Mean_loss: 1.34476161,  training time: 3.85
progress:  95%|[34m[0m| 19/20 [01:16<00:03,  3.74s/it]progress: 100%|[34m[0m| 20/20 [01:16<00:00,  3.77s/it]progress: 100%|[34m[0m| 20/20 [01:16<00:00,  3.84s/it]
+ --num_envs 4 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for model in 15x13+mix+SD2 15x5+mix+SD2
+ for n_m in '$n_m_options'
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp17/DAN/finetuning/15x5+mix+SD2/15x13_4 --model_suffix free --finetuning_model 15x5+mix+SD2 --max_updates 20 --n_j 15 --n_m 13 --op_per_job 4
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x13+mix
save model name:  15x13+mix+free
./trained_network/SD2/15x5+mix+SD2.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/15x5+mix+SD2.pth
vali data :./data/data_train_vali/SD2/15x13+mix


progress:   0%|[34m          [0m| 0/20 [00:00<?, ?it/s]                                                Episode 1	 reward: -2.51	 makespan: 248.90	 Mean_loss: 0.28787896,  training time: 2.48
progress:   0%|[34m          [0m| 0/20 [00:02<?, ?it/s]progress:   5%|[34m         [0m| 1/20 [00:02<00:47,  2.49s/it]                                                        Episode 2	 reward: -2.66	 makespan: 263.05	 Mean_loss: 0.12122052,  training time: 1.29
progress:   5%|[34m         [0m| 1/20 [00:03<00:47,  2.49s/it]progress:  10%|[34m         [0m| 2/20 [00:03<00:32,  1.81s/it]                                                        Episode 3	 reward: -2.46	 makespan: 243.35	 Mean_loss: 0.03857320,  training time: 1.29
progress:  10%|[34m         [0m| 2/20 [00:05<00:32,  1.81s/it]progress:  15%|[34m        [0m| 3/20 [00:05<00:26,  1.57s/it]                                                        Episode 4	 reward: -2.58	 makespan: 255.00	 Mean_loss: 0.03475858,  training time: 1.28
progress:  15%|[34m        [0m| 3/20 [00:06<00:26,  1.57s/it]progress:  20%|[34m        [0m| 4/20 [00:06<00:23,  1.46s/it]                                                        Episode 5	 reward: -2.53	 makespan: 250.85	 Mean_loss: 0.02751094,  training time: 1.28
progress:  20%|[34m        [0m| 4/20 [00:07<00:23,  1.46s/it]progress:  25%|[34m       [0m| 5/20 [00:07<00:20,  1.40s/it]                                                        Episode 6	 reward: -2.55	 makespan: 252.65	 Mean_loss: 0.05002990,  training time: 1.28
progress:  25%|[34m       [0m| 5/20 [00:08<00:20,  1.40s/it]progress:  30%|[34m       [0m| 6/20 [00:08<00:19,  1.36s/it]                                                        Episode 7	 reward: -2.47	 makespan: 244.50	 Mean_loss: 0.05197320,  training time: 1.28
progress:  30%|[34m       [0m| 6/20 [00:10<00:19,  1.36s/it]progress:  35%|[34m      [0m| 7/20 [00:10<00:17,  1.34s/it]                                                        Episode 8	 reward: -2.52	 makespan: 249.20	 Mean_loss: 0.01661594,  training time: 1.27
progress:  35%|[34m      [0m| 7/20 [00:11<00:17,  1.34s/it]progress:  40%|[34m      [0m| 8/20 [00:11<00:15,  1.32s/it]                                                        Episode 9	 reward: -2.54	 makespan: 251.50	 Mean_loss: 0.00786938,  training time: 1.37
progress:  40%|[34m      [0m| 8/20 [00:12<00:15,  1.32s/it]progress:  45%|[34m     [0m| 9/20 [00:12<00:14,  1.34s/it]                                                        Episode 10	 reward: -2.56	 makespan: 253.05	 Mean_loss: 0.01576025,  training time: 1.32
progress:  45%|[34m     [0m| 9/20 [00:14<00:14,  1.34s/it]progress:  50%|[34m     [0m| 10/20 [00:14<00:13,  1.33s/it]                                                         Episode 11	 reward: -2.56	 makespan: 253.35	 Mean_loss: 0.01001469,  training time: 1.33
progress:  50%|[34m     [0m| 10/20 [00:15<00:13,  1.33s/it]progress:  55%|[34m    [0m| 11/20 [00:15<00:11,  1.33s/it]                                                         Episode 12	 reward: -2.43	 makespan: 240.65	 Mean_loss: 0.01399423,  training time: 1.40
progress:  55%|[34m    [0m| 11/20 [00:16<00:11,  1.33s/it]progress:  60%|[34m    [0m| 12/20 [00:16<00:10,  1.35s/it]                                                         Episode 13	 reward: -2.47	 makespan: 244.90	 Mean_loss: 0.02072513,  training time: 1.28
progress:  60%|[34m    [0m| 12/20 [00:18<00:10,  1.35s/it]progress:  65%|[34m   [0m| 13/20 [00:18<00:09,  1.33s/it]                                                         Episode 14	 reward: -2.52	 makespan: 249.50	 Mean_loss: 0.02334480,  training time: 1.31
progress:  65%|[34m   [0m| 13/20 [00:19<00:09,  1.33s/it]progress:  70%|[34m   [0m| 14/20 [00:19<00:07,  1.33s/it]                                                         Episode 15	 reward: -2.45	 makespan: 242.80	 Mean_loss: 0.00738165,  training time: 1.35
progress:  70%|[34m   [0m| 14/20 [00:20<00:07,  1.33s/it]progress:  75%|[34m  [0m| 15/20 [00:20<00:06,  1.33s/it]                                                         Episode 16	 reward: -2.48	 makespan: 245.40	 Mean_loss: 0.00728914,  training time: 1.35
progress:  75%|[34m  [0m| 15/20 [00:22<00:06,  1.33s/it]progress:  80%|[34m  [0m| 16/20 [00:22<00:05,  1.34s/it]                                                         Episode 17	 reward: -2.48	 makespan: 245.50	 Mean_loss: 0.02459752,  training time: 1.28
progress:  80%|[34m  [0m| 16/20 [00:23<00:05,  1.34s/it]progress:  85%|[34m [0m| 17/20 [00:23<00:03,  1.32s/it]                                                         Episode 18	 reward: -2.51	 makespan: 248.65	 Mean_loss: 0.03150299,  training time: 1.29
progress:  85%|[34m [0m| 17/20 [00:24<00:03,  1.32s/it]progress:  90%|[34m [0m| 18/20 [00:24<00:02,  1.31s/it]                                                         Episode 19	 reward: -2.58	 makespan: 255.15	 Mean_loss: 0.02289594,  training time: 1.36
progress:  90%|[34m [0m| 18/20 [00:26<00:02,  1.31s/it]progress:  95%|[34m[0m| 19/20 [00:26<00:01,  1.33s/it]                                                         Episode 20	 reward: -2.52	 makespan: 249.80	 Mean_loss: 0.02178037,  training time: 1.27
progress:  95%|[34m[0m| 19/20 [00:27<00:01,  1.33s/it]progress: 100%|[34m[0m| 20/20 [00:27<00:00,  1.31s/it]progress: 100%|[34m[0m| 20/20 [00:27<00:00,  1.37s/it]
+ --num_envs 4 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp17/DAN/finetuning/15x5+mix+SD2/15x13_7 --model_suffix free --finetuning_model 15x5+mix+SD2 --max_updates 20 --n_j 15 --n_m 13 --op_per_job 7
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x13+mix
save model name:  15x13+mix+free
./trained_network/SD2/15x5+mix+SD2.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/15x5+mix+SD2.pth
vali data :./data/data_train_vali/SD2/15x13+mix


progress:   0%|[34m          [0m| 0/20 [00:00<?, ?it/s]                                                Episode 1	 reward: -4.28	 makespan: 423.75	 Mean_loss: 0.58030641,  training time: 3.52
progress:   0%|[34m          [0m| 0/20 [00:03<?, ?it/s]progress:   5%|[34m         [0m| 1/20 [00:03<01:07,  3.53s/it]                                                        Episode 2	 reward: -4.13	 makespan: 409.20	 Mean_loss: 0.04525802,  training time: 2.41
progress:   5%|[34m         [0m| 1/20 [00:05<01:07,  3.53s/it]progress:  10%|[34m         [0m| 2/20 [00:05<00:51,  2.88s/it]                                                        Episode 3	 reward: -4.33	 makespan: 428.95	 Mean_loss: -0.30460835,  training time: 2.50
progress:  10%|[34m         [0m| 2/20 [00:08<00:51,  2.88s/it]progress:  15%|[34m        [0m| 3/20 [00:08<00:46,  2.71s/it]                                                        Episode 4	 reward: -4.30	 makespan: 425.30	 Mean_loss: -0.30250227,  training time: 2.39
progress:  15%|[34m        [0m| 3/20 [00:10<00:46,  2.71s/it]progress:  20%|[34m        [0m| 4/20 [00:10<00:41,  2.59s/it]                                                        Episode 5	 reward: -4.33	 makespan: 428.45	 Mean_loss: -0.08365034,  training time: 2.42
progress:  20%|[34m        [0m| 4/20 [00:13<00:41,  2.59s/it]progress:  25%|[34m       [0m| 5/20 [00:13<00:37,  2.53s/it]                                                        Episode 6	 reward: -4.27	 makespan: 423.15	 Mean_loss: -0.26090252,  training time: 2.42
progress:  25%|[34m       [0m| 5/20 [00:15<00:37,  2.53s/it]progress:  30%|[34m       [0m| 6/20 [00:15<00:34,  2.49s/it]                                                        Episode 7	 reward: -4.25	 makespan: 421.10	 Mean_loss: -0.55526048,  training time: 2.31
progress:  30%|[34m       [0m| 6/20 [00:18<00:34,  2.49s/it]progress:  35%|[34m      [0m| 7/20 [00:18<00:31,  2.43s/it]                                                        Episode 8	 reward: -4.37	 makespan: 432.65	 Mean_loss: -0.17233731,  training time: 2.35
progress:  35%|[34m      [0m| 7/20 [00:20<00:31,  2.43s/it]progress:  40%|[34m      [0m| 8/20 [00:20<00:28,  2.41s/it]                                                        Episode 9	 reward: -4.39	 makespan: 435.10	 Mean_loss: 0.24466997,  training time: 2.30
progress:  40%|[34m      [0m| 8/20 [00:22<00:28,  2.41s/it]progress:  45%|[34m     [0m| 9/20 [00:22<00:26,  2.38s/it]                                                        Episode 10	 reward: -4.43	 makespan: 438.15	 Mean_loss: -0.45515025,  training time: 2.36
progress:  45%|[34m     [0m| 9/20 [00:25<00:26,  2.38s/it]progress:  50%|[34m     [0m| 10/20 [00:25<00:23,  2.38s/it]                                                         Episode 11	 reward: -4.31	 makespan: 426.35	 Mean_loss: -0.22492617,  training time: 2.30
progress:  50%|[34m     [0m| 10/20 [00:27<00:23,  2.38s/it]progress:  55%|[34m    [0m| 11/20 [00:27<00:21,  2.36s/it]                                                         Episode 12	 reward: -4.24	 makespan: 419.40	 Mean_loss: -0.21771833,  training time: 2.39
progress:  55%|[34m    [0m| 11/20 [00:29<00:21,  2.36s/it]progress:  60%|[34m    [0m| 12/20 [00:29<00:18,  2.37s/it]                                                         Episode 13	 reward: -4.31	 makespan: 426.45	 Mean_loss: 0.19227058,  training time: 2.43
progress:  60%|[34m    [0m| 12/20 [00:32<00:18,  2.37s/it]progress:  65%|[34m   [0m| 13/20 [00:32<00:16,  2.39s/it]                                                         Episode 14	 reward: -4.30	 makespan: 425.60	 Mean_loss: 0.22761974,  training time: 2.31
progress:  65%|[34m   [0m| 13/20 [00:34<00:16,  2.39s/it]progress:  70%|[34m   [0m| 14/20 [00:34<00:14,  2.37s/it]                                                         Episode 15	 reward: -4.38	 makespan: 433.65	 Mean_loss: -0.03489731,  training time: 2.38
progress:  70%|[34m   [0m| 14/20 [00:36<00:14,  2.37s/it]progress:  75%|[34m  [0m| 15/20 [00:36<00:11,  2.37s/it]                                                         Episode 16	 reward: -4.33	 makespan: 428.50	 Mean_loss: -0.38563725,  training time: 2.33
progress:  75%|[34m  [0m| 15/20 [00:39<00:11,  2.37s/it]progress:  80%|[34m  [0m| 16/20 [00:39<00:09,  2.36s/it]                                                         Episode 17	 reward: -4.38	 makespan: 433.60	 Mean_loss: 0.16681148,  training time: 2.38
progress:  80%|[34m  [0m| 16/20 [00:41<00:09,  2.36s/it]progress:  85%|[34m [0m| 17/20 [00:41<00:07,  2.37s/it]                                                         Episode 18	 reward: -4.34	 makespan: 429.60	 Mean_loss: 0.16772819,  training time: 2.36
progress:  85%|[34m [0m| 17/20 [00:43<00:07,  2.37s/it]progress:  90%|[34m [0m| 18/20 [00:43<00:04,  2.37s/it]                                                         Episode 19	 reward: -4.18	 makespan: 414.30	 Mean_loss: -0.15397875,  training time: 2.28
progress:  90%|[34m [0m| 18/20 [00:46<00:04,  2.37s/it]progress:  95%|[34m[0m| 19/20 [00:46<00:02,  2.34s/it]                                                         Episode 20	 reward: -4.28	 makespan: 423.40	 Mean_loss: -0.51172990,  training time: 2.30
progress:  95%|[34m[0m| 19/20 [00:48<00:02,  2.34s/it]progress: 100%|[34m[0m| 20/20 [00:48<00:00,  2.33s/it]progress: 100%|[34m[0m| 20/20 [00:48<00:00,  2.43s/it]
+ --num_envs 4 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp17/DAN/finetuning/15x5+mix+SD2/15x13_10 --model_suffix free --finetuning_model 15x5+mix+SD2 --max_updates 20 --n_j 15 --n_m 13 --op_per_job 10
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x13+mix
save model name:  15x13+mix+free
./trained_network/SD2/15x5+mix+SD2.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/15x5+mix+SD2.pth
vali data :./data/data_train_vali/SD2/15x13+mix


progress:   0%|[34m          [0m| 0/20 [00:00<?, ?it/s]                                                Episode 1	 reward: -5.86	 makespan: 579.65	 Mean_loss: 0.23248394,  training time: 4.49
progress:   0%|[34m          [0m| 0/20 [00:04<?, ?it/s]progress:   5%|[34m         [0m| 1/20 [00:04<01:25,  4.50s/it]                                                        Episode 2	 reward: -6.01	 makespan: 594.70	 Mean_loss: 0.17277282,  training time: 3.30
progress:   5%|[34m         [0m| 1/20 [00:07<01:25,  4.50s/it]progress:  10%|[34m         [0m| 2/20 [00:07<01:08,  3.79s/it]                                                        Episode 3	 reward: -5.88	 makespan: 581.75	 Mean_loss: 0.11757450,  training time: 3.36
progress:  10%|[34m         [0m| 2/20 [00:11<01:08,  3.79s/it]progress:  15%|[34m        [0m| 3/20 [00:11<01:01,  3.60s/it]                                                        Episode 4	 reward: -5.75	 makespan: 568.85	 Mean_loss: 0.11863991,  training time: 3.37
progress:  15%|[34m        [0m| 3/20 [00:14<01:01,  3.60s/it]progress:  20%|[34m        [0m| 4/20 [00:14<00:56,  3.51s/it]                                                        Episode 5	 reward: -5.89	 makespan: 582.95	 Mean_loss: 0.09323618,  training time: 3.44
progress:  20%|[34m        [0m| 4/20 [00:17<00:56,  3.51s/it]progress:  25%|[34m       [0m| 5/20 [00:17<00:52,  3.49s/it]                                                        Episode 6	 reward: -5.92	 makespan: 586.25	 Mean_loss: 0.08258212,  training time: 3.47
progress:  25%|[34m       [0m| 5/20 [00:21<00:52,  3.49s/it]progress:  30%|[34m       [0m| 6/20 [00:21<00:48,  3.49s/it]                                                        Episode 7	 reward: -5.87	 makespan: 581.20	 Mean_loss: 0.08823811,  training time: 3.34
progress:  30%|[34m       [0m| 6/20 [00:24<00:48,  3.49s/it]progress:  35%|[34m      [0m| 7/20 [00:24<00:44,  3.44s/it]                                                        Episode 8	 reward: -5.97	 makespan: 590.65	 Mean_loss: 0.07276941,  training time: 3.41
progress:  35%|[34m      [0m| 7/20 [00:28<00:44,  3.44s/it]progress:  40%|[34m      [0m| 8/20 [00:28<00:41,  3.43s/it]                                                        Episode 9	 reward: -5.89	 makespan: 582.65	 Mean_loss: 0.08247470,  training time: 3.32
progress:  40%|[34m      [0m| 8/20 [00:31<00:41,  3.43s/it]progress:  45%|[34m     [0m| 9/20 [00:31<00:37,  3.40s/it]                                                        Episode 10	 reward: -5.93	 makespan: 587.15	 Mean_loss: 0.07406463,  training time: 3.33
progress:  45%|[34m     [0m| 9/20 [00:34<00:37,  3.40s/it]progress:  50%|[34m     [0m| 10/20 [00:34<00:33,  3.38s/it]                                                         Episode 11	 reward: -6.00	 makespan: 594.25	 Mean_loss: 0.06847189,  training time: 3.34
progress:  50%|[34m     [0m| 10/20 [00:38<00:33,  3.38s/it]progress:  55%|[34m    [0m| 11/20 [00:38<00:30,  3.37s/it]                                                         Episode 12	 reward: -5.93	 makespan: 587.55	 Mean_loss: 0.06125765,  training time: 3.30
progress:  55%|[34m    [0m| 11/20 [00:41<00:30,  3.37s/it]progress:  60%|[34m    [0m| 12/20 [00:41<00:26,  3.35s/it]                                                         Episode 13	 reward: -5.97	 makespan: 591.40	 Mean_loss: 0.07239087,  training time: 3.27
progress:  60%|[34m    [0m| 12/20 [00:44<00:26,  3.35s/it]progress:  65%|[34m   [0m| 13/20 [00:44<00:23,  3.33s/it]                                                         Episode 14	 reward: -5.89	 makespan: 582.95	 Mean_loss: 0.06004682,  training time: 3.28
progress:  65%|[34m   [0m| 13/20 [00:48<00:23,  3.33s/it]progress:  70%|[34m   [0m| 14/20 [00:48<00:19,  3.32s/it]                                                         Episode 15	 reward: -5.93	 makespan: 587.50	 Mean_loss: 0.06359155,  training time: 3.27
progress:  70%|[34m   [0m| 14/20 [00:51<00:19,  3.32s/it]progress:  75%|[34m  [0m| 15/20 [00:51<00:16,  3.31s/it]                                                         Episode 16	 reward: -5.95	 makespan: 589.35	 Mean_loss: 0.08119789,  training time: 3.44
progress:  75%|[34m  [0m| 15/20 [00:54<00:16,  3.31s/it]progress:  80%|[34m  [0m| 16/20 [00:54<00:13,  3.35s/it]                                                         Episode 17	 reward: -5.80	 makespan: 574.15	 Mean_loss: 0.05386471,  training time: 3.37
progress:  80%|[34m  [0m| 16/20 [00:58<00:13,  3.35s/it]progress:  85%|[34m [0m| 17/20 [00:58<00:10,  3.36s/it]                                                         Episode 18	 reward: -5.91	 makespan: 584.90	 Mean_loss: 0.06649102,  training time: 3.30
progress:  85%|[34m [0m| 17/20 [01:01<00:10,  3.36s/it]progress:  90%|[34m [0m| 18/20 [01:01<00:06,  3.34s/it]                                                         Episode 19	 reward: -5.81	 makespan: 575.50	 Mean_loss: 0.05409091,  training time: 3.26
progress:  90%|[34m [0m| 18/20 [01:04<00:06,  3.34s/it]progress:  95%|[34m[0m| 19/20 [01:04<00:03,  3.32s/it]                                                         Episode 20	 reward: -5.88	 makespan: 581.90	 Mean_loss: 0.06562752,  training time: 3.27
progress:  95%|[34m[0m| 19/20 [01:08<00:03,  3.32s/it]progress: 100%|[34m[0m| 20/20 [01:08<00:00,  3.31s/it]progress: 100%|[34m[0m| 20/20 [01:08<00:00,  3.40s/it]
+ --num_envs 4 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp17/DAN/finetuning/15x5+mix+SD2/15x13_12 --model_suffix free --finetuning_model 15x5+mix+SD2 --max_updates 20 --n_j 15 --n_m 13 --op_per_job 12
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x13+mix
save model name:  15x13+mix+free
./trained_network/SD2/15x5+mix+SD2.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/15x5+mix+SD2.pth
vali data :./data/data_train_vali/SD2/15x13+mix


progress:   0%|[34m          [0m| 0/20 [00:00<?, ?it/s]                                                Episode 1	 reward: -6.82	 makespan: 675.40	 Mean_loss: 0.32735521,  training time: 5.45
progress:   0%|[34m          [0m| 0/20 [00:05<?, ?it/s]progress:   5%|[34m         [0m| 1/20 [00:05<01:43,  5.46s/it]                                                        Episode 2	 reward: -6.85	 makespan: 678.00	 Mean_loss: 0.18723297,  training time: 4.41
progress:   5%|[34m         [0m| 1/20 [00:09<01:43,  5.46s/it]progress:  10%|[34m         [0m| 2/20 [00:09<01:27,  4.85s/it]                                                        Episode 3	 reward: -6.83	 makespan: 675.75	 Mean_loss: 0.14590560,  training time: 4.29
progress:  10%|[34m         [0m| 2/20 [00:14<01:27,  4.85s/it]progress:  15%|[34m        [0m| 3/20 [00:14<01:18,  4.59s/it]                                                        Episode 4	 reward: -6.88	 makespan: 681.00	 Mean_loss: 0.13313675,  training time: 4.21
progress:  15%|[34m        [0m| 3/20 [00:18<01:18,  4.59s/it]progress:  20%|[34m        [0m| 4/20 [00:18<01:11,  4.45s/it]                                                        Episode 5	 reward: -6.86	 makespan: 679.55	 Mean_loss: 0.14074722,  training time: 4.34
progress:  20%|[34m        [0m| 4/20 [00:22<01:11,  4.45s/it]progress:  25%|[34m       [0m| 5/20 [00:22<01:06,  4.41s/it]                                                        Episode 6	 reward: -6.80	 makespan: 673.00	 Mean_loss: 0.14908540,  training time: 4.36
progress:  25%|[34m       [0m| 5/20 [00:27<01:06,  4.41s/it]progress:  30%|[34m       [0m| 6/20 [00:27<01:01,  4.40s/it]                                                        Episode 7	 reward: -6.82	 makespan: 675.35	 Mean_loss: 0.09464559,  training time: 4.47
progress:  30%|[34m       [0m| 6/20 [00:31<01:01,  4.40s/it]progress:  35%|[34m      [0m| 7/20 [00:31<00:57,  4.42s/it]                                                        Episode 8	 reward: -6.82	 makespan: 674.75	 Mean_loss: 0.11751543,  training time: 4.17
progress:  35%|[34m      [0m| 7/20 [00:35<00:57,  4.42s/it]progress:  40%|[34m      [0m| 8/20 [00:35<00:52,  4.35s/it]                                                        Episode 9	 reward: -6.79	 makespan: 672.25	 Mean_loss: 0.12059173,  training time: 4.18
progress:  40%|[34m      [0m| 8/20 [00:39<00:52,  4.35s/it]progress:  45%|[34m     [0m| 9/20 [00:39<00:47,  4.30s/it]                                                        Episode 10	 reward: -6.79	 makespan: 672.15	 Mean_loss: 0.09390829,  training time: 4.23
progress:  45%|[34m     [0m| 9/20 [00:44<00:47,  4.30s/it]progress:  50%|[34m     [0m| 10/20 [00:44<00:42,  4.28s/it]                                                         Episode 11	 reward: -6.82	 makespan: 675.65	 Mean_loss: 0.08975023,  training time: 4.19
progress:  50%|[34m     [0m| 10/20 [00:48<00:42,  4.28s/it]progress:  55%|[34m    [0m| 11/20 [00:48<00:38,  4.25s/it]                                                         Episode 12	 reward: -6.89	 makespan: 682.00	 Mean_loss: 0.10837969,  training time: 4.18
progress:  55%|[34m    [0m| 11/20 [00:52<00:38,  4.25s/it]progress:  60%|[34m    [0m| 12/20 [00:52<00:33,  4.23s/it]                                                         Episode 13	 reward: -6.82	 makespan: 674.90	 Mean_loss: 0.09703749,  training time: 4.17
progress:  60%|[34m    [0m| 12/20 [00:56<00:33,  4.23s/it]progress:  65%|[34m   [0m| 13/20 [00:56<00:29,  4.22s/it]                                                         Episode 14	 reward: -6.79	 makespan: 672.10	 Mean_loss: 0.11929318,  training time: 4.22
progress:  65%|[34m   [0m| 13/20 [01:00<00:29,  4.22s/it]progress:  70%|[34m   [0m| 14/20 [01:00<00:25,  4.22s/it]                                                         Episode 15	 reward: -6.85	 makespan: 677.80	 Mean_loss: 0.10822756,  training time: 4.20
progress:  70%|[34m   [0m| 14/20 [01:05<00:25,  4.22s/it]progress:  75%|[34m  [0m| 15/20 [01:05<00:21,  4.22s/it]                                                         Episode 16	 reward: -6.73	 makespan: 665.85	 Mean_loss: 0.10726885,  training time: 4.20
progress:  75%|[34m  [0m| 15/20 [01:09<00:21,  4.22s/it]progress:  80%|[34m  [0m| 16/20 [01:09<00:16,  4.21s/it]                                                         Episode 17	 reward: -6.93	 makespan: 686.40	 Mean_loss: 0.12724300,  training time: 4.23
progress:  80%|[34m  [0m| 16/20 [01:13<00:16,  4.21s/it]progress:  85%|[34m [0m| 17/20 [01:13<00:12,  4.22s/it]                                                         Episode 18	 reward: -6.82	 makespan: 675.35	 Mean_loss: 0.10767668,  training time: 4.18
progress:  85%|[34m [0m| 17/20 [01:17<00:12,  4.22s/it]progress:  90%|[34m [0m| 18/20 [01:17<00:08,  4.21s/it]                                                         Episode 19	 reward: -7.00	 makespan: 693.25	 Mean_loss: 0.11376595,  training time: 4.23
progress:  90%|[34m [0m| 18/20 [01:22<00:08,  4.21s/it]progress:  95%|[34m[0m| 19/20 [01:22<00:04,  4.22s/it]                                                         Episode 20	 reward: -6.82	 makespan: 675.25	 Mean_loss: 0.09141959,  training time: 4.17
progress:  95%|[34m[0m| 19/20 [01:26<00:04,  4.22s/it]progress: 100%|[34m[0m| 20/20 [01:26<00:00,  4.21s/it]progress: 100%|[34m[0m| 20/20 [01:26<00:00,  4.31s/it]
+ --num_envs 4 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for n_m in '$n_m_options'
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp17/DAN/finetuning/15x5+mix+SD2/15x10_4 --model_suffix free --finetuning_model 15x5+mix+SD2 --max_updates 20 --n_j 15 --n_m 10 --op_per_job 4
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x10+mix
save model name:  15x10+mix+free
./trained_network/SD2/15x5+mix+SD2.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/15x5+mix+SD2.pth
vali data :./data/data_train_vali/SD2/15x10+mix


progress:   0%|[34m          [0m| 0/20 [00:00<?, ?it/s]                                                Episode 1	 reward: -2.77	 makespan: 274.55	 Mean_loss: 0.25577182,  training time: 2.39
progress:   0%|[34m          [0m| 0/20 [00:02<?, ?it/s]progress:   5%|[34m         [0m| 1/20 [00:02<00:45,  2.40s/it]                                                        Episode 2	 reward: -2.79	 makespan: 276.55	 Mean_loss: 0.13891351,  training time: 1.31
progress:   5%|[34m         [0m| 1/20 [00:03<00:45,  2.40s/it]progress:  10%|[34m         [0m| 2/20 [00:03<00:31,  1.76s/it]                                                        Episode 3	 reward: -2.72	 makespan: 269.30	 Mean_loss: 0.07364870,  training time: 1.39
progress:  10%|[34m         [0m| 2/20 [00:05<00:31,  1.76s/it]progress:  15%|[34m        [0m| 3/20 [00:05<00:27,  1.59s/it]                                                        Episode 4	 reward: -2.76	 makespan: 273.20	 Mean_loss: 0.06600514,  training time: 1.33
progress:  15%|[34m        [0m| 3/20 [00:06<00:27,  1.59s/it]progress:  20%|[34m        [0m| 4/20 [00:06<00:23,  1.49s/it]                                                        Episode 5	 reward: -2.71	 makespan: 268.05	 Mean_loss: 0.04784188,  training time: 1.32
progress:  20%|[34m        [0m| 4/20 [00:07<00:23,  1.49s/it]progress:  25%|[34m       [0m| 5/20 [00:07<00:21,  1.43s/it]                                                        Episode 6	 reward: -2.69	 makespan: 265.85	 Mean_loss: 0.01824957,  training time: 1.33
progress:  25%|[34m       [0m| 5/20 [00:09<00:21,  1.43s/it]progress:  30%|[34m       [0m| 6/20 [00:09<00:19,  1.40s/it]                                                        Episode 7	 reward: -2.74	 makespan: 271.75	 Mean_loss: 0.03653228,  training time: 1.33
progress:  30%|[34m       [0m| 6/20 [00:10<00:19,  1.40s/it]progress:  35%|[34m      [0m| 7/20 [00:10<00:17,  1.38s/it]                                                        Episode 8	 reward: -2.93	 makespan: 290.35	 Mean_loss: 0.05378102,  training time: 1.30
progress:  35%|[34m      [0m| 7/20 [00:11<00:17,  1.38s/it]progress:  40%|[34m      [0m| 8/20 [00:11<00:16,  1.36s/it]                                                        Episode 9	 reward: -2.76	 makespan: 273.15	 Mean_loss: 0.05409312,  training time: 1.33
progress:  40%|[34m      [0m| 8/20 [00:13<00:16,  1.36s/it]progress:  45%|[34m     [0m| 9/20 [00:13<00:14,  1.35s/it]                                                        Episode 10	 reward: -2.78	 makespan: 274.85	 Mean_loss: 0.06765752,  training time: 1.38
progress:  45%|[34m     [0m| 9/20 [00:14<00:14,  1.35s/it]progress:  50%|[34m     [0m| 10/20 [00:14<00:13,  1.36s/it]                                                         Episode 11	 reward: -2.79	 makespan: 276.25	 Mean_loss: 0.04221261,  training time: 1.37
progress:  50%|[34m     [0m| 10/20 [00:15<00:13,  1.36s/it]progress:  55%|[34m    [0m| 11/20 [00:15<00:12,  1.36s/it]                                                         Episode 12	 reward: -2.74	 makespan: 271.45	 Mean_loss: 0.08891819,  training time: 1.45
progress:  55%|[34m    [0m| 11/20 [00:17<00:12,  1.36s/it]progress:  60%|[34m    [0m| 12/20 [00:17<00:11,  1.39s/it]                                                         Episode 13	 reward: -2.82	 makespan: 278.80	 Mean_loss: 0.02740403,  training time: 1.32
progress:  60%|[34m    [0m| 12/20 [00:18<00:11,  1.39s/it]progress:  65%|[34m   [0m| 13/20 [00:18<00:09,  1.37s/it]                                                         Episode 14	 reward: -2.85	 makespan: 281.70	 Mean_loss: 0.03439285,  training time: 1.34
progress:  65%|[34m   [0m| 13/20 [00:19<00:09,  1.37s/it]progress:  70%|[34m   [0m| 14/20 [00:19<00:08,  1.36s/it]                                                         Episode 15	 reward: -2.78	 makespan: 274.90	 Mean_loss: 0.03042404,  training time: 1.33
progress:  70%|[34m   [0m| 14/20 [00:21<00:08,  1.36s/it]progress:  75%|[34m  [0m| 15/20 [00:21<00:06,  1.36s/it]                                                         Episode 16	 reward: -2.74	 makespan: 271.05	 Mean_loss: 0.01607977,  training time: 1.32
progress:  75%|[34m  [0m| 15/20 [00:22<00:06,  1.36s/it]progress:  80%|[34m  [0m| 16/20 [00:22<00:05,  1.35s/it]                                                         Episode 17	 reward: -2.84	 makespan: 281.20	 Mean_loss: 0.03406767,  training time: 1.31
progress:  80%|[34m  [0m| 16/20 [00:23<00:05,  1.35s/it]progress:  85%|[34m [0m| 17/20 [00:23<00:04,  1.34s/it]                                                         Episode 18	 reward: -2.84	 makespan: 281.50	 Mean_loss: 0.04933352,  training time: 1.32
progress:  85%|[34m [0m| 17/20 [00:25<00:04,  1.34s/it]progress:  90%|[34m [0m| 18/20 [00:25<00:02,  1.33s/it]                                                         Episode 19	 reward: -2.71	 makespan: 268.15	 Mean_loss: 0.03800117,  training time: 1.32
progress:  90%|[34m [0m| 18/20 [00:26<00:02,  1.33s/it]progress:  95%|[34m[0m| 19/20 [00:26<00:01,  1.33s/it]                                                         Episode 20	 reward: -2.73	 makespan: 270.20	 Mean_loss: 0.03221052,  training time: 1.32
progress:  95%|[34m[0m| 19/20 [00:27<00:01,  1.33s/it]progress: 100%|[34m[0m| 20/20 [00:27<00:00,  1.33s/it]progress: 100%|[34m[0m| 20/20 [00:27<00:00,  1.40s/it]
+ --num_envs 4 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp17/DAN/finetuning/15x5+mix+SD2/15x10_7 --model_suffix free --finetuning_model 15x5+mix+SD2 --max_updates 20 --n_j 15 --n_m 10 --op_per_job 7
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x10+mix
save model name:  15x10+mix+free
./trained_network/SD2/15x5+mix+SD2.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/15x5+mix+SD2.pth
vali data :./data/data_train_vali/SD2/15x10+mix


progress:   0%|[34m          [0m| 0/20 [00:00<?, ?it/s]                                                Episode 1	 reward: -4.60	 makespan: 455.00	 Mean_loss: 0.52741766,  training time: 3.58
progress:   0%|[34m          [0m| 0/20 [00:03<?, ?it/s]progress:   5%|[34m         [0m| 1/20 [00:03<01:08,  3.59s/it]                                                        Episode 2	 reward: -4.73	 makespan: 468.05	 Mean_loss: 0.19466707,  training time: 2.24
progress:   5%|[34m         [0m| 1/20 [00:05<01:08,  3.59s/it]progress:  10%|[34m         [0m| 2/20 [00:05<00:50,  2.80s/it]                                                        Episode 3	 reward: -4.60	 makespan: 455.70	 Mean_loss: -0.46447930,  training time: 2.26
progress:  10%|[34m         [0m| 2/20 [00:08<00:50,  2.80s/it]progress:  15%|[34m        [0m| 3/20 [00:08<00:43,  2.55s/it]                                                        Episode 4	 reward: -4.66	 makespan: 461.70	 Mean_loss: -0.25380838,  training time: 2.23
progress:  15%|[34m        [0m| 3/20 [00:10<00:43,  2.55s/it]progress:  20%|[34m        [0m| 4/20 [00:10<00:38,  2.43s/it]                                                        Episode 5	 reward: -4.68	 makespan: 463.80	 Mean_loss: 0.06703006,  training time: 2.27
progress:  20%|[34m        [0m| 4/20 [00:12<00:38,  2.43s/it]progress:  25%|[34m       [0m| 5/20 [00:12<00:35,  2.38s/it]                                                        Episode 6	 reward: -4.66	 makespan: 461.70	 Mean_loss: -0.28290427,  training time: 2.30
progress:  25%|[34m       [0m| 5/20 [00:14<00:35,  2.38s/it]progress:  30%|[34m       [0m| 6/20 [00:14<00:32,  2.35s/it]                                                        Episode 7	 reward: -4.51	 makespan: 446.10	 Mean_loss: 0.43533099,  training time: 2.29
progress:  30%|[34m       [0m| 6/20 [00:17<00:32,  2.35s/it]progress:  35%|[34m      [0m| 7/20 [00:17<00:30,  2.34s/it]                                                        Episode 8	 reward: -4.77	 makespan: 472.20	 Mean_loss: 0.37445843,  training time: 2.29
progress:  35%|[34m      [0m| 7/20 [00:19<00:30,  2.34s/it]progress:  40%|[34m      [0m| 8/20 [00:19<00:27,  2.32s/it]                                                        Episode 9	 reward: -4.53	 makespan: 448.75	 Mean_loss: 0.48252004,  training time: 2.21
progress:  40%|[34m      [0m| 8/20 [00:21<00:27,  2.32s/it]progress:  45%|[34m     [0m| 9/20 [00:21<00:25,  2.29s/it]                                                        Episode 10	 reward: -4.73	 makespan: 468.60	 Mean_loss: -0.07866427,  training time: 2.22
progress:  45%|[34m     [0m| 9/20 [00:23<00:25,  2.29s/it]progress:  50%|[34m     [0m| 10/20 [00:23<00:22,  2.27s/it]                                                         Episode 11	 reward: -4.64	 makespan: 459.25	 Mean_loss: 0.17114788,  training time: 2.29
progress:  50%|[34m     [0m| 10/20 [00:26<00:22,  2.27s/it]progress:  55%|[34m    [0m| 11/20 [00:26<00:20,  2.28s/it]                                                         Episode 12	 reward: -4.62	 makespan: 457.20	 Mean_loss: 0.29616439,  training time: 2.27
progress:  55%|[34m    [0m| 11/20 [00:28<00:20,  2.28s/it]progress:  60%|[34m    [0m| 12/20 [00:28<00:18,  2.28s/it]                                                         Episode 13	 reward: -4.61	 makespan: 456.55	 Mean_loss: -0.60542953,  training time: 2.41
progress:  60%|[34m    [0m| 12/20 [00:30<00:18,  2.28s/it]progress:  65%|[34m   [0m| 13/20 [00:30<00:16,  2.32s/it]                                                         Episode 14	 reward: -4.56	 makespan: 451.10	 Mean_loss: 0.21531339,  training time: 2.20
progress:  65%|[34m   [0m| 13/20 [00:33<00:16,  2.32s/it]progress:  70%|[34m   [0m| 14/20 [00:33<00:13,  2.29s/it]                                                         Episode 15	 reward: -4.66	 makespan: 461.50	 Mean_loss: 0.23971742,  training time: 2.20
progress:  70%|[34m   [0m| 14/20 [00:35<00:13,  2.29s/it]progress:  75%|[34m  [0m| 15/20 [00:35<00:11,  2.26s/it]                                                         Episode 16	 reward: -4.55	 makespan: 450.35	 Mean_loss: -0.07674742,  training time: 2.30
progress:  75%|[34m  [0m| 15/20 [00:37<00:11,  2.26s/it]progress:  80%|[34m  [0m| 16/20 [00:37<00:09,  2.28s/it]                                                         Episode 17	 reward: -4.64	 makespan: 459.05	 Mean_loss: 0.24551889,  training time: 2.28
progress:  80%|[34m  [0m| 16/20 [00:39<00:09,  2.28s/it]progress:  85%|[34m [0m| 17/20 [00:39<00:06,  2.28s/it]                                                         Episode 18	 reward: -4.53	 makespan: 448.65	 Mean_loss: 0.47111964,  training time: 2.21
progress:  85%|[34m [0m| 17/20 [00:42<00:06,  2.28s/it]progress:  90%|[34m [0m| 18/20 [00:42<00:04,  2.26s/it]                                                         Episode 19	 reward: -4.53	 makespan: 448.75	 Mean_loss: -0.53839654,  training time: 2.33
progress:  90%|[34m [0m| 18/20 [00:44<00:04,  2.26s/it]progress:  95%|[34m[0m| 19/20 [00:44<00:02,  2.28s/it]                                                         Episode 20	 reward: -4.58	 makespan: 453.50	 Mean_loss: 0.03264184,  training time: 2.28
progress:  95%|[34m[0m| 19/20 [00:46<00:02,  2.28s/it]progress: 100%|[34m[0m| 20/20 [00:46<00:00,  2.28s/it]progress: 100%|[34m[0m| 20/20 [00:46<00:00,  2.34s/it]
+ --num_envs 4 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp17/DAN/finetuning/15x5+mix+SD2/15x10_10 --model_suffix free --finetuning_model 15x5+mix+SD2 --max_updates 20 --n_j 15 --n_m 10 --op_per_job 10
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x10+mix
save model name:  15x10+mix+free
./trained_network/SD2/15x5+mix+SD2.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/15x5+mix+SD2.pth
vali data :./data/data_train_vali/SD2/15x10+mix


progress:   0%|[34m          [0m| 0/20 [00:00<?, ?it/s]                                                Episode 1	 reward: -6.34	 makespan: 627.65	 Mean_loss: 0.22528775,  training time: 4.29
progress:   0%|[34m          [0m| 0/20 [00:04<?, ?it/s]progress:   5%|[34m         [0m| 1/20 [00:04<01:21,  4.30s/it]                                                        Episode 2	 reward: -6.36	 makespan: 629.70	 Mean_loss: 0.14680190,  training time: 3.24
progress:   5%|[34m         [0m| 1/20 [00:07<01:21,  4.30s/it]progress:  10%|[34m         [0m| 2/20 [00:07<01:06,  3.68s/it]                                                        Episode 3	 reward: -6.17	 makespan: 611.00	 Mean_loss: 0.11558183,  training time: 3.18
progress:  10%|[34m         [0m| 2/20 [00:10<01:06,  3.68s/it]progress:  15%|[34m        [0m| 3/20 [00:10<00:58,  3.46s/it]                                                        Episode 4	 reward: -6.18	 makespan: 611.95	 Mean_loss: 0.07946668,  training time: 3.18
progress:  15%|[34m        [0m| 3/20 [00:13<00:58,  3.46s/it]progress:  20%|[34m        [0m| 4/20 [00:13<00:53,  3.35s/it]                                                        Episode 5	 reward: -6.28	 makespan: 621.60	 Mean_loss: 0.08371194,  training time: 3.19
progress:  20%|[34m        [0m| 4/20 [00:17<00:53,  3.35s/it]progress:  25%|[34m       [0m| 5/20 [00:17<00:49,  3.29s/it]                                                        Episode 6	 reward: -6.25	 makespan: 619.05	 Mean_loss: 0.09328603,  training time: 3.30
progress:  25%|[34m       [0m| 5/20 [00:20<00:49,  3.29s/it]progress:  30%|[34m       [0m| 6/20 [00:20<00:46,  3.30s/it]                                                        Episode 7	 reward: -6.18	 makespan: 611.85	 Mean_loss: 0.07229565,  training time: 3.26
progress:  30%|[34m       [0m| 6/20 [00:23<00:46,  3.30s/it]progress:  35%|[34m      [0m| 7/20 [00:23<00:42,  3.29s/it]                                                        Episode 8	 reward: -6.13	 makespan: 606.40	 Mean_loss: 0.07250699,  training time: 3.17
progress:  35%|[34m      [0m| 7/20 [00:26<00:42,  3.29s/it]progress:  40%|[34m      [0m| 8/20 [00:26<00:39,  3.25s/it]                                                        Episode 9	 reward: -6.20	 makespan: 614.00	 Mean_loss: 0.06835545,  training time: 3.12
progress:  40%|[34m      [0m| 8/20 [00:29<00:39,  3.25s/it]progress:  45%|[34m     [0m| 9/20 [00:29<00:35,  3.21s/it]                                                        Episode 10	 reward: -6.27	 makespan: 620.55	 Mean_loss: 0.07849008,  training time: 3.15
progress:  45%|[34m     [0m| 9/20 [00:33<00:35,  3.21s/it]progress:  50%|[34m     [0m| 10/20 [00:33<00:31,  3.20s/it]                                                         Episode 11	 reward: -6.20	 makespan: 614.00	 Mean_loss: 0.06424205,  training time: 3.17
progress:  50%|[34m     [0m| 10/20 [00:36<00:31,  3.20s/it]progress:  55%|[34m    [0m| 11/20 [00:36<00:28,  3.19s/it]                                                         Episode 12	 reward: -6.07	 makespan: 600.75	 Mean_loss: 0.06623433,  training time: 3.13
progress:  55%|[34m    [0m| 11/20 [00:39<00:28,  3.19s/it]progress:  60%|[34m    [0m| 12/20 [00:39<00:25,  3.17s/it]                                                         Episode 13	 reward: -6.13	 makespan: 607.15	 Mean_loss: 0.07036278,  training time: 3.26
progress:  60%|[34m    [0m| 12/20 [00:42<00:25,  3.17s/it]progress:  65%|[34m   [0m| 13/20 [00:42<00:22,  3.20s/it]                                                         Episode 14	 reward: -6.07	 makespan: 600.50	 Mean_loss: 0.07920900,  training time: 3.11
progress:  65%|[34m   [0m| 13/20 [00:45<00:22,  3.20s/it]progress:  70%|[34m   [0m| 14/20 [00:45<00:19,  3.17s/it]                                                         Episode 15	 reward: -6.18	 makespan: 611.85	 Mean_loss: 0.06725082,  training time: 3.17
progress:  70%|[34m   [0m| 14/20 [00:49<00:19,  3.17s/it]progress:  75%|[34m  [0m| 15/20 [00:49<00:15,  3.18s/it]                                                         Episode 16	 reward: -6.17	 makespan: 611.00	 Mean_loss: 0.05847504,  training time: 3.17
progress:  75%|[34m  [0m| 15/20 [00:52<00:15,  3.18s/it]progress:  80%|[34m  [0m| 16/20 [00:52<00:12,  3.18s/it]                                                         Episode 17	 reward: -6.19	 makespan: 612.45	 Mean_loss: 0.06589016,  training time: 3.15
progress:  80%|[34m  [0m| 16/20 [00:55<00:12,  3.18s/it]progress:  85%|[34m [0m| 17/20 [00:55<00:09,  3.17s/it]                                                         Episode 18	 reward: -6.06	 makespan: 599.80	 Mean_loss: 0.07464954,  training time: 3.20
progress:  85%|[34m [0m| 17/20 [00:58<00:09,  3.17s/it]progress:  90%|[34m [0m| 18/20 [00:58<00:06,  3.18s/it]                                                         Episode 19	 reward: -6.06	 makespan: 599.45	 Mean_loss: 0.04892534,  training time: 3.19
progress:  90%|[34m [0m| 18/20 [01:01<00:06,  3.18s/it]progress:  95%|[34m[0m| 19/20 [01:01<00:03,  3.18s/it]                                                         Episode 20	 reward: -6.22	 makespan: 616.10	 Mean_loss: 0.05577341,  training time: 3.20
progress:  95%|[34m[0m| 19/20 [01:04<00:03,  3.18s/it]progress: 100%|[34m[0m| 20/20 [01:04<00:00,  3.19s/it]progress: 100%|[34m[0m| 20/20 [01:04<00:00,  3.25s/it]
+ --num_envs 4 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp17/DAN/finetuning/15x5+mix+SD2/15x10_12 --model_suffix free --finetuning_model 15x5+mix+SD2 --max_updates 20 --n_j 15 --n_m 10 --op_per_job 12
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x10+mix
save model name:  15x10+mix+free
./trained_network/SD2/15x5+mix+SD2.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/15x5+mix+SD2.pth
vali data :./data/data_train_vali/SD2/15x10+mix


progress:   0%|[34m          [0m| 0/20 [00:00<?, ?it/s]                                                Episode 1	 reward: -7.49	 makespan: 741.75	 Mean_loss: 0.24089487,  training time: 5.32
progress:   0%|[34m          [0m| 0/20 [00:05<?, ?it/s]progress:   5%|[34m         [0m| 1/20 [00:05<01:41,  5.33s/it]                                                        Episode 2	 reward: -7.50	 makespan: 742.30	 Mean_loss: 0.15998457,  training time: 4.19
progress:   5%|[34m         [0m| 1/20 [00:09<01:41,  5.33s/it]progress:  10%|[34m         [0m| 2/20 [00:09<01:23,  4.66s/it]                                                        Episode 3	 reward: -7.46	 makespan: 738.85	 Mean_loss: 0.13517453,  training time: 4.08
progress:  10%|[34m         [0m| 2/20 [00:13<01:23,  4.66s/it]progress:  15%|[34m        [0m| 3/20 [00:13<01:14,  4.40s/it]                                                        Episode 4	 reward: -7.36	 makespan: 728.15	 Mean_loss: 0.10554667,  training time: 4.09
progress:  15%|[34m        [0m| 3/20 [00:17<01:14,  4.40s/it]progress:  20%|[34m        [0m| 4/20 [00:17<01:08,  4.28s/it]                                                        Episode 5	 reward: -7.49	 makespan: 741.35	 Mean_loss: 0.11271282,  training time: 4.20
progress:  20%|[34m        [0m| 4/20 [00:21<01:08,  4.28s/it]progress:  25%|[34m       [0m| 5/20 [00:21<01:03,  4.26s/it]                                                        Episode 6	 reward: -7.28	 makespan: 721.05	 Mean_loss: 0.10213274,  training time: 4.09
progress:  25%|[34m       [0m| 5/20 [00:26<01:03,  4.26s/it]progress:  30%|[34m       [0m| 6/20 [00:26<00:58,  4.20s/it]                                                        Episode 7	 reward: -7.55	 makespan: 747.75	 Mean_loss: 0.10870543,  training time: 4.02
progress:  30%|[34m       [0m| 6/20 [00:30<00:58,  4.20s/it]progress:  35%|[34m      [0m| 7/20 [00:30<00:53,  4.14s/it]                                                        Episode 8	 reward: -7.54	 makespan: 746.00	 Mean_loss: 0.09350093,  training time: 4.01
progress:  35%|[34m      [0m| 7/20 [00:34<00:53,  4.14s/it]progress:  40%|[34m      [0m| 8/20 [00:34<00:49,  4.10s/it]                                                        Episode 9	 reward: -7.51	 makespan: 743.40	 Mean_loss: 0.07926310,  training time: 4.02
progress:  40%|[34m      [0m| 8/20 [00:38<00:49,  4.10s/it]progress:  45%|[34m     [0m| 9/20 [00:38<00:44,  4.08s/it]                                                        Episode 10	 reward: -7.58	 makespan: 750.80	 Mean_loss: 0.10429540,  training time: 4.03
progress:  45%|[34m     [0m| 9/20 [00:42<00:44,  4.08s/it]progress:  50%|[34m     [0m| 10/20 [00:42<00:40,  4.07s/it]                                                         Episode 11	 reward: -7.66	 makespan: 758.65	 Mean_loss: 0.11667893,  training time: 4.01
progress:  50%|[34m     [0m| 10/20 [00:46<00:40,  4.07s/it]progress:  55%|[34m    [0m| 11/20 [00:46<00:36,  4.05s/it]                                                         Episode 12	 reward: -7.41	 makespan: 733.40	 Mean_loss: 0.07102289,  training time: 4.00
progress:  55%|[34m    [0m| 11/20 [00:50<00:36,  4.05s/it]progress:  60%|[34m    [0m| 12/20 [00:50<00:32,  4.04s/it]                                                         Episode 13	 reward: -7.61	 makespan: 753.15	 Mean_loss: 0.12469900,  training time: 3.99
progress:  60%|[34m    [0m| 12/20 [00:54<00:32,  4.04s/it]progress:  65%|[34m   [0m| 13/20 [00:54<00:28,  4.03s/it]                                                         Episode 14	 reward: -7.35	 makespan: 727.55	 Mean_loss: 0.09693924,  training time: 4.11
progress:  65%|[34m   [0m| 13/20 [00:58<00:28,  4.03s/it]progress:  70%|[34m   [0m| 14/20 [00:58<00:24,  4.05s/it]                                                         Episode 15	 reward: -7.49	 makespan: 741.65	 Mean_loss: 0.10152181,  training time: 4.02
progress:  70%|[34m   [0m| 14/20 [01:02<00:24,  4.05s/it]progress:  75%|[34m  [0m| 15/20 [01:02<00:20,  4.04s/it]                                                         Episode 16	 reward: -7.41	 makespan: 733.85	 Mean_loss: 0.09763671,  training time: 3.99
progress:  75%|[34m  [0m| 15/20 [01:06<00:20,  4.04s/it]progress:  80%|[34m  [0m| 16/20 [01:06<00:16,  4.03s/it]                                                         Episode 17	 reward: -7.45	 makespan: 737.10	 Mean_loss: 0.07209479,  training time: 4.01
progress:  80%|[34m  [0m| 16/20 [01:10<00:16,  4.03s/it]progress:  85%|[34m [0m| 17/20 [01:10<00:12,  4.03s/it]                                                         Episode 18	 reward: -7.40	 makespan: 732.90	 Mean_loss: 0.07287940,  training time: 4.00
progress:  85%|[34m [0m| 17/20 [01:14<00:12,  4.03s/it]progress:  90%|[34m [0m| 18/20 [01:14<00:08,  4.02s/it]                                                         Episode 19	 reward: -7.45	 makespan: 737.90	 Mean_loss: 0.08370718,  training time: 4.00
progress:  90%|[34m [0m| 18/20 [01:18<00:08,  4.02s/it]progress:  95%|[34m[0m| 19/20 [01:18<00:04,  4.02s/it]                                                         Episode 20	 reward: -7.49	 makespan: 741.30	 Mean_loss: 0.08538187,  training time: 4.05
progress:  95%|[34m[0m| 19/20 [01:22<00:04,  4.02s/it]progress: 100%|[34m[0m| 20/20 [01:22<00:00,  4.03s/it]progress: 100%|[34m[0m| 20/20 [01:22<00:00,  4.12s/it]
+ --num_envs 4 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for n_m in '$n_m_options'
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp17/DAN/finetuning/15x5+mix+SD2/15x7_4 --model_suffix free --finetuning_model 15x5+mix+SD2 --max_updates 20 --n_j 15 --n_m 7 --op_per_job 4
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x7+mix
save model name:  15x7+mix+free
./trained_network/SD2/15x5+mix+SD2.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/15x5+mix+SD2.pth
vali data :./data/data_train_vali/SD2/15x7+mix


progress:   0%|[34m          [0m| 0/20 [00:00<?, ?it/s]                                                Episode 1	 reward: -3.50	 makespan: 346.30	 Mean_loss: 0.27404058,  training time: 2.39
progress:   0%|[34m          [0m| 0/20 [00:02<?, ?it/s]progress:   5%|[34m         [0m| 1/20 [00:02<00:45,  2.39s/it]                                                        Episode 2	 reward: -3.49	 makespan: 345.65	 Mean_loss: 0.12573954,  training time: 1.32
progress:   5%|[34m         [0m| 1/20 [00:03<00:45,  2.39s/it]progress:  10%|[34m         [0m| 2/20 [00:03<00:31,  1.77s/it]                                                        Episode 3	 reward: -3.50	 makespan: 346.45	 Mean_loss: 0.10733593,  training time: 1.24
progress:  10%|[34m         [0m| 2/20 [00:04<00:31,  1.77s/it]progress:  15%|[34m        [0m| 3/20 [00:04<00:25,  1.53s/it]                                                        Episode 4	 reward: -3.54	 makespan: 350.15	 Mean_loss: 0.10273340,  training time: 1.21
progress:  15%|[34m        [0m| 3/20 [00:06<00:25,  1.53s/it]progress:  20%|[34m        [0m| 4/20 [00:06<00:22,  1.40s/it]                                                        Episode 5	 reward: -3.46	 makespan: 342.10	 Mean_loss: 0.08075365,  training time: 1.22
progress:  20%|[34m        [0m| 4/20 [00:07<00:22,  1.40s/it]progress:  25%|[34m       [0m| 5/20 [00:07<00:20,  1.34s/it]                                                        Episode 6	 reward: -3.52	 makespan: 348.30	 Mean_loss: 0.09662710,  training time: 1.29
progress:  25%|[34m       [0m| 5/20 [00:08<00:20,  1.34s/it]progress:  30%|[34m       [0m| 6/20 [00:08<00:18,  1.33s/it]                                                        Episode 7	 reward: -3.48	 makespan: 344.15	 Mean_loss: 0.08062022,  training time: 1.25
progress:  30%|[34m       [0m| 6/20 [00:09<00:18,  1.33s/it]progress:  35%|[34m      [0m| 7/20 [00:09<00:16,  1.30s/it]                                                        Episode 8	 reward: -3.47	 makespan: 343.90	 Mean_loss: 0.07533220,  training time: 1.20
progress:  35%|[34m      [0m| 7/20 [00:11<00:16,  1.30s/it]progress:  40%|[34m      [0m| 8/20 [00:11<00:15,  1.27s/it]                                                        Episode 9	 reward: -3.61	 makespan: 357.10	 Mean_loss: 0.10639356,  training time: 1.21
progress:  40%|[34m      [0m| 8/20 [00:12<00:15,  1.27s/it]progress:  45%|[34m     [0m| 9/20 [00:12<00:13,  1.25s/it]                                                        Episode 10	 reward: -3.42	 makespan: 338.30	 Mean_loss: 0.09750446,  training time: 1.24
progress:  45%|[34m     [0m| 9/20 [00:13<00:13,  1.25s/it]progress:  50%|[34m     [0m| 10/20 [00:13<00:12,  1.25s/it]                                                         Episode 11	 reward: -3.55	 makespan: 351.15	 Mean_loss: 0.08692629,  training time: 1.28
progress:  50%|[34m     [0m| 10/20 [00:14<00:12,  1.25s/it]progress:  55%|[34m    [0m| 11/20 [00:14<00:11,  1.26s/it]                                                         Episode 12	 reward: -3.61	 makespan: 357.35	 Mean_loss: 0.09260733,  training time: 1.34
progress:  55%|[34m    [0m| 11/20 [00:16<00:11,  1.26s/it]progress:  60%|[34m    [0m| 12/20 [00:16<00:10,  1.29s/it]                                                         Episode 13	 reward: -3.51	 makespan: 347.95	 Mean_loss: 0.07750888,  training time: 1.22
progress:  60%|[34m    [0m| 12/20 [00:17<00:10,  1.29s/it]progress:  65%|[34m   [0m| 13/20 [00:17<00:08,  1.27s/it]                                                         Episode 14	 reward: -3.46	 makespan: 342.30	 Mean_loss: 0.10245838,  training time: 1.25
progress:  65%|[34m   [0m| 13/20 [00:18<00:08,  1.27s/it]progress:  70%|[34m   [0m| 14/20 [00:18<00:07,  1.27s/it]                                                         Episode 15	 reward: -3.48	 makespan: 344.60	 Mean_loss: 0.07198129,  training time: 1.28
progress:  70%|[34m   [0m| 14/20 [00:20<00:07,  1.27s/it]progress:  75%|[34m  [0m| 15/20 [00:20<00:06,  1.28s/it]                                                         Episode 16	 reward: -3.51	 makespan: 347.55	 Mean_loss: 0.07456086,  training time: 1.29
progress:  75%|[34m  [0m| 15/20 [00:21<00:06,  1.28s/it]progress:  80%|[34m  [0m| 16/20 [00:21<00:05,  1.28s/it]                                                         Episode 17	 reward: -3.47	 makespan: 343.05	 Mean_loss: 0.07492308,  training time: 1.23
progress:  80%|[34m  [0m| 16/20 [00:22<00:05,  1.28s/it]progress:  85%|[34m [0m| 17/20 [00:22<00:03,  1.27s/it]                                                         Episode 18	 reward: -3.57	 makespan: 353.20	 Mean_loss: 0.10277621,  training time: 1.30
progress:  85%|[34m [0m| 17/20 [00:23<00:03,  1.27s/it]progress:  90%|[34m [0m| 18/20 [00:23<00:02,  1.28s/it]                                                         Episode 19	 reward: -3.56	 makespan: 351.95	 Mean_loss: 0.08059718,  training time: 1.37
progress:  90%|[34m [0m| 18/20 [00:25<00:02,  1.28s/it]progress:  95%|[34m[0m| 19/20 [00:25<00:01,  1.31s/it]                                                         Episode 20	 reward: -3.68	 makespan: 364.20	 Mean_loss: 0.09391139,  training time: 1.25
progress:  95%|[34m[0m| 19/20 [00:26<00:01,  1.31s/it]progress: 100%|[34m[0m| 20/20 [00:26<00:00,  1.29s/it]progress: 100%|[34m[0m| 20/20 [00:26<00:00,  1.33s/it]
+ --num_envs 4 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp17/DAN/finetuning/15x5+mix+SD2/15x7_7 --model_suffix free --finetuning_model 15x5+mix+SD2 --max_updates 20 --n_j 15 --n_m 7 --op_per_job 7
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x7+mix
save model name:  15x7+mix+free
./trained_network/SD2/15x5+mix+SD2.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/15x5+mix+SD2.pth
vali data :./data/data_train_vali/SD2/15x7+mix


progress:   0%|[34m          [0m| 0/20 [00:00<?, ?it/s]                                                Episode 1	 reward: -5.45	 makespan: 539.55	 Mean_loss: 0.75743854,  training time: 3.33
progress:   0%|[34m          [0m| 0/20 [00:03<?, ?it/s]progress:   5%|[34m         [0m| 1/20 [00:03<01:03,  3.34s/it]                                                        Episode 2	 reward: -5.39	 makespan: 533.20	 Mean_loss: 0.19580457,  training time: 2.37
progress:   5%|[34m         [0m| 1/20 [00:05<01:03,  3.34s/it]progress:  10%|[34m         [0m| 2/20 [00:05<00:49,  2.77s/it]                                                        Episode 3	 reward: -5.43	 makespan: 537.80	 Mean_loss: -0.19280776,  training time: 2.24
progress:  10%|[34m         [0m| 2/20 [00:07<00:49,  2.77s/it]progress:  15%|[34m        [0m| 3/20 [00:07<00:43,  2.53s/it]                                                        Episode 4	 reward: -5.47	 makespan: 541.90	 Mean_loss: 0.49895003,  training time: 2.23
progress:  15%|[34m        [0m| 3/20 [00:10<00:43,  2.53s/it]progress:  20%|[34m        [0m| 4/20 [00:10<00:38,  2.42s/it]                                                        Episode 5	 reward: -5.58	 makespan: 552.75	 Mean_loss: 0.84063160,  training time: 2.23
progress:  20%|[34m        [0m| 4/20 [00:12<00:38,  2.42s/it]progress:  25%|[34m       [0m| 5/20 [00:12<00:35,  2.35s/it]                                                        Episode 6	 reward: -5.47	 makespan: 541.95	 Mean_loss: -0.40177596,  training time: 2.23
progress:  25%|[34m       [0m| 5/20 [00:14<00:35,  2.35s/it]progress:  30%|[34m       [0m| 6/20 [00:14<00:32,  2.31s/it]                                                        Episode 7	 reward: -5.59	 makespan: 553.80	 Mean_loss: -0.72145623,  training time: 2.21
progress:  30%|[34m       [0m| 6/20 [00:16<00:32,  2.31s/it]progress:  35%|[34m      [0m| 7/20 [00:16<00:29,  2.28s/it]                                                        Episode 8	 reward: -5.58	 makespan: 552.10	 Mean_loss: 0.13764179,  training time: 2.21
progress:  35%|[34m      [0m| 7/20 [00:19<00:29,  2.28s/it]progress:  40%|[34m      [0m| 8/20 [00:19<00:27,  2.26s/it]                                                        Episode 9	 reward: -5.45	 makespan: 539.75	 Mean_loss: 0.50457829,  training time: 2.21
progress:  40%|[34m      [0m| 8/20 [00:21<00:27,  2.26s/it]progress:  45%|[34m     [0m| 9/20 [00:21<00:24,  2.25s/it]                                                        Episode 10	 reward: -5.38	 makespan: 533.00	 Mean_loss: -0.04088145,  training time: 2.28
progress:  45%|[34m     [0m| 9/20 [00:23<00:24,  2.25s/it]progress:  50%|[34m     [0m| 10/20 [00:23<00:22,  2.26s/it]                                                         Episode 11	 reward: -5.53	 makespan: 547.60	 Mean_loss: 0.38493046,  training time: 2.22
progress:  50%|[34m     [0m| 10/20 [00:25<00:22,  2.26s/it]progress:  55%|[34m    [0m| 11/20 [00:25<00:20,  2.25s/it]                                                         Episode 12	 reward: -5.44	 makespan: 538.10	 Mean_loss: -0.26285753,  training time: 2.16
progress:  55%|[34m    [0m| 11/20 [00:27<00:20,  2.25s/it]progress:  60%|[34m    [0m| 12/20 [00:27<00:17,  2.22s/it]                                                         Episode 13	 reward: -5.61	 makespan: 555.15	 Mean_loss: -0.25352725,  training time: 2.29
progress:  60%|[34m    [0m| 12/20 [00:30<00:17,  2.22s/it]progress:  65%|[34m   [0m| 13/20 [00:30<00:15,  2.24s/it]                                                         Episode 14	 reward: -5.60	 makespan: 553.95	 Mean_loss: 0.19259121,  training time: 2.21
progress:  65%|[34m   [0m| 13/20 [00:32<00:15,  2.24s/it]progress:  70%|[34m   [0m| 14/20 [00:32<00:13,  2.24s/it]                                                         Episode 15	 reward: -5.65	 makespan: 559.20	 Mean_loss: 0.10087293,  training time: 2.23
progress:  70%|[34m   [0m| 14/20 [00:34<00:13,  2.24s/it]progress:  75%|[34m  [0m| 15/20 [00:34<00:11,  2.23s/it]                                                         Episode 16	 reward: -5.57	 makespan: 551.50	 Mean_loss: 0.68808579,  training time: 2.17
progress:  75%|[34m  [0m| 15/20 [00:36<00:11,  2.23s/it]progress:  80%|[34m  [0m| 16/20 [00:36<00:08,  2.22s/it]                                                         Episode 17	 reward: -5.47	 makespan: 541.35	 Mean_loss: -0.21135077,  training time: 2.16
progress:  80%|[34m  [0m| 16/20 [00:39<00:08,  2.22s/it]progress:  85%|[34m [0m| 17/20 [00:39<00:06,  2.20s/it]                                                         Episode 18	 reward: -5.45	 makespan: 540.00	 Mean_loss: 0.09775157,  training time: 2.19
progress:  85%|[34m [0m| 17/20 [00:41<00:06,  2.20s/it]progress:  90%|[34m [0m| 18/20 [00:41<00:04,  2.20s/it]                                                         Episode 19	 reward: -5.62	 makespan: 556.55	 Mean_loss: -0.01184816,  training time: 2.20
progress:  90%|[34m [0m| 18/20 [00:43<00:04,  2.20s/it]progress:  95%|[34m[0m| 19/20 [00:43<00:02,  2.20s/it]                                                         Episode 20	 reward: -5.52	 makespan: 546.25	 Mean_loss: -0.04371083,  training time: 2.20
progress:  95%|[34m[0m| 19/20 [00:45<00:02,  2.20s/it]progress: 100%|[34m[0m| 20/20 [00:45<00:00,  2.20s/it]progress: 100%|[34m[0m| 20/20 [00:45<00:00,  2.28s/it]
+ --num_envs 4 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp17/DAN/finetuning/15x5+mix+SD2/15x7_10 --model_suffix free --finetuning_model 15x5+mix+SD2 --max_updates 20 --n_j 15 --n_m 7 --op_per_job 10
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x7+mix
save model name:  15x7+mix+free
./trained_network/SD2/15x5+mix+SD2.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/15x5+mix+SD2.pth
vali data :./data/data_train_vali/SD2/15x7+mix


progress:   0%|[34m          [0m| 0/20 [00:00<?, ?it/s]                                                Episode 1	 reward: -7.70	 makespan: 762.20	 Mean_loss: 0.17692918,  training time: 4.17
progress:   0%|[34m          [0m| 0/20 [00:04<?, ?it/s]progress:   5%|[34m         [0m| 1/20 [00:04<01:19,  4.18s/it]                                                        Episode 2	 reward: -7.42	 makespan: 734.30	 Mean_loss: 0.11908923,  training time: 3.02
progress:   5%|[34m         [0m| 1/20 [00:07<01:19,  4.18s/it]progress:  10%|[34m         [0m| 2/20 [00:07<01:02,  3.50s/it]                                                        Episode 3	 reward: -7.57	 makespan: 748.95	 Mean_loss: 0.11304273,  training time: 3.05
progress:  10%|[34m         [0m| 2/20 [00:10<01:02,  3.50s/it]progress:  15%|[34m        [0m| 3/20 [00:10<00:56,  3.30s/it]                                                        Episode 4	 reward: -7.63	 makespan: 754.90	 Mean_loss: 0.10284154,  training time: 3.07
progress:  15%|[34m        [0m| 3/20 [00:13<00:56,  3.30s/it]progress:  20%|[34m        [0m| 4/20 [00:13<00:51,  3.21s/it]                                                        Episode 5	 reward: -7.45	 makespan: 737.55	 Mean_loss: 0.07098845,  training time: 3.08
progress:  20%|[34m        [0m| 4/20 [00:16<00:51,  3.21s/it]progress:  25%|[34m       [0m| 5/20 [00:16<00:47,  3.16s/it]                                                        Episode 6	 reward: -7.52	 makespan: 744.15	 Mean_loss: 0.07621712,  training time: 3.18
progress:  25%|[34m       [0m| 5/20 [00:19<00:47,  3.16s/it]progress:  30%|[34m       [0m| 6/20 [00:19<00:44,  3.17s/it]                                                        Episode 7	 reward: -7.56	 makespan: 748.60	 Mean_loss: 0.09246348,  training time: 3.15
progress:  30%|[34m       [0m| 6/20 [00:22<00:44,  3.17s/it]progress:  35%|[34m      [0m| 7/20 [00:22<00:41,  3.17s/it]                                                        Episode 8	 reward: -7.57	 makespan: 749.10	 Mean_loss: 0.06313901,  training time: 3.10
progress:  35%|[34m      [0m| 7/20 [00:25<00:41,  3.17s/it]progress:  40%|[34m      [0m| 8/20 [00:25<00:37,  3.15s/it]                                                        Episode 9	 reward: -7.48	 makespan: 740.25	 Mean_loss: 0.06657446,  training time: 3.06
progress:  40%|[34m      [0m| 8/20 [00:28<00:37,  3.15s/it]progress:  45%|[34m     [0m| 9/20 [00:28<00:34,  3.12s/it]                                                        Episode 10	 reward: -7.53	 makespan: 745.00	 Mean_loss: 0.07424099,  training time: 3.06
progress:  45%|[34m     [0m| 9/20 [00:31<00:34,  3.12s/it]progress:  50%|[34m     [0m| 10/20 [00:31<00:31,  3.10s/it]                                                         Episode 11	 reward: -7.51	 makespan: 743.05	 Mean_loss: 0.07273392,  training time: 3.11
progress:  50%|[34m     [0m| 10/20 [00:35<00:31,  3.10s/it]progress:  55%|[34m    [0m| 11/20 [00:35<00:27,  3.11s/it]                                                         Episode 12	 reward: -7.49	 makespan: 741.95	 Mean_loss: 0.06335042,  training time: 3.03
progress:  55%|[34m    [0m| 11/20 [00:38<00:27,  3.11s/it]progress:  60%|[34m    [0m| 12/20 [00:38<00:24,  3.09s/it]                                                         Episode 13	 reward: -7.58	 makespan: 750.40	 Mean_loss: 0.07196100,  training time: 3.16
progress:  60%|[34m    [0m| 12/20 [00:41<00:24,  3.09s/it]progress:  65%|[34m   [0m| 13/20 [00:41<00:21,  3.11s/it]                                                         Episode 14	 reward: -7.52	 makespan: 744.65	 Mean_loss: 0.07440086,  training time: 3.05
progress:  65%|[34m   [0m| 13/20 [00:44<00:21,  3.11s/it]progress:  70%|[34m   [0m| 14/20 [00:44<00:18,  3.10s/it]                                                         Episode 15	 reward: -7.58	 makespan: 750.55	 Mean_loss: 0.06524947,  training time: 3.07
progress:  70%|[34m   [0m| 14/20 [00:47<00:18,  3.10s/it]progress:  75%|[34m  [0m| 15/20 [00:47<00:15,  3.09s/it]                                                         Episode 16	 reward: -7.57	 makespan: 749.75	 Mean_loss: 0.05876563,  training time: 3.07
progress:  75%|[34m  [0m| 15/20 [00:50<00:15,  3.09s/it]progress:  80%|[34m  [0m| 16/20 [00:50<00:12,  3.09s/it]                                                         Episode 17	 reward: -7.50	 makespan: 742.85	 Mean_loss: 0.05810637,  training time: 3.07
progress:  80%|[34m  [0m| 16/20 [00:53<00:12,  3.09s/it]progress:  85%|[34m [0m| 17/20 [00:53<00:09,  3.08s/it]                                                         Episode 18	 reward: -7.51	 makespan: 743.65	 Mean_loss: 0.06600601,  training time: 3.06
progress:  85%|[34m [0m| 17/20 [00:56<00:09,  3.08s/it]progress:  90%|[34m [0m| 18/20 [00:56<00:06,  3.08s/it]                                                         Episode 19	 reward: -7.50	 makespan: 742.35	 Mean_loss: 0.06499210,  training time: 3.06
progress:  90%|[34m [0m| 18/20 [00:59<00:06,  3.08s/it]progress:  95%|[34m[0m| 19/20 [00:59<00:03,  3.07s/it]                                                         Episode 20	 reward: -7.51	 makespan: 743.55	 Mean_loss: 0.06057986,  training time: 3.10
progress:  95%|[34m[0m| 19/20 [01:02<00:03,  3.07s/it]progress: 100%|[34m[0m| 20/20 [01:02<00:00,  3.08s/it]progress: 100%|[34m[0m| 20/20 [01:02<00:00,  3.14s/it]
+ --num_envs 4 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp17/DAN/finetuning/15x5+mix+SD2/15x7_12 --model_suffix free --finetuning_model 15x5+mix+SD2 --max_updates 20 --n_j 15 --n_m 7 --op_per_job 12
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x7+mix
save model name:  15x7+mix+free
./trained_network/SD2/15x5+mix+SD2.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/15x5+mix+SD2.pth
vali data :./data/data_train_vali/SD2/15x7+mix


progress:   0%|[34m          [0m| 0/20 [00:00<?, ?it/s]                                                Episode 1	 reward: -9.11	 makespan: 902.15	 Mean_loss: 0.21705744,  training time: 4.97
progress:   0%|[34m          [0m| 0/20 [00:04<?, ?it/s]progress:   5%|[34m         [0m| 1/20 [00:04<01:34,  4.98s/it]                                                        Episode 2	 reward: -9.02	 makespan: 893.10	 Mean_loss: 0.13164273,  training time: 3.85
progress:   5%|[34m         [0m| 1/20 [00:08<01:34,  4.98s/it]progress:  10%|[34m         [0m| 2/20 [00:08<01:17,  4.32s/it]                                                        Episode 3	 reward: -8.97	 makespan: 887.90	 Mean_loss: 0.12358159,  training time: 3.87
progress:  10%|[34m         [0m| 2/20 [00:12<01:17,  4.32s/it]progress:  15%|[34m        [0m| 3/20 [00:12<01:09,  4.12s/it]                                                        Episode 4	 reward: -9.28	 makespan: 919.05	 Mean_loss: 0.15237419,  training time: 3.87
progress:  15%|[34m        [0m| 3/20 [00:16<01:09,  4.12s/it]progress:  20%|[34m        [0m| 4/20 [00:16<01:04,  4.02s/it]                                                        Episode 5	 reward: -9.05	 makespan: 895.95	 Mean_loss: 0.15032193,  training time: 4.00
progress:  20%|[34m        [0m| 4/20 [00:20<01:04,  4.02s/it]progress:  25%|[34m       [0m| 5/20 [00:20<01:00,  4.02s/it]                                                        Episode 6	 reward: -9.34	 makespan: 925.10	 Mean_loss: 0.14320810,  training time: 3.90
progress:  25%|[34m       [0m| 5/20 [00:24<01:00,  4.02s/it]progress:  30%|[34m       [0m| 6/20 [00:24<00:55,  3.98s/it]                                                        Episode 7	 reward: -9.17	 makespan: 908.05	 Mean_loss: 0.11156749,  training time: 4.00
progress:  30%|[34m       [0m| 6/20 [00:28<00:55,  3.98s/it]progress:  35%|[34m      [0m| 7/20 [00:28<00:51,  3.99s/it]                                                        Episode 8	 reward: -9.17	 makespan: 907.45	 Mean_loss: 0.13162407,  training time: 4.08
progress:  35%|[34m      [0m| 7/20 [00:32<00:51,  3.99s/it]progress:  40%|[34m      [0m| 8/20 [00:32<00:48,  4.02s/it]                                                        Episode 9	 reward: -9.19	 makespan: 909.75	 Mean_loss: 0.13309018,  training time: 3.89
progress:  40%|[34m      [0m| 8/20 [00:36<00:48,  4.02s/it]progress:  45%|[34m     [0m| 9/20 [00:36<00:43,  3.98s/it]                                                        Episode 10	 reward: -9.16	 makespan: 906.90	 Mean_loss: 0.09167605,  training time: 3.93
progress:  45%|[34m     [0m| 9/20 [00:40<00:43,  3.98s/it]progress:  50%|[34m     [0m| 10/20 [00:40<00:39,  3.97s/it]                                                         Episode 11	 reward: -9.13	 makespan: 903.45	 Mean_loss: 0.09403741,  training time: 3.87
progress:  50%|[34m     [0m| 10/20 [00:44<00:39,  3.97s/it]progress:  55%|[34m    [0m| 11/20 [00:44<00:35,  3.94s/it]                                                         Episode 12	 reward: -9.29	 makespan: 920.15	 Mean_loss: 0.09300811,  training time: 3.90
progress:  55%|[34m    [0m| 11/20 [00:48<00:35,  3.94s/it]progress:  60%|[34m    [0m| 12/20 [00:48<00:31,  3.93s/it]                                                         Episode 13	 reward: -9.07	 makespan: 898.10	 Mean_loss: 0.10320909,  training time: 3.84
progress:  60%|[34m    [0m| 12/20 [00:52<00:31,  3.93s/it]progress:  65%|[34m   [0m| 13/20 [00:52<00:27,  3.91s/it]                                                         Episode 14	 reward: -9.27	 makespan: 917.80	 Mean_loss: 0.12267249,  training time: 3.85
progress:  65%|[34m   [0m| 13/20 [00:55<00:27,  3.91s/it]progress:  70%|[34m   [0m| 14/20 [00:55<00:23,  3.89s/it]                                                         Episode 15	 reward: -9.05	 makespan: 896.25	 Mean_loss: 0.08467703,  training time: 3.90
progress:  70%|[34m   [0m| 14/20 [00:59<00:23,  3.89s/it]progress:  75%|[34m  [0m| 15/20 [00:59<00:19,  3.90s/it]                                                         Episode 16	 reward: -9.09	 makespan: 899.60	 Mean_loss: 0.10161492,  training time: 3.82
progress:  75%|[34m  [0m| 15/20 [01:03<00:19,  3.90s/it]progress:  80%|[34m  [0m| 16/20 [01:03<00:15,  3.88s/it]                                                         Episode 17	 reward: -9.17	 makespan: 907.65	 Mean_loss: 0.08262336,  training time: 3.82
progress:  80%|[34m  [0m| 16/20 [01:07<00:15,  3.88s/it]progress:  85%|[34m [0m| 17/20 [01:07<00:11,  3.86s/it]                                                         Episode 18	 reward: -9.21	 makespan: 911.40	 Mean_loss: 0.12730817,  training time: 3.84
progress:  85%|[34m [0m| 17/20 [01:11<00:11,  3.86s/it]progress:  90%|[34m [0m| 18/20 [01:11<00:07,  3.86s/it]                                                         Episode 19	 reward: -9.23	 makespan: 913.85	 Mean_loss: 0.09440313,  training time: 4.04
progress:  90%|[34m [0m| 18/20 [01:15<00:07,  3.86s/it]progress:  95%|[34m[0m| 19/20 [01:15<00:03,  3.92s/it]                                                         Episode 20	 reward: -9.05	 makespan: 896.15	 Mean_loss: 0.11855762,  training time: 3.92
progress:  95%|[34m[0m| 19/20 [01:19<00:03,  3.92s/it]progress: 100%|[34m[0m| 20/20 [01:19<00:00,  3.92s/it]progress: 100%|[34m[0m| 20/20 [01:19<00:00,  3.97s/it]
+ --num_envs 4 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for n_m in '$n_m_options'
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp17/DAN/finetuning/15x5+mix+SD2/15x5_4 --model_suffix free --finetuning_model 15x5+mix+SD2 --max_updates 20 --n_j 15 --n_m 5 --op_per_job 4
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x5+mix
save model name:  15x5+mix+free
./trained_network/SD2/15x5+mix+SD2.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/15x5+mix+SD2.pth
vali data :./data/data_train_vali/SD2/15x5+mix


progress:   0%|[34m          [0m| 0/20 [00:00<?, ?it/s]                                                Episode 1	 reward: -4.52	 makespan: 447.30	 Mean_loss: 0.23811997,  training time: 2.36
progress:   0%|[34m          [0m| 0/20 [00:02<?, ?it/s]progress:   5%|[34m         [0m| 1/20 [00:02<00:45,  2.37s/it]                                                        Episode 2	 reward: -4.51	 makespan: 446.05	 Mean_loss: 0.08962662,  training time: 1.25
progress:   5%|[34m         [0m| 1/20 [00:03<00:45,  2.37s/it]progress:  10%|[34m         [0m| 2/20 [00:03<00:30,  1.72s/it]                                                        Episode 3	 reward: -4.49	 makespan: 444.55	 Mean_loss: 0.08192421,  training time: 1.33
progress:  10%|[34m         [0m| 2/20 [00:04<00:30,  1.72s/it]progress:  15%|[34m        [0m| 3/20 [00:04<00:26,  1.54s/it]                                                        Episode 4	 reward: -4.45	 makespan: 440.45	 Mean_loss: 0.11839893,  training time: 1.25
progress:  15%|[34m        [0m| 3/20 [00:06<00:26,  1.54s/it]progress:  20%|[34m        [0m| 4/20 [00:06<00:22,  1.43s/it]                                                        Episode 5	 reward: -4.50	 makespan: 445.80	 Mean_loss: 0.08432587,  training time: 1.22
progress:  20%|[34m        [0m| 4/20 [00:07<00:22,  1.43s/it]progress:  25%|[34m       [0m| 5/20 [00:07<00:20,  1.36s/it]                                                        Episode 6	 reward: -4.55	 makespan: 450.85	 Mean_loss: 0.08116047,  training time: 1.22
progress:  25%|[34m       [0m| 5/20 [00:08<00:20,  1.36s/it]progress:  30%|[34m       [0m| 6/20 [00:08<00:18,  1.31s/it]                                                        Episode 7	 reward: -4.47	 makespan: 442.50	 Mean_loss: 0.09333961,  training time: 1.22
progress:  30%|[34m       [0m| 6/20 [00:09<00:18,  1.31s/it]progress:  35%|[34m      [0m| 7/20 [00:09<00:16,  1.29s/it]                                                        Episode 8	 reward: -4.48	 makespan: 443.55	 Mean_loss: 0.08974613,  training time: 1.22
progress:  35%|[34m      [0m| 7/20 [00:11<00:16,  1.29s/it]progress:  40%|[34m      [0m| 8/20 [00:11<00:15,  1.27s/it]                                                        Episode 9	 reward: -4.46	 makespan: 441.85	 Mean_loss: 0.07865006,  training time: 1.32
progress:  40%|[34m      [0m| 8/20 [00:12<00:15,  1.27s/it]progress:  45%|[34m     [0m| 9/20 [00:12<00:14,  1.29s/it]                                                        Episode 10	 reward: -4.61	 makespan: 455.90	 Mean_loss: 0.07870273,  training time: 1.28
progress:  45%|[34m     [0m| 9/20 [00:13<00:14,  1.29s/it]progress:  50%|[34m     [0m| 10/20 [00:13<00:12,  1.29s/it]                                                         Episode 11	 reward: -4.50	 makespan: 445.45	 Mean_loss: 0.11724523,  training time: 1.21
progress:  50%|[34m     [0m| 10/20 [00:14<00:12,  1.29s/it]progress:  55%|[34m    [0m| 11/20 [00:14<00:11,  1.26s/it]                                                         Episode 12	 reward: -4.51	 makespan: 446.70	 Mean_loss: 0.04469223,  training time: 1.39
progress:  55%|[34m    [0m| 11/20 [00:16<00:11,  1.26s/it]progress:  60%|[34m    [0m| 12/20 [00:16<00:10,  1.30s/it]                                                         Episode 13	 reward: -4.55	 makespan: 450.60	 Mean_loss: 0.00725301,  training time: 1.29
progress:  60%|[34m    [0m| 12/20 [00:17<00:10,  1.30s/it]progress:  65%|[34m   [0m| 13/20 [00:17<00:09,  1.30s/it]                                                         Episode 14	 reward: -4.48	 makespan: 443.70	 Mean_loss: 0.08852032,  training time: 1.27
progress:  65%|[34m   [0m| 13/20 [00:18<00:09,  1.30s/it]progress:  70%|[34m   [0m| 14/20 [00:18<00:07,  1.29s/it]                                                         Episode 15	 reward: -4.61	 makespan: 456.50	 Mean_loss: 0.03069657,  training time: 1.27
progress:  70%|[34m   [0m| 14/20 [00:20<00:07,  1.29s/it]progress:  75%|[34m  [0m| 15/20 [00:20<00:06,  1.29s/it]                                                         Episode 16	 reward: -4.52	 makespan: 447.35	 Mean_loss: 0.04763047,  training time: 1.35
progress:  75%|[34m  [0m| 15/20 [00:21<00:06,  1.29s/it]progress:  80%|[34m  [0m| 16/20 [00:21<00:05,  1.31s/it]                                                         Episode 17	 reward: -4.58	 makespan: 453.45	 Mean_loss: 0.02414634,  training time: 1.24
progress:  80%|[34m  [0m| 16/20 [00:22<00:05,  1.31s/it]progress:  85%|[34m [0m| 17/20 [00:22<00:03,  1.29s/it]                                                         Episode 18	 reward: -4.54	 makespan: 449.05	 Mean_loss: 0.03349137,  training time: 1.22
progress:  85%|[34m [0m| 17/20 [00:24<00:03,  1.29s/it]progress:  90%|[34m [0m| 18/20 [00:24<00:02,  1.27s/it]                                                         Episode 19	 reward: -4.61	 makespan: 456.75	 Mean_loss: 0.07328827,  training time: 1.29
progress:  90%|[34m [0m| 18/20 [00:25<00:02,  1.27s/it]progress:  95%|[34m[0m| 19/20 [00:25<00:01,  1.28s/it]                                                         Episode 20	 reward: -4.48	 makespan: 443.15	 Mean_loss: 0.04194111,  training time: 1.22
progress:  95%|[34m[0m| 19/20 [00:26<00:01,  1.28s/it]progress: 100%|[34m[0m| 20/20 [00:26<00:00,  1.26s/it]progress: 100%|[34m[0m| 20/20 [00:26<00:00,  1.33s/it]
+ --num_envs 4 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp17/DAN/finetuning/15x5+mix+SD2/15x5_7 --model_suffix free --finetuning_model 15x5+mix+SD2 --max_updates 20 --n_j 15 --n_m 5 --op_per_job 7
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x5+mix
save model name:  15x5+mix+free
./trained_network/SD2/15x5+mix+SD2.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/15x5+mix+SD2.pth
vali data :./data/data_train_vali/SD2/15x5+mix


progress:   0%|[34m          [0m| 0/20 [00:00<?, ?it/s]                                                Episode 1	 reward: -7.42	 makespan: 734.90	 Mean_loss: 0.25977376,  training time: 3.26
progress:   0%|[34m          [0m| 0/20 [00:03<?, ?it/s]progress:   5%|[34m         [0m| 1/20 [00:03<01:02,  3.27s/it]                                                        Episode 2	 reward: -7.43	 makespan: 735.55	 Mean_loss: 0.29417434,  training time: 2.18
progress:   5%|[34m         [0m| 1/20 [00:05<01:02,  3.27s/it]progress:  10%|[34m         [0m| 2/20 [00:05<00:47,  2.63s/it]                                                        Episode 3	 reward: -7.38	 makespan: 731.00	 Mean_loss: -0.00374060,  training time: 2.19
progress:  10%|[34m         [0m| 2/20 [00:07<00:47,  2.63s/it]progress:  15%|[34m        [0m| 3/20 [00:07<00:41,  2.43s/it]                                                        Episode 4	 reward: -7.55	 makespan: 747.15	 Mean_loss: -0.28527081,  training time: 2.23
progress:  15%|[34m        [0m| 3/20 [00:09<00:41,  2.43s/it]progress:  20%|[34m        [0m| 4/20 [00:09<00:37,  2.35s/it]                                                        Episode 5	 reward: -7.40	 makespan: 732.30	 Mean_loss: -0.12797329,  training time: 2.16
progress:  20%|[34m        [0m| 4/20 [00:12<00:37,  2.35s/it]progress:  25%|[34m       [0m| 5/20 [00:12<00:34,  2.29s/it]                                                        Episode 6	 reward: -7.35	 makespan: 728.05	 Mean_loss: -0.05731661,  training time: 2.18
progress:  25%|[34m       [0m| 5/20 [00:14<00:34,  2.29s/it]progress:  30%|[34m       [0m| 6/20 [00:14<00:31,  2.25s/it]                                                        Episode 7	 reward: -7.45	 makespan: 737.30	 Mean_loss: -0.32278043,  training time: 2.28
progress:  30%|[34m       [0m| 6/20 [00:16<00:31,  2.25s/it]progress:  35%|[34m      [0m| 7/20 [00:16<00:29,  2.26s/it]                                                        Episode 8	 reward: -7.44	 makespan: 736.65	 Mean_loss: -0.49082601,  training time: 2.19
progress:  35%|[34m      [0m| 7/20 [00:18<00:29,  2.26s/it]progress:  40%|[34m      [0m| 8/20 [00:18<00:26,  2.24s/it]                                                        Episode 9	 reward: -7.48	 makespan: 740.35	 Mean_loss: -0.16535029,  training time: 2.17
progress:  40%|[34m      [0m| 8/20 [00:20<00:26,  2.24s/it]progress:  45%|[34m     [0m| 9/20 [00:20<00:24,  2.22s/it]                                                        Episode 10	 reward: -7.41	 makespan: 733.15	 Mean_loss: -0.45168996,  training time: 2.18
progress:  45%|[34m     [0m| 9/20 [00:23<00:24,  2.22s/it]progress:  50%|[34m     [0m| 10/20 [00:23<00:22,  2.21s/it]                                                         Episode 11	 reward: -7.38	 makespan: 730.15	 Mean_loss: -0.56949079,  training time: 2.13
progress:  50%|[34m     [0m| 10/20 [00:25<00:22,  2.21s/it]progress:  55%|[34m    [0m| 11/20 [00:25<00:19,  2.19s/it]                                                         Episode 12	 reward: -7.48	 makespan: 740.05	 Mean_loss: -0.48689973,  training time: 2.18
progress:  55%|[34m    [0m| 11/20 [00:27<00:19,  2.19s/it]progress:  60%|[34m    [0m| 12/20 [00:27<00:17,  2.19s/it]                                                         Episode 13	 reward: -7.44	 makespan: 736.65	 Mean_loss: 0.20593771,  training time: 2.28
progress:  60%|[34m    [0m| 12/20 [00:29<00:17,  2.19s/it]progress:  65%|[34m   [0m| 13/20 [00:29<00:15,  2.22s/it]                                                         Episode 14	 reward: -7.33	 makespan: 725.95	 Mean_loss: -0.39277551,  training time: 2.17
progress:  65%|[34m   [0m| 13/20 [00:31<00:15,  2.22s/it]progress:  70%|[34m   [0m| 14/20 [00:31<00:13,  2.21s/it]                                                         Episode 15	 reward: -7.44	 makespan: 736.15	 Mean_loss: -0.43132430,  training time: 2.17
progress:  70%|[34m   [0m| 14/20 [00:34<00:13,  2.21s/it]progress:  75%|[34m  [0m| 15/20 [00:34<00:10,  2.20s/it]                                                         Episode 16	 reward: -7.33	 makespan: 726.05	 Mean_loss: -0.17016542,  training time: 2.16
progress:  75%|[34m  [0m| 15/20 [00:36<00:10,  2.20s/it]progress:  80%|[34m  [0m| 16/20 [00:36<00:08,  2.19s/it]                                                         Episode 17	 reward: -7.36	 makespan: 728.55	 Mean_loss: -0.54749727,  training time: 2.17
progress:  80%|[34m  [0m| 16/20 [00:38<00:08,  2.19s/it]progress:  85%|[34m [0m| 17/20 [00:38<00:06,  2.18s/it]                                                         Episode 18	 reward: -7.36	 makespan: 728.65	 Mean_loss: -0.51354891,  training time: 2.17
progress:  85%|[34m [0m| 17/20 [00:40<00:06,  2.18s/it]progress:  90%|[34m [0m| 18/20 [00:40<00:04,  2.18s/it]                                                         Episode 19	 reward: -7.33	 makespan: 725.65	 Mean_loss: -0.15316767,  training time: 2.19
progress:  90%|[34m [0m| 18/20 [00:42<00:04,  2.18s/it]progress:  95%|[34m[0m| 19/20 [00:42<00:02,  2.19s/it]                                                         Episode 20	 reward: -7.46	 makespan: 738.05	 Mean_loss: 0.09361506,  training time: 2.16
progress:  95%|[34m[0m| 19/20 [00:44<00:02,  2.19s/it]progress: 100%|[34m[0m| 20/20 [00:44<00:00,  2.18s/it]progress: 100%|[34m[0m| 20/20 [00:44<00:00,  2.25s/it]
+ --num_envs 4 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp17/DAN/finetuning/15x5+mix+SD2/15x5_10 --model_suffix free --finetuning_model 15x5+mix+SD2 --max_updates 20 --n_j 15 --n_m 5 --op_per_job 10
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x5+mix
save model name:  15x5+mix+free
./trained_network/SD2/15x5+mix+SD2.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/15x5+mix+SD2.pth
vali data :./data/data_train_vali/SD2/15x5+mix


progress:   0%|[34m          [0m| 0/20 [00:00<?, ?it/s]                                                Episode 1	 reward: -10.34	 makespan: 1023.40	 Mean_loss: 0.08705766,  training time: 4.12
progress:   0%|[34m          [0m| 0/20 [00:04<?, ?it/s]progress:   5%|[34m         [0m| 1/20 [00:04<01:18,  4.13s/it]                                                        Episode 2	 reward: -10.37	 makespan: 1026.70	 Mean_loss: 0.07242968,  training time: 3.11
progress:   5%|[34m         [0m| 1/20 [00:07<01:18,  4.13s/it]progress:  10%|[34m         [0m| 2/20 [00:07<01:03,  3.53s/it]                                                        Episode 3	 reward: -10.34	 makespan: 1023.20	 Mean_loss: 0.08828184,  training time: 3.06
progress:  10%|[34m         [0m| 2/20 [00:10<01:03,  3.53s/it]progress:  15%|[34m        [0m| 3/20 [00:10<00:56,  3.32s/it]                                                        Episode 4	 reward: -10.30	 makespan: 1020.00	 Mean_loss: 0.06808022,  training time: 3.05
progress:  15%|[34m        [0m| 3/20 [00:13<00:56,  3.32s/it]progress:  20%|[34m        [0m| 4/20 [00:13<00:51,  3.21s/it]                                                        Episode 5	 reward: -10.40	 makespan: 1030.00	 Mean_loss: 0.07895606,  training time: 3.03
progress:  20%|[34m        [0m| 4/20 [00:16<00:51,  3.21s/it]progress:  25%|[34m       [0m| 5/20 [00:16<00:47,  3.15s/it]                                                        Episode 6	 reward: -10.32	 makespan: 1021.70	 Mean_loss: 0.07236554,  training time: 3.17
progress:  25%|[34m       [0m| 5/20 [00:19<00:47,  3.15s/it]progress:  30%|[34m       [0m| 6/20 [00:19<00:44,  3.16s/it]                                                        Episode 7	 reward: -10.43	 makespan: 1033.05	 Mean_loss: 0.06801567,  training time: 3.14
progress:  30%|[34m       [0m| 6/20 [00:22<00:44,  3.16s/it]progress:  35%|[34m      [0m| 7/20 [00:22<00:41,  3.16s/it]                                                        Episode 8	 reward: -10.35	 makespan: 1024.95	 Mean_loss: 0.07202764,  training time: 3.00
progress:  35%|[34m      [0m| 7/20 [00:25<00:41,  3.16s/it]progress:  40%|[34m      [0m| 8/20 [00:25<00:37,  3.11s/it]                                                        Episode 9	 reward: -10.38	 makespan: 1027.60	 Mean_loss: 0.06274923,  training time: 3.09
progress:  40%|[34m      [0m| 8/20 [00:28<00:37,  3.11s/it]progress:  45%|[34m     [0m| 9/20 [00:28<00:34,  3.10s/it]                                                        Episode 10	 reward: -10.47	 makespan: 1036.70	 Mean_loss: 0.06639536,  training time: 2.99
progress:  45%|[34m     [0m| 9/20 [00:31<00:34,  3.10s/it]progress:  50%|[34m     [0m| 10/20 [00:31<00:30,  3.07s/it]                                                         Episode 11	 reward: -10.28	 makespan: 1018.00	 Mean_loss: 0.06730934,  training time: 2.97
progress:  50%|[34m     [0m| 10/20 [00:34<00:30,  3.07s/it]progress:  55%|[34m    [0m| 11/20 [00:34<00:27,  3.04s/it]                                                         Episode 12	 reward: -10.32	 makespan: 1022.05	 Mean_loss: 0.08223559,  training time: 3.14
progress:  55%|[34m    [0m| 11/20 [00:37<00:27,  3.04s/it]progress:  60%|[34m    [0m| 12/20 [00:37<00:24,  3.08s/it]                                                         Episode 13	 reward: -10.32	 makespan: 1022.05	 Mean_loss: 0.08343019,  training time: 2.99
progress:  60%|[34m    [0m| 12/20 [00:40<00:24,  3.08s/it]progress:  65%|[34m   [0m| 13/20 [00:40<00:21,  3.05s/it]                                                         Episode 14	 reward: -10.34	 makespan: 1024.10	 Mean_loss: 0.05845204,  training time: 2.97
progress:  65%|[34m   [0m| 13/20 [00:43<00:21,  3.05s/it]progress:  70%|[34m   [0m| 14/20 [00:43<00:18,  3.03s/it]                                                         Episode 15	 reward: -10.37	 makespan: 1026.65	 Mean_loss: 0.07670492,  training time: 2.97
progress:  70%|[34m   [0m| 14/20 [00:46<00:18,  3.03s/it]progress:  75%|[34m  [0m| 15/20 [00:46<00:15,  3.01s/it]                                                         Episode 16	 reward: -10.41	 makespan: 1030.20	 Mean_loss: 0.07693239,  training time: 2.96
progress:  75%|[34m  [0m| 15/20 [00:49<00:15,  3.01s/it]progress:  80%|[34m  [0m| 16/20 [00:49<00:11,  3.00s/it]                                                         Episode 17	 reward: -10.45	 makespan: 1034.50	 Mean_loss: 0.06620108,  training time: 2.96
progress:  80%|[34m  [0m| 16/20 [00:52<00:11,  3.00s/it]progress:  85%|[34m [0m| 17/20 [00:52<00:08,  2.99s/it]                                                         Episode 18	 reward: -10.48	 makespan: 1037.90	 Mean_loss: 0.06597840,  training time: 2.96
progress:  85%|[34m [0m| 17/20 [00:55<00:08,  2.99s/it]progress:  90%|[34m [0m| 18/20 [00:55<00:05,  2.98s/it]                                                         Episode 19	 reward: -10.30	 makespan: 1019.50	 Mean_loss: 0.08062813,  training time: 3.03
progress:  90%|[34m [0m| 18/20 [00:58<00:05,  2.98s/it]progress:  95%|[34m[0m| 19/20 [00:58<00:02,  3.00s/it]                                                         Episode 20	 reward: -10.32	 makespan: 1021.25	 Mean_loss: 0.06592802,  training time: 2.98
progress:  95%|[34m[0m| 19/20 [01:01<00:02,  3.00s/it]progress: 100%|[34m[0m| 20/20 [01:01<00:00,  2.99s/it]progress: 100%|[34m[0m| 20/20 [01:01<00:00,  3.09s/it]
+ --num_envs 4 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp17/DAN/finetuning/15x5+mix+SD2/15x5_12 --model_suffix free --finetuning_model 15x5+mix+SD2 --max_updates 20 --n_j 15 --n_m 5 --op_per_job 12
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x5+mix
save model name:  15x5+mix+free
./trained_network/SD2/15x5+mix+SD2.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/15x5+mix+SD2.pth
vali data :./data/data_train_vali/SD2/15x5+mix


progress:   0%|[34m          [0m| 0/20 [00:00<?, ?it/s]                                                Episode 1	 reward: -12.51	 makespan: 1238.40	 Mean_loss: 0.18367366,  training time: 5.11
progress:   0%|[34m          [0m| 0/20 [00:05<?, ?it/s]progress:   5%|[34m         [0m| 1/20 [00:05<01:37,  5.12s/it]                                                        Episode 2	 reward: -12.63	 makespan: 1250.00	 Mean_loss: 0.17467934,  training time: 3.86
progress:   5%|[34m         [0m| 1/20 [00:08<01:37,  5.12s/it]progress:  10%|[34m         [0m| 2/20 [00:08<01:18,  4.38s/it]                                                        Episode 3	 reward: -12.60	 makespan: 1247.65	 Mean_loss: 0.14754209,  training time: 3.89
progress:  10%|[34m         [0m| 2/20 [00:12<01:18,  4.38s/it]progress:  15%|[34m        [0m| 3/20 [00:12<01:10,  4.16s/it]                                                        Episode 4	 reward: -12.56	 makespan: 1243.05	 Mean_loss: 0.16582872,  training time: 3.80
progress:  15%|[34m        [0m| 3/20 [00:16<01:10,  4.16s/it]progress:  20%|[34m        [0m| 4/20 [00:16<01:04,  4.02s/it]                                                        Episode 5	 reward: -12.48	 makespan: 1235.75	 Mean_loss: 0.18636625,  training time: 3.99
progress:  20%|[34m        [0m| 4/20 [00:20<01:04,  4.02s/it]progress:  25%|[34m       [0m| 5/20 [00:20<01:00,  4.01s/it]                                                        Episode 6	 reward: -12.48	 makespan: 1235.60	 Mean_loss: 0.17534663,  training time: 3.82
progress:  25%|[34m       [0m| 5/20 [00:24<01:00,  4.01s/it]progress:  30%|[34m       [0m| 6/20 [00:24<00:55,  3.95s/it]                                                        Episode 7	 reward: -12.50	 makespan: 1237.40	 Mean_loss: 0.17662857,  training time: 3.79
progress:  30%|[34m       [0m| 6/20 [00:28<00:55,  3.95s/it]progress:  35%|[34m      [0m| 7/20 [00:28<00:50,  3.90s/it]                                                        Episode 8	 reward: -12.52	 makespan: 1239.00	 Mean_loss: 0.14686215,  training time: 3.79
progress:  35%|[34m      [0m| 7/20 [00:32<00:50,  3.90s/it]progress:  40%|[34m      [0m| 8/20 [00:32<00:46,  3.87s/it]                                                        Episode 9	 reward: -12.61	 makespan: 1248.80	 Mean_loss: 0.15213707,  training time: 4.02
progress:  40%|[34m      [0m| 8/20 [00:36<00:46,  3.87s/it]progress:  45%|[34m     [0m| 9/20 [00:36<00:43,  3.92s/it]                                                        Episode 10	 reward: -12.55	 makespan: 1242.10	 Mean_loss: 0.17789808,  training time: 3.77
progress:  45%|[34m     [0m| 9/20 [00:39<00:43,  3.92s/it]progress:  50%|[34m     [0m| 10/20 [00:39<00:38,  3.87s/it]                                                         Episode 11	 reward: -12.65	 makespan: 1252.25	 Mean_loss: 0.16468287,  training time: 3.75
progress:  50%|[34m     [0m| 10/20 [00:43<00:38,  3.87s/it]progress:  55%|[34m    [0m| 11/20 [00:43<00:34,  3.84s/it]                                                         Episode 12	 reward: -12.62	 makespan: 1249.10	 Mean_loss: 0.16124493,  training time: 3.83
progress:  55%|[34m    [0m| 11/20 [00:47<00:34,  3.84s/it]progress:  60%|[34m    [0m| 12/20 [00:47<00:30,  3.84s/it]                                                         Episode 13	 reward: -12.54	 makespan: 1241.00	 Mean_loss: 0.15874371,  training time: 3.78
progress:  60%|[34m    [0m| 12/20 [00:51<00:30,  3.84s/it]progress:  65%|[34m   [0m| 13/20 [00:51<00:26,  3.82s/it]                                                         Episode 14	 reward: -12.57	 makespan: 1244.20	 Mean_loss: 0.15279908,  training time: 3.75
progress:  65%|[34m   [0m| 13/20 [00:55<00:26,  3.82s/it]progress:  70%|[34m   [0m| 14/20 [00:55<00:22,  3.80s/it]                                                         Episode 15	 reward: -12.42	 makespan: 1229.15	 Mean_loss: 0.13266332,  training time: 3.83
progress:  70%|[34m   [0m| 14/20 [00:58<00:22,  3.80s/it]progress:  75%|[34m  [0m| 15/20 [00:58<00:19,  3.81s/it]                                                         Episode 16	 reward: -12.59	 makespan: 1246.90	 Mean_loss: 0.14708644,  training time: 3.83
progress:  75%|[34m  [0m| 15/20 [01:02<00:19,  3.81s/it]progress:  80%|[34m  [0m| 16/20 [01:02<00:15,  3.82s/it]                                                         Episode 17	 reward: -12.50	 makespan: 1237.85	 Mean_loss: 0.15807647,  training time: 3.74
progress:  80%|[34m  [0m| 16/20 [01:06<00:15,  3.82s/it]progress:  85%|[34m [0m| 17/20 [01:06<00:11,  3.80s/it]                                                         Episode 18	 reward: -12.59	 makespan: 1246.00	 Mean_loss: 0.17304820,  training time: 3.77
progress:  85%|[34m [0m| 17/20 [01:10<00:11,  3.80s/it]progress:  90%|[34m [0m| 18/20 [01:10<00:07,  3.79s/it]                                                         Episode 19	 reward: -12.59	 makespan: 1246.75	 Mean_loss: 0.15101470,  training time: 3.78
progress:  90%|[34m [0m| 18/20 [01:14<00:07,  3.79s/it]progress:  95%|[34m[0m| 19/20 [01:14<00:03,  3.79s/it]                                                         Episode 20	 reward: -12.63	 makespan: 1250.75	 Mean_loss: 0.16592763,  training time: 3.74
progress:  95%|[34m[0m| 19/20 [01:17<00:03,  3.79s/it]progress: 100%|[34m[0m| 20/20 [01:17<00:00,  3.78s/it]progress: 100%|[34m[0m| 20/20 [01:17<00:00,  3.89s/it]
+ --num_envs 4 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ model_suffix=exp17_500_512_3
+ logdir_maml=./runs/exp17/maml
+ n_j=15
+ for model in 'maml+$model_suffix'
+ for n_m in '$n_m_options'
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp17/maml/finetuning/maml+exp17_500_512_3/15x13_4 --model_suffix free --finetuning_model maml+exp17_500_512_3 --max_updates 20 --n_j 15 --n_m 13 --op_per_job 4 --num_envs 4 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x13+mix
save model name:  15x13+mix+free
./trained_network/SD2/maml+exp17_500_512_3.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/maml+exp17_500_512_3.pth
vali data :./data/data_train_vali/SD2/15x13+mix


progress:   0%|[34m          [0m| 0/20 [00:00<?, ?it/s]                                                Episode 1	 reward: -2.61	 makespan: 258.25	 Mean_loss: 5.90750170,  training time: 2.14
progress:   0%|[34m          [0m| 0/20 [00:02<?, ?it/s]progress:   5%|[34m         [0m| 1/20 [00:02<00:40,  2.14s/it]                                                        Episode 2	 reward: -2.83	 makespan: 280.25	 Mean_loss: 1.10009897,  training time: 1.07
progress:   5%|[34m         [0m| 1/20 [00:03<00:40,  2.14s/it]progress:  10%|[34m         [0m| 2/20 [00:03<00:27,  1.51s/it]                                                        Episode 3	 reward: -3.04	 makespan: 300.50	 Mean_loss: 0.41796052,  training time: 1.07
progress:  10%|[34m         [0m| 2/20 [00:04<00:27,  1.51s/it]progress:  15%|[34m        [0m| 3/20 [00:04<00:22,  1.31s/it]                                                        Episode 4	 reward: -2.55	 makespan: 252.00	 Mean_loss: 0.23366651,  training time: 1.02
progress:  15%|[34m        [0m| 3/20 [00:05<00:22,  1.31s/it]progress:  20%|[34m        [0m| 4/20 [00:05<00:19,  1.20s/it]                                                        Episode 5	 reward: -3.17	 makespan: 313.50	 Mean_loss: 0.24666762,  training time: 1.09
progress:  20%|[34m        [0m| 4/20 [00:06<00:19,  1.20s/it]progress:  25%|[34m       [0m| 5/20 [00:06<00:17,  1.16s/it]                                                        Episode 6	 reward: -2.91	 makespan: 287.75	 Mean_loss: 0.11029901,  training time: 1.05
progress:  25%|[34m       [0m| 5/20 [00:07<00:17,  1.16s/it]progress:  30%|[34m       [0m| 6/20 [00:07<00:15,  1.12s/it]                                                        Episode 7	 reward: -2.84	 makespan: 281.50	 Mean_loss: 0.09790976,  training time: 1.05
progress:  30%|[34m       [0m| 6/20 [00:08<00:15,  1.12s/it]progress:  35%|[34m      [0m| 7/20 [00:08<00:14,  1.10s/it]                                                        Episode 8	 reward: -3.12	 makespan: 309.25	 Mean_loss: 0.10406048,  training time: 1.04
progress:  35%|[34m      [0m| 7/20 [00:09<00:14,  1.10s/it]progress:  40%|[34m      [0m| 8/20 [00:09<00:12,  1.08s/it]                                                        Episode 9	 reward: -2.89	 makespan: 286.00	 Mean_loss: 0.07700361,  training time: 1.08
progress:  40%|[34m      [0m| 8/20 [00:10<00:12,  1.08s/it]progress:  45%|[34m     [0m| 9/20 [00:10<00:11,  1.08s/it]                                                        Episode 10	 reward: -2.85	 makespan: 282.25	 Mean_loss: 0.04662061,  training time: 1.06
progress:  45%|[34m     [0m| 9/20 [00:11<00:11,  1.08s/it]progress:  50%|[34m     [0m| 10/20 [00:11<00:10,  1.08s/it]                                                         Episode 11	 reward: -2.94	 makespan: 291.00	 Mean_loss: 0.03972448,  training time: 1.06
progress:  50%|[34m     [0m| 10/20 [00:12<00:10,  1.08s/it]progress:  55%|[34m    [0m| 11/20 [00:12<00:09,  1.07s/it]                                                         Episode 12	 reward: -2.72	 makespan: 269.75	 Mean_loss: 0.04142299,  training time: 1.19
progress:  55%|[34m    [0m| 11/20 [00:13<00:09,  1.07s/it]progress:  60%|[34m    [0m| 12/20 [00:13<00:08,  1.11s/it]                                                         Episode 13	 reward: -2.65	 makespan: 262.50	 Mean_loss: 0.04191795,  training time: 1.08
progress:  60%|[34m    [0m| 12/20 [00:15<00:08,  1.11s/it]progress:  65%|[34m   [0m| 13/20 [00:15<00:07,  1.10s/it]                                                         Episode 14	 reward: -2.77	 makespan: 273.75	 Mean_loss: 0.03213103,  training time: 1.09
progress:  65%|[34m   [0m| 13/20 [00:16<00:07,  1.10s/it]progress:  70%|[34m   [0m| 14/20 [00:16<00:06,  1.10s/it]                                                         Episode 15	 reward: -2.63	 makespan: 260.00	 Mean_loss: 0.01709942,  training time: 1.06
progress:  70%|[34m   [0m| 14/20 [00:17<00:06,  1.10s/it]progress:  75%|[34m  [0m| 15/20 [00:17<00:05,  1.09s/it]                                                         Episode 16	 reward: -2.70	 makespan: 267.25	 Mean_loss: 0.01338486,  training time: 1.09
progress:  75%|[34m  [0m| 15/20 [00:18<00:05,  1.09s/it]progress:  80%|[34m  [0m| 16/20 [00:18<00:04,  1.09s/it]                                                         Episode 17	 reward: -2.48	 makespan: 245.50	 Mean_loss: 0.03034066,  training time: 1.07
progress:  80%|[34m  [0m| 16/20 [00:19<00:04,  1.09s/it]progress:  85%|[34m [0m| 17/20 [00:19<00:03,  1.08s/it]                                                         Episode 18	 reward: -2.60	 makespan: 257.25	 Mean_loss: 0.01648859,  training time: 1.07
progress:  85%|[34m [0m| 17/20 [00:20<00:03,  1.08s/it]progress:  90%|[34m [0m| 18/20 [00:20<00:02,  1.08s/it]                                                         Episode 19	 reward: -2.61	 makespan: 258.50	 Mean_loss: 0.01331308,  training time: 1.06
progress:  90%|[34m [0m| 18/20 [00:21<00:02,  1.08s/it]progress:  95%|[34m[0m| 19/20 [00:21<00:01,  1.07s/it]                                                         Episode 20	 reward: -2.74	 makespan: 271.50	 Mean_loss: 0.03525509,  training time: 1.06
progress:  95%|[34m[0m| 19/20 [00:22<00:01,  1.07s/it]progress: 100%|[34m[0m| 20/20 [00:22<00:00,  1.07s/it]progress: 100%|[34m[0m| 20/20 [00:22<00:00,  1.13s/it]
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp17/maml/finetuning/maml+exp17_500_512_3/15x13_7 --model_suffix free --finetuning_model maml+exp17_500_512_3 --max_updates 20 --n_j 15 --n_m 13 --op_per_job 7 --num_envs 4 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x13+mix
save model name:  15x13+mix+free
./trained_network/SD2/maml+exp17_500_512_3.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/maml+exp17_500_512_3.pth
vali data :./data/data_train_vali/SD2/15x13+mix


progress:   0%|[34m          [0m| 0/20 [00:00<?, ?it/s]                                                Episode 1	 reward: -4.59	 makespan: 454.75	 Mean_loss: 4.69525433,  training time: 2.81
progress:   0%|[34m          [0m| 0/20 [00:02<?, ?it/s]progress:   5%|[34m         [0m| 1/20 [00:02<00:53,  2.81s/it]                                                        Episode 2	 reward: -4.76	 makespan: 471.50	 Mean_loss: 1.11808443,  training time: 1.76
progress:   5%|[34m         [0m| 1/20 [00:04<00:53,  2.81s/it]progress:  10%|[34m         [0m| 2/20 [00:04<00:39,  2.19s/it]                                                        Episode 3	 reward: -4.76	 makespan: 471.50	 Mean_loss: 0.38812849,  training time: 1.72
progress:  10%|[34m         [0m| 2/20 [00:06<00:39,  2.19s/it]progress:  15%|[34m        [0m| 3/20 [00:06<00:33,  1.97s/it]                                                        Episode 4	 reward: -4.50	 makespan: 445.25	 Mean_loss: 0.26918444,  training time: 1.74
progress:  15%|[34m        [0m| 3/20 [00:08<00:33,  1.97s/it]progress:  20%|[34m        [0m| 4/20 [00:08<00:30,  1.88s/it]                                                        Episode 5	 reward: -4.62	 makespan: 457.75	 Mean_loss: 0.17955978,  training time: 1.74
progress:  20%|[34m        [0m| 4/20 [00:09<00:30,  1.88s/it]progress:  25%|[34m       [0m| 5/20 [00:09<00:27,  1.83s/it]                                                        Episode 6	 reward: -4.77	 makespan: 472.00	 Mean_loss: 0.10530424,  training time: 1.72
progress:  25%|[34m       [0m| 5/20 [00:11<00:27,  1.83s/it]progress:  30%|[34m       [0m| 6/20 [00:11<00:25,  1.79s/it]                                                        Episode 7	 reward: -4.64	 makespan: 459.50	 Mean_loss: 0.05990950,  training time: 1.67
progress:  30%|[34m       [0m| 6/20 [00:13<00:25,  1.79s/it]progress:  35%|[34m      [0m| 7/20 [00:13<00:22,  1.75s/it]                                                        Episode 8	 reward: -4.53	 makespan: 448.25	 Mean_loss: 0.04811362,  training time: 1.72
progress:  35%|[34m      [0m| 7/20 [00:14<00:22,  1.75s/it]progress:  40%|[34m      [0m| 8/20 [00:14<00:20,  1.74s/it]                                                        Episode 9	 reward: -4.35	 makespan: 431.00	 Mean_loss: 0.07575388,  training time: 1.75
progress:  40%|[34m      [0m| 8/20 [00:16<00:20,  1.74s/it]progress:  45%|[34m     [0m| 9/20 [00:16<00:19,  1.75s/it]                                                        Episode 10	 reward: -4.53	 makespan: 448.75	 Mean_loss: 0.08718056,  training time: 1.74
progress:  45%|[34m     [0m| 9/20 [00:18<00:19,  1.75s/it]progress:  50%|[34m     [0m| 10/20 [00:18<00:17,  1.74s/it]                                                         Episode 11	 reward: -4.46	 makespan: 441.75	 Mean_loss: 0.07419408,  training time: 1.70
progress:  50%|[34m     [0m| 10/20 [00:20<00:17,  1.74s/it]progress:  55%|[34m    [0m| 11/20 [00:20<00:15,  1.73s/it]                                                         Episode 12	 reward: -5.16	 makespan: 511.25	 Mean_loss: 0.11534800,  training time: 1.73
progress:  55%|[34m    [0m| 11/20 [00:21<00:15,  1.73s/it]progress:  60%|[34m    [0m| 12/20 [00:21<00:13,  1.73s/it]                                                         Episode 13	 reward: -4.33	 makespan: 429.00	 Mean_loss: 0.08586232,  training time: 1.84
progress:  60%|[34m    [0m| 12/20 [00:23<00:13,  1.73s/it]progress:  65%|[34m   [0m| 13/20 [00:23<00:12,  1.77s/it]                                                         Episode 14	 reward: -4.37	 makespan: 432.25	 Mean_loss: 0.10291155,  training time: 1.73
progress:  65%|[34m   [0m| 13/20 [00:25<00:12,  1.77s/it]progress:  70%|[34m   [0m| 14/20 [00:25<00:10,  1.76s/it]                                                         Episode 15	 reward: -4.71	 makespan: 466.00	 Mean_loss: 0.04468005,  training time: 1.74
progress:  70%|[34m   [0m| 14/20 [00:27<00:10,  1.76s/it]progress:  75%|[34m  [0m| 15/20 [00:27<00:08,  1.75s/it]                                                         Episode 16	 reward: -4.50	 makespan: 445.25	 Mean_loss: 0.03724460,  training time: 1.74
progress:  75%|[34m  [0m| 15/20 [00:28<00:08,  1.75s/it]progress:  80%|[34m  [0m| 16/20 [00:28<00:06,  1.75s/it]                                                         Episode 17	 reward: -4.52	 makespan: 447.50	 Mean_loss: 0.04038838,  training time: 1.73
progress:  80%|[34m  [0m| 16/20 [00:30<00:06,  1.75s/it]progress:  85%|[34m [0m| 17/20 [00:30<00:05,  1.74s/it]                                                         Episode 18	 reward: -4.54	 makespan: 449.00	 Mean_loss: 0.03779820,  training time: 1.71
progress:  85%|[34m [0m| 17/20 [00:32<00:05,  1.74s/it]progress:  90%|[34m [0m| 18/20 [00:32<00:03,  1.73s/it]                                                         Episode 19	 reward: -4.61	 makespan: 456.75	 Mean_loss: 0.04170383,  training time: 1.72
progress:  90%|[34m [0m| 18/20 [00:34<00:03,  1.73s/it]progress:  95%|[34m[0m| 19/20 [00:34<00:01,  1.73s/it]                                                         Episode 20	 reward: -4.42	 makespan: 438.00	 Mean_loss: 0.03617565,  training time: 1.72
progress:  95%|[34m[0m| 19/20 [00:35<00:01,  1.73s/it]progress: 100%|[34m[0m| 20/20 [00:35<00:00,  1.73s/it]progress: 100%|[34m[0m| 20/20 [00:35<00:00,  1.79s/it]
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp17/maml/finetuning/maml+exp17_500_512_3/15x13_10 --model_suffix free --finetuning_model maml+exp17_500_512_3 --max_updates 20 --n_j 15 --n_m 13 --op_per_job 10 --num_envs 4 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x13+mix
save model name:  15x13+mix+free
./trained_network/SD2/maml+exp17_500_512_3.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/maml+exp17_500_512_3.pth
vali data :./data/data_train_vali/SD2/15x13+mix


progress:   0%|[34m          [0m| 0/20 [00:00<?, ?it/s]                                                Episode 1	 reward: -5.87	 makespan: 580.75	 Mean_loss: 3.18742585,  training time: 3.50
progress:   0%|[34m          [0m| 0/20 [00:03<?, ?it/s]progress:   5%|[34m         [0m| 1/20 [00:03<01:06,  3.50s/it]                                                        Episode 2	 reward: -6.03	 makespan: 596.50	 Mean_loss: 0.96235251,  training time: 2.45
progress:   5%|[34m         [0m| 1/20 [00:05<01:06,  3.50s/it]progress:  10%|[34m         [0m| 2/20 [00:05<00:51,  2.88s/it]                                                        Episode 3	 reward: -6.13	 makespan: 607.00	 Mean_loss: 0.60166132,  training time: 2.43
progress:  10%|[34m         [0m| 2/20 [00:08<00:51,  2.88s/it]progress:  15%|[34m        [0m| 3/20 [00:08<00:45,  2.68s/it]                                                        Episode 4	 reward: -6.10	 makespan: 604.00	 Mean_loss: 0.32202575,  training time: 2.49
progress:  15%|[34m        [0m| 3/20 [00:10<00:45,  2.68s/it]progress:  20%|[34m        [0m| 4/20 [00:10<00:41,  2.60s/it]                                                        Episode 5	 reward: -6.36	 makespan: 630.00	 Mean_loss: 0.17066577,  training time: 2.43
progress:  20%|[34m        [0m| 4/20 [00:13<00:41,  2.60s/it]progress:  25%|[34m       [0m| 5/20 [00:13<00:38,  2.54s/it]                                                        Episode 6	 reward: -6.06	 makespan: 599.50	 Mean_loss: 0.21678290,  training time: 2.53
progress:  25%|[34m       [0m| 5/20 [00:15<00:38,  2.54s/it]progress:  30%|[34m       [0m| 6/20 [00:15<00:35,  2.54s/it]                                                        Episode 7	 reward: -5.97	 makespan: 591.00	 Mean_loss: 0.10160964,  training time: 2.41
progress:  30%|[34m       [0m| 6/20 [00:18<00:35,  2.54s/it]progress:  35%|[34m      [0m| 7/20 [00:18<00:32,  2.50s/it]                                                        Episode 8	 reward: -5.85	 makespan: 579.50	 Mean_loss: 0.05449638,  training time: 2.42
progress:  35%|[34m      [0m| 7/20 [00:20<00:32,  2.50s/it]progress:  40%|[34m      [0m| 8/20 [00:20<00:29,  2.47s/it]                                                        Episode 9	 reward: -5.62	 makespan: 556.00	 Mean_loss: 0.04322572,  training time: 2.43
progress:  40%|[34m      [0m| 8/20 [00:23<00:29,  2.47s/it]progress:  45%|[34m     [0m| 9/20 [00:23<00:27,  2.46s/it]                                                        Episode 10	 reward: -5.86	 makespan: 580.50	 Mean_loss: 0.04194019,  training time: 2.42
progress:  45%|[34m     [0m| 9/20 [00:25<00:27,  2.46s/it]progress:  50%|[34m     [0m| 10/20 [00:25<00:24,  2.45s/it]                                                         Episode 11	 reward: -5.76	 makespan: 569.75	 Mean_loss: 0.03356189,  training time: 2.41
progress:  50%|[34m     [0m| 10/20 [00:27<00:24,  2.45s/it]progress:  55%|[34m    [0m| 11/20 [00:27<00:21,  2.44s/it]                                                         Episode 12	 reward: -6.01	 makespan: 594.50	 Mean_loss: 0.03864501,  training time: 2.42
progress:  55%|[34m    [0m| 11/20 [00:30<00:21,  2.44s/it]progress:  60%|[34m    [0m| 12/20 [00:30<00:19,  2.43s/it]                                                         Episode 13	 reward: -5.78	 makespan: 572.00	 Mean_loss: 0.03614929,  training time: 2.41
progress:  60%|[34m    [0m| 12/20 [00:32<00:19,  2.43s/it]progress:  65%|[34m   [0m| 13/20 [00:32<00:16,  2.42s/it]                                                         Episode 14	 reward: -6.21	 makespan: 614.75	 Mean_loss: 0.03062205,  training time: 2.41
progress:  65%|[34m   [0m| 13/20 [00:35<00:16,  2.42s/it]progress:  70%|[34m   [0m| 14/20 [00:35<00:14,  2.42s/it]                                                         Episode 15	 reward: -6.04	 makespan: 597.75	 Mean_loss: 0.03855458,  training time: 2.42
progress:  70%|[34m   [0m| 14/20 [00:37<00:14,  2.42s/it]progress:  75%|[34m  [0m| 15/20 [00:37<00:12,  2.42s/it]                                                         Episode 16	 reward: -6.26	 makespan: 619.25	 Mean_loss: 0.03997684,  training time: 2.41
progress:  75%|[34m  [0m| 15/20 [00:39<00:12,  2.42s/it]progress:  80%|[34m  [0m| 16/20 [00:39<00:09,  2.42s/it]                                                         Episode 17	 reward: -6.39	 makespan: 633.00	 Mean_loss: 0.03700569,  training time: 2.42
progress:  80%|[34m  [0m| 16/20 [00:42<00:09,  2.42s/it]progress:  85%|[34m [0m| 17/20 [00:42<00:07,  2.42s/it]                                                         Episode 18	 reward: -6.17	 makespan: 611.25	 Mean_loss: 0.03871074,  training time: 2.41
progress:  85%|[34m [0m| 17/20 [00:44<00:07,  2.42s/it]progress:  90%|[34m [0m| 18/20 [00:44<00:04,  2.42s/it]                                                         Episode 19	 reward: -6.56	 makespan: 649.00	 Mean_loss: 0.03556656,  training time: 2.42
progress:  90%|[34m [0m| 18/20 [00:47<00:04,  2.42s/it]progress:  95%|[34m[0m| 19/20 [00:47<00:02,  2.42s/it]                                                         Episode 20	 reward: -6.62	 makespan: 655.25	 Mean_loss: 0.05430603,  training time: 2.41
progress:  95%|[34m[0m| 19/20 [00:49<00:02,  2.42s/it]progress: 100%|[34m[0m| 20/20 [00:49<00:00,  2.42s/it]progress: 100%|[34m[0m| 20/20 [00:49<00:00,  2.48s/it]
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp17/maml/finetuning/maml+exp17_500_512_3/15x13_12 --model_suffix free --finetuning_model maml+exp17_500_512_3 --max_updates 20 --n_j 15 --n_m 13 --op_per_job 12 --num_envs 4 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x13+mix
save model name:  15x13+mix+free
./trained_network/SD2/maml+exp17_500_512_3.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/maml+exp17_500_512_3.pth
vali data :./data/data_train_vali/SD2/15x13+mix


progress:   0%|[34m          [0m| 0/20 [00:00<?, ?it/s]                                                Episode 1	 reward: -6.72	 makespan: 665.25	 Mean_loss: 2.44392395,  training time: 3.97
progress:   0%|[34m          [0m| 0/20 [00:03<?, ?it/s]progress:   5%|[34m         [0m| 1/20 [00:03<01:15,  3.97s/it]                                                        Episode 2	 reward: -7.39	 makespan: 732.00	 Mean_loss: 0.70753062,  training time: 2.86
progress:   5%|[34m         [0m| 1/20 [00:06<01:15,  3.97s/it]progress:  10%|[34m         [0m| 2/20 [00:06<00:59,  3.32s/it]                                                        Episode 3	 reward: -7.71	 makespan: 763.75	 Mean_loss: 0.48865777,  training time: 2.89
progress:  10%|[34m         [0m| 2/20 [00:09<00:59,  3.32s/it]progress:  15%|[34m        [0m| 3/20 [00:09<00:53,  3.12s/it]                                                        Episode 4	 reward: -7.20	 makespan: 713.00	 Mean_loss: 0.31039214,  training time: 2.85
progress:  15%|[34m        [0m| 3/20 [00:12<00:53,  3.12s/it]progress:  20%|[34m        [0m| 4/20 [00:12<00:48,  3.02s/it]                                                        Episode 5	 reward: -8.08	 makespan: 799.75	 Mean_loss: 0.28057832,  training time: 2.99
progress:  20%|[34m        [0m| 4/20 [00:15<00:48,  3.02s/it]progress:  25%|[34m       [0m| 5/20 [00:15<00:45,  3.01s/it]                                                        Episode 6	 reward: -7.61	 makespan: 753.50	 Mean_loss: 0.20251502,  training time: 2.86
progress:  25%|[34m       [0m| 5/20 [00:18<00:45,  3.01s/it]progress:  30%|[34m       [0m| 6/20 [00:18<00:41,  2.96s/it]                                                        Episode 7	 reward: -6.94	 makespan: 687.00	 Mean_loss: 0.12706754,  training time: 2.87
progress:  30%|[34m       [0m| 6/20 [00:21<00:41,  2.96s/it]progress:  35%|[34m      [0m| 7/20 [00:21<00:38,  2.93s/it]                                                        Episode 8	 reward: -7.30	 makespan: 722.50	 Mean_loss: 0.14905331,  training time: 2.87
progress:  35%|[34m      [0m| 7/20 [00:24<00:38,  2.93s/it]progress:  40%|[34m      [0m| 8/20 [00:24<00:34,  2.91s/it]                                                        Episode 9	 reward: -7.62	 makespan: 754.25	 Mean_loss: 0.13724650,  training time: 2.87
progress:  40%|[34m      [0m| 8/20 [00:27<00:34,  2.91s/it]progress:  45%|[34m     [0m| 9/20 [00:27<00:31,  2.90s/it]                                                        Episode 10	 reward: -6.77	 makespan: 670.25	 Mean_loss: 0.09424198,  training time: 2.87
progress:  45%|[34m     [0m| 9/20 [00:29<00:31,  2.90s/it]progress:  50%|[34m     [0m| 10/20 [00:29<00:28,  2.89s/it]                                                         Episode 11	 reward: -7.64	 makespan: 756.50	 Mean_loss: 0.08652698,  training time: 2.88
progress:  50%|[34m     [0m| 10/20 [00:32<00:28,  2.89s/it]progress:  55%|[34m    [0m| 11/20 [00:32<00:25,  2.89s/it]                                                         Episode 12	 reward: -7.49	 makespan: 742.00	 Mean_loss: 0.09815726,  training time: 2.86
progress:  55%|[34m    [0m| 11/20 [00:35<00:25,  2.89s/it]progress:  60%|[34m    [0m| 12/20 [00:35<00:23,  2.88s/it]                                                         Episode 13	 reward: -7.12	 makespan: 705.25	 Mean_loss: 0.08331800,  training time: 2.87
progress:  60%|[34m    [0m| 12/20 [00:38<00:23,  2.88s/it]progress:  65%|[34m   [0m| 13/20 [00:38<00:20,  2.88s/it]                                                         Episode 14	 reward: -7.24	 makespan: 716.50	 Mean_loss: 0.07222573,  training time: 2.95
progress:  65%|[34m   [0m| 13/20 [00:41<00:20,  2.88s/it]progress:  70%|[34m   [0m| 14/20 [00:41<00:17,  2.90s/it]                                                         Episode 15	 reward: -7.54	 makespan: 746.75	 Mean_loss: 0.06702535,  training time: 2.87
progress:  70%|[34m   [0m| 14/20 [00:44<00:17,  2.90s/it]progress:  75%|[34m  [0m| 15/20 [00:44<00:14,  2.89s/it]                                                         Episode 16	 reward: -7.38	 makespan: 731.00	 Mean_loss: 0.08067678,  training time: 2.87
progress:  75%|[34m  [0m| 15/20 [00:47<00:14,  2.89s/it]progress:  80%|[34m  [0m| 16/20 [00:47<00:11,  2.88s/it]                                                         Episode 17	 reward: -7.07	 makespan: 700.25	 Mean_loss: 0.04606663,  training time: 2.86
progress:  80%|[34m  [0m| 16/20 [00:50<00:11,  2.88s/it]progress:  85%|[34m [0m| 17/20 [00:50<00:08,  2.88s/it]                                                         Episode 18	 reward: -7.04	 makespan: 696.75	 Mean_loss: 0.04930376,  training time: 2.85
progress:  85%|[34m [0m| 17/20 [00:52<00:08,  2.88s/it]progress:  90%|[34m [0m| 18/20 [00:52<00:05,  2.87s/it]                                                         Episode 19	 reward: -6.99	 makespan: 692.00	 Mean_loss: 0.05555152,  training time: 2.86
progress:  90%|[34m [0m| 18/20 [00:55<00:05,  2.87s/it]progress:  95%|[34m[0m| 19/20 [00:55<00:02,  2.87s/it]                                                         Episode 20	 reward: -6.92	 makespan: 685.25	 Mean_loss: 0.05156681,  training time: 2.85
progress:  95%|[34m[0m| 19/20 [00:58<00:02,  2.87s/it]progress: 100%|[34m[0m| 20/20 [00:58<00:00,  2.86s/it]progress: 100%|[34m[0m| 20/20 [00:58<00:00,  2.93s/it]
+ for n_m in '$n_m_options'
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp17/maml/finetuning/maml+exp17_500_512_3/15x10_4 --model_suffix free --finetuning_model maml+exp17_500_512_3 --max_updates 20 --n_j 15 --n_m 10 --op_per_job 4 --num_envs 4 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x10+mix
save model name:  15x10+mix+free
./trained_network/SD2/maml+exp17_500_512_3.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/maml+exp17_500_512_3.pth
vali data :./data/data_train_vali/SD2/15x10+mix


progress:   0%|[34m          [0m| 0/20 [00:00<?, ?it/s]                                                Episode 1	 reward: -2.83	 makespan: 279.75	 Mean_loss: 5.83715105,  training time: 2.19
progress:   0%|[34m          [0m| 0/20 [00:02<?, ?it/s]progress:   5%|[34m         [0m| 1/20 [00:02<00:41,  2.19s/it]                                                        Episode 2	 reward: -2.90	 makespan: 286.75	 Mean_loss: 1.08244848,  training time: 1.02
progress:   5%|[34m         [0m| 1/20 [00:03<00:41,  2.19s/it]progress:  10%|[34m         [0m| 2/20 [00:03<00:27,  1.50s/it]                                                        Episode 3	 reward: -2.86	 makespan: 283.50	 Mean_loss: 0.28969032,  training time: 1.04
progress:  10%|[34m         [0m| 2/20 [00:04<00:27,  1.50s/it]progress:  15%|[34m        [0m| 3/20 [00:04<00:21,  1.29s/it]                                                        Episode 4	 reward: -2.98	 makespan: 295.50	 Mean_loss: 0.28735036,  training time: 1.07
progress:  15%|[34m        [0m| 3/20 [00:05<00:21,  1.29s/it]progress:  20%|[34m        [0m| 4/20 [00:05<00:19,  1.20s/it]                                                        Episode 5	 reward: -2.99	 makespan: 296.50	 Mean_loss: 0.15516570,  training time: 1.03
progress:  20%|[34m        [0m| 4/20 [00:06<00:19,  1.20s/it]progress:  25%|[34m       [0m| 5/20 [00:06<00:17,  1.14s/it]                                                        Episode 6	 reward: -2.81	 makespan: 278.50	 Mean_loss: 0.06015715,  training time: 1.01
progress:  25%|[34m       [0m| 5/20 [00:07<00:17,  1.14s/it]progress:  30%|[34m       [0m| 6/20 [00:07<00:15,  1.10s/it]                                                        Episode 7	 reward: -2.89	 makespan: 286.50	 Mean_loss: 0.06381997,  training time: 1.04
progress:  30%|[34m       [0m| 6/20 [00:08<00:15,  1.10s/it]progress:  35%|[34m      [0m| 7/20 [00:08<00:14,  1.08s/it]                                                        Episode 8	 reward: -2.58	 makespan: 255.50	 Mean_loss: 0.07479337,  training time: 1.08
progress:  35%|[34m      [0m| 7/20 [00:09<00:14,  1.08s/it]progress:  40%|[34m      [0m| 8/20 [00:09<00:12,  1.08s/it]                                                        Episode 9	 reward: -2.72	 makespan: 269.75	 Mean_loss: 0.06707590,  training time: 1.04
progress:  40%|[34m      [0m| 8/20 [00:10<00:12,  1.08s/it]progress:  45%|[34m     [0m| 9/20 [00:10<00:11,  1.07s/it]                                                        Episode 10	 reward: -2.95	 makespan: 292.25	 Mean_loss: 0.05933509,  training time: 1.02
progress:  45%|[34m     [0m| 9/20 [00:11<00:11,  1.07s/it]progress:  50%|[34m     [0m| 10/20 [00:11<00:10,  1.05s/it]                                                         Episode 11	 reward: -2.75	 makespan: 272.25	 Mean_loss: 0.03490128,  training time: 1.02
progress:  50%|[34m     [0m| 10/20 [00:12<00:10,  1.05s/it]progress:  55%|[34m    [0m| 11/20 [00:12<00:09,  1.04s/it]                                                         Episode 12	 reward: -2.88	 makespan: 285.25	 Mean_loss: 0.04496802,  training time: 1.16
progress:  55%|[34m    [0m| 11/20 [00:13<00:09,  1.04s/it]progress:  60%|[34m    [0m| 12/20 [00:13<00:08,  1.08s/it]                                                         Episode 13	 reward: -2.92	 makespan: 288.75	 Mean_loss: 0.02020474,  training time: 1.04
progress:  60%|[34m    [0m| 12/20 [00:14<00:08,  1.08s/it]progress:  65%|[34m   [0m| 13/20 [00:14<00:07,  1.07s/it]                                                         Episode 14	 reward: -2.80	 makespan: 277.50	 Mean_loss: 0.01025536,  training time: 1.03
progress:  65%|[34m   [0m| 13/20 [00:15<00:07,  1.07s/it]progress:  70%|[34m   [0m| 14/20 [00:15<00:06,  1.06s/it]                                                         Episode 15	 reward: -3.01	 makespan: 298.25	 Mean_loss: 0.03507208,  training time: 1.04
progress:  70%|[34m   [0m| 14/20 [00:16<00:06,  1.06s/it]progress:  75%|[34m  [0m| 15/20 [00:16<00:05,  1.05s/it]                                                         Episode 16	 reward: -3.12	 makespan: 308.75	 Mean_loss: 0.05615866,  training time: 1.15
progress:  75%|[34m  [0m| 15/20 [00:17<00:05,  1.05s/it]progress:  80%|[34m  [0m| 16/20 [00:17<00:04,  1.08s/it]                                                         Episode 17	 reward: -2.89	 makespan: 286.25	 Mean_loss: 0.04137195,  training time: 1.03
progress:  80%|[34m  [0m| 16/20 [00:19<00:04,  1.08s/it]progress:  85%|[34m [0m| 17/20 [00:19<00:03,  1.07s/it]                                                         Episode 18	 reward: -3.08	 makespan: 304.50	 Mean_loss: 0.02505507,  training time: 1.01
progress:  85%|[34m [0m| 17/20 [00:20<00:03,  1.07s/it]progress:  90%|[34m [0m| 18/20 [00:20<00:02,  1.05s/it]                                                         Episode 19	 reward: -2.66	 makespan: 263.50	 Mean_loss: 0.03390467,  training time: 1.02
progress:  90%|[34m [0m| 18/20 [00:21<00:02,  1.05s/it]progress:  95%|[34m[0m| 19/20 [00:21<00:01,  1.04s/it]                                                         Episode 20	 reward: -2.73	 makespan: 270.25	 Mean_loss: 0.02286022,  training time: 1.03
progress:  95%|[34m[0m| 19/20 [00:22<00:01,  1.04s/it]progress: 100%|[34m[0m| 20/20 [00:22<00:00,  1.04s/it]progress: 100%|[34m[0m| 20/20 [00:22<00:00,  1.10s/it]
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp17/maml/finetuning/maml+exp17_500_512_3/15x10_7 --model_suffix free --finetuning_model maml+exp17_500_512_3 --max_updates 20 --n_j 15 --n_m 10 --op_per_job 7 --num_envs 4 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x10+mix
save model name:  15x10+mix+free
./trained_network/SD2/maml+exp17_500_512_3.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/maml+exp17_500_512_3.pth
vali data :./data/data_train_vali/SD2/15x10+mix


progress:   0%|[34m          [0m| 0/20 [00:00<?, ?it/s]                                                Episode 1	 reward: -5.08	 makespan: 503.25	 Mean_loss: 5.20591402,  training time: 2.77
progress:   0%|[34m          [0m| 0/20 [00:02<?, ?it/s]progress:   5%|[34m         [0m| 1/20 [00:02<00:52,  2.77s/it]                                                        Episode 2	 reward: -5.37	 makespan: 532.00	 Mean_loss: 1.23768377,  training time: 1.70
progress:   5%|[34m         [0m| 1/20 [00:04<00:52,  2.77s/it]progress:  10%|[34m         [0m| 2/20 [00:04<00:38,  2.14s/it]                                                        Episode 3	 reward: -5.99	 makespan: 593.50	 Mean_loss: 0.54255772,  training time: 1.73
progress:  10%|[34m         [0m| 2/20 [00:06<00:38,  2.14s/it]progress:  15%|[34m        [0m| 3/20 [00:06<00:33,  1.95s/it]                                                        Episode 4	 reward: -5.40	 makespan: 534.75	 Mean_loss: 0.32627115,  training time: 1.69
progress:  15%|[34m        [0m| 3/20 [00:07<00:33,  1.95s/it]progress:  20%|[34m        [0m| 4/20 [00:07<00:29,  1.85s/it]                                                        Episode 5	 reward: -5.42	 makespan: 536.50	 Mean_loss: 0.22422287,  training time: 1.68
progress:  20%|[34m        [0m| 4/20 [00:09<00:29,  1.85s/it]progress:  25%|[34m       [0m| 5/20 [00:09<00:26,  1.79s/it]                                                        Episode 6	 reward: -5.76	 makespan: 569.75	 Mean_loss: 0.17775175,  training time: 1.68
progress:  25%|[34m       [0m| 5/20 [00:11<00:26,  1.79s/it]progress:  30%|[34m       [0m| 6/20 [00:11<00:24,  1.75s/it]                                                        Episode 7	 reward: -5.93	 makespan: 587.50	 Mean_loss: 0.14233947,  training time: 1.68
progress:  30%|[34m       [0m| 6/20 [00:12<00:24,  1.75s/it]progress:  35%|[34m      [0m| 7/20 [00:12<00:22,  1.73s/it]                                                        Episode 8	 reward: -5.39	 makespan: 533.75	 Mean_loss: 0.14483188,  training time: 1.70
progress:  35%|[34m      [0m| 7/20 [00:14<00:22,  1.73s/it]progress:  40%|[34m      [0m| 8/20 [00:14<00:20,  1.72s/it]                                                        Episode 9	 reward: -5.32	 makespan: 526.50	 Mean_loss: 0.10098037,  training time: 1.68
progress:  40%|[34m      [0m| 8/20 [00:16<00:20,  1.72s/it]progress:  45%|[34m     [0m| 9/20 [00:16<00:18,  1.71s/it]                                                        Episode 10	 reward: -5.76	 makespan: 570.50	 Mean_loss: 0.09068266,  training time: 1.68
progress:  45%|[34m     [0m| 9/20 [00:18<00:18,  1.71s/it]progress:  50%|[34m     [0m| 10/20 [00:18<00:17,  1.70s/it]                                                         Episode 11	 reward: -5.65	 makespan: 559.00	 Mean_loss: 0.05615737,  training time: 1.69
progress:  50%|[34m     [0m| 10/20 [00:19<00:17,  1.70s/it]progress:  55%|[34m    [0m| 11/20 [00:19<00:15,  1.70s/it]                                                         Episode 12	 reward: -5.51	 makespan: 545.75	 Mean_loss: 0.08585443,  training time: 1.69
progress:  55%|[34m    [0m| 11/20 [00:21<00:15,  1.70s/it]progress:  60%|[34m    [0m| 12/20 [00:21<00:13,  1.70s/it]                                                         Episode 13	 reward: -5.43	 makespan: 538.00	 Mean_loss: 0.08382507,  training time: 1.77
progress:  60%|[34m    [0m| 12/20 [00:23<00:13,  1.70s/it]progress:  65%|[34m   [0m| 13/20 [00:23<00:12,  1.72s/it]                                                         Episode 14	 reward: -5.80	 makespan: 574.00	 Mean_loss: 0.09406427,  training time: 1.69
progress:  65%|[34m   [0m| 13/20 [00:24<00:12,  1.72s/it]progress:  70%|[34m   [0m| 14/20 [00:24<00:10,  1.71s/it]                                                         Episode 15	 reward: -5.05	 makespan: 500.25	 Mean_loss: 0.05883622,  training time: 1.69
progress:  70%|[34m   [0m| 14/20 [00:26<00:10,  1.71s/it]progress:  75%|[34m  [0m| 15/20 [00:26<00:08,  1.70s/it]                                                         Episode 16	 reward: -5.84	 makespan: 578.50	 Mean_loss: 0.10790703,  training time: 1.67
progress:  75%|[34m  [0m| 15/20 [00:28<00:08,  1.70s/it]progress:  80%|[34m  [0m| 16/20 [00:28<00:06,  1.70s/it]                                                         Episode 17	 reward: -5.98	 makespan: 592.25	 Mean_loss: 0.11056644,  training time: 1.69
progress:  80%|[34m  [0m| 16/20 [00:29<00:06,  1.70s/it]progress:  85%|[34m [0m| 17/20 [00:29<00:05,  1.69s/it]                                                         Episode 18	 reward: -5.18	 makespan: 512.75	 Mean_loss: 0.07617215,  training time: 1.68
progress:  85%|[34m [0m| 17/20 [00:31<00:05,  1.69s/it]progress:  90%|[34m [0m| 18/20 [00:31<00:03,  1.69s/it]                                                         Episode 19	 reward: -5.64	 makespan: 558.50	 Mean_loss: 0.06956637,  training time: 1.65
progress:  90%|[34m [0m| 18/20 [00:33<00:03,  1.69s/it]progress:  95%|[34m[0m| 19/20 [00:33<00:01,  1.68s/it]                                                         Episode 20	 reward: -5.70	 makespan: 564.75	 Mean_loss: 0.06228205,  training time: 1.71
progress:  95%|[34m[0m| 19/20 [00:34<00:01,  1.68s/it]progress: 100%|[34m[0m| 20/20 [00:34<00:00,  1.69s/it]progress: 100%|[34m[0m| 20/20 [00:34<00:00,  1.75s/it]
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp17/maml/finetuning/maml+exp17_500_512_3/15x10_10 --model_suffix free --finetuning_model maml+exp17_500_512_3 --max_updates 20 --n_j 15 --n_m 10 --op_per_job 10 --num_envs 4 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x10+mix
save model name:  15x10+mix+free
./trained_network/SD2/maml+exp17_500_512_3.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/maml+exp17_500_512_3.pth
vali data :./data/data_train_vali/SD2/15x10+mix


progress:   0%|[34m          [0m| 0/20 [00:00<?, ?it/s]                                                Episode 1	 reward: -6.97	 makespan: 690.00	 Mean_loss: 5.29952478,  training time: 3.44
progress:   0%|[34m          [0m| 0/20 [00:03<?, ?it/s]progress:   5%|[34m         [0m| 1/20 [00:03<01:05,  3.44s/it]                                                        Episode 2	 reward: -7.07	 makespan: 699.50	 Mean_loss: 1.14353049,  training time: 2.37
progress:   5%|[34m         [0m| 1/20 [00:05<01:05,  3.44s/it]progress:  10%|[34m         [0m| 2/20 [00:05<00:50,  2.81s/it]                                                        Episode 3	 reward: -7.92	 makespan: 784.25	 Mean_loss: 0.78052044,  training time: 2.39
progress:  10%|[34m         [0m| 2/20 [00:08<00:50,  2.81s/it]progress:  15%|[34m        [0m| 3/20 [00:08<00:44,  2.62s/it]                                                        Episode 4	 reward: -7.83	 makespan: 775.50	 Mean_loss: 0.72577888,  training time: 2.37
progress:  15%|[34m        [0m| 3/20 [00:10<00:44,  2.62s/it]progress:  20%|[34m        [0m| 4/20 [00:10<00:40,  2.52s/it]                                                        Episode 5	 reward: -8.25	 makespan: 816.75	 Mean_loss: 0.42928529,  training time: 2.36
progress:  20%|[34m        [0m| 4/20 [00:12<00:40,  2.52s/it]progress:  25%|[34m       [0m| 5/20 [00:12<00:36,  2.46s/it]                                                        Episode 6	 reward: -8.59	 makespan: 850.25	 Mean_loss: 0.29603231,  training time: 2.45
progress:  25%|[34m       [0m| 5/20 [00:15<00:36,  2.46s/it]progress:  30%|[34m       [0m| 6/20 [00:15<00:34,  2.46s/it]                                                        Episode 7	 reward: -8.17	 makespan: 809.25	 Mean_loss: 0.38681966,  training time: 2.41
progress:  30%|[34m       [0m| 6/20 [00:17<00:34,  2.46s/it]progress:  35%|[34m      [0m| 7/20 [00:17<00:31,  2.44s/it]                                                        Episode 8	 reward: -8.18	 makespan: 809.75	 Mean_loss: 0.28532696,  training time: 2.38
progress:  35%|[34m      [0m| 7/20 [00:20<00:31,  2.44s/it]progress:  40%|[34m      [0m| 8/20 [00:20<00:29,  2.42s/it]                                                        Episode 9	 reward: -7.76	 makespan: 768.50	 Mean_loss: 0.20069028,  training time: 2.37
progress:  40%|[34m      [0m| 8/20 [00:22<00:29,  2.42s/it]progress:  45%|[34m     [0m| 9/20 [00:22<00:26,  2.41s/it]                                                        Episode 10	 reward: -8.42	 makespan: 834.00	 Mean_loss: 0.22219536,  training time: 2.37
progress:  45%|[34m     [0m| 9/20 [00:24<00:26,  2.41s/it]progress:  50%|[34m     [0m| 10/20 [00:24<00:23,  2.40s/it]                                                         Episode 11	 reward: -8.51	 makespan: 842.25	 Mean_loss: 0.21397284,  training time: 2.37
progress:  50%|[34m     [0m| 10/20 [00:27<00:23,  2.40s/it]progress:  55%|[34m    [0m| 11/20 [00:27<00:21,  2.39s/it]                                                         Episode 12	 reward: -8.21	 makespan: 813.00	 Mean_loss: 0.23722407,  training time: 2.37
progress:  55%|[34m    [0m| 11/20 [00:29<00:21,  2.39s/it]progress:  60%|[34m    [0m| 12/20 [00:29<00:19,  2.38s/it]                                                         Episode 13	 reward: -8.40	 makespan: 832.00	 Mean_loss: 0.19260497,  training time: 2.38
progress:  60%|[34m    [0m| 12/20 [00:32<00:19,  2.38s/it]progress:  65%|[34m   [0m| 13/20 [00:32<00:16,  2.38s/it]                                                         Episode 14	 reward: -8.44	 makespan: 836.00	 Mean_loss: 0.21874711,  training time: 2.38
progress:  65%|[34m   [0m| 13/20 [00:34<00:16,  2.38s/it]progress:  70%|[34m   [0m| 14/20 [00:34<00:14,  2.38s/it]                                                         Episode 15	 reward: -8.70	 makespan: 861.25	 Mean_loss: 0.19537812,  training time: 2.37
progress:  70%|[34m   [0m| 14/20 [00:36<00:14,  2.38s/it]progress:  75%|[34m  [0m| 15/20 [00:36<00:11,  2.38s/it]                                                         Episode 16	 reward: -8.93	 makespan: 884.00	 Mean_loss: 0.19118097,  training time: 2.39
progress:  75%|[34m  [0m| 15/20 [00:39<00:11,  2.38s/it]progress:  80%|[34m  [0m| 16/20 [00:39<00:09,  2.38s/it]                                                         Episode 17	 reward: -7.95	 makespan: 787.00	 Mean_loss: 0.15717357,  training time: 2.38
progress:  80%|[34m  [0m| 16/20 [00:41<00:09,  2.38s/it]progress:  85%|[34m [0m| 17/20 [00:41<00:07,  2.38s/it]                                                         Episode 18	 reward: -8.61	 makespan: 852.25	 Mean_loss: 0.15694736,  training time: 2.38
progress:  85%|[34m [0m| 17/20 [00:43<00:07,  2.38s/it]progress:  90%|[34m [0m| 18/20 [00:43<00:04,  2.38s/it]                                                         Episode 19	 reward: -7.91	 makespan: 783.25	 Mean_loss: 0.17034133,  training time: 2.37
progress:  90%|[34m [0m| 18/20 [00:46<00:04,  2.38s/it]progress:  95%|[34m[0m| 19/20 [00:46<00:02,  2.38s/it]                                                         Episode 20	 reward: -7.98	 makespan: 790.50	 Mean_loss: 0.13045272,  training time: 2.38
progress:  95%|[34m[0m| 19/20 [00:48<00:02,  2.38s/it]progress: 100%|[34m[0m| 20/20 [00:48<00:00,  2.38s/it]progress: 100%|[34m[0m| 20/20 [00:48<00:00,  2.44s/it]
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp17/maml/finetuning/maml+exp17_500_512_3/15x10_12 --model_suffix free --finetuning_model maml+exp17_500_512_3 --max_updates 20 --n_j 15 --n_m 10 --op_per_job 12 --num_envs 4 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x10+mix
save model name:  15x10+mix+free
./trained_network/SD2/maml+exp17_500_512_3.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/maml+exp17_500_512_3.pth
vali data :./data/data_train_vali/SD2/15x10+mix


progress:   0%|[34m          [0m| 0/20 [00:00<?, ?it/s]                                                Episode 1	 reward: -9.19	 makespan: 910.25	 Mean_loss: 7.21384430,  training time: 3.92
progress:   0%|[34m          [0m| 0/20 [00:03<?, ?it/s]progress:   5%|[34m         [0m| 1/20 [00:03<01:14,  3.92s/it]                                                        Episode 2	 reward: -9.15	 makespan: 906.25	 Mean_loss: 0.54992318,  training time: 2.84
progress:   5%|[34m         [0m| 1/20 [00:06<01:14,  3.92s/it]progress:  10%|[34m         [0m| 2/20 [00:06<00:59,  3.28s/it]                                                        Episode 3	 reward: -9.68	 makespan: 958.00	 Mean_loss: 0.99889398,  training time: 2.83
progress:  10%|[34m         [0m| 2/20 [00:09<00:59,  3.28s/it]progress:  15%|[34m        [0m| 3/20 [00:09<00:52,  3.08s/it]                                                        Episode 4	 reward: -8.93	 makespan: 884.00	 Mean_loss: 0.61679322,  training time: 2.82
progress:  15%|[34m        [0m| 3/20 [00:12<00:52,  3.08s/it]progress:  20%|[34m        [0m| 4/20 [00:12<00:47,  2.98s/it]                                                        Episode 5	 reward: -9.63	 makespan: 953.50	 Mean_loss: 0.67930400,  training time: 2.95
progress:  20%|[34m        [0m| 4/20 [00:15<00:47,  2.98s/it]progress:  25%|[34m       [0m| 5/20 [00:15<00:44,  2.97s/it]                                                        Episode 6	 reward: -9.79	 makespan: 969.25	 Mean_loss: 0.55076510,  training time: 2.82
progress:  25%|[34m       [0m| 5/20 [00:18<00:44,  2.97s/it]progress:  30%|[34m       [0m| 6/20 [00:18<00:40,  2.92s/it]                                                        Episode 7	 reward: -10.07	 makespan: 997.25	 Mean_loss: 0.64126819,  training time: 2.89
progress:  30%|[34m       [0m| 6/20 [00:21<00:40,  2.92s/it]progress:  35%|[34m      [0m| 7/20 [00:21<00:37,  2.91s/it]                                                        Episode 8	 reward: -9.44	 makespan: 934.75	 Mean_loss: 0.40435395,  training time: 2.83
progress:  35%|[34m      [0m| 7/20 [00:23<00:37,  2.91s/it]progress:  40%|[34m      [0m| 8/20 [00:23<00:34,  2.88s/it]                                                        Episode 9	 reward: -9.71	 makespan: 961.00	 Mean_loss: 0.48429555,  training time: 2.84
progress:  40%|[34m      [0m| 8/20 [00:26<00:34,  2.88s/it]progress:  45%|[34m     [0m| 9/20 [00:26<00:31,  2.87s/it]                                                        Episode 10	 reward: -9.65	 makespan: 955.75	 Mean_loss: 0.54433262,  training time: 2.82
progress:  45%|[34m     [0m| 9/20 [00:29<00:31,  2.87s/it]progress:  50%|[34m     [0m| 10/20 [00:29<00:28,  2.85s/it]                                                         Episode 11	 reward: -10.00	 makespan: 990.00	 Mean_loss: 0.43578166,  training time: 2.82
progress:  50%|[34m     [0m| 10/20 [00:32<00:28,  2.85s/it]progress:  55%|[34m    [0m| 11/20 [00:32<00:25,  2.84s/it]                                                         Episode 12	 reward: -9.58	 makespan: 948.00	 Mean_loss: 0.35122842,  training time: 2.83
progress:  55%|[34m    [0m| 11/20 [00:35<00:25,  2.84s/it]progress:  60%|[34m    [0m| 12/20 [00:35<00:22,  2.84s/it]                                                         Episode 13	 reward: -9.54	 makespan: 944.00	 Mean_loss: 0.40223521,  training time: 2.83
progress:  60%|[34m    [0m| 12/20 [00:38<00:22,  2.84s/it]progress:  65%|[34m   [0m| 13/20 [00:38<00:19,  2.84s/it]                                                         Episode 14	 reward: -9.34	 makespan: 924.75	 Mean_loss: 0.35777324,  training time: 2.82
progress:  65%|[34m   [0m| 13/20 [00:40<00:19,  2.84s/it]progress:  70%|[34m   [0m| 14/20 [00:40<00:16,  2.83s/it]                                                         Episode 15	 reward: -10.32	 makespan: 1021.50	 Mean_loss: 0.36464238,  training time: 2.81
progress:  70%|[34m   [0m| 14/20 [00:43<00:16,  2.83s/it]progress:  75%|[34m  [0m| 15/20 [00:43<00:14,  2.83s/it]                                                         Episode 16	 reward: -9.79	 makespan: 969.00	 Mean_loss: 0.23254210,  training time: 2.82
progress:  75%|[34m  [0m| 15/20 [00:46<00:14,  2.83s/it]progress:  80%|[34m  [0m| 16/20 [00:46<00:11,  2.83s/it]                                                         Episode 17	 reward: -11.14	 makespan: 1102.50	 Mean_loss: 0.36041918,  training time: 2.83
progress:  80%|[34m  [0m| 16/20 [00:49<00:11,  2.83s/it]progress:  85%|[34m [0m| 17/20 [00:49<00:08,  2.83s/it]                                                         Episode 18	 reward: -9.32	 makespan: 922.25	 Mean_loss: 0.38700524,  training time: 2.81
progress:  85%|[34m [0m| 17/20 [00:52<00:08,  2.83s/it]progress:  90%|[34m [0m| 18/20 [00:52<00:05,  2.82s/it]                                                         Episode 19	 reward: -10.21	 makespan: 1010.75	 Mean_loss: 0.31112027,  training time: 2.81
progress:  90%|[34m [0m| 18/20 [00:54<00:05,  2.82s/it]progress:  95%|[34m[0m| 19/20 [00:54<00:02,  2.82s/it]                                                         Episode 20	 reward: -9.67	 makespan: 957.00	 Mean_loss: 0.26911831,  training time: 2.81
progress:  95%|[34m[0m| 19/20 [00:57<00:02,  2.82s/it]progress: 100%|[34m[0m| 20/20 [00:57<00:00,  2.82s/it]progress: 100%|[34m[0m| 20/20 [00:57<00:00,  2.89s/it]
+ for n_m in '$n_m_options'
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp17/maml/finetuning/maml+exp17_500_512_3/15x7_4 --model_suffix free --finetuning_model maml+exp17_500_512_3 --max_updates 20 --n_j 15 --n_m 7 --op_per_job 4 --num_envs 4 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x7+mix
save model name:  15x7+mix+free
./trained_network/SD2/maml+exp17_500_512_3.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/maml+exp17_500_512_3.pth
vali data :./data/data_train_vali/SD2/15x7+mix


progress:   0%|[34m          [0m| 0/20 [00:00<?, ?it/s]                                                Episode 1	 reward: -4.18	 makespan: 414.25	 Mean_loss: 2.03468561,  training time: 2.18
progress:   0%|[34m          [0m| 0/20 [00:02<?, ?it/s]progress:   5%|[34m         [0m| 1/20 [00:02<00:41,  2.18s/it]                                                        Episode 2	 reward: -3.99	 makespan: 394.75	 Mean_loss: 0.54799330,  training time: 1.03
progress:   5%|[34m         [0m| 1/20 [00:03<00:41,  2.18s/it]progress:  10%|[34m         [0m| 2/20 [00:03<00:27,  1.50s/it]                                                        Episode 3	 reward: -3.91	 makespan: 387.25	 Mean_loss: 0.39743716,  training time: 1.04
progress:  10%|[34m         [0m| 2/20 [00:04<00:27,  1.50s/it]progress:  15%|[34m        [0m| 3/20 [00:04<00:21,  1.29s/it]                                                        Episode 4	 reward: -4.02	 makespan: 397.75	 Mean_loss: 0.21220361,  training time: 0.99
progress:  15%|[34m        [0m| 3/20 [00:05<00:21,  1.29s/it]progress:  20%|[34m        [0m| 4/20 [00:05<00:18,  1.17s/it]                                                        Episode 5	 reward: -3.56	 makespan: 352.25	 Mean_loss: 0.11464747,  training time: 1.03
progress:  20%|[34m        [0m| 4/20 [00:06<00:18,  1.17s/it]progress:  25%|[34m       [0m| 5/20 [00:06<00:16,  1.12s/it]                                                        Episode 6	 reward: -3.77	 makespan: 372.75	 Mean_loss: 0.09954947,  training time: 1.00
progress:  25%|[34m       [0m| 5/20 [00:07<00:16,  1.12s/it]progress:  30%|[34m       [0m| 6/20 [00:07<00:15,  1.08s/it]                                                        Episode 7	 reward: -3.71	 makespan: 367.75	 Mean_loss: 0.08477743,  training time: 1.05
progress:  30%|[34m       [0m| 6/20 [00:08<00:15,  1.08s/it]progress:  35%|[34m      [0m| 7/20 [00:08<00:13,  1.07s/it]                                                        Episode 8	 reward: -3.75	 makespan: 371.00	 Mean_loss: 0.09891404,  training time: 1.00
progress:  35%|[34m      [0m| 7/20 [00:09<00:13,  1.07s/it]progress:  40%|[34m      [0m| 8/20 [00:09<00:12,  1.05s/it]                                                        Episode 9	 reward: -3.45	 makespan: 341.50	 Mean_loss: 0.03194570,  training time: 1.05
progress:  40%|[34m      [0m| 8/20 [00:10<00:12,  1.05s/it]progress:  45%|[34m     [0m| 9/20 [00:10<00:11,  1.05s/it]                                                        Episode 10	 reward: -3.63	 makespan: 359.25	 Mean_loss: 0.04302953,  training time: 0.99
progress:  45%|[34m     [0m| 9/20 [00:11<00:11,  1.05s/it]progress:  50%|[34m     [0m| 10/20 [00:11<00:10,  1.03s/it]                                                         Episode 11	 reward: -3.62	 makespan: 358.25	 Mean_loss: 0.04465532,  training time: 0.99
progress:  50%|[34m     [0m| 10/20 [00:12<00:10,  1.03s/it]progress:  55%|[34m    [0m| 11/20 [00:12<00:09,  1.02s/it]                                                         Episode 12	 reward: -3.74	 makespan: 370.50	 Mean_loss: 0.03282014,  training time: 1.11
progress:  55%|[34m    [0m| 11/20 [00:13<00:09,  1.02s/it]progress:  60%|[34m    [0m| 12/20 [00:13<00:08,  1.05s/it]                                                         Episode 13	 reward: -3.47	 makespan: 343.75	 Mean_loss: 0.03106777,  training time: 1.01
progress:  60%|[34m    [0m| 12/20 [00:14<00:08,  1.05s/it]progress:  65%|[34m   [0m| 13/20 [00:14<00:07,  1.04s/it]                                                         Episode 14	 reward: -3.32	 makespan: 329.00	 Mean_loss: 0.05357410,  training time: 1.00
progress:  65%|[34m   [0m| 13/20 [00:15<00:07,  1.04s/it]progress:  70%|[34m   [0m| 14/20 [00:15<00:06,  1.03s/it]                                                         Episode 15	 reward: -3.80	 makespan: 376.25	 Mean_loss: 0.04414177,  training time: 0.99
progress:  70%|[34m   [0m| 14/20 [00:16<00:06,  1.03s/it]progress:  75%|[34m  [0m| 15/20 [00:16<00:05,  1.02s/it]                                                         Episode 16	 reward: -4.11	 makespan: 407.00	 Mean_loss: 0.03266796,  training time: 1.00
progress:  75%|[34m  [0m| 15/20 [00:17<00:05,  1.02s/it]progress:  80%|[34m  [0m| 16/20 [00:17<00:04,  1.01s/it]                                                         Episode 17	 reward: -4.00	 makespan: 396.25	 Mean_loss: 0.03151308,  training time: 1.03
progress:  80%|[34m  [0m| 16/20 [00:18<00:04,  1.01s/it]progress:  85%|[34m [0m| 17/20 [00:18<00:03,  1.02s/it]                                                         Episode 18	 reward: -4.20	 makespan: 416.25	 Mean_loss: 0.03421754,  training time: 0.98
progress:  85%|[34m [0m| 17/20 [00:19<00:03,  1.02s/it]progress:  90%|[34m [0m| 18/20 [00:19<00:02,  1.01s/it]                                                         Episode 19	 reward: -4.07	 makespan: 403.00	 Mean_loss: 0.03823067,  training time: 1.05
progress:  90%|[34m [0m| 18/20 [00:20<00:02,  1.01s/it]progress:  95%|[34m[0m| 19/20 [00:20<00:01,  1.02s/it]                                                         Episode 20	 reward: -4.07	 makespan: 403.00	 Mean_loss: 0.01541976,  training time: 1.03
progress:  95%|[34m[0m| 19/20 [00:21<00:01,  1.02s/it]progress: 100%|[34m[0m| 20/20 [00:21<00:00,  1.02s/it]progress: 100%|[34m[0m| 20/20 [00:21<00:00,  1.08s/it]
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp17/maml/finetuning/maml+exp17_500_512_3/15x7_7 --model_suffix free --finetuning_model maml+exp17_500_512_3 --max_updates 20 --n_j 15 --n_m 7 --op_per_job 7 --num_envs 4 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x7+mix
save model name:  15x7+mix+free
./trained_network/SD2/maml+exp17_500_512_3.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/maml+exp17_500_512_3.pth
vali data :./data/data_train_vali/SD2/15x7+mix


progress:   0%|[34m          [0m| 0/20 [00:00<?, ?it/s]                                                Episode 1	 reward: -6.63	 makespan: 656.25	 Mean_loss: 4.52421284,  training time: 2.73
progress:   0%|[34m          [0m| 0/20 [00:02<?, ?it/s]progress:   5%|[34m         [0m| 1/20 [00:02<00:51,  2.73s/it]                                                        Episode 2	 reward: -8.05	 makespan: 796.75	 Mean_loss: 1.36573362,  training time: 1.66
progress:   5%|[34m         [0m| 1/20 [00:04<00:51,  2.73s/it]progress:  10%|[34m         [0m| 2/20 [00:04<00:37,  2.10s/it]                                                        Episode 3	 reward: -8.04	 makespan: 796.00	 Mean_loss: 1.40675020,  training time: 1.68
progress:  10%|[34m         [0m| 2/20 [00:06<00:37,  2.10s/it]progress:  15%|[34m        [0m| 3/20 [00:06<00:32,  1.91s/it]                                                        Episode 4	 reward: -8.09	 makespan: 801.25	 Mean_loss: 0.59311396,  training time: 1.68
progress:  15%|[34m        [0m| 3/20 [00:07<00:32,  1.91s/it]progress:  20%|[34m        [0m| 4/20 [00:07<00:29,  1.82s/it]                                                        Episode 5	 reward: -7.39	 makespan: 731.75	 Mean_loss: 0.49432683,  training time: 1.63
progress:  20%|[34m        [0m| 4/20 [00:09<00:29,  1.82s/it]progress:  25%|[34m       [0m| 5/20 [00:09<00:26,  1.75s/it]                                                        Episode 6	 reward: -7.92	 makespan: 783.75	 Mean_loss: 0.40607342,  training time: 1.67
progress:  25%|[34m       [0m| 5/20 [00:11<00:26,  1.75s/it]progress:  30%|[34m       [0m| 6/20 [00:11<00:24,  1.72s/it]                                                        Episode 7	 reward: -7.60	 makespan: 752.50	 Mean_loss: 0.34708261,  training time: 1.64
progress:  30%|[34m       [0m| 6/20 [00:12<00:24,  1.72s/it]progress:  35%|[34m      [0m| 7/20 [00:12<00:22,  1.70s/it]                                                        Episode 8	 reward: -8.14	 makespan: 806.00	 Mean_loss: 0.34963754,  training time: 1.67
progress:  35%|[34m      [0m| 7/20 [00:14<00:22,  1.70s/it]progress:  40%|[34m      [0m| 8/20 [00:14<00:20,  1.69s/it]                                                        Episode 9	 reward: -8.52	 makespan: 843.25	 Mean_loss: 0.35558262,  training time: 1.65
progress:  40%|[34m      [0m| 8/20 [00:16<00:20,  1.69s/it]progress:  45%|[34m     [0m| 9/20 [00:16<00:18,  1.68s/it]                                                        Episode 10	 reward: -8.24	 makespan: 815.75	 Mean_loss: 0.33774716,  training time: 1.65
progress:  45%|[34m     [0m| 9/20 [00:17<00:18,  1.68s/it]progress:  50%|[34m     [0m| 10/20 [00:17<00:16,  1.67s/it]                                                         Episode 11	 reward: -8.14	 makespan: 805.75	 Mean_loss: 0.29773033,  training time: 1.69
progress:  50%|[34m     [0m| 10/20 [00:19<00:16,  1.67s/it]progress:  55%|[34m    [0m| 11/20 [00:19<00:15,  1.68s/it]                                                         Episode 12	 reward: -7.70	 makespan: 762.50	 Mean_loss: 0.32983595,  training time: 1.67
progress:  55%|[34m    [0m| 11/20 [00:21<00:15,  1.68s/it]progress:  60%|[34m    [0m| 12/20 [00:21<00:13,  1.67s/it]                                                         Episode 13	 reward: -8.01	 makespan: 792.50	 Mean_loss: 0.23179851,  training time: 1.75
progress:  60%|[34m    [0m| 12/20 [00:22<00:13,  1.67s/it]progress:  65%|[34m   [0m| 13/20 [00:22<00:11,  1.70s/it]                                                         Episode 14	 reward: -7.59	 makespan: 751.00	 Mean_loss: 0.24268140,  training time: 1.66
progress:  65%|[34m   [0m| 13/20 [00:24<00:11,  1.70s/it]progress:  70%|[34m   [0m| 14/20 [00:24<00:10,  1.69s/it]                                                         Episode 15	 reward: -7.96	 makespan: 787.75	 Mean_loss: 0.22761875,  training time: 1.68
progress:  70%|[34m   [0m| 14/20 [00:26<00:10,  1.69s/it]progress:  75%|[34m  [0m| 15/20 [00:26<00:08,  1.68s/it]                                                         Episode 16	 reward: -8.10	 makespan: 802.00	 Mean_loss: 0.19226122,  training time: 1.67
progress:  75%|[34m  [0m| 15/20 [00:27<00:08,  1.68s/it]progress:  80%|[34m  [0m| 16/20 [00:27<00:06,  1.68s/it]                                                         Episode 17	 reward: -8.19	 makespan: 810.50	 Mean_loss: 0.21957931,  training time: 1.68
progress:  80%|[34m  [0m| 16/20 [00:29<00:06,  1.68s/it]progress:  85%|[34m [0m| 17/20 [00:29<00:05,  1.68s/it]                                                         Episode 18	 reward: -8.65	 makespan: 856.25	 Mean_loss: 0.35532612,  training time: 1.68
progress:  85%|[34m [0m| 17/20 [00:31<00:05,  1.68s/it]progress:  90%|[34m [0m| 18/20 [00:31<00:03,  1.68s/it]                                                         Episode 19	 reward: -7.83	 makespan: 774.75	 Mean_loss: 0.30186450,  training time: 1.68
progress:  90%|[34m [0m| 18/20 [00:32<00:03,  1.68s/it]progress:  95%|[34m[0m| 19/20 [00:32<00:01,  1.68s/it]                                                         Episode 20	 reward: -8.07	 makespan: 799.25	 Mean_loss: 0.17959383,  training time: 1.68
progress:  95%|[34m[0m| 19/20 [00:34<00:01,  1.68s/it]progress: 100%|[34m[0m| 20/20 [00:34<00:00,  1.68s/it]progress: 100%|[34m[0m| 20/20 [00:34<00:00,  1.73s/it]
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp17/maml/finetuning/maml+exp17_500_512_3/15x7_10 --model_suffix free --finetuning_model maml+exp17_500_512_3 --max_updates 20 --n_j 15 --n_m 7 --op_per_job 10 --num_envs 4 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x7+mix
save model name:  15x7+mix+free
./trained_network/SD2/maml+exp17_500_512_3.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/maml+exp17_500_512_3.pth
vali data :./data/data_train_vali/SD2/15x7+mix


progress:   0%|[34m          [0m| 0/20 [00:00<?, ?it/s]                                                Episode 1	 reward: -9.79	 makespan: 969.00	 Mean_loss: 7.60232925,  training time: 3.42
progress:   0%|[34m          [0m| 0/20 [00:03<?, ?it/s]progress:   5%|[34m         [0m| 1/20 [00:03<01:04,  3.42s/it]                                                        Episode 2	 reward: -9.18	 makespan: 909.00	 Mean_loss: 0.65671653,  training time: 2.35
progress:   5%|[34m         [0m| 1/20 [00:05<01:04,  3.42s/it]progress:  10%|[34m         [0m| 2/20 [00:05<00:50,  2.79s/it]                                                        Episode 3	 reward: -9.28	 makespan: 918.25	 Mean_loss: 0.92677534,  training time: 2.35
progress:  10%|[34m         [0m| 2/20 [00:08<00:50,  2.79s/it]progress:  15%|[34m        [0m| 3/20 [00:08<00:44,  2.59s/it]                                                        Episode 4	 reward: -8.90	 makespan: 880.75	 Mean_loss: 0.61289001,  training time: 2.35
progress:  15%|[34m        [0m| 3/20 [00:10<00:44,  2.59s/it]progress:  20%|[34m        [0m| 4/20 [00:10<00:39,  2.49s/it]                                                        Episode 5	 reward: -9.24	 makespan: 914.75	 Mean_loss: 0.60912323,  training time: 2.34
progress:  20%|[34m        [0m| 4/20 [00:12<00:39,  2.49s/it]progress:  25%|[34m       [0m| 5/20 [00:12<00:36,  2.44s/it]                                                        Episode 6	 reward: -9.33	 makespan: 923.50	 Mean_loss: 0.69102502,  training time: 2.46
progress:  25%|[34m       [0m| 5/20 [00:15<00:36,  2.44s/it]progress:  30%|[34m       [0m| 6/20 [00:15<00:34,  2.45s/it]                                                        Episode 7	 reward: -9.37	 makespan: 928.00	 Mean_loss: 0.36925921,  training time: 2.34
progress:  30%|[34m       [0m| 6/20 [00:17<00:34,  2.45s/it]progress:  35%|[34m      [0m| 7/20 [00:17<00:31,  2.41s/it]                                                        Episode 8	 reward: -9.41	 makespan: 932.00	 Mean_loss: 0.48534191,  training time: 2.35
progress:  35%|[34m      [0m| 7/20 [00:19<00:31,  2.41s/it]progress:  40%|[34m      [0m| 8/20 [00:19<00:28,  2.40s/it]                                                        Episode 9	 reward: -9.24	 makespan: 914.50	 Mean_loss: 0.35609382,  training time: 2.35
progress:  40%|[34m      [0m| 8/20 [00:22<00:28,  2.40s/it]progress:  45%|[34m     [0m| 9/20 [00:22<00:26,  2.38s/it]                                                        Episode 10	 reward: -9.75	 makespan: 965.25	 Mean_loss: 0.41117495,  training time: 2.34
progress:  45%|[34m     [0m| 9/20 [00:24<00:26,  2.38s/it]progress:  50%|[34m     [0m| 10/20 [00:24<00:23,  2.37s/it]                                                         Episode 11	 reward: -9.72	 makespan: 962.75	 Mean_loss: 0.38675743,  training time: 2.35
progress:  50%|[34m     [0m| 10/20 [00:27<00:23,  2.37s/it]progress:  55%|[34m    [0m| 11/20 [00:27<00:21,  2.36s/it]                                                         Episode 12	 reward: -9.53	 makespan: 943.75	 Mean_loss: 0.29471481,  training time: 2.36
progress:  55%|[34m    [0m| 11/20 [00:29<00:21,  2.36s/it]progress:  60%|[34m    [0m| 12/20 [00:29<00:18,  2.36s/it]                                                         Episode 13	 reward: -9.61	 makespan: 951.50	 Mean_loss: 0.23608240,  training time: 2.34
progress:  60%|[34m    [0m| 12/20 [00:31<00:18,  2.36s/it]progress:  65%|[34m   [0m| 13/20 [00:31<00:16,  2.36s/it]                                                         Episode 14	 reward: -9.97	 makespan: 986.75	 Mean_loss: 0.16573355,  training time: 2.35
progress:  65%|[34m   [0m| 13/20 [00:34<00:16,  2.36s/it]progress:  70%|[34m   [0m| 14/20 [00:34<00:14,  2.35s/it]                                                         Episode 15	 reward: -10.25	 makespan: 1014.50	 Mean_loss: 0.22760123,  training time: 2.35
progress:  70%|[34m   [0m| 14/20 [00:36<00:14,  2.35s/it]progress:  75%|[34m  [0m| 15/20 [00:36<00:11,  2.35s/it]                                                         Episode 16	 reward: -9.89	 makespan: 979.25	 Mean_loss: 0.22320610,  training time: 2.34
progress:  75%|[34m  [0m| 15/20 [00:38<00:11,  2.35s/it]progress:  80%|[34m  [0m| 16/20 [00:38<00:09,  2.35s/it]                                                         Episode 17	 reward: -9.75	 makespan: 965.00	 Mean_loss: 0.21005824,  training time: 2.35
progress:  80%|[34m  [0m| 16/20 [00:41<00:09,  2.35s/it]progress:  85%|[34m [0m| 17/20 [00:41<00:07,  2.35s/it]                                                         Episode 18	 reward: -9.89	 makespan: 979.50	 Mean_loss: 0.20416062,  training time: 2.34
progress:  85%|[34m [0m| 17/20 [00:43<00:07,  2.35s/it]progress:  90%|[34m [0m| 18/20 [00:43<00:04,  2.35s/it]                                                         Episode 19	 reward: -9.77	 makespan: 967.25	 Mean_loss: 0.17207514,  training time: 2.34
progress:  90%|[34m [0m| 18/20 [00:45<00:04,  2.35s/it]progress:  95%|[34m[0m| 19/20 [00:45<00:02,  2.35s/it]                                                         Episode 20	 reward: -9.43	 makespan: 933.25	 Mean_loss: 0.17811759,  training time: 2.35
progress:  95%|[34m[0m| 19/20 [00:48<00:02,  2.35s/it]progress: 100%|[34m[0m| 20/20 [00:48<00:00,  2.35s/it]progress: 100%|[34m[0m| 20/20 [00:48<00:00,  2.41s/it]
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp17/maml/finetuning/maml+exp17_500_512_3/15x7_12 --model_suffix free --finetuning_model maml+exp17_500_512_3 --max_updates 20 --n_j 15 --n_m 7 --op_per_job 12 --num_envs 4 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x7+mix
save model name:  15x7+mix+free
./trained_network/SD2/maml+exp17_500_512_3.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/maml+exp17_500_512_3.pth
vali data :./data/data_train_vali/SD2/15x7+mix


progress:   0%|[34m          [0m| 0/20 [00:00<?, ?it/s]                                                Episode 1	 reward: -12.62	 makespan: 1249.25	 Mean_loss: 7.25444984,  training time: 3.96
progress:   0%|[34m          [0m| 0/20 [00:03<?, ?it/s]progress:   5%|[34m         [0m| 1/20 [00:03<01:15,  3.97s/it]                                                        Episode 2	 reward: -11.97	 makespan: 1185.50	 Mean_loss: 0.93768126,  training time: 2.81
progress:   5%|[34m         [0m| 1/20 [00:06<01:15,  3.97s/it]progress:  10%|[34m         [0m| 2/20 [00:06<00:59,  3.29s/it]                                                        Episode 3	 reward: -12.57	 makespan: 1244.25	 Mean_loss: 1.20622361,  training time: 2.75
progress:  10%|[34m         [0m| 2/20 [00:09<00:59,  3.29s/it]progress:  15%|[34m        [0m| 3/20 [00:09<00:51,  3.04s/it]                                                        Episode 4	 reward: -11.76	 makespan: 1163.75	 Mean_loss: 1.06194699,  training time: 2.77
progress:  15%|[34m        [0m| 3/20 [00:12<00:51,  3.04s/it]progress:  20%|[34m        [0m| 4/20 [00:12<00:46,  2.94s/it]                                                        Episode 5	 reward: -11.56	 makespan: 1144.50	 Mean_loss: 1.07380247,  training time: 2.88
progress:  20%|[34m        [0m| 4/20 [00:15<00:46,  2.94s/it]progress:  25%|[34m       [0m| 5/20 [00:15<00:43,  2.92s/it]                                                        Episode 6	 reward: -12.66	 makespan: 1253.75	 Mean_loss: 0.75961322,  training time: 2.76
progress:  25%|[34m       [0m| 5/20 [00:17<00:43,  2.92s/it]progress:  30%|[34m       [0m| 6/20 [00:17<00:40,  2.86s/it]                                                        Episode 7	 reward: -12.37	 makespan: 1224.50	 Mean_loss: 0.83990866,  training time: 2.80
progress:  30%|[34m       [0m| 6/20 [00:20<00:40,  2.86s/it]progress:  35%|[34m      [0m| 7/20 [00:20<00:36,  2.84s/it]                                                        Episode 8	 reward: -11.38	 makespan: 1126.25	 Mean_loss: 0.80016327,  training time: 2.75
progress:  35%|[34m      [0m| 7/20 [00:23<00:36,  2.84s/it]progress:  40%|[34m      [0m| 8/20 [00:23<00:33,  2.81s/it]                                                        Episode 9	 reward: -13.07	 makespan: 1294.25	 Mean_loss: 0.60421073,  training time: 2.75
progress:  40%|[34m      [0m| 8/20 [00:26<00:33,  2.81s/it]progress:  45%|[34m     [0m| 9/20 [00:26<00:30,  2.79s/it]                                                        Episode 10	 reward: -12.26	 makespan: 1213.50	 Mean_loss: 0.60407329,  training time: 2.76
progress:  45%|[34m     [0m| 9/20 [00:29<00:30,  2.79s/it]progress:  50%|[34m     [0m| 10/20 [00:29<00:27,  2.78s/it]                                                         Episode 11	 reward: -12.01	 makespan: 1188.75	 Mean_loss: 0.43406922,  training time: 2.77
progress:  50%|[34m     [0m| 10/20 [00:31<00:27,  2.78s/it]progress:  55%|[34m    [0m| 11/20 [00:31<00:25,  2.78s/it]                                                         Episode 12	 reward: -12.26	 makespan: 1213.75	 Mean_loss: 0.50754046,  training time: 2.77
progress:  55%|[34m    [0m| 11/20 [00:34<00:25,  2.78s/it]progress:  60%|[34m    [0m| 12/20 [00:34<00:22,  2.78s/it]                                                         Episode 13	 reward: -11.91	 makespan: 1178.75	 Mean_loss: 0.64349955,  training time: 2.75
progress:  60%|[34m    [0m| 12/20 [00:37<00:22,  2.78s/it]progress:  65%|[34m   [0m| 13/20 [00:37<00:19,  2.77s/it]                                                         Episode 14	 reward: -12.78	 makespan: 1265.25	 Mean_loss: 0.39992458,  training time: 2.76
progress:  65%|[34m   [0m| 13/20 [00:40<00:19,  2.77s/it]progress:  70%|[34m   [0m| 14/20 [00:40<00:16,  2.77s/it]                                                         Episode 15	 reward: -12.58	 makespan: 1245.25	 Mean_loss: 0.47115606,  training time: 2.76
progress:  70%|[34m   [0m| 14/20 [00:42<00:16,  2.77s/it]progress:  75%|[34m  [0m| 15/20 [00:42<00:13,  2.77s/it]                                                         Episode 16	 reward: -13.87	 makespan: 1373.00	 Mean_loss: 0.40534073,  training time: 2.74
progress:  75%|[34m  [0m| 15/20 [00:45<00:13,  2.77s/it]progress:  80%|[34m  [0m| 16/20 [00:45<00:11,  2.76s/it]                                                         Episode 17	 reward: -13.05	 makespan: 1292.25	 Mean_loss: 0.39506847,  training time: 2.75
progress:  80%|[34m  [0m| 16/20 [00:48<00:11,  2.76s/it]progress:  85%|[34m [0m| 17/20 [00:48<00:08,  2.76s/it]                                                         Episode 18	 reward: -13.66	 makespan: 1352.00	 Mean_loss: 0.36440384,  training time: 2.76
progress:  85%|[34m [0m| 17/20 [00:51<00:08,  2.76s/it]progress:  90%|[34m [0m| 18/20 [00:51<00:05,  2.76s/it]                                                         Episode 19	 reward: -13.58	 makespan: 1344.50	 Mean_loss: 0.24755509,  training time: 2.74
progress:  90%|[34m [0m| 18/20 [00:53<00:05,  2.76s/it]progress:  95%|[34m[0m| 19/20 [00:53<00:02,  2.75s/it]                                                         Episode 20	 reward: -13.65	 makespan: 1351.25	 Mean_loss: 0.52476585,  training time: 2.74
progress:  95%|[34m[0m| 19/20 [00:56<00:02,  2.75s/it]progress: 100%|[34m[0m| 20/20 [00:56<00:00,  2.75s/it]progress: 100%|[34m[0m| 20/20 [00:56<00:00,  2.83s/it]
+ for n_m in '$n_m_options'
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp17/maml/finetuning/maml+exp17_500_512_3/15x5_4 --model_suffix free --finetuning_model maml+exp17_500_512_3 --max_updates 20 --n_j 15 --n_m 5 --op_per_job 4 --num_envs 4 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x5+mix
save model name:  15x5+mix+free
./trained_network/SD2/maml+exp17_500_512_3.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/maml+exp17_500_512_3.pth
vali data :./data/data_train_vali/SD2/15x5+mix


progress:   0%|[34m          [0m| 0/20 [00:00<?, ?it/s]                                                Episode 1	 reward: -5.76	 makespan: 569.75	 Mean_loss: 2.59250975,  training time: 2.19
progress:   0%|[34m          [0m| 0/20 [00:02<?, ?it/s]progress:   5%|[34m         [0m| 1/20 [00:02<00:41,  2.19s/it]                                                        Episode 2	 reward: -6.53	 makespan: 646.50	 Mean_loss: 1.15264881,  training time: 1.05
progress:   5%|[34m         [0m| 1/20 [00:03<00:41,  2.19s/it]progress:  10%|[34m         [0m| 2/20 [00:03<00:27,  1.52s/it]                                                        Episode 3	 reward: -5.93	 makespan: 587.25	 Mean_loss: 0.62745488,  training time: 1.07
progress:  10%|[34m         [0m| 2/20 [00:04<00:27,  1.52s/it]progress:  15%|[34m        [0m| 3/20 [00:04<00:22,  1.32s/it]                                                        Episode 4	 reward: -5.80	 makespan: 574.50	 Mean_loss: 0.38812444,  training time: 1.01
progress:  15%|[34m        [0m| 3/20 [00:05<00:22,  1.32s/it]progress:  20%|[34m        [0m| 4/20 [00:05<00:19,  1.19s/it]                                                        Episode 5	 reward: -6.03	 makespan: 596.75	 Mean_loss: 0.24505666,  training time: 1.00
progress:  20%|[34m        [0m| 4/20 [00:06<00:19,  1.19s/it]progress:  25%|[34m       [0m| 5/20 [00:06<00:16,  1.13s/it]                                                        Episode 6	 reward: -5.91	 makespan: 584.75	 Mean_loss: 0.22549593,  training time: 1.00
progress:  25%|[34m       [0m| 5/20 [00:07<00:16,  1.13s/it]progress:  30%|[34m       [0m| 6/20 [00:07<00:15,  1.08s/it]                                                        Episode 7	 reward: -6.17	 makespan: 611.00	 Mean_loss: 0.23757187,  training time: 1.06
progress:  30%|[34m       [0m| 6/20 [00:08<00:15,  1.08s/it]progress:  35%|[34m      [0m| 7/20 [00:08<00:13,  1.07s/it]                                                        Episode 8	 reward: -5.95	 makespan: 589.25	 Mean_loss: 0.20459753,  training time: 1.07
progress:  35%|[34m      [0m| 7/20 [00:09<00:13,  1.07s/it]progress:  40%|[34m      [0m| 8/20 [00:09<00:12,  1.07s/it]                                                        Episode 9	 reward: -6.01	 makespan: 595.00	 Mean_loss: 0.20070246,  training time: 1.06
progress:  40%|[34m      [0m| 8/20 [00:10<00:12,  1.07s/it]progress:  45%|[34m     [0m| 9/20 [00:10<00:11,  1.07s/it]                                                        Episode 10	 reward: -6.16	 makespan: 609.75	 Mean_loss: 0.25163028,  training time: 1.05
progress:  45%|[34m     [0m| 9/20 [00:11<00:11,  1.07s/it]progress:  50%|[34m     [0m| 10/20 [00:11<00:10,  1.06s/it]                                                         Episode 11	 reward: -5.78	 makespan: 572.00	 Mean_loss: 0.14282405,  training time: 1.04
progress:  50%|[34m     [0m| 10/20 [00:12<00:10,  1.06s/it]progress:  55%|[34m    [0m| 11/20 [00:12<00:09,  1.06s/it]                                                         Episode 12	 reward: -6.19	 makespan: 613.00	 Mean_loss: 0.13428240,  training time: 1.17
progress:  55%|[34m    [0m| 11/20 [00:13<00:09,  1.06s/it]progress:  60%|[34m    [0m| 12/20 [00:13<00:08,  1.09s/it]                                                         Episode 13	 reward: -5.72	 makespan: 566.00	 Mean_loss: 0.14621703,  training time: 1.06
progress:  60%|[34m    [0m| 12/20 [00:14<00:08,  1.09s/it]progress:  65%|[34m   [0m| 13/20 [00:14<00:07,  1.08s/it]                                                         Episode 14	 reward: -6.12	 makespan: 606.00	 Mean_loss: 0.13854837,  training time: 1.01
progress:  65%|[34m   [0m| 13/20 [00:15<00:07,  1.08s/it]progress:  70%|[34m   [0m| 14/20 [00:15<00:06,  1.06s/it]                                                         Episode 15	 reward: -6.56	 makespan: 649.00	 Mean_loss: 0.23668610,  training time: 1.00
progress:  70%|[34m   [0m| 14/20 [00:16<00:06,  1.06s/it]progress:  75%|[34m  [0m| 15/20 [00:16<00:05,  1.04s/it]                                                         Episode 16	 reward: -6.51	 makespan: 644.25	 Mean_loss: 0.13584119,  training time: 0.99
progress:  75%|[34m  [0m| 15/20 [00:17<00:05,  1.04s/it]progress:  80%|[34m  [0m| 16/20 [00:17<00:04,  1.03s/it]                                                         Episode 17	 reward: -6.26	 makespan: 620.00	 Mean_loss: 0.16850632,  training time: 1.00
progress:  80%|[34m  [0m| 16/20 [00:18<00:04,  1.03s/it]progress:  85%|[34m [0m| 17/20 [00:18<00:03,  1.02s/it]                                                         Episode 18	 reward: -6.22	 makespan: 616.00	 Mean_loss: 0.14422153,  training time: 1.03
progress:  85%|[34m [0m| 17/20 [00:19<00:03,  1.02s/it]progress:  90%|[34m [0m| 18/20 [00:19<00:02,  1.02s/it]                                                         Episode 19	 reward: -6.56	 makespan: 649.75	 Mean_loss: 0.16083044,  training time: 1.07
progress:  90%|[34m [0m| 18/20 [00:20<00:02,  1.02s/it]progress:  95%|[34m[0m| 19/20 [00:20<00:01,  1.04s/it]                                                         Episode 20	 reward: -6.09	 makespan: 602.50	 Mean_loss: 0.12389852,  training time: 1.05
progress:  95%|[34m[0m| 19/20 [00:22<00:01,  1.04s/it]progress: 100%|[34m[0m| 20/20 [00:22<00:00,  1.04s/it]progress: 100%|[34m[0m| 20/20 [00:22<00:00,  1.10s/it]
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp17/maml/finetuning/maml+exp17_500_512_3/15x5_7 --model_suffix free --finetuning_model maml+exp17_500_512_3 --max_updates 20 --n_j 15 --n_m 5 --op_per_job 7 --num_envs 4 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x5+mix
save model name:  15x5+mix+free
./trained_network/SD2/maml+exp17_500_512_3.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/maml+exp17_500_512_3.pth
vali data :./data/data_train_vali/SD2/15x5+mix


progress:   0%|[34m          [0m| 0/20 [00:00<?, ?it/s]                                                Episode 1	 reward: -8.85	 makespan: 876.00	 Mean_loss: 6.91230392,  training time: 2.83
progress:   0%|[34m          [0m| 0/20 [00:02<?, ?it/s]progress:   5%|[34m         [0m| 1/20 [00:02<00:53,  2.83s/it]                                                        Episode 2	 reward: -8.92	 makespan: 883.50	 Mean_loss: 1.02557039,  training time: 1.69
progress:   5%|[34m         [0m| 1/20 [00:04<00:53,  2.83s/it]progress:  10%|[34m         [0m| 2/20 [00:04<00:38,  2.16s/it]                                                        Episode 3	 reward: -9.59	 makespan: 949.50	 Mean_loss: 1.76168251,  training time: 1.70
progress:  10%|[34m         [0m| 2/20 [00:06<00:38,  2.16s/it]progress:  15%|[34m        [0m| 3/20 [00:06<00:33,  1.95s/it]                                                        Episode 4	 reward: -9.71	 makespan: 961.00	 Mean_loss: 1.42609000,  training time: 1.73
progress:  15%|[34m        [0m| 3/20 [00:07<00:33,  1.95s/it]progress:  20%|[34m        [0m| 4/20 [00:07<00:29,  1.87s/it]                                                        Episode 5	 reward: -10.39	 makespan: 1028.75	 Mean_loss: 1.00634146,  training time: 1.69
progress:  20%|[34m        [0m| 4/20 [00:09<00:29,  1.87s/it]progress:  25%|[34m       [0m| 5/20 [00:09<00:27,  1.80s/it]                                                        Episode 6	 reward: -10.69	 makespan: 1058.75	 Mean_loss: 1.04751539,  training time: 1.76
progress:  25%|[34m       [0m| 5/20 [00:11<00:27,  1.80s/it]progress:  30%|[34m       [0m| 6/20 [00:11<00:25,  1.79s/it]                                                        Episode 7	 reward: -10.13	 makespan: 1002.75	 Mean_loss: 0.95974946,  training time: 1.70
progress:  30%|[34m       [0m| 6/20 [00:13<00:25,  1.79s/it]progress:  35%|[34m      [0m| 7/20 [00:13<00:22,  1.76s/it]                                                        Episode 8	 reward: -11.19	 makespan: 1108.25	 Mean_loss: 0.94769412,  training time: 1.71
progress:  35%|[34m      [0m| 7/20 [00:14<00:22,  1.76s/it]progress:  40%|[34m      [0m| 8/20 [00:14<00:20,  1.74s/it]                                                        Episode 9	 reward: -11.47	 makespan: 1135.75	 Mean_loss: 0.90096658,  training time: 1.75
progress:  40%|[34m      [0m| 8/20 [00:16<00:20,  1.74s/it]progress:  45%|[34m     [0m| 9/20 [00:16<00:19,  1.75s/it]                                                        Episode 10	 reward: -11.01	 makespan: 1090.00	 Mean_loss: 0.72494811,  training time: 1.72
progress:  45%|[34m     [0m| 9/20 [00:18<00:19,  1.75s/it]progress:  50%|[34m     [0m| 10/20 [00:18<00:17,  1.74s/it]                                                         Episode 11	 reward: -10.56	 makespan: 1045.25	 Mean_loss: 0.92651474,  training time: 1.72
progress:  50%|[34m     [0m| 10/20 [00:20<00:17,  1.74s/it]progress:  55%|[34m    [0m| 11/20 [00:20<00:15,  1.73s/it]                                                         Episode 12	 reward: -10.93	 makespan: 1082.25	 Mean_loss: 0.90177083,  training time: 1.70
progress:  55%|[34m    [0m| 11/20 [00:21<00:15,  1.73s/it]progress:  60%|[34m    [0m| 12/20 [00:21<00:13,  1.73s/it]                                                         Episode 13	 reward: -10.84	 makespan: 1072.75	 Mean_loss: 0.54434198,  training time: 1.82
progress:  60%|[34m    [0m| 12/20 [00:23<00:13,  1.73s/it]progress:  65%|[34m   [0m| 13/20 [00:23<00:12,  1.75s/it]                                                         Episode 14	 reward: -10.98	 makespan: 1087.25	 Mean_loss: 0.60572779,  training time: 1.70
progress:  65%|[34m   [0m| 13/20 [00:25<00:12,  1.75s/it]progress:  70%|[34m   [0m| 14/20 [00:25<00:10,  1.74s/it]                                                         Episode 15	 reward: -10.96	 makespan: 1085.25	 Mean_loss: 0.49902773,  training time: 1.71
progress:  70%|[34m   [0m| 14/20 [00:26<00:10,  1.74s/it]progress:  75%|[34m  [0m| 15/20 [00:26<00:08,  1.73s/it]                                                         Episode 16	 reward: -10.98	 makespan: 1087.50	 Mean_loss: 0.68282425,  training time: 1.71
progress:  75%|[34m  [0m| 15/20 [00:28<00:08,  1.73s/it]progress:  80%|[34m  [0m| 16/20 [00:28<00:06,  1.72s/it]                                                         Episode 17	 reward: -11.20	 makespan: 1108.50	 Mean_loss: 0.47035819,  training time: 1.70
progress:  80%|[34m  [0m| 16/20 [00:30<00:06,  1.72s/it]progress:  85%|[34m [0m| 17/20 [00:30<00:05,  1.72s/it]                                                         Episode 18	 reward: -11.09	 makespan: 1098.25	 Mean_loss: 0.68850917,  training time: 1.71
progress:  85%|[34m [0m| 17/20 [00:32<00:05,  1.72s/it]progress:  90%|[34m [0m| 18/20 [00:32<00:03,  1.72s/it]                                                         Episode 19	 reward: -10.73	 makespan: 1062.25	 Mean_loss: 0.53921592,  training time: 1.71
progress:  90%|[34m [0m| 18/20 [00:33<00:03,  1.72s/it]progress:  95%|[34m[0m| 19/20 [00:33<00:01,  1.71s/it]                                                         Episode 20	 reward: -10.21	 makespan: 1011.00	 Mean_loss: 0.61514628,  training time: 1.71
progress:  95%|[34m[0m| 19/20 [00:35<00:01,  1.71s/it]progress: 100%|[34m[0m| 20/20 [00:35<00:00,  1.71s/it]progress: 100%|[34m[0m| 20/20 [00:35<00:00,  1.78s/it]
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp17/maml/finetuning/maml+exp17_500_512_3/15x5_10 --model_suffix free --finetuning_model maml+exp17_500_512_3 --max_updates 20 --n_j 15 --n_m 5 --op_per_job 10 --num_envs 4 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x5+mix
save model name:  15x5+mix+free
./trained_network/SD2/maml+exp17_500_512_3.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/maml+exp17_500_512_3.pth
vali data :./data/data_train_vali/SD2/15x5+mix


progress:   0%|[34m          [0m| 0/20 [00:00<?, ?it/s]                                                Episode 1	 reward: -12.92	 makespan: 1278.75	 Mean_loss: 6.99779081,  training time: 3.51
progress:   0%|[34m          [0m| 0/20 [00:03<?, ?it/s]progress:   5%|[34m         [0m| 1/20 [00:03<01:06,  3.51s/it]                                                        Episode 2	 reward: -11.95	 makespan: 1183.00	 Mean_loss: 1.13559794,  training time: 2.31
progress:   5%|[34m         [0m| 1/20 [00:05<01:06,  3.51s/it]progress:  10%|[34m         [0m| 2/20 [00:05<00:50,  2.81s/it]                                                        Episode 3	 reward: -12.28	 makespan: 1216.00	 Mean_loss: 1.37875032,  training time: 2.32
progress:  10%|[34m         [0m| 2/20 [00:08<00:50,  2.81s/it]progress:  15%|[34m        [0m| 3/20 [00:08<00:43,  2.58s/it]                                                        Episode 4	 reward: -11.80	 makespan: 1168.25	 Mean_loss: 1.23617411,  training time: 2.31
progress:  15%|[34m        [0m| 3/20 [00:10<00:43,  2.58s/it]progress:  20%|[34m        [0m| 4/20 [00:10<00:39,  2.48s/it]                                                        Episode 5	 reward: -12.09	 makespan: 1196.75	 Mean_loss: 1.03232491,  training time: 2.34
progress:  20%|[34m        [0m| 4/20 [00:12<00:39,  2.48s/it]progress:  25%|[34m       [0m| 5/20 [00:12<00:36,  2.43s/it]                                                        Episode 6	 reward: -11.92	 makespan: 1180.00	 Mean_loss: 0.99832720,  training time: 2.49
progress:  25%|[34m       [0m| 5/20 [00:15<00:36,  2.43s/it]progress:  30%|[34m       [0m| 6/20 [00:15<00:34,  2.45s/it]                                                        Episode 7	 reward: -11.68	 makespan: 1156.50	 Mean_loss: 0.75770760,  training time: 2.33
progress:  30%|[34m       [0m| 6/20 [00:17<00:34,  2.45s/it]progress:  35%|[34m      [0m| 7/20 [00:17<00:31,  2.41s/it]                                                        Episode 8	 reward: -12.12	 makespan: 1200.00	 Mean_loss: 0.54875827,  training time: 2.32
progress:  35%|[34m      [0m| 7/20 [00:19<00:31,  2.41s/it]progress:  40%|[34m      [0m| 8/20 [00:19<00:28,  2.38s/it]                                                        Episode 9	 reward: -11.97	 makespan: 1185.50	 Mean_loss: 0.41353542,  training time: 2.34
progress:  40%|[34m      [0m| 8/20 [00:22<00:28,  2.38s/it]progress:  45%|[34m     [0m| 9/20 [00:22<00:26,  2.37s/it]                                                        Episode 10	 reward: -11.93	 makespan: 1181.25	 Mean_loss: 0.38377267,  training time: 2.33
progress:  45%|[34m     [0m| 9/20 [00:24<00:26,  2.37s/it]progress:  50%|[34m     [0m| 10/20 [00:24<00:23,  2.36s/it]                                                         Episode 11	 reward: -11.65	 makespan: 1153.00	 Mean_loss: 0.30187649,  training time: 2.32
progress:  50%|[34m     [0m| 10/20 [00:26<00:23,  2.36s/it]progress:  55%|[34m    [0m| 11/20 [00:26<00:21,  2.35s/it]                                                         Episode 12	 reward: -12.03	 makespan: 1191.00	 Mean_loss: 0.26972798,  training time: 2.32
progress:  55%|[34m    [0m| 11/20 [00:29<00:21,  2.35s/it]progress:  60%|[34m    [0m| 12/20 [00:29<00:18,  2.34s/it]                                                         Episode 13	 reward: -11.85	 makespan: 1173.25	 Mean_loss: 0.38686043,  training time: 2.32
progress:  60%|[34m    [0m| 12/20 [00:31<00:18,  2.34s/it]progress:  65%|[34m   [0m| 13/20 [00:31<00:16,  2.34s/it]                                                         Episode 14	 reward: -11.94	 makespan: 1182.00	 Mean_loss: 0.32530779,  training time: 2.33
progress:  65%|[34m   [0m| 13/20 [00:33<00:16,  2.34s/it]progress:  70%|[34m   [0m| 14/20 [00:33<00:14,  2.33s/it]                                                         Episode 15	 reward: -11.96	 makespan: 1183.75	 Mean_loss: 0.29757199,  training time: 2.34
progress:  70%|[34m   [0m| 14/20 [00:36<00:14,  2.33s/it]progress:  75%|[34m  [0m| 15/20 [00:36<00:11,  2.33s/it]                                                         Episode 16	 reward: -12.20	 makespan: 1208.25	 Mean_loss: 0.23476763,  training time: 2.32
progress:  75%|[34m  [0m| 15/20 [00:38<00:11,  2.33s/it]progress:  80%|[34m  [0m| 16/20 [00:38<00:09,  2.33s/it]                                                         Episode 17	 reward: -12.20	 makespan: 1207.75	 Mean_loss: 0.22050384,  training time: 2.32
progress:  80%|[34m  [0m| 16/20 [00:40<00:09,  2.33s/it]progress:  85%|[34m [0m| 17/20 [00:40<00:06,  2.33s/it]                                                         Episode 18	 reward: -11.75	 makespan: 1163.25	 Mean_loss: 0.23337571,  training time: 2.33
progress:  85%|[34m [0m| 17/20 [00:43<00:06,  2.33s/it]progress:  90%|[34m [0m| 18/20 [00:43<00:04,  2.33s/it]                                                         Episode 19	 reward: -12.28	 makespan: 1215.25	 Mean_loss: 0.26308563,  training time: 2.34
progress:  90%|[34m [0m| 18/20 [00:45<00:04,  2.33s/it]progress:  95%|[34m[0m| 19/20 [00:45<00:02,  2.33s/it]                                                         Episode 20	 reward: -12.00	 makespan: 1188.25	 Mean_loss: 0.20401123,  training time: 2.31
progress:  95%|[34m[0m| 19/20 [00:47<00:02,  2.33s/it]progress: 100%|[34m[0m| 20/20 [00:47<00:00,  2.33s/it]progress: 100%|[34m[0m| 20/20 [00:47<00:00,  2.39s/it]
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp17/maml/finetuning/maml+exp17_500_512_3/15x5_12 --model_suffix free --finetuning_model maml+exp17_500_512_3 --max_updates 20 --n_j 15 --n_m 5 --op_per_job 12 --num_envs 4 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/15x5+mix
save model name:  15x5+mix+free
./trained_network/SD2/maml+exp17_500_512_3.pth
-------------------------Training Setting-------------------------
source : SD2
model name :./trained_network/SD2/maml+exp17_500_512_3.pth
vali data :./data/data_train_vali/SD2/15x5+mix


progress:   0%|[34m          [0m| 0/20 [00:00<?, ?it/s]                                                Episode 1	 reward: -15.31	 makespan: 1515.50	 Mean_loss: 7.41403818,  training time: 3.93
progress:   0%|[34m          [0m| 0/20 [00:03<?, ?it/s]progress:   5%|[34m         [0m| 1/20 [00:03<01:14,  3.93s/it]                                                        Episode 2	 reward: -16.50	 makespan: 1633.50	 Mean_loss: 2.24336100,  training time: 2.78
progress:   5%|[34m         [0m| 1/20 [00:06<01:14,  3.93s/it]progress:  10%|[34m         [0m| 2/20 [00:06<00:58,  3.25s/it]                                                        Episode 3	 reward: -16.96	 makespan: 1679.00	 Mean_loss: 2.36009574,  training time: 2.77
progress:  10%|[34m         [0m| 2/20 [00:09<00:58,  3.25s/it]progress:  15%|[34m        [0m| 3/20 [00:09<00:51,  3.03s/it]                                                        Episode 4	 reward: -16.26	 makespan: 1609.50	 Mean_loss: 2.12500572,  training time: 2.77
progress:  15%|[34m        [0m| 3/20 [00:12<00:51,  3.03s/it]progress:  20%|[34m        [0m| 4/20 [00:12<00:46,  2.93s/it]                                                        Episode 5	 reward: -15.30	 makespan: 1514.50	 Mean_loss: 1.96313667,  training time: 2.88
progress:  20%|[34m        [0m| 4/20 [00:15<00:46,  2.93s/it]progress:  25%|[34m       [0m| 5/20 [00:15<00:43,  2.91s/it]                                                        Episode 6	 reward: -15.10	 makespan: 1495.00	 Mean_loss: 1.97336805,  training time: 2.77
progress:  25%|[34m       [0m| 5/20 [00:17<00:43,  2.91s/it]progress:  30%|[34m       [0m| 6/20 [00:17<00:40,  2.87s/it]                                                        Episode 7	 reward: -15.23	 makespan: 1507.75	 Mean_loss: 1.52152407,  training time: 2.76
progress:  30%|[34m       [0m| 6/20 [00:20<00:40,  2.87s/it]progress:  35%|[34m      [0m| 7/20 [00:20<00:36,  2.83s/it]                                                        Episode 8	 reward: -15.81	 makespan: 1565.50	 Mean_loss: 1.06799352,  training time: 2.76
progress:  35%|[34m      [0m| 7/20 [00:23<00:36,  2.83s/it]progress:  40%|[34m      [0m| 8/20 [00:23<00:33,  2.81s/it]                                                        Episode 9	 reward: -15.53	 makespan: 1537.50	 Mean_loss: 1.37123156,  training time: 2.76
progress:  40%|[34m      [0m| 8/20 [00:26<00:33,  2.81s/it]progress:  45%|[34m     [0m| 9/20 [00:26<00:30,  2.80s/it]                                                        Episode 10	 reward: -16.02	 makespan: 1585.75	 Mean_loss: 0.98436069,  training time: 2.77
progress:  45%|[34m     [0m| 9/20 [00:28<00:30,  2.80s/it]progress:  50%|[34m     [0m| 10/20 [00:28<00:27,  2.79s/it]                                                         Episode 11	 reward: -16.41	 makespan: 1624.50	 Mean_loss: 0.89162838,  training time: 2.77
progress:  50%|[34m     [0m| 10/20 [00:31<00:27,  2.79s/it]progress:  55%|[34m    [0m| 11/20 [00:31<00:25,  2.78s/it]                                                         Episode 12	 reward: -16.12	 makespan: 1595.75	 Mean_loss: 0.90485752,  training time: 2.76
progress:  55%|[34m    [0m| 11/20 [00:34<00:25,  2.78s/it]progress:  60%|[34m    [0m| 12/20 [00:34<00:22,  2.78s/it]                                                         Episode 13	 reward: -16.33	 makespan: 1616.25	 Mean_loss: 0.81927860,  training time: 2.76
progress:  60%|[34m    [0m| 12/20 [00:37<00:22,  2.78s/it]progress:  65%|[34m   [0m| 13/20 [00:37<00:19,  2.77s/it]                                                         Episode 14	 reward: -15.83	 makespan: 1566.75	 Mean_loss: 0.98960543,  training time: 2.75
progress:  65%|[34m   [0m| 13/20 [00:40<00:19,  2.77s/it]progress:  70%|[34m   [0m| 14/20 [00:40<00:16,  2.77s/it]                                                         Episode 15	 reward: -16.71	 makespan: 1654.25	 Mean_loss: 1.07808065,  training time: 2.74
progress:  70%|[34m   [0m| 14/20 [00:42<00:16,  2.77s/it]progress:  75%|[34m  [0m| 15/20 [00:42<00:13,  2.76s/it]                                                         Episode 16	 reward: -16.78	 makespan: 1661.00	 Mean_loss: 0.70732218,  training time: 2.73
progress:  75%|[34m  [0m| 15/20 [00:45<00:13,  2.76s/it]progress:  80%|[34m  [0m| 16/20 [00:45<00:11,  2.75s/it]                                                         Episode 17	 reward: -16.43	 makespan: 1626.75	 Mean_loss: 0.75708443,  training time: 2.82
progress:  80%|[34m  [0m| 16/20 [00:48<00:11,  2.75s/it]progress:  85%|[34m [0m| 17/20 [00:48<00:08,  2.77s/it]                                                         Episode 18	 reward: -17.86	 makespan: 1767.75	 Mean_loss: 1.05786240,  training time: 2.76
progress:  85%|[34m [0m| 17/20 [00:51<00:08,  2.77s/it]progress:  90%|[34m [0m| 18/20 [00:51<00:05,  2.77s/it]                                                         Episode 19	 reward: -17.14	 makespan: 1696.50	 Mean_loss: 0.57452458,  training time: 2.77
progress:  90%|[34m [0m| 18/20 [00:53<00:05,  2.77s/it]progress:  95%|[34m[0m| 19/20 [00:53<00:02,  2.77s/it]                                                         Episode 20	 reward: -16.26	 makespan: 1610.00	 Mean_loss: 0.89859265,  training time: 2.76
progress:  95%|[34m[0m| 19/20 [00:56<00:02,  2.77s/it]progress: 100%|[34m[0m| 20/20 [00:56<00:00,  2.77s/it]progress: 100%|[34m[0m| 20/20 [00:56<00:00,  2.83s/it]

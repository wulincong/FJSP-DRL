+ source /work/home/lxx_hzau/.bashrc
++ export CUDA_HOME=/usr/local/cuda-11.1/
++ CUDA_HOME=/usr/local/cuda-11.1/
++ export PATH=/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
++ PATH=/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
++ '[' -f /etc/bashrc ']'
++ . /etc/bashrc
+++ '[' '' ']'
+++ shopt -q login_shell
+++ '[' 5235 -gt 199 ']'
++++ /usr/bin/id -gn
++++ /usr/bin/id -un
+++ '[' ac6owqc808 = lxx_hzau ']'
+++ umask 022
+++ SHELL=/bin/bash
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/256term.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/256term.sh
++++ local256=truecolor
++++ '[' -n truecolor ']'
++++ case "$TERM" in
++++ export TERM
++++ '[' -n '' ']'
++++ unset local256
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/bash_completion.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/bash_completion.sh
++++ '[' -z '4.2.46(2)-release' -o -z '' -o -n '' ']'
++++ return
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/colorgrep.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/colorgrep.sh
++++ /usr/libexec/grepconf.sh -c
++++ alias 'grep=grep --color=auto'
++++ alias 'egrep=egrep --color=auto'
++++ alias 'fgrep=fgrep --color=auto'
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/colorls.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/colorls.sh
++++ '[' '!' -t 0 ']'
++++ return
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/cvs.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/cvs.sh
++++ export CVS_RSH=ssh
++++ CVS_RSH=ssh
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/dawning.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/dawning.sh
++++ export GRIDVIEW_HOME=/opt/gridview
++++ GRIDVIEW_HOME=/opt/gridview
++++ export MUNGE_HOME=/opt/gridview/munge
++++ MUNGE_HOME=/opt/gridview/munge
++++ export PATH=/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
++++ PATH=/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
++++ export LD_LIBRARY_PATH=/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/sharp/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/lib:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:
++++ LD_LIBRARY_PATH=/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/sharp/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/lib:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:
++++ export MANPATH=/opt/gridview/munge/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/share/man:/opt/rh/devtoolset-7/root/usr/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:
++++ MANPATH=/opt/gridview/munge/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/share/man:/opt/rh/devtoolset-7/root/usr/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:
++++ export C_INCLUDE_PATH=/opt/gridview/munge/include:/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/rh/devtoolset-7/root/usr/include/c++/7:/opt/rh/devtoolset-7/root/usr/include/c++/7/x86_64-redhat-linux:/opt/gridview/slurm/include:/opt/gridview/munge/include:
++++ C_INCLUDE_PATH=/opt/gridview/munge/include:/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/rh/devtoolset-7/root/usr/include/c++/7:/opt/rh/devtoolset-7/root/usr/include/c++/7/x86_64-redhat-linux:/opt/gridview/slurm/include:/opt/gridview/munge/include:
++++ export SLURM_HOME=/opt/gridview/slurm
++++ SLURM_HOME=/opt/gridview/slurm
++++ export PATH=/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
++++ PATH=/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin/
++++ export LD_LIBRARY_PATH=/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/sharp/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/lib:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:
++++ LD_LIBRARY_PATH=/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/sharp/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/lib:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:
++++ export MANPATH=/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/share/man:/opt/rh/devtoolset-7/root/usr/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:
++++ MANPATH=/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/share/man:/opt/rh/devtoolset-7/root/usr/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:
++++ export C_INCLUDE_PATH=/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/rh/devtoolset-7/root/usr/include/c++/7:/opt/rh/devtoolset-7/root/usr/include/c++/7/x86_64-redhat-linux:/opt/gridview/slurm/include:/opt/gridview/munge/include:
++++ C_INCLUDE_PATH=/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/gridview/slurm/include:/opt/gridview/munge/include:/opt/rh/devtoolset-7/root/usr/include/c++/7:/opt/rh/devtoolset-7/root/usr/include/c++/7/x86_64-redhat-linux:/opt/gridview/slurm/include:/opt/gridview/munge/include:
++++ export SLURM_PMIX_DIRECT_CONN=true
++++ SLURM_PMIX_DIRECT_CONN=true
++++ export SLURM_PMIX_DIRECT_CONN_UCX=true
++++ SLURM_PMIX_DIRECT_CONN_UCX=true
++++ export SLURM_PMIX_DIRECT_CONN_EARLY=true
++++ SLURM_PMIX_DIRECT_CONN_EARLY=true
++++ export SLURM_PMIX_TIMEOUT=3000
++++ SLURM_PMIX_TIMEOUT=3000
++++ '[' lxx_hzau '!=' root ']'
++++ export SQUEUE_USERS=lxx_hzau
++++ SQUEUE_USERS=lxx_hzau
++++ export SCONTROL_JOB_USERS=lxx_hzau
++++ SCONTROL_JOB_USERS=lxx_hzau
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/flatpak.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/flatpak.sh
++++ '[' /exports/share:/var/lib/flatpak/exports/share:/usr/local/share:/usr/share = /work/home/lxx_hzau/.local/share/flatpak/exports/share:/var/lib/flatpak/exports/share:/usr/local/share:/usr/share ']'
++++ export XDG_DATA_DIRS
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/gnome-ssh-askpass.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/gnome-ssh-askpass.sh
++++ SSH_ASKPASS=/usr/libexec/openssh/gnome-ssh-askpass
++++ export SSH_ASKPASS
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/kde.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/kde.sh
++++ '[' -z /usr ']'
++++ '[' -z '' ']'
++++ grep --color=auto -qs '^PRELINKING=yes' /etc/sysconfig/prelink
++++ for libdir in /usr/lib64 /usr/lib
++++ '[' -n /usr/lib64/kde4/plugins:/usr/lib/kde4/plugins ']'
++++ echo /usr/lib64/kde4/plugins:/usr/lib/kde4/plugins
++++ /bin/grep -q /usr/lib64/kde4/plugins
++++ for libdir in /usr/lib64 /usr/lib
++++ '[' -n /usr/lib64/kde4/plugins:/usr/lib/kde4/plugins ']'
++++ echo /usr/lib64/kde4/plugins:/usr/lib/kde4/plugins
++++ /bin/grep -q /usr/lib/kde4/plugins
++++ '[' '!' -d /work/home/lxx_hzau/.local/share ']'
++++ unset libdir
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/lang.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/lang.sh
++++ sourced=0
++++ '[' -n en_US.UTF-8 ']'
++++ saved_lang=en_US.UTF-8
++++ '[' -f /work/home/lxx_hzau/.i18n ']'
++++ LANG=en_US.UTF-8
++++ unset saved_lang
++++ '[' 0 = 1 ']'
++++ unset sourced
++++ unset langfile
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/less.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/less.sh
++++ '[' -x /usr/bin/lesspipe.sh ']'
++++ export 'LESSOPEN=||/usr/bin/lesspipe.sh %s'
++++ LESSOPEN='||/usr/bin/lesspipe.sh %s'
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/mc.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/mc.sh
++++ '[' -n '4.2.46(2)-release' ']'
++++ alias 'mc=. /usr/libexec/mc/mc-wrapper.sh'
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/modules.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/modules.sh
++++++ /bin/ps -p 4583 -ocomm=
+++++ /bin/basename slurm_script
++++ shell=slurm_script
++++ '[' -f /usr/share/Modules/init/slurm_script ']'
++++ . /usr/share/Modules/init/sh
+++++ MODULESHOME=/usr/share/Modules
+++++ export MODULESHOME
+++++ '[' compiler/devtoolset/7.3.1:mpi/hpcx/gcc-7.3.1 = '' ']'
+++++ '[' /public/software/modules/base:/public/software/modules/apps = '' ']'
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/mpi-selector.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/mpi-selector.sh
++++ mpi_selector_dir=/var/mpi-selector/data
++++ mpi_selector_homefile=/work/home/lxx_hzau/.mpi-selector
++++ mpi_selector_sysfile=/etc/sysconfig/mpi-selector
++++ mpi_selection=
++++ test -f /work/home/lxx_hzau/.mpi-selector
++++ test -f /etc/sysconfig/mpi-selector
++++ test '' '!=' '' -a -f /var/mpi-selector/data/.sh
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/PackageKit.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/PackageKit.sh
++++ [[ -n '' ]]
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/perl-homedir.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/perl-homedir.sh
++++ PERL_HOMEDIR=1
++++ '[' -f /etc/sysconfig/perl-homedir ']'
++++ '[' -f /work/home/lxx_hzau/.perl-homedir ']'
++++ alias 'perlll=eval `perl -Mlocal::lib`'
++++ '[' x1 = x1 ']'
+++++ perl -Mlocal::lib
++++ eval
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/qt-graphicssystem.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/qt-graphicssystem.sh
++++ '[' -z 1 -a -z '' ']'
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/qt.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/qt.sh
++++ '[' -z /usr/lib64/qt-3.3 ']'
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/rocm-gcc-mpi-default.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/rocm-gcc-mpi-default.sh
++++ [[ lxx_hzau =~ root|xhadmin ]]
++++ module use /public/software/modules/apps
+++++ /usr/bin/modulecmd sh use /public/software/modules/apps
++++ eval
++++ module use /public/software/modules/base
+++++ /usr/bin/modulecmd sh use /public/software/modules/base
++++ eval
++++ module unuse /opt/hpc/software/modules
+++++ /usr/bin/modulecmd sh unuse /opt/hpc/software/modules
++++ eval
++++ module load compiler/devtoolset/7.3.1
+++++ /usr/bin/modulecmd sh load compiler/devtoolset/7.3.1
++++ eval
++++ module load mpi/hpcx/gcc-7.3.1
+++++ /usr/bin/modulecmd sh load mpi/hpcx/gcc-7.3.1
++++ eval
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/vte.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/vte.sh
++++ '[' -n '4.2.46(2)-release' -o -n '' ']'
++++ [[ hxB == *i* ]]
++++ return 0
+++ for i in '/etc/profile.d/*.sh'
+++ '[' -r /etc/profile.d/which2.sh ']'
+++ '[' '' ']'
+++ . /etc/profile.d/which2.sh
++++ alias 'which=alias | /usr/bin/which --tty-only --read-alias --show-dot --show-tilde'
+++ unset i
+++ unset -f pathmunge
+++ /work/home/lxx_hzau/miniconda3/bin/conda shell.bash hook
++ __conda_setup='export CONDA_EXE='\''/work/home/lxx_hzau/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/work/home/lxx_hzau/miniconda3/bin/python'\''

# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
__conda_exe() (
    "$CONDA_EXE" $_CE_M $_CE_CONDA "$@"
)

__conda_hashr() {
    if [ -n "${ZSH_VERSION:+x}" ]; then
        \rehash
    elif [ -n "${POSH_VERSION:+x}" ]; then
        :  # pass
    else
        \hash -r
    fi
}

__conda_activate() {
    if [ -n "${CONDA_PS1_BACKUP:+x}" ]; then
        # Handle transition from shell activated with conda <= 4.3 to a subsequent activation
        # after conda updated to >= 4.4. See issue #6173.
        PS1="$CONDA_PS1_BACKUP"
        \unset CONDA_PS1_BACKUP
    fi
    \local ask_conda
    ask_conda="$(PS1="${PS1:-}" __conda_exe shell.posix "$@")" || \return
    \eval "$ask_conda"
    __conda_hashr
}

__conda_reactivate() {
    \local ask_conda
    ask_conda="$(PS1="${PS1:-}" __conda_exe shell.posix reactivate)" || \return
    \eval "$ask_conda"
    __conda_hashr
}

conda() {
    \local cmd="${1-__missing__}"
    case "$cmd" in
        activate|deactivate)
            __conda_activate "$@"
            ;;
        install|update|upgrade|remove|uninstall)
            __conda_exe "$@" || \return
            __conda_reactivate
            ;;
        *)
            __conda_exe "$@"
            ;;
    esac
}

if [ -z "${CONDA_SHLVL+x}" ]; then
    \export CONDA_SHLVL=0
    # In dev-mode CONDA_EXE is python.exe and on Windows
    # it is in a different relative location to condabin.
    if [ -n "${_CE_CONDA:+x}" ] && [ -n "${WINDIR+x}" ]; then
        PATH="$(\dirname "$CONDA_EXE")/condabin${PATH:+":${PATH}"}"
    else
        PATH="$(\dirname "$(\dirname "$CONDA_EXE")")/condabin${PATH:+":${PATH}"}"
    fi
    \export PATH

    # We'\''re not allowing PS1 to be unbound. It must at least be set.
    # However, we'\''re not exporting it, which can cause problems when starting a second shell
    # via a first shell (i.e. starting zsh from bash).
    if [ -z "${PS1+x}" ]; then
        PS1=
    fi
fi

conda activate base'
++ '[' 0 -eq 0 ']'
++ eval 'export CONDA_EXE='\''/work/home/lxx_hzau/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/work/home/lxx_hzau/miniconda3/bin/python'\''

# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
__conda_exe() (
    "$CONDA_EXE" $_CE_M $_CE_CONDA "$@"
)

__conda_hashr() {
    if [ -n "${ZSH_VERSION:+x}" ]; then
        \rehash
    elif [ -n "${POSH_VERSION:+x}" ]; then
        :  # pass
    else
        \hash -r
    fi
}

__conda_activate() {
    if [ -n "${CONDA_PS1_BACKUP:+x}" ]; then
        # Handle transition from shell activated with conda <= 4.3 to a subsequent activation
        # after conda updated to >= 4.4. See issue #6173.
        PS1="$CONDA_PS1_BACKUP"
        \unset CONDA_PS1_BACKUP
    fi
    \local ask_conda
    ask_conda="$(PS1="${PS1:-}" __conda_exe shell.posix "$@")" || \return
    \eval "$ask_conda"
    __conda_hashr
}

__conda_reactivate() {
    \local ask_conda
    ask_conda="$(PS1="${PS1:-}" __conda_exe shell.posix reactivate)" || \return
    \eval "$ask_conda"
    __conda_hashr
}

conda() {
    \local cmd="${1-__missing__}"
    case "$cmd" in
        activate|deactivate)
            __conda_activate "$@"
            ;;
        install|update|upgrade|remove|uninstall)
            __conda_exe "$@" || \return
            __conda_reactivate
            ;;
        *)
            __conda_exe "$@"
            ;;
    esac
}

if [ -z "${CONDA_SHLVL+x}" ]; then
    \export CONDA_SHLVL=0
    # In dev-mode CONDA_EXE is python.exe and on Windows
    # it is in a different relative location to condabin.
    if [ -n "${_CE_CONDA:+x}" ] && [ -n "${WINDIR+x}" ]; then
        PATH="$(\dirname "$CONDA_EXE")/condabin${PATH:+":${PATH}"}"
    else
        PATH="$(\dirname "$(\dirname "$CONDA_EXE")")/condabin${PATH:+":${PATH}"}"
    fi
    \export PATH

    # We'\''re not allowing PS1 to be unbound. It must at least be set.
    # However, we'\''re not exporting it, which can cause problems when starting a second shell
    # via a first shell (i.e. starting zsh from bash).
    if [ -z "${PS1+x}" ]; then
        PS1=
    fi
fi

conda activate base'
+++ export CONDA_EXE=/work/home/lxx_hzau/miniconda3/bin/conda
+++ CONDA_EXE=/work/home/lxx_hzau/miniconda3/bin/conda
+++ export _CE_M=
+++ _CE_M=
+++ export _CE_CONDA=
+++ _CE_CONDA=
+++ export CONDA_PYTHON_EXE=/work/home/lxx_hzau/miniconda3/bin/python
+++ CONDA_PYTHON_EXE=/work/home/lxx_hzau/miniconda3/bin/python
+++ '[' -z x ']'
+++ conda activate base
+++ local cmd=activate
+++ case "$cmd" in
+++ __conda_activate activate base
+++ '[' -n '' ']'
+++ local ask_conda
++++ PS1=
++++ __conda_exe shell.posix activate base
++++ /work/home/lxx_hzau/miniconda3/bin/conda shell.posix activate base
+++ ask_conda='PS1='\''(base) '\''
export PATH='\''/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin'\''
export CONDA_SHLVL='\''1'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export PYTHONPATH='\''/work/home/lxx_hzau/project/FJSP-DRL-main'\'''
+++ eval 'PS1='\''(base) '\''
export PATH='\''/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin'\''
export CONDA_SHLVL='\''1'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export PYTHONPATH='\''/work/home/lxx_hzau/project/FJSP-DRL-main'\'''
++++ PS1='(base) '
++++ export PATH=/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin
++++ PATH=/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin
++++ export CONDA_SHLVL=1
++++ CONDA_SHLVL=1
++++ export 'CONDA_PROMPT_MODIFIER=(base) '
++++ CONDA_PROMPT_MODIFIER='(base) '
++++ export PYTHONPATH=/work/home/lxx_hzau/project/FJSP-DRL-main
++++ PYTHONPATH=/work/home/lxx_hzau/project/FJSP-DRL-main
+++ __conda_hashr
+++ '[' -n '' ']'
+++ '[' -n '' ']'
+++ hash -r
++ unset __conda_setup
++ export PATH=/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin
++ PATH=/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin
+ module load nvidia/cuda/11.6
++ /usr/bin/modulecmd sh load nvidia/cuda/11.6
+ eval CUDA_HOME=/public/software/compiler/nvidia/cuda/11.6.0:/usr/local/cuda-11.1/ ';export' 'CUDA_HOME;CUDA_ROOT=/public/software/compiler/nvidia/cuda/11.6.0' ';export' 'CUDA_ROOT;INCLUDE=/public/software/compiler/nvidia/cuda/11.6.0/include:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/include:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/include:/opt/hpc/software/mpi/hpcx/v2.11.0/sharp/include:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/include' ';export' 'INCLUDE;LD_LIBRARY_PATH=/public/software/compiler/nvidia/cuda/11.6.0/lib64:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/sharp/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/lib:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:' ';export' 'LD_LIBRARY_PATH;LOADEDMODULES=compiler/devtoolset/7.3.1:mpi/hpcx/gcc-7.3.1:nvidia/cuda/11.6' ';export' 'LOADEDMODULES;PATH=/public/software/compiler/nvidia/cuda/11.6.0/bin:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin' ';export' 'PATH;_LMFILES_=/public/software/modules/base/compiler/devtoolset/7.3.1:/public/software/modules/base/mpi/hpcx/gcc-7.3.1:/public/software/modules/base/nvidia/cuda/11.6' ';export' '_LMFILES_;'
++ CUDA_HOME=/public/software/compiler/nvidia/cuda/11.6.0:/usr/local/cuda-11.1/
++ export CUDA_HOME
++ CUDA_ROOT=/public/software/compiler/nvidia/cuda/11.6.0
++ export CUDA_ROOT
++ INCLUDE=/public/software/compiler/nvidia/cuda/11.6.0/include:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/include:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/include:/opt/hpc/software/mpi/hpcx/v2.11.0/sharp/include:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/include
++ export INCLUDE
++ LD_LIBRARY_PATH=/public/software/compiler/nvidia/cuda/11.6.0/lib64:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/sharp/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/lib:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib:
++ export LD_LIBRARY_PATH
++ LOADEDMODULES=compiler/devtoolset/7.3.1:mpi/hpcx/gcc-7.3.1:nvidia/cuda/11.6
++ export LOADEDMODULES
++ PATH=/public/software/compiler/nvidia/cuda/11.6.0/bin:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin
++ export PATH
++ _LMFILES_=/public/software/modules/base/compiler/devtoolset/7.3.1:/public/software/modules/base/mpi/hpcx/gcc-7.3.1:/public/software/modules/base/nvidia/cuda/11.6
++ export _LMFILES_
+ conda activate RL-torch
+ local cmd=activate
+ case "$cmd" in
+ __conda_activate activate RL-torch
+ '[' -n '' ']'
+ local ask_conda
++ PS1='(base) '
++ __conda_exe shell.posix activate RL-torch
++ /work/home/lxx_hzau/miniconda3/bin/conda shell.posix activate RL-torch
+ ask_conda='PS1='\''(RL-torch) '\''
export PATH='\''/public/software/compiler/nvidia/cuda/11.6.0/bin:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/envs/RL-torch/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin'\''
export CONDA_PREFIX='\''/work/home/lxx_hzau/miniconda3/envs/RL-torch'\''
export CONDA_SHLVL='\''2'\''
export CONDA_DEFAULT_ENV='\''RL-torch'\''
export CONDA_PROMPT_MODIFIER='\''(RL-torch) '\''
export PYTHONPATH='\''/work/home/lxx_hzau/project/FJSP-DRL-main'\''
export CONDA_PREFIX_1='\''/work/home/lxx_hzau/miniconda3'\''
export CONDA_EXE='\''/work/home/lxx_hzau/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/work/home/lxx_hzau/miniconda3/bin/python'\'''
+ eval 'PS1='\''(RL-torch) '\''
export PATH='\''/public/software/compiler/nvidia/cuda/11.6.0/bin:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/envs/RL-torch/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin'\''
export CONDA_PREFIX='\''/work/home/lxx_hzau/miniconda3/envs/RL-torch'\''
export CONDA_SHLVL='\''2'\''
export CONDA_DEFAULT_ENV='\''RL-torch'\''
export CONDA_PROMPT_MODIFIER='\''(RL-torch) '\''
export PYTHONPATH='\''/work/home/lxx_hzau/project/FJSP-DRL-main'\''
export CONDA_PREFIX_1='\''/work/home/lxx_hzau/miniconda3'\''
export CONDA_EXE='\''/work/home/lxx_hzau/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/work/home/lxx_hzau/miniconda3/bin/python'\'''
++ PS1='(RL-torch) '
++ export PATH=/public/software/compiler/nvidia/cuda/11.6.0/bin:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/envs/RL-torch/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin
++ PATH=/public/software/compiler/nvidia/cuda/11.6.0/bin:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/work/home/lxx_hzau/local/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/work/home/lxx_hzau/local/bin:/work/home/lxx_hzau/miniconda3/envs/RL-torch/bin:/work/home/lxx_hzau/miniconda3/condabin:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/work/home/lxx_hzau/perl5/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/local/bin:/usr/bin:/usr/local/cuda-11.1/bin:/usr/local/sbin:/usr/sbin:/usr/local/cuda-11.1/bin:/work/home/lxx_hzau/.local/bin:/work/home/lxx_hzau/bin:/usr/local/cuda-11.1/bin:/usr/local/cuda-11.1/bin
++ export CONDA_PREFIX=/work/home/lxx_hzau/miniconda3/envs/RL-torch
++ CONDA_PREFIX=/work/home/lxx_hzau/miniconda3/envs/RL-torch
++ export CONDA_SHLVL=2
++ CONDA_SHLVL=2
++ export CONDA_DEFAULT_ENV=RL-torch
++ CONDA_DEFAULT_ENV=RL-torch
++ export 'CONDA_PROMPT_MODIFIER=(RL-torch) '
++ CONDA_PROMPT_MODIFIER='(RL-torch) '
++ export PYTHONPATH=/work/home/lxx_hzau/project/FJSP-DRL-main
++ PYTHONPATH=/work/home/lxx_hzau/project/FJSP-DRL-main
++ export CONDA_PREFIX_1=/work/home/lxx_hzau/miniconda3
++ CONDA_PREFIX_1=/work/home/lxx_hzau/miniconda3
++ export CONDA_EXE=/work/home/lxx_hzau/miniconda3/bin/conda
++ CONDA_EXE=/work/home/lxx_hzau/miniconda3/bin/conda
++ export _CE_M=
++ _CE_M=
++ export _CE_CONDA=
++ _CE_CONDA=
++ export CONDA_PYTHON_EXE=/work/home/lxx_hzau/miniconda3/bin/python
++ CONDA_PYTHON_EXE=/work/home/lxx_hzau/miniconda3/bin/python
+ __conda_hashr
+ '[' -n '' ']'
+ '[' -n '' ']'
+ hash -r
+ t=train
+ exp=exp18
+ echo exp18
exp18
+ cat
Áî®Êñ∞ÁöÑÂÖÉÂ≠¶‰π†ÁÆóÊ≥ïÂÅö‰πãÂâçÁöÑÂÆûÈ™å Êú∫Âô®Êï∞ÁöÑÂ¢ûÂä†ÂíåÂáèÂ∞ë
+ n_j_options='20 20 20 20 20 20'
+ n_m_options='5  7  10 13 15 20'
+ op_per_job_options='5 5 5 5 5 5'
+ logdir=./runs/exp18/exp18-3
+ hidden_dim_actor=512
+ hidden_dim_critic=512
+ num_mlp_layers_actor=3
+ num_mlp_layers_critic=3
+ num_envs=10
+ num_tasks=6
+ meta_iterations=203
+ max_updates_maml=500
+ max_updates_finetune=21
+ lr=0.003
+ logdir_dan=./runs/exp18/exp18-3/DAN
+ python train/DAN.py --n_j 20 --n_m 5 --op_per_job 5 --data_source SD2 --model_suffix SD2_exp18_3 --logdir ./runs/exp18/exp18-3/DAN/train_model/20x5x5 --max_updates 500
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x5+mix
save model name:  20x5+mix+SD2_exp18_3
-------------------------Training Setting-------------------------
source : SD2
model name :20x5+mix+SD2_exp18_3
vali data :./data/data_train_vali/SD2/20x5+mix


progress:   0%|[34m          [0m| 0/500 [00:00<?, ?it/s]                                                 Episode 1	 reward: -11.55	 makespan: 1143.75	 Mean_loss: 9.11856461,  training time: 4.82
progress:   0%|[34m          [0m| 0/500 [00:04<?, ?it/s]progress:   0%|[34m          [0m| 1/500 [00:04<40:03,  4.82s/it]                                                         Episode 2	 reward: -10.93	 makespan: 1082.40	 Mean_loss: 7.44118690,  training time: 1.93
progress:   0%|[34m          [0m| 1/500 [00:06<40:03,  4.82s/it]progress:   0%|[34m          [0m| 2/500 [00:06<25:54,  3.12s/it]                                                         Episode 3	 reward: -11.01	 makespan: 1090.10	 Mean_loss: 7.68168736,  training time: 1.93
progress:   0%|[34m          [0m| 2/500 [00:08<25:54,  3.12s/it]progress:   1%|[34m          [0m| 3/500 [00:08<21:21,  2.58s/it]                                                         Episode 4	 reward: -10.95	 makespan: 1084.50	 Mean_loss: 7.08047771,  training time: 1.92
progress:   1%|[34m          [0m| 3/500 [00:10<21:21,  2.58s/it]progress:   1%|[34m          [0m| 4/500 [00:10<19:10,  2.32s/it]                                                         Episode 5	 reward: -10.90	 makespan: 1079.10	 Mean_loss: 6.30920410,  training time: 1.92
progress:   1%|[34m          [0m| 4/500 [00:12<19:10,  2.32s/it]progress:   1%|[34m          [0m| 5/500 [00:12<17:56,  2.18s/it]                                                         Episode 6	 reward: -10.60	 makespan: 1049.10	 Mean_loss: 5.21165991,  training time: 1.93
progress:   1%|[34m          [0m| 5/500 [00:14<17:56,  2.18s/it]progress:   1%|[34m          [0m| 6/500 [00:14<17:13,  2.09s/it]                                                         Episode 7	 reward: -10.86	 makespan: 1075.45	 Mean_loss: 4.68958092,  training time: 2.01
progress:   1%|[34m          [0m| 6/500 [00:16<17:13,  2.09s/it]progress:   1%|[34m‚ñè         [0m| 7/500 [00:16<16:57,  2.06s/it]                                                         Episode 8	 reward: -10.99	 makespan: 1087.70	 Mean_loss: 4.13090658,  training time: 1.97
progress:   1%|[34m‚ñè         [0m| 7/500 [00:18<16:57,  2.06s/it]progress:   2%|[34m‚ñè         [0m| 8/500 [00:18<16:40,  2.03s/it]                                                         Episode 9	 reward: -10.83	 makespan: 1071.75	 Mean_loss: 3.60375571,  training time: 2.00
progress:   2%|[34m‚ñè         [0m| 8/500 [00:20<16:40,  2.03s/it]progress:   2%|[34m‚ñè         [0m| 9/500 [00:20<16:34,  2.02s/it]                                                         Episode 10	 reward: -11.19	 makespan: 1108.10	 Mean_loss: 3.72346544,  training time: 1.98
progress:   2%|[34m‚ñè         [0m| 9/500 [00:22<16:34,  2.02s/it]progress:   2%|[34m‚ñè         [0m| 10/500 [00:22<16:25,  2.01s/it]                                                          Episode 11	 reward: -10.94	 makespan: 1083.15	 Mean_loss: 3.82453346,  training time: 1.95
progress:   2%|[34m‚ñè         [0m| 10/500 [00:24<16:25,  2.01s/it]progress:   2%|[34m‚ñè         [0m| 11/500 [00:24<16:14,  1.99s/it]                                                          Episode 12	 reward: -10.66	 makespan: 1055.55	 Mean_loss: 3.65935969,  training time: 1.96
progress:   2%|[34m‚ñè         [0m| 11/500 [00:26<16:14,  1.99s/it]progress:   2%|[34m‚ñè         [0m| 12/500 [00:26<16:07,  1.98s/it]                                                          Episode 13	 reward: -10.73	 makespan: 1061.90	 Mean_loss: 4.06060314,  training time: 1.94
progress:   2%|[34m‚ñè         [0m| 12/500 [00:28<16:07,  1.98s/it]progress:   3%|[34m‚ñé         [0m| 13/500 [00:28<15:59,  1.97s/it]                                                          Episode 14	 reward: -10.54	 makespan: 1043.20	 Mean_loss: 4.05608797,  training time: 1.94
progress:   3%|[34m‚ñé         [0m| 13/500 [00:30<15:59,  1.97s/it]progress:   3%|[34m‚ñé         [0m| 14/500 [00:30<15:52,  1.96s/it]                                                          Episode 15	 reward: -10.74	 makespan: 1063.25	 Mean_loss: 4.13387346,  training time: 1.93
progress:   3%|[34m‚ñé         [0m| 14/500 [00:32<15:52,  1.96s/it]progress:   3%|[34m‚ñé         [0m| 15/500 [00:32<15:47,  1.95s/it]                                                          Episode 16	 reward: -10.85	 makespan: 1074.55	 Mean_loss: 3.87046552,  training time: 1.93
progress:   3%|[34m‚ñé         [0m| 15/500 [00:34<15:47,  1.95s/it]progress:   3%|[34m‚ñé         [0m| 16/500 [00:34<15:42,  1.95s/it]                                                          Episode 17	 reward: -10.89	 makespan: 1077.65	 Mean_loss: 3.82367158,  training time: 1.94
progress:   3%|[34m‚ñé         [0m| 16/500 [00:36<15:42,  1.95s/it]progress:   3%|[34m‚ñé         [0m| 17/500 [00:36<15:40,  1.95s/it]                                                          Episode 18	 reward: -10.83	 makespan: 1071.80	 Mean_loss: 3.43206644,  training time: 1.94
progress:   3%|[34m‚ñé         [0m| 17/500 [00:37<15:40,  1.95s/it]progress:   4%|[34m‚ñé         [0m| 18/500 [00:37<15:37,  1.95s/it]                                                          Episode 19	 reward: -11.04	 makespan: 1092.65	 Mean_loss: 3.09351730,  training time: 2.02
progress:   4%|[34m‚ñé         [0m| 18/500 [00:39<15:37,  1.95s/it]progress:   4%|[34m‚ñç         [0m| 19/500 [00:39<15:47,  1.97s/it]                                                          Episode 20	 reward: -10.85	 makespan: 1073.80	 Mean_loss: 2.57997108,  training time: 2.05
progress:   4%|[34m‚ñç         [0m| 19/500 [00:42<15:47,  1.97s/it]progress:   4%|[34m‚ñç         [0m| 20/500 [00:42<15:56,  1.99s/it]                                                          Episode 21	 reward: -10.75	 makespan: 1064.55	 Mean_loss: 2.14361072,  training time: 1.99
progress:   4%|[34m‚ñç         [0m| 20/500 [00:44<15:56,  1.99s/it]progress:   4%|[34m‚ñç         [0m| 21/500 [00:44<15:54,  1.99s/it]                                                          Episode 22	 reward: -10.84	 makespan: 1073.35	 Mean_loss: 2.11127877,  training time: 1.94
progress:   4%|[34m‚ñç         [0m| 21/500 [00:45<15:54,  1.99s/it]progress:   4%|[34m‚ñç         [0m| 22/500 [00:45<15:44,  1.98s/it]                                                          Episode 23	 reward: -10.27	 makespan: 1017.00	 Mean_loss: 1.65381229,  training time: 1.98
progress:   4%|[34m‚ñç         [0m| 22/500 [00:47<15:44,  1.98s/it]progress:   5%|[34m‚ñç         [0m| 23/500 [00:47<15:43,  1.98s/it]                                                          Episode 24	 reward: -10.68	 makespan: 1057.60	 Mean_loss: 1.55315006,  training time: 1.96
progress:   5%|[34m‚ñç         [0m| 23/500 [00:49<15:43,  1.98s/it]progress:   5%|[34m‚ñç         [0m| 24/500 [00:49<15:39,  1.97s/it]                                                          Episode 25	 reward: -10.45	 makespan: 1034.95	 Mean_loss: 1.61023521,  training time: 1.88
progress:   5%|[34m‚ñç         [0m| 24/500 [00:51<15:39,  1.97s/it]progress:   5%|[34m‚ñå         [0m| 25/500 [00:51<15:24,  1.95s/it]                                                          Episode 26	 reward: -10.54	 makespan: 1043.60	 Mean_loss: 1.42965996,  training time: 2.09
progress:   5%|[34m‚ñå         [0m| 25/500 [00:53<15:24,  1.95s/it]progress:   5%|[34m‚ñå         [0m| 26/500 [00:53<15:42,  1.99s/it]                                                          Episode 27	 reward: -10.39	 makespan: 1028.60	 Mean_loss: 1.63901615,  training time: 1.92
progress:   5%|[34m‚ñå         [0m| 26/500 [00:55<15:42,  1.99s/it]progress:   5%|[34m‚ñå         [0m| 27/500 [00:55<15:31,  1.97s/it]                                                          Episode 28	 reward: -10.03	 makespan: 993.05	 Mean_loss: 1.39066720,  training time: 1.95
progress:   5%|[34m‚ñå         [0m| 27/500 [00:57<15:31,  1.97s/it]progress:   6%|[34m‚ñå         [0m| 28/500 [00:57<15:27,  1.96s/it]                                                          Episode 29	 reward: -10.03	 makespan: 992.60	 Mean_loss: 1.31971717,  training time: 1.93
progress:   6%|[34m‚ñå         [0m| 28/500 [00:59<15:27,  1.96s/it]progress:   6%|[34m‚ñå         [0m| 29/500 [00:59<15:20,  1.95s/it]                                                          Episode 30	 reward: -9.73	 makespan: 963.75	 Mean_loss: 1.15338206,  training time: 1.95
progress:   6%|[34m‚ñå         [0m| 29/500 [01:01<15:20,  1.95s/it]progress:   6%|[34m‚ñå         [0m| 30/500 [01:01<15:17,  1.95s/it]                                                          Episode 31	 reward: -9.53	 makespan: 943.05	 Mean_loss: 1.11640072,  training time: 1.99
progress:   6%|[34m‚ñå         [0m| 30/500 [01:03<15:17,  1.95s/it]progress:   6%|[34m‚ñå         [0m| 31/500 [01:03<15:20,  1.96s/it]                                                          Episode 32	 reward: -9.68	 makespan: 958.75	 Mean_loss: 1.04477811,  training time: 2.09
progress:   6%|[34m‚ñå         [0m| 31/500 [01:05<15:20,  1.96s/it]progress:   6%|[34m‚ñã         [0m| 32/500 [01:05<15:36,  2.00s/it]                                                          Episode 33	 reward: -9.69	 makespan: 959.75	 Mean_loss: 1.10963631,  training time: 2.00
progress:   6%|[34m‚ñã         [0m| 32/500 [01:07<15:36,  2.00s/it]progress:   7%|[34m‚ñã         [0m| 33/500 [01:07<15:34,  2.00s/it]                                                          Episode 34	 reward: -9.49	 makespan: 940.00	 Mean_loss: 1.06263328,  training time: 2.06
progress:   7%|[34m‚ñã         [0m| 33/500 [01:09<15:34,  2.00s/it]progress:   7%|[34m‚ñã         [0m| 34/500 [01:09<15:40,  2.02s/it]                                                          Episode 35	 reward: -9.52	 makespan: 942.70	 Mean_loss: 0.87105161,  training time: 1.94
progress:   7%|[34m‚ñã         [0m| 34/500 [01:11<15:40,  2.02s/it]progress:   7%|[34m‚ñã         [0m| 35/500 [01:11<15:28,  2.00s/it]                                                          Episode 36	 reward: -9.52	 makespan: 942.60	 Mean_loss: 0.88705027,  training time: 1.95
progress:   7%|[34m‚ñã         [0m| 35/500 [01:13<15:28,  2.00s/it]progress:   7%|[34m‚ñã         [0m| 36/500 [01:13<15:20,  1.98s/it]                                                          Episode 37	 reward: -9.35	 makespan: 925.30	 Mean_loss: 0.82594150,  training time: 1.96
progress:   7%|[34m‚ñã         [0m| 36/500 [01:15<15:20,  1.98s/it]progress:   7%|[34m‚ñã         [0m| 37/500 [01:15<15:15,  1.98s/it]                                                          Episode 38	 reward: -9.20	 makespan: 910.45	 Mean_loss: 0.84284514,  training time: 1.95
progress:   7%|[34m‚ñã         [0m| 37/500 [01:17<15:15,  1.98s/it]progress:   8%|[34m‚ñä         [0m| 38/500 [01:17<15:09,  1.97s/it]                                                          Episode 39	 reward: -9.52	 makespan: 942.20	 Mean_loss: 0.95879602,  training time: 2.01
progress:   8%|[34m‚ñä         [0m| 38/500 [01:19<15:09,  1.97s/it]progress:   8%|[34m‚ñä         [0m| 39/500 [01:19<15:13,  1.98s/it]                                                          Episode 40	 reward: -9.52	 makespan: 942.95	 Mean_loss: 0.88769841,  training time: 1.95
progress:   8%|[34m‚ñä         [0m| 39/500 [01:21<15:13,  1.98s/it]progress:   8%|[34m‚ñä         [0m| 40/500 [01:21<15:08,  1.97s/it]                                                          Episode 41	 reward: -8.78	 makespan: 869.10	 Mean_loss: 0.77002978,  training time: 1.97
progress:   8%|[34m‚ñä         [0m| 40/500 [01:23<15:08,  1.97s/it]progress:   8%|[34m‚ñä         [0m| 41/500 [01:23<15:06,  1.97s/it]                                                          Episode 42	 reward: -8.65	 makespan: 855.90	 Mean_loss: 0.64809483,  training time: 2.07
progress:   8%|[34m‚ñä         [0m| 41/500 [01:25<15:06,  1.97s/it]progress:   8%|[34m‚ñä         [0m| 42/500 [01:25<15:18,  2.00s/it]                                                          Episode 43	 reward: -8.83	 makespan: 874.05	 Mean_loss: 0.53583330,  training time: 1.95
progress:   8%|[34m‚ñä         [0m| 42/500 [01:27<15:18,  2.00s/it]progress:   9%|[34m‚ñä         [0m| 43/500 [01:27<15:09,  1.99s/it]                                                          Episode 44	 reward: -8.82	 makespan: 873.65	 Mean_loss: 0.49812877,  training time: 2.06
progress:   9%|[34m‚ñä         [0m| 43/500 [01:29<15:09,  1.99s/it]progress:   9%|[34m‚ñâ         [0m| 44/500 [01:29<15:16,  2.01s/it]                                                          Episode 45	 reward: -8.82	 makespan: 872.95	 Mean_loss: 0.41848734,  training time: 2.01
progress:   9%|[34m‚ñâ         [0m| 44/500 [01:31<15:16,  2.01s/it]progress:   9%|[34m‚ñâ         [0m| 45/500 [01:31<15:14,  2.01s/it]                                                          Episode 46	 reward: -8.79	 makespan: 870.55	 Mean_loss: 0.46439439,  training time: 2.06
progress:   9%|[34m‚ñâ         [0m| 45/500 [01:33<15:14,  2.01s/it]progress:   9%|[34m‚ñâ         [0m| 46/500 [01:33<15:19,  2.03s/it]                                                          Episode 47	 reward: -8.76	 makespan: 867.15	 Mean_loss: 0.45169547,  training time: 1.93
progress:   9%|[34m‚ñâ         [0m| 46/500 [01:35<15:19,  2.03s/it]progress:   9%|[34m‚ñâ         [0m| 47/500 [01:35<15:04,  2.00s/it]                                                          Episode 48	 reward: -8.75	 makespan: 866.55	 Mean_loss: 0.45115355,  training time: 1.96
progress:   9%|[34m‚ñâ         [0m| 47/500 [01:37<15:04,  2.00s/it]progress:  10%|[34m‚ñâ         [0m| 48/500 [01:37<14:57,  1.99s/it]                                                          Episode 49	 reward: -8.54	 makespan: 845.15	 Mean_loss: 0.42518002,  training time: 1.94
progress:  10%|[34m‚ñâ         [0m| 48/500 [01:39<14:57,  1.99s/it]progress:  10%|[34m‚ñâ         [0m| 49/500 [01:39<14:49,  1.97s/it]                                                          Episode 50	 reward: -8.49	 makespan: 840.35	 Mean_loss: 0.39692608,  training time: 1.93
progress:  10%|[34m‚ñâ         [0m| 49/500 [01:41<14:49,  1.97s/it]progress:  10%|[34m‚ñà         [0m| 50/500 [01:41<14:42,  1.96s/it]                                                          Episode 51	 reward: -8.68	 makespan: 859.15	 Mean_loss: 0.46238074,  training time: 1.93
progress:  10%|[34m‚ñà         [0m| 50/500 [01:43<14:42,  1.96s/it]progress:  10%|[34m‚ñà         [0m| 51/500 [01:43<14:36,  1.95s/it]                                                          Episode 52	 reward: -8.55	 makespan: 846.90	 Mean_loss: 0.41075280,  training time: 1.96
progress:  10%|[34m‚ñà         [0m| 51/500 [01:45<14:36,  1.95s/it]progress:  10%|[34m‚ñà         [0m| 52/500 [01:45<14:36,  1.96s/it]                                                          Episode 53	 reward: -8.62	 makespan: 853.55	 Mean_loss: 0.40370083,  training time: 1.99
progress:  10%|[34m‚ñà         [0m| 52/500 [01:47<14:36,  1.96s/it]progress:  11%|[34m‚ñà         [0m| 53/500 [01:47<14:39,  1.97s/it]                                                          Episode 54	 reward: -8.72	 makespan: 863.25	 Mean_loss: 0.45274040,  training time: 1.95
progress:  11%|[34m‚ñà         [0m| 53/500 [01:49<14:39,  1.97s/it]progress:  11%|[34m‚ñà         [0m| 54/500 [01:49<14:34,  1.96s/it]                                                          Episode 55	 reward: -8.47	 makespan: 838.25	 Mean_loss: 0.40198165,  training time: 2.07
progress:  11%|[34m‚ñà         [0m| 54/500 [01:51<14:34,  1.96s/it]progress:  11%|[34m‚ñà         [0m| 55/500 [01:51<14:46,  1.99s/it]                                                          Episode 56	 reward: -8.46	 makespan: 837.15	 Mean_loss: 0.30628553,  training time: 2.01
progress:  11%|[34m‚ñà         [0m| 55/500 [01:53<14:46,  1.99s/it]progress:  11%|[34m‚ñà         [0m| 56/500 [01:53<14:46,  2.00s/it]                                                          Episode 57	 reward: -8.29	 makespan: 820.75	 Mean_loss: 0.39045739,  training time: 1.97
progress:  11%|[34m‚ñà         [0m| 56/500 [01:55<14:46,  2.00s/it]progress:  11%|[34m‚ñà‚ñè        [0m| 57/500 [01:55<14:40,  1.99s/it]                                                          Episode 58	 reward: -8.56	 makespan: 847.00	 Mean_loss: 0.37238833,  training time: 1.96
progress:  11%|[34m‚ñà‚ñè        [0m| 57/500 [01:57<14:40,  1.99s/it]progress:  12%|[34m‚ñà‚ñè        [0m| 58/500 [01:57<14:35,  1.98s/it]                                                          Episode 59	 reward: -8.52	 makespan: 843.20	 Mean_loss: 0.31665677,  training time: 1.96
progress:  12%|[34m‚ñà‚ñè        [0m| 58/500 [01:59<14:35,  1.98s/it]progress:  12%|[34m‚ñà‚ñè        [0m| 59/500 [01:59<14:31,  1.98s/it]                                                          Episode 60	 reward: -8.53	 makespan: 844.60	 Mean_loss: 0.32974815,  training time: 1.97
progress:  12%|[34m‚ñà‚ñè        [0m| 59/500 [02:01<14:31,  1.98s/it]progress:  12%|[34m‚ñà‚ñè        [0m| 60/500 [02:01<14:29,  1.98s/it]                                                          Episode 61	 reward: -8.65	 makespan: 856.55	 Mean_loss: 0.38581094,  training time: 2.00
progress:  12%|[34m‚ñà‚ñè        [0m| 60/500 [02:03<14:29,  1.98s/it]progress:  12%|[34m‚ñà‚ñè        [0m| 61/500 [02:03<14:30,  1.98s/it]                                                          Episode 62	 reward: -8.40	 makespan: 832.05	 Mean_loss: 0.37443593,  training time: 2.00
progress:  12%|[34m‚ñà‚ñè        [0m| 61/500 [02:05<14:30,  1.98s/it]progress:  12%|[34m‚ñà‚ñè        [0m| 62/500 [02:05<14:31,  1.99s/it]                                                          Episode 63	 reward: -8.54	 makespan: 845.40	 Mean_loss: 0.36627698,  training time: 1.94
progress:  12%|[34m‚ñà‚ñè        [0m| 62/500 [02:07<14:31,  1.99s/it]progress:  13%|[34m‚ñà‚ñé        [0m| 63/500 [02:07<14:23,  1.98s/it]                                                          Episode 64	 reward: -8.61	 makespan: 852.15	 Mean_loss: 0.34388289,  training time: 1.97
progress:  13%|[34m‚ñà‚ñé        [0m| 63/500 [02:09<14:23,  1.98s/it]progress:  13%|[34m‚ñà‚ñé        [0m| 64/500 [02:09<14:21,  1.97s/it]                                                          Episode 65	 reward: -8.49	 makespan: 840.35	 Mean_loss: 0.33467621,  training time: 1.96
progress:  13%|[34m‚ñà‚ñé        [0m| 64/500 [02:11<14:21,  1.97s/it]progress:  13%|[34m‚ñà‚ñé        [0m| 65/500 [02:11<14:16,  1.97s/it]                                                          Episode 66	 reward: -8.42	 makespan: 833.25	 Mean_loss: 0.31999245,  training time: 1.95
progress:  13%|[34m‚ñà‚ñé        [0m| 65/500 [02:13<14:16,  1.97s/it]progress:  13%|[34m‚ñà‚ñé        [0m| 66/500 [02:13<14:12,  1.96s/it]                                                          Episode 67	 reward: -8.27	 makespan: 818.70	 Mean_loss: 0.28909171,  training time: 1.97
progress:  13%|[34m‚ñà‚ñé        [0m| 66/500 [02:15<14:12,  1.96s/it]progress:  13%|[34m‚ñà‚ñé        [0m| 67/500 [02:15<14:11,  1.97s/it]                                                          Episode 68	 reward: -8.24	 makespan: 816.00	 Mean_loss: 0.29807761,  training time: 1.95
progress:  13%|[34m‚ñà‚ñé        [0m| 67/500 [02:16<14:11,  1.97s/it]progress:  14%|[34m‚ñà‚ñé        [0m| 68/500 [02:16<14:07,  1.96s/it]                                                          Episode 69	 reward: -8.26	 makespan: 817.70	 Mean_loss: 0.30798903,  training time: 1.94
progress:  14%|[34m‚ñà‚ñé        [0m| 68/500 [02:18<14:07,  1.96s/it]progress:  14%|[34m‚ñà‚ñç        [0m| 69/500 [02:18<14:02,  1.95s/it]                                                          Episode 70	 reward: -8.03	 makespan: 794.65	 Mean_loss: 0.28803560,  training time: 1.96
progress:  14%|[34m‚ñà‚ñç        [0m| 69/500 [02:20<14:02,  1.95s/it]progress:  14%|[34m‚ñà‚ñç        [0m| 70/500 [02:20<14:01,  1.96s/it]                                                          Episode 71	 reward: -8.13	 makespan: 805.20	 Mean_loss: 0.30243015,  training time: 1.94
progress:  14%|[34m‚ñà‚ñç        [0m| 70/500 [02:22<14:01,  1.96s/it]progress:  14%|[34m‚ñà‚ñç        [0m| 71/500 [02:22<13:57,  1.95s/it]                                                          Episode 72	 reward: -7.99	 makespan: 791.20	 Mean_loss: 0.25897101,  training time: 1.94
progress:  14%|[34m‚ñà‚ñç        [0m| 71/500 [02:24<13:57,  1.95s/it]progress:  14%|[34m‚ñà‚ñç        [0m| 72/500 [02:24<13:53,  1.95s/it]                                                          Episode 73	 reward: -7.81	 makespan: 773.60	 Mean_loss: 0.27554151,  training time: 1.99
progress:  14%|[34m‚ñà‚ñç        [0m| 72/500 [02:26<13:53,  1.95s/it]progress:  15%|[34m‚ñà‚ñç        [0m| 73/500 [02:26<13:57,  1.96s/it]                                                          Episode 74	 reward: -8.15	 makespan: 806.55	 Mean_loss: 0.25731614,  training time: 2.00
progress:  15%|[34m‚ñà‚ñç        [0m| 73/500 [02:28<13:57,  1.96s/it]progress:  15%|[34m‚ñà‚ñç        [0m| 74/500 [02:28<14:00,  1.97s/it]                                                          Episode 75	 reward: -8.08	 makespan: 799.70	 Mean_loss: 0.26448521,  training time: 1.98
progress:  15%|[34m‚ñà‚ñç        [0m| 74/500 [02:30<14:00,  1.97s/it]progress:  15%|[34m‚ñà‚ñå        [0m| 75/500 [02:30<13:59,  1.97s/it]                                                          Episode 76	 reward: -7.95	 makespan: 786.85	 Mean_loss: 0.33521444,  training time: 1.93
progress:  15%|[34m‚ñà‚ñå        [0m| 75/500 [02:32<13:59,  1.97s/it]progress:  15%|[34m‚ñà‚ñå        [0m| 76/500 [02:32<13:51,  1.96s/it]                                                          Episode 77	 reward: -8.04	 makespan: 795.85	 Mean_loss: 0.23967263,  training time: 1.94
progress:  15%|[34m‚ñà‚ñå        [0m| 76/500 [02:34<13:51,  1.96s/it]progress:  15%|[34m‚ñà‚ñå        [0m| 77/500 [02:34<13:46,  1.95s/it]                                                          Episode 78	 reward: -8.04	 makespan: 796.15	 Mean_loss: 0.29354492,  training time: 1.94
progress:  15%|[34m‚ñà‚ñå        [0m| 77/500 [02:36<13:46,  1.95s/it]progress:  16%|[34m‚ñà‚ñå        [0m| 78/500 [02:36<13:43,  1.95s/it]                                                          Episode 79	 reward: -7.96	 makespan: 787.65	 Mean_loss: 0.21811008,  training time: 1.96
progress:  16%|[34m‚ñà‚ñå        [0m| 78/500 [02:38<13:43,  1.95s/it]progress:  16%|[34m‚ñà‚ñå        [0m| 79/500 [02:38<13:42,  1.95s/it]                                                          Episode 80	 reward: -7.84	 makespan: 775.85	 Mean_loss: 0.25481129,  training time: 2.06
progress:  16%|[34m‚ñà‚ñå        [0m| 79/500 [02:40<13:42,  1.95s/it]progress:  16%|[34m‚ñà‚ñå        [0m| 80/500 [02:40<13:54,  1.99s/it]                                                          Episode 81	 reward: -7.96	 makespan: 788.40	 Mean_loss: 0.17766707,  training time: 1.98
progress:  16%|[34m‚ñà‚ñå        [0m| 80/500 [02:42<13:54,  1.99s/it]progress:  16%|[34m‚ñà‚ñå        [0m| 81/500 [02:42<13:50,  1.98s/it]                                                          Episode 82	 reward: -7.88	 makespan: 779.75	 Mean_loss: 0.18189794,  training time: 2.03
progress:  16%|[34m‚ñà‚ñå        [0m| 81/500 [02:44<13:50,  1.98s/it]progress:  16%|[34m‚ñà‚ñã        [0m| 82/500 [02:44<13:54,  2.00s/it]                                                          Episode 83	 reward: -7.90	 makespan: 782.50	 Mean_loss: 0.21441118,  training time: 2.01
progress:  16%|[34m‚ñà‚ñã        [0m| 82/500 [02:46<13:54,  2.00s/it]progress:  17%|[34m‚ñà‚ñã        [0m| 83/500 [02:46<13:55,  2.00s/it]                                                          Episode 84	 reward: -7.91	 makespan: 783.55	 Mean_loss: 0.24517174,  training time: 1.93
progress:  17%|[34m‚ñà‚ñã        [0m| 83/500 [02:48<13:55,  2.00s/it]progress:  17%|[34m‚ñà‚ñã        [0m| 84/500 [02:48<13:44,  1.98s/it]                                                          Episode 85	 reward: -7.83	 makespan: 774.90	 Mean_loss: 0.18691954,  training time: 1.94
progress:  17%|[34m‚ñà‚ñã        [0m| 84/500 [02:50<13:44,  1.98s/it]progress:  17%|[34m‚ñà‚ñã        [0m| 85/500 [02:50<13:37,  1.97s/it]                                                          Episode 86	 reward: -7.76	 makespan: 768.65	 Mean_loss: 0.21604902,  training time: 1.93
progress:  17%|[34m‚ñà‚ñã        [0m| 85/500 [02:52<13:37,  1.97s/it]progress:  17%|[34m‚ñà‚ñã        [0m| 86/500 [02:52<13:30,  1.96s/it]                                                          Episode 87	 reward: -7.82	 makespan: 773.70	 Mean_loss: 0.18528518,  training time: 2.01
progress:  17%|[34m‚ñà‚ñã        [0m| 86/500 [02:54<13:30,  1.96s/it]progress:  17%|[34m‚ñà‚ñã        [0m| 87/500 [02:54<13:35,  1.97s/it]                                                          Episode 88	 reward: -7.88	 makespan: 780.05	 Mean_loss: 0.18123759,  training time: 1.93
progress:  17%|[34m‚ñà‚ñã        [0m| 87/500 [02:56<13:35,  1.97s/it]progress:  18%|[34m‚ñà‚ñä        [0m| 88/500 [02:56<13:28,  1.96s/it]                                                          Episode 89	 reward: -7.95	 makespan: 787.45	 Mean_loss: 0.19628519,  training time: 1.94
progress:  18%|[34m‚ñà‚ñä        [0m| 88/500 [02:58<13:28,  1.96s/it]progress:  18%|[34m‚ñà‚ñä        [0m| 89/500 [02:58<13:23,  1.95s/it]                                                          Episode 90	 reward: -7.87	 makespan: 779.60	 Mean_loss: 0.16048114,  training time: 1.96
progress:  18%|[34m‚ñà‚ñä        [0m| 89/500 [03:00<13:23,  1.95s/it]progress:  18%|[34m‚ñà‚ñä        [0m| 90/500 [03:00<13:22,  1.96s/it]                                                          Episode 91	 reward: -7.70	 makespan: 762.00	 Mean_loss: 0.17447785,  training time: 1.94
progress:  18%|[34m‚ñà‚ñä        [0m| 90/500 [03:02<13:22,  1.96s/it]progress:  18%|[34m‚ñà‚ñä        [0m| 91/500 [03:02<13:17,  1.95s/it]                                                          Episode 92	 reward: -7.80	 makespan: 772.40	 Mean_loss: 0.18790360,  training time: 1.94
progress:  18%|[34m‚ñà‚ñä        [0m| 91/500 [03:04<13:17,  1.95s/it]progress:  18%|[34m‚ñà‚ñä        [0m| 92/500 [03:04<13:14,  1.95s/it]                                                          Episode 93	 reward: -7.73	 makespan: 765.70	 Mean_loss: 0.17594035,  training time: 2.00
progress:  18%|[34m‚ñà‚ñä        [0m| 92/500 [03:06<13:14,  1.95s/it]progress:  19%|[34m‚ñà‚ñä        [0m| 93/500 [03:06<13:19,  1.96s/it]                                                          Episode 94	 reward: -7.85	 makespan: 776.70	 Mean_loss: 0.17630577,  training time: 1.96
progress:  19%|[34m‚ñà‚ñä        [0m| 93/500 [03:08<13:19,  1.96s/it]progress:  19%|[34m‚ñà‚ñâ        [0m| 94/500 [03:08<13:17,  1.96s/it]                                                          Episode 95	 reward: -7.72	 makespan: 764.40	 Mean_loss: 0.15026434,  training time: 1.99
progress:  19%|[34m‚ñà‚ñâ        [0m| 94/500 [03:10<13:17,  1.96s/it]progress:  19%|[34m‚ñà‚ñâ        [0m| 95/500 [03:10<13:18,  1.97s/it]                                                          Episode 96	 reward: -7.59	 makespan: 751.65	 Mean_loss: 0.13897796,  training time: 1.97
progress:  19%|[34m‚ñà‚ñâ        [0m| 95/500 [03:12<13:18,  1.97s/it]progress:  19%|[34m‚ñà‚ñâ        [0m| 96/500 [03:12<13:17,  1.97s/it]                                                          Episode 97	 reward: -7.73	 makespan: 765.45	 Mean_loss: 0.18263392,  training time: 2.04
progress:  19%|[34m‚ñà‚ñâ        [0m| 96/500 [03:14<13:17,  1.97s/it]progress:  19%|[34m‚ñà‚ñâ        [0m| 97/500 [03:14<13:22,  1.99s/it]                                                          Episode 98	 reward: -7.67	 makespan: 758.85	 Mean_loss: 0.16180384,  training time: 2.03
progress:  19%|[34m‚ñà‚ñâ        [0m| 97/500 [03:16<13:22,  1.99s/it]progress:  20%|[34m‚ñà‚ñâ        [0m| 98/500 [03:16<13:25,  2.00s/it]                                                          Episode 99	 reward: -7.74	 makespan: 766.25	 Mean_loss: 0.15896735,  training time: 2.01
progress:  20%|[34m‚ñà‚ñâ        [0m| 98/500 [03:18<13:25,  2.00s/it]progress:  20%|[34m‚ñà‚ñâ        [0m| 99/500 [03:18<13:24,  2.01s/it]                                                          Episode 100	 reward: -7.65	 makespan: 757.70	 Mean_loss: 0.16488242,  training time: 2.00
progress:  20%|[34m‚ñà‚ñâ        [0m| 99/500 [03:20<13:24,  2.01s/it]progress:  20%|[34m‚ñà‚ñà        [0m| 100/500 [03:20<13:21,  2.00s/it]                                                           Episode 101	 reward: -7.77	 makespan: 769.60	 Mean_loss: 0.20488998,  training time: 2.02
progress:  20%|[34m‚ñà‚ñà        [0m| 100/500 [03:22<13:21,  2.00s/it]progress:  20%|[34m‚ñà‚ñà        [0m| 101/500 [03:22<13:21,  2.01s/it]                                                           Episode 102	 reward: -7.83	 makespan: 775.00	 Mean_loss: 0.20866141,  training time: 1.97
progress:  20%|[34m‚ñà‚ñà        [0m| 101/500 [03:24<13:21,  2.01s/it]progress:  20%|[34m‚ñà‚ñà        [0m| 102/500 [03:24<13:15,  2.00s/it]                                                           Episode 103	 reward: -7.85	 makespan: 777.25	 Mean_loss: 0.20321976,  training time: 1.92
progress:  20%|[34m‚ñà‚ñà        [0m| 102/500 [03:26<13:15,  2.00s/it]progress:  21%|[34m‚ñà‚ñà        [0m| 103/500 [03:26<13:04,  1.98s/it]                                                           Episode 104	 reward: -7.55	 makespan: 747.40	 Mean_loss: 0.22194538,  training time: 2.10
progress:  21%|[34m‚ñà‚ñà        [0m| 103/500 [03:28<13:04,  1.98s/it]progress:  21%|[34m‚ñà‚ñà        [0m| 104/500 [03:28<13:17,  2.01s/it]                                                           Episode 105	 reward: -7.81	 makespan: 773.60	 Mean_loss: 0.22802860,  training time: 1.99
progress:  21%|[34m‚ñà‚ñà        [0m| 104/500 [03:30<13:17,  2.01s/it]progress:  21%|[34m‚ñà‚ñà        [0m| 105/500 [03:30<13:12,  2.01s/it]                                                           Episode 106	 reward: -7.72	 makespan: 764.50	 Mean_loss: 0.23600683,  training time: 2.02
progress:  21%|[34m‚ñà‚ñà        [0m| 105/500 [03:32<13:12,  2.01s/it]progress:  21%|[34m‚ñà‚ñà        [0m| 106/500 [03:32<13:12,  2.01s/it]                                                           Episode 107	 reward: -7.67	 makespan: 759.50	 Mean_loss: 0.14893337,  training time: 1.90
progress:  21%|[34m‚ñà‚ñà        [0m| 106/500 [03:34<13:12,  2.01s/it]progress:  21%|[34m‚ñà‚ñà‚ñè       [0m| 107/500 [03:34<12:57,  1.98s/it]                                                           Episode 108	 reward: -7.72	 makespan: 764.20	 Mean_loss: 0.17867815,  training time: 1.92
progress:  21%|[34m‚ñà‚ñà‚ñè       [0m| 107/500 [03:35<12:57,  1.98s/it]progress:  22%|[34m‚ñà‚ñà‚ñè       [0m| 108/500 [03:35<12:48,  1.96s/it]                                                           Episode 109	 reward: -7.73	 makespan: 764.85	 Mean_loss: 0.22402893,  training time: 1.97
progress:  22%|[34m‚ñà‚ñà‚ñè       [0m| 108/500 [03:37<12:48,  1.96s/it]progress:  22%|[34m‚ñà‚ñà‚ñè       [0m| 109/500 [03:37<12:48,  1.96s/it]                                                           Episode 110	 reward: -7.77	 makespan: 769.45	 Mean_loss: 0.20790015,  training time: 1.93
progress:  22%|[34m‚ñà‚ñà‚ñè       [0m| 109/500 [03:39<12:48,  1.96s/it]progress:  22%|[34m‚ñà‚ñà‚ñè       [0m| 110/500 [03:39<12:42,  1.96s/it]                                                           Episode 111	 reward: -7.79	 makespan: 771.45	 Mean_loss: 0.17822500,  training time: 1.99
progress:  22%|[34m‚ñà‚ñà‚ñè       [0m| 110/500 [03:41<12:42,  1.96s/it]progress:  22%|[34m‚ñà‚ñà‚ñè       [0m| 111/500 [03:41<12:45,  1.97s/it]                                                           Episode 112	 reward: -7.63	 makespan: 755.40	 Mean_loss: 0.19068444,  training time: 1.93
progress:  22%|[34m‚ñà‚ñà‚ñè       [0m| 111/500 [03:43<12:45,  1.97s/it]progress:  22%|[34m‚ñà‚ñà‚ñè       [0m| 112/500 [03:43<12:38,  1.96s/it]                                                           Episode 113	 reward: -7.81	 makespan: 773.00	 Mean_loss: 0.17678492,  training time: 1.97
progress:  22%|[34m‚ñà‚ñà‚ñè       [0m| 112/500 [03:45<12:38,  1.96s/it]progress:  23%|[34m‚ñà‚ñà‚ñé       [0m| 113/500 [03:45<12:38,  1.96s/it]                                                           Episode 114	 reward: -7.81	 makespan: 773.15	 Mean_loss: 0.19711989,  training time: 1.95
progress:  23%|[34m‚ñà‚ñà‚ñé       [0m| 113/500 [03:47<12:38,  1.96s/it]progress:  23%|[34m‚ñà‚ñà‚ñé       [0m| 114/500 [03:47<12:35,  1.96s/it]                                                           Episode 115	 reward: -7.76	 makespan: 768.35	 Mean_loss: 0.22898738,  training time: 1.96
progress:  23%|[34m‚ñà‚ñà‚ñé       [0m| 114/500 [03:49<12:35,  1.96s/it]progress:  23%|[34m‚ñà‚ñà‚ñé       [0m| 115/500 [03:49<12:34,  1.96s/it]                                                           Episode 116	 reward: -7.70	 makespan: 761.95	 Mean_loss: 0.19697101,  training time: 1.94
progress:  23%|[34m‚ñà‚ñà‚ñé       [0m| 115/500 [03:51<12:34,  1.96s/it]progress:  23%|[34m‚ñà‚ñà‚ñé       [0m| 116/500 [03:51<12:29,  1.95s/it]                                                           Episode 117	 reward: -7.70	 makespan: 762.35	 Mean_loss: 0.20689991,  training time: 1.96
progress:  23%|[34m‚ñà‚ñà‚ñé       [0m| 116/500 [03:53<12:29,  1.95s/it]progress:  23%|[34m‚ñà‚ñà‚ñé       [0m| 117/500 [03:53<12:28,  1.96s/it]                                                           Episode 118	 reward: -7.79	 makespan: 771.30	 Mean_loss: 0.21933816,  training time: 1.95
progress:  23%|[34m‚ñà‚ñà‚ñé       [0m| 117/500 [03:55<12:28,  1.96s/it]progress:  24%|[34m‚ñà‚ñà‚ñé       [0m| 118/500 [03:55<12:26,  1.96s/it]                                                           Episode 119	 reward: -7.62	 makespan: 754.75	 Mean_loss: 0.15713194,  training time: 1.93
progress:  24%|[34m‚ñà‚ñà‚ñé       [0m| 118/500 [03:57<12:26,  1.96s/it]progress:  24%|[34m‚ñà‚ñà‚ñç       [0m| 119/500 [03:57<12:22,  1.95s/it]                                                           Episode 120	 reward: -7.56	 makespan: 748.85	 Mean_loss: 0.18096626,  training time: 1.92
progress:  24%|[34m‚ñà‚ñà‚ñç       [0m| 119/500 [03:59<12:22,  1.95s/it]progress:  24%|[34m‚ñà‚ñà‚ñç       [0m| 120/500 [03:59<12:17,  1.94s/it]                                                           Episode 121	 reward: -7.83	 makespan: 775.45	 Mean_loss: 0.21280600,  training time: 1.98
progress:  24%|[34m‚ñà‚ñà‚ñç       [0m| 120/500 [04:01<12:17,  1.94s/it]progress:  24%|[34m‚ñà‚ñà‚ñç       [0m| 121/500 [04:01<12:20,  1.95s/it]                                                           Episode 122	 reward: -7.86	 makespan: 778.10	 Mean_loss: 0.21154183,  training time: 1.91
progress:  24%|[34m‚ñà‚ñà‚ñç       [0m| 121/500 [04:03<12:20,  1.95s/it]progress:  24%|[34m‚ñà‚ñà‚ñç       [0m| 122/500 [04:03<12:13,  1.94s/it]                                                           Episode 123	 reward: -7.66	 makespan: 757.90	 Mean_loss: 0.21757396,  training time: 1.91
progress:  24%|[34m‚ñà‚ñà‚ñç       [0m| 122/500 [04:05<12:13,  1.94s/it]progress:  25%|[34m‚ñà‚ñà‚ñç       [0m| 123/500 [04:05<12:08,  1.93s/it]                                                           Episode 124	 reward: -7.67	 makespan: 759.65	 Mean_loss: 0.19836858,  training time: 1.92
progress:  25%|[34m‚ñà‚ñà‚ñç       [0m| 123/500 [04:07<12:08,  1.93s/it]progress:  25%|[34m‚ñà‚ñà‚ñç       [0m| 124/500 [04:07<12:05,  1.93s/it]                                                           Episode 125	 reward: -7.68	 makespan: 759.90	 Mean_loss: 0.16830403,  training time: 1.92
progress:  25%|[34m‚ñà‚ñà‚ñç       [0m| 124/500 [04:09<12:05,  1.93s/it]progress:  25%|[34m‚ñà‚ñà‚ñå       [0m| 125/500 [04:09<12:02,  1.93s/it]                                                           Episode 126	 reward: -7.62	 makespan: 754.60	 Mean_loss: 0.15116037,  training time: 1.94
progress:  25%|[34m‚ñà‚ñà‚ñå       [0m| 125/500 [04:11<12:02,  1.93s/it]progress:  25%|[34m‚ñà‚ñà‚ñå       [0m| 126/500 [04:11<12:02,  1.93s/it]                                                           Episode 127	 reward: -7.68	 makespan: 760.35	 Mean_loss: 0.17746405,  training time: 1.93
progress:  25%|[34m‚ñà‚ñà‚ñå       [0m| 126/500 [04:12<12:02,  1.93s/it]progress:  25%|[34m‚ñà‚ñà‚ñå       [0m| 127/500 [04:12<12:00,  1.93s/it]                                                           Episode 128	 reward: -7.58	 makespan: 750.30	 Mean_loss: 0.14876245,  training time: 1.88
progress:  25%|[34m‚ñà‚ñà‚ñå       [0m| 127/500 [04:14<12:00,  1.93s/it]progress:  26%|[34m‚ñà‚ñà‚ñå       [0m| 128/500 [04:14<11:53,  1.92s/it]                                                           Episode 129	 reward: -7.69	 makespan: 761.00	 Mean_loss: 0.20746903,  training time: 1.91
progress:  26%|[34m‚ñà‚ñà‚ñå       [0m| 128/500 [04:16<11:53,  1.92s/it]progress:  26%|[34m‚ñà‚ñà‚ñå       [0m| 129/500 [04:16<11:50,  1.91s/it]                                                           Episode 130	 reward: -7.54	 makespan: 746.10	 Mean_loss: 0.13411181,  training time: 1.93
progress:  26%|[34m‚ñà‚ñà‚ñå       [0m| 129/500 [04:18<11:50,  1.91s/it]progress:  26%|[34m‚ñà‚ñà‚ñå       [0m| 130/500 [04:18<11:50,  1.92s/it]                                                           Episode 131	 reward: -7.72	 makespan: 764.25	 Mean_loss: 0.16065660,  training time: 1.92
progress:  26%|[34m‚ñà‚ñà‚ñå       [0m| 130/500 [04:20<11:50,  1.92s/it]progress:  26%|[34m‚ñà‚ñà‚ñå       [0m| 131/500 [04:20<11:48,  1.92s/it]                                                           Episode 132	 reward: -7.59	 makespan: 751.50	 Mean_loss: 0.19201076,  training time: 1.92
progress:  26%|[34m‚ñà‚ñà‚ñå       [0m| 131/500 [04:22<11:48,  1.92s/it]progress:  26%|[34m‚ñà‚ñà‚ñã       [0m| 132/500 [04:22<11:47,  1.92s/it]                                                           Episode 133	 reward: -7.73	 makespan: 765.50	 Mean_loss: 0.18096216,  training time: 1.94
progress:  26%|[34m‚ñà‚ñà‚ñã       [0m| 132/500 [04:24<11:47,  1.92s/it]progress:  27%|[34m‚ñà‚ñà‚ñã       [0m| 133/500 [04:24<11:47,  1.93s/it]                                                           Episode 134	 reward: -7.59	 makespan: 751.70	 Mean_loss: 0.18082580,  training time: 1.89
progress:  27%|[34m‚ñà‚ñà‚ñã       [0m| 133/500 [04:26<11:47,  1.93s/it]progress:  27%|[34m‚ñà‚ñà‚ñã       [0m| 134/500 [04:26<11:41,  1.92s/it]                                                           Episode 135	 reward: -7.59	 makespan: 750.95	 Mean_loss: 0.16044267,  training time: 1.92
progress:  27%|[34m‚ñà‚ñà‚ñã       [0m| 134/500 [04:28<11:41,  1.92s/it]progress:  27%|[34m‚ñà‚ñà‚ñã       [0m| 135/500 [04:28<11:39,  1.92s/it]                                                           Episode 136	 reward: -7.54	 makespan: 746.70	 Mean_loss: 0.13696620,  training time: 1.93
progress:  27%|[34m‚ñà‚ñà‚ñã       [0m| 135/500 [04:30<11:39,  1.92s/it]progress:  27%|[34m‚ñà‚ñà‚ñã       [0m| 136/500 [04:30<11:39,  1.92s/it]                                                           Episode 137	 reward: -7.63	 makespan: 755.75	 Mean_loss: 0.18243526,  training time: 1.89
progress:  27%|[34m‚ñà‚ñà‚ñã       [0m| 136/500 [04:32<11:39,  1.92s/it]progress:  27%|[34m‚ñà‚ñà‚ñã       [0m| 137/500 [04:32<11:33,  1.91s/it]                                                           Episode 138	 reward: -7.49	 makespan: 741.30	 Mean_loss: 0.13789804,  training time: 1.92
progress:  27%|[34m‚ñà‚ñà‚ñã       [0m| 137/500 [04:34<11:33,  1.91s/it]progress:  28%|[34m‚ñà‚ñà‚ñä       [0m| 138/500 [04:34<11:33,  1.92s/it]                                                           Episode 139	 reward: -7.50	 makespan: 742.25	 Mean_loss: 0.18150584,  training time: 1.95
progress:  28%|[34m‚ñà‚ñà‚ñä       [0m| 138/500 [04:35<11:33,  1.92s/it]progress:  28%|[34m‚ñà‚ñà‚ñä       [0m| 139/500 [04:35<11:35,  1.93s/it]                                                           Episode 140	 reward: -7.55	 makespan: 747.90	 Mean_loss: 0.13149998,  training time: 1.92
progress:  28%|[34m‚ñà‚ñà‚ñä       [0m| 139/500 [04:37<11:35,  1.93s/it]progress:  28%|[34m‚ñà‚ñà‚ñä       [0m| 140/500 [04:37<11:33,  1.93s/it]                                                           Episode 141	 reward: -7.16	 makespan: 708.90	 Mean_loss: 0.13431625,  training time: 1.98
progress:  28%|[34m‚ñà‚ñà‚ñä       [0m| 140/500 [04:39<11:33,  1.93s/it]progress:  28%|[34m‚ñà‚ñà‚ñä       [0m| 141/500 [04:39<11:37,  1.94s/it]                                                           Episode 142	 reward: -7.24	 makespan: 716.50	 Mean_loss: 0.12351693,  training time: 1.94
progress:  28%|[34m‚ñà‚ñà‚ñä       [0m| 141/500 [04:41<11:37,  1.94s/it]progress:  28%|[34m‚ñà‚ñà‚ñä       [0m| 142/500 [04:41<11:34,  1.94s/it]                                                           Episode 143	 reward: -7.16	 makespan: 708.40	 Mean_loss: 0.12415153,  training time: 1.95
progress:  28%|[34m‚ñà‚ñà‚ñä       [0m| 142/500 [04:43<11:34,  1.94s/it]progress:  29%|[34m‚ñà‚ñà‚ñä       [0m| 143/500 [04:43<11:34,  1.94s/it]                                                           Episode 144	 reward: -7.26	 makespan: 718.30	 Mean_loss: 0.13603579,  training time: 1.92
progress:  29%|[34m‚ñà‚ñà‚ñä       [0m| 143/500 [04:45<11:34,  1.94s/it]progress:  29%|[34m‚ñà‚ñà‚ñâ       [0m| 144/500 [04:45<11:29,  1.94s/it]                                                           Episode 145	 reward: -7.17	 makespan: 709.85	 Mean_loss: 0.15600631,  training time: 1.92
progress:  29%|[34m‚ñà‚ñà‚ñâ       [0m| 144/500 [04:47<11:29,  1.94s/it]progress:  29%|[34m‚ñà‚ñà‚ñâ       [0m| 145/500 [04:47<11:25,  1.93s/it]                                                           Episode 146	 reward: -7.26	 makespan: 718.85	 Mean_loss: 0.14736912,  training time: 1.92
progress:  29%|[34m‚ñà‚ñà‚ñâ       [0m| 145/500 [04:49<11:25,  1.93s/it]progress:  29%|[34m‚ñà‚ñà‚ñâ       [0m| 146/500 [04:49<11:22,  1.93s/it]                                                           Episode 147	 reward: -7.30	 makespan: 722.70	 Mean_loss: 0.16452166,  training time: 1.92
progress:  29%|[34m‚ñà‚ñà‚ñâ       [0m| 146/500 [04:51<11:22,  1.93s/it]progress:  29%|[34m‚ñà‚ñà‚ñâ       [0m| 147/500 [04:51<11:20,  1.93s/it]                                                           Episode 148	 reward: -7.38	 makespan: 730.55	 Mean_loss: 0.13940156,  training time: 1.99
progress:  29%|[34m‚ñà‚ñà‚ñâ       [0m| 147/500 [04:53<11:20,  1.93s/it]progress:  30%|[34m‚ñà‚ñà‚ñâ       [0m| 148/500 [04:53<11:24,  1.95s/it]                                                           Episode 149	 reward: -7.27	 makespan: 720.05	 Mean_loss: 0.12091333,  training time: 1.94
progress:  30%|[34m‚ñà‚ñà‚ñâ       [0m| 148/500 [04:55<11:24,  1.95s/it]progress:  30%|[34m‚ñà‚ñà‚ñâ       [0m| 149/500 [04:55<11:22,  1.94s/it]                                                           Episode 150	 reward: -7.19	 makespan: 711.40	 Mean_loss: 0.16383073,  training time: 1.94
progress:  30%|[34m‚ñà‚ñà‚ñâ       [0m| 149/500 [04:57<11:22,  1.94s/it]progress:  30%|[34m‚ñà‚ñà‚ñà       [0m| 150/500 [04:57<11:19,  1.94s/it]                                                           Episode 151	 reward: -7.19	 makespan: 712.20	 Mean_loss: 0.10795382,  training time: 1.89
progress:  30%|[34m‚ñà‚ñà‚ñà       [0m| 150/500 [04:59<11:19,  1.94s/it]progress:  30%|[34m‚ñà‚ñà‚ñà       [0m| 151/500 [04:59<11:12,  1.93s/it]                                                           Episode 152	 reward: -7.36	 makespan: 729.10	 Mean_loss: 0.13142736,  training time: 1.91
progress:  30%|[34m‚ñà‚ñà‚ñà       [0m| 151/500 [05:01<11:12,  1.93s/it]progress:  30%|[34m‚ñà‚ñà‚ñà       [0m| 152/500 [05:01<11:08,  1.92s/it]                                                           Episode 153	 reward: -7.34	 makespan: 727.15	 Mean_loss: 0.13107948,  training time: 1.89
progress:  30%|[34m‚ñà‚ñà‚ñà       [0m| 152/500 [05:02<11:08,  1.92s/it]progress:  31%|[34m‚ñà‚ñà‚ñà       [0m| 153/500 [05:02<11:03,  1.91s/it]                                                           Episode 154	 reward: -7.19	 makespan: 711.70	 Mean_loss: 0.14004241,  training time: 1.88
progress:  31%|[34m‚ñà‚ñà‚ñà       [0m| 153/500 [05:04<11:03,  1.91s/it]progress:  31%|[34m‚ñà‚ñà‚ñà       [0m| 154/500 [05:04<10:58,  1.90s/it]                                                           Episode 155	 reward: -7.18	 makespan: 711.00	 Mean_loss: 0.11902568,  training time: 1.90
progress:  31%|[34m‚ñà‚ñà‚ñà       [0m| 154/500 [05:06<10:58,  1.90s/it]progress:  31%|[34m‚ñà‚ñà‚ñà       [0m| 155/500 [05:06<10:55,  1.90s/it]                                                           Episode 156	 reward: -7.28	 makespan: 720.70	 Mean_loss: 0.13506261,  training time: 1.90
progress:  31%|[34m‚ñà‚ñà‚ñà       [0m| 155/500 [05:08<10:55,  1.90s/it]progress:  31%|[34m‚ñà‚ñà‚ñà       [0m| 156/500 [05:08<10:53,  1.90s/it]                                                           Episode 157	 reward: -7.31	 makespan: 723.35	 Mean_loss: 0.11487912,  training time: 1.90
progress:  31%|[34m‚ñà‚ñà‚ñà       [0m| 156/500 [05:10<10:53,  1.90s/it]progress:  31%|[34m‚ñà‚ñà‚ñà‚ñè      [0m| 157/500 [05:10<10:51,  1.90s/it]                                                           Episode 158	 reward: -7.30	 makespan: 723.15	 Mean_loss: 0.13866279,  training time: 1.91
progress:  31%|[34m‚ñà‚ñà‚ñà‚ñè      [0m| 157/500 [05:12<10:51,  1.90s/it]progress:  32%|[34m‚ñà‚ñà‚ñà‚ñè      [0m| 158/500 [05:12<10:50,  1.90s/it]                                                           Episode 159	 reward: -7.27	 makespan: 719.70	 Mean_loss: 0.13873383,  training time: 1.89
progress:  32%|[34m‚ñà‚ñà‚ñà‚ñè      [0m| 158/500 [05:14<10:50,  1.90s/it]progress:  32%|[34m‚ñà‚ñà‚ñà‚ñè      [0m| 159/500 [05:14<10:48,  1.90s/it]                                                           Episode 160	 reward: -7.16	 makespan: 708.60	 Mean_loss: 0.10168419,  training time: 1.92
progress:  32%|[34m‚ñà‚ñà‚ñà‚ñè      [0m| 159/500 [05:16<10:48,  1.90s/it]progress:  32%|[34m‚ñà‚ñà‚ñà‚ñè      [0m| 160/500 [05:16<10:48,  1.91s/it]                                                           Episode 161	 reward: -7.53	 makespan: 745.15	 Mean_loss: 0.12998386,  training time: 1.96
progress:  32%|[34m‚ñà‚ñà‚ñà‚ñè      [0m| 160/500 [05:18<10:48,  1.91s/it]progress:  32%|[34m‚ñà‚ñà‚ñà‚ñè      [0m| 161/500 [05:18<10:51,  1.92s/it]                                                           Episode 162	 reward: -7.58	 makespan: 750.35	 Mean_loss: 0.12888198,  training time: 1.86
progress:  32%|[34m‚ñà‚ñà‚ñà‚ñè      [0m| 161/500 [05:20<10:51,  1.92s/it]progress:  32%|[34m‚ñà‚ñà‚ñà‚ñè      [0m| 162/500 [05:20<10:43,  1.90s/it]                                                           Episode 163	 reward: -7.45	 makespan: 737.70	 Mean_loss: 0.11296687,  training time: 1.90
progress:  32%|[34m‚ñà‚ñà‚ñà‚ñè      [0m| 162/500 [05:22<10:43,  1.90s/it]progress:  33%|[34m‚ñà‚ñà‚ñà‚ñé      [0m| 163/500 [05:22<10:41,  1.90s/it]                                                           Episode 164	 reward: -7.59	 makespan: 751.50	 Mean_loss: 0.15195659,  training time: 1.88
progress:  33%|[34m‚ñà‚ñà‚ñà‚ñé      [0m| 163/500 [05:23<10:41,  1.90s/it]progress:  33%|[34m‚ñà‚ñà‚ñà‚ñé      [0m| 164/500 [05:23<10:37,  1.90s/it]                                                           Episode 165	 reward: -7.44	 makespan: 737.05	 Mean_loss: 0.11295506,  training time: 1.91
progress:  33%|[34m‚ñà‚ñà‚ñà‚ñé      [0m| 164/500 [05:25<10:37,  1.90s/it]progress:  33%|[34m‚ñà‚ñà‚ñà‚ñé      [0m| 165/500 [05:25<10:36,  1.90s/it]                                                           Episode 166	 reward: -7.51	 makespan: 743.95	 Mean_loss: 0.14427371,  training time: 1.91
progress:  33%|[34m‚ñà‚ñà‚ñà‚ñé      [0m| 165/500 [05:27<10:36,  1.90s/it]progress:  33%|[34m‚ñà‚ñà‚ñà‚ñé      [0m| 166/500 [05:27<10:35,  1.90s/it]                                                           Episode 167	 reward: -7.45	 makespan: 737.30	 Mean_loss: 0.12665053,  training time: 1.91
progress:  33%|[34m‚ñà‚ñà‚ñà‚ñé      [0m| 166/500 [05:29<10:35,  1.90s/it]progress:  33%|[34m‚ñà‚ñà‚ñà‚ñé      [0m| 167/500 [05:29<10:34,  1.91s/it]                                                           Episode 168	 reward: -7.56	 makespan: 748.90	 Mean_loss: 0.12041380,  training time: 1.90
progress:  33%|[34m‚ñà‚ñà‚ñà‚ñé      [0m| 167/500 [05:31<10:34,  1.91s/it]progress:  34%|[34m‚ñà‚ñà‚ñà‚ñé      [0m| 168/500 [05:31<10:32,  1.91s/it]                                                           Episode 169	 reward: -7.47	 makespan: 739.80	 Mean_loss: 0.14856154,  training time: 1.86
progress:  34%|[34m‚ñà‚ñà‚ñà‚ñé      [0m| 168/500 [05:33<10:32,  1.91s/it]progress:  34%|[34m‚ñà‚ñà‚ñà‚ñç      [0m| 169/500 [05:33<10:26,  1.89s/it]                                                           Episode 170	 reward: -7.51	 makespan: 743.80	 Mean_loss: 0.14016607,  training time: 1.91
progress:  34%|[34m‚ñà‚ñà‚ñà‚ñç      [0m| 169/500 [05:35<10:26,  1.89s/it]progress:  34%|[34m‚ñà‚ñà‚ñà‚ñç      [0m| 170/500 [05:35<10:26,  1.90s/it]                                                           Episode 171	 reward: -7.63	 makespan: 755.00	 Mean_loss: 0.15751037,  training time: 1.88
progress:  34%|[34m‚ñà‚ñà‚ñà‚ñç      [0m| 170/500 [05:37<10:26,  1.90s/it]progress:  34%|[34m‚ñà‚ñà‚ñà‚ñç      [0m| 171/500 [05:37<10:22,  1.89s/it]                                                           Episode 172	 reward: -7.57	 makespan: 748.95	 Mean_loss: 0.12488820,  training time: 1.96
progress:  34%|[34m‚ñà‚ñà‚ñà‚ñç      [0m| 171/500 [05:39<10:22,  1.89s/it]progress:  34%|[34m‚ñà‚ñà‚ñà‚ñç      [0m| 172/500 [05:39<10:27,  1.91s/it]                                                           Episode 173	 reward: -7.65	 makespan: 756.95	 Mean_loss: 0.13186048,  training time: 1.92
progress:  34%|[34m‚ñà‚ñà‚ñà‚ñç      [0m| 172/500 [05:41<10:27,  1.91s/it]progress:  35%|[34m‚ñà‚ñà‚ñà‚ñç      [0m| 173/500 [05:41<10:26,  1.92s/it]                                                           Episode 174	 reward: -7.53	 makespan: 745.00	 Mean_loss: 0.14441258,  training time: 1.85
progress:  35%|[34m‚ñà‚ñà‚ñà‚ñç      [0m| 173/500 [05:42<10:26,  1.92s/it]progress:  35%|[34m‚ñà‚ñà‚ñà‚ñç      [0m| 174/500 [05:42<10:18,  1.90s/it]                                                           Episode 175	 reward: -7.50	 makespan: 742.20	 Mean_loss: 0.15090671,  training time: 1.90
progress:  35%|[34m‚ñà‚ñà‚ñà‚ñç      [0m| 174/500 [05:44<10:18,  1.90s/it]progress:  35%|[34m‚ñà‚ñà‚ñà‚ñå      [0m| 175/500 [05:44<10:16,  1.90s/it]                                                           Episode 176	 reward: -7.52	 makespan: 744.95	 Mean_loss: 0.16901688,  training time: 1.90
progress:  35%|[34m‚ñà‚ñà‚ñà‚ñå      [0m| 175/500 [05:46<10:16,  1.90s/it]progress:  35%|[34m‚ñà‚ñà‚ñà‚ñå      [0m| 176/500 [05:46<10:15,  1.90s/it]                                                           Episode 177	 reward: -7.45	 makespan: 737.55	 Mean_loss: 0.12047009,  training time: 1.98
progress:  35%|[34m‚ñà‚ñà‚ñà‚ñå      [0m| 176/500 [05:48<10:15,  1.90s/it]progress:  35%|[34m‚ñà‚ñà‚ñà‚ñå      [0m| 177/500 [05:48<10:21,  1.92s/it]                                                           Episode 178	 reward: -7.63	 makespan: 755.40	 Mean_loss: 0.13499913,  training time: 1.92
progress:  35%|[34m‚ñà‚ñà‚ñà‚ñå      [0m| 177/500 [05:50<10:21,  1.92s/it]progress:  36%|[34m‚ñà‚ñà‚ñà‚ñå      [0m| 178/500 [05:50<10:19,  1.92s/it]                                                           Episode 179	 reward: -7.62	 makespan: 754.00	 Mean_loss: 0.15866418,  training time: 1.87
progress:  36%|[34m‚ñà‚ñà‚ñà‚ñå      [0m| 178/500 [05:52<10:19,  1.92s/it]progress:  36%|[34m‚ñà‚ñà‚ñà‚ñå      [0m| 179/500 [05:52<10:12,  1.91s/it]                                                           Episode 180	 reward: -7.42	 makespan: 734.35	 Mean_loss: 0.12881401,  training time: 1.93
progress:  36%|[34m‚ñà‚ñà‚ñà‚ñå      [0m| 179/500 [05:54<10:12,  1.91s/it]progress:  36%|[34m‚ñà‚ñà‚ñà‚ñå      [0m| 180/500 [05:54<10:13,  1.92s/it]                                                           Episode 181	 reward: -7.51	 makespan: 743.20	 Mean_loss: 0.15367204,  training time: 1.96
progress:  36%|[34m‚ñà‚ñà‚ñà‚ñå      [0m| 180/500 [05:56<10:13,  1.92s/it]progress:  36%|[34m‚ñà‚ñà‚ñà‚ñå      [0m| 181/500 [05:56<10:15,  1.93s/it]                                                           Episode 182	 reward: -7.53	 makespan: 745.70	 Mean_loss: 0.17008813,  training time: 1.93
progress:  36%|[34m‚ñà‚ñà‚ñà‚ñå      [0m| 181/500 [05:58<10:15,  1.93s/it]progress:  36%|[34m‚ñà‚ñà‚ñà‚ñã      [0m| 182/500 [05:58<10:13,  1.93s/it]                                                           Episode 183	 reward: -7.41	 makespan: 733.15	 Mean_loss: 0.12865292,  training time: 1.88
progress:  36%|[34m‚ñà‚ñà‚ñà‚ñã      [0m| 182/500 [06:00<10:13,  1.93s/it]progress:  37%|[34m‚ñà‚ñà‚ñà‚ñã      [0m| 183/500 [06:00<10:07,  1.92s/it]                                                           Episode 184	 reward: -7.49	 makespan: 741.30	 Mean_loss: 0.13212301,  training time: 1.89
progress:  37%|[34m‚ñà‚ñà‚ñà‚ñã      [0m| 183/500 [06:02<10:07,  1.92s/it]progress:  37%|[34m‚ñà‚ñà‚ñà‚ñã      [0m| 184/500 [06:02<10:02,  1.91s/it]                                                           Episode 185	 reward: -7.45	 makespan: 737.85	 Mean_loss: 0.13922489,  training time: 1.91
progress:  37%|[34m‚ñà‚ñà‚ñà‚ñã      [0m| 184/500 [06:04<10:02,  1.91s/it]progress:  37%|[34m‚ñà‚ñà‚ñà‚ñã      [0m| 185/500 [06:04<10:01,  1.91s/it]                                                           Episode 186	 reward: -7.43	 makespan: 735.20	 Mean_loss: 0.10241379,  training time: 1.88
progress:  37%|[34m‚ñà‚ñà‚ñà‚ñã      [0m| 185/500 [06:05<10:01,  1.91s/it]progress:  37%|[34m‚ñà‚ñà‚ñà‚ñã      [0m| 186/500 [06:05<09:56,  1.90s/it]                                                           Episode 187	 reward: -7.45	 makespan: 737.35	 Mean_loss: 0.13082679,  training time: 1.89
progress:  37%|[34m‚ñà‚ñà‚ñà‚ñã      [0m| 186/500 [06:07<09:56,  1.90s/it]progress:  37%|[34m‚ñà‚ñà‚ñà‚ñã      [0m| 187/500 [06:07<09:53,  1.90s/it]                                                           Episode 188	 reward: -7.49	 makespan: 741.55	 Mean_loss: 0.15140456,  training time: 1.90
progress:  37%|[34m‚ñà‚ñà‚ñà‚ñã      [0m| 187/500 [06:09<09:53,  1.90s/it]progress:  38%|[34m‚ñà‚ñà‚ñà‚ñä      [0m| 188/500 [06:09<09:51,  1.90s/it]                                                           Episode 189	 reward: -7.42	 makespan: 734.45	 Mean_loss: 0.10934398,  training time: 1.91
progress:  38%|[34m‚ñà‚ñà‚ñà‚ñä      [0m| 188/500 [06:11<09:51,  1.90s/it]progress:  38%|[34m‚ñà‚ñà‚ñà‚ñä      [0m| 189/500 [06:11<09:51,  1.90s/it]                                                           Episode 190	 reward: -7.44	 makespan: 736.90	 Mean_loss: 0.14558190,  training time: 1.90
progress:  38%|[34m‚ñà‚ñà‚ñà‚ñä      [0m| 189/500 [06:13<09:51,  1.90s/it]progress:  38%|[34m‚ñà‚ñà‚ñà‚ñä      [0m| 190/500 [06:13<09:49,  1.90s/it]                                                           Episode 191	 reward: -7.38	 makespan: 730.85	 Mean_loss: 0.09230576,  training time: 1.91
progress:  38%|[34m‚ñà‚ñà‚ñà‚ñä      [0m| 190/500 [06:15<09:49,  1.90s/it]progress:  38%|[34m‚ñà‚ñà‚ñà‚ñä      [0m| 191/500 [06:15<09:48,  1.90s/it]                                                           Episode 192	 reward: -7.50	 makespan: 742.20	 Mean_loss: 0.12338836,  training time: 1.91
progress:  38%|[34m‚ñà‚ñà‚ñà‚ñä      [0m| 191/500 [06:17<09:48,  1.90s/it]progress:  38%|[34m‚ñà‚ñà‚ñà‚ñä      [0m| 192/500 [06:17<09:47,  1.91s/it]                                                           Episode 193	 reward: -7.64	 makespan: 756.10	 Mean_loss: 0.14390868,  training time: 1.92
progress:  38%|[34m‚ñà‚ñà‚ñà‚ñä      [0m| 192/500 [06:19<09:47,  1.91s/it]progress:  39%|[34m‚ñà‚ñà‚ñà‚ñä      [0m| 193/500 [06:19<09:46,  1.91s/it]                                                           Episode 194	 reward: -7.40	 makespan: 733.05	 Mean_loss: 0.10896063,  training time: 1.92
progress:  39%|[34m‚ñà‚ñà‚ñà‚ñä      [0m| 193/500 [06:21<09:46,  1.91s/it]progress:  39%|[34m‚ñà‚ñà‚ñà‚ñâ      [0m| 194/500 [06:21<09:45,  1.91s/it]                                                           Episode 195	 reward: -7.42	 makespan: 734.10	 Mean_loss: 0.12543514,  training time: 1.93
progress:  39%|[34m‚ñà‚ñà‚ñà‚ñâ      [0m| 194/500 [06:23<09:45,  1.91s/it]progress:  39%|[34m‚ñà‚ñà‚ñà‚ñâ      [0m| 195/500 [06:23<09:45,  1.92s/it]                                                           Episode 196	 reward: -7.38	 makespan: 730.65	 Mean_loss: 0.11234489,  training time: 1.93
progress:  39%|[34m‚ñà‚ñà‚ñà‚ñâ      [0m| 195/500 [06:25<09:45,  1.92s/it]progress:  39%|[34m‚ñà‚ñà‚ñà‚ñâ      [0m| 196/500 [06:25<09:44,  1.92s/it]                                                           Episode 197	 reward: -7.50	 makespan: 742.80	 Mean_loss: 0.11690377,  training time: 1.93
progress:  39%|[34m‚ñà‚ñà‚ñà‚ñâ      [0m| 196/500 [06:26<09:44,  1.92s/it]progress:  39%|[34m‚ñà‚ñà‚ñà‚ñâ      [0m| 197/500 [06:26<09:42,  1.92s/it]                                                           Episode 198	 reward: -7.42	 makespan: 734.10	 Mean_loss: 0.09917099,  training time: 1.94
progress:  39%|[34m‚ñà‚ñà‚ñà‚ñâ      [0m| 197/500 [06:28<09:42,  1.92s/it]progress:  40%|[34m‚ñà‚ñà‚ñà‚ñâ      [0m| 198/500 [06:28<09:42,  1.93s/it]                                                           Episode 199	 reward: -7.38	 makespan: 730.75	 Mean_loss: 0.09978764,  training time: 1.91
progress:  40%|[34m‚ñà‚ñà‚ñà‚ñâ      [0m| 198/500 [06:30<09:42,  1.93s/it]progress:  40%|[34m‚ñà‚ñà‚ñà‚ñâ      [0m| 199/500 [06:30<09:39,  1.92s/it]                                                           Episode 200	 reward: -7.58	 makespan: 750.70	 Mean_loss: 0.11567815,  training time: 2.00
progress:  40%|[34m‚ñà‚ñà‚ñà‚ñâ      [0m| 199/500 [06:32<09:39,  1.92s/it]progress:  40%|[34m‚ñà‚ñà‚ñà‚ñà      [0m| 200/500 [06:32<09:44,  1.95s/it]                                                           Episode 201	 reward: -7.47	 makespan: 739.10	 Mean_loss: 0.11795809,  training time: 1.95
progress:  40%|[34m‚ñà‚ñà‚ñà‚ñà      [0m| 200/500 [06:34<09:44,  1.95s/it]progress:  40%|[34m‚ñà‚ñà‚ñà‚ñà      [0m| 201/500 [06:34<09:42,  1.95s/it]                                                           Episode 202	 reward: -7.34	 makespan: 727.15	 Mean_loss: 0.14097895,  training time: 1.97
progress:  40%|[34m‚ñà‚ñà‚ñà‚ñà      [0m| 201/500 [06:36<09:42,  1.95s/it]progress:  40%|[34m‚ñà‚ñà‚ñà‚ñà      [0m| 202/500 [06:36<09:42,  1.96s/it]                                                           Episode 203	 reward: -7.39	 makespan: 731.75	 Mean_loss: 0.12176156,  training time: 1.89
progress:  40%|[34m‚ñà‚ñà‚ñà‚ñà      [0m| 202/500 [06:38<09:42,  1.96s/it]progress:  41%|[34m‚ñà‚ñà‚ñà‚ñà      [0m| 203/500 [06:38<09:35,  1.94s/it]                                                           Episode 204	 reward: -7.34	 makespan: 726.90	 Mean_loss: 0.08672421,  training time: 1.92
progress:  41%|[34m‚ñà‚ñà‚ñà‚ñà      [0m| 203/500 [06:40<09:35,  1.94s/it]progress:  41%|[34m‚ñà‚ñà‚ñà‚ñà      [0m| 204/500 [06:40<09:31,  1.93s/it]                                                           Episode 205	 reward: -7.29	 makespan: 722.10	 Mean_loss: 0.09331284,  training time: 1.90
progress:  41%|[34m‚ñà‚ñà‚ñà‚ñà      [0m| 204/500 [06:42<09:31,  1.93s/it]progress:  41%|[34m‚ñà‚ñà‚ñà‚ñà      [0m| 205/500 [06:42<09:27,  1.92s/it]                                                           Episode 206	 reward: -7.32	 makespan: 724.45	 Mean_loss: 0.10029461,  training time: 1.91
progress:  41%|[34m‚ñà‚ñà‚ñà‚ñà      [0m| 205/500 [06:44<09:27,  1.92s/it]progress:  41%|[34m‚ñà‚ñà‚ñà‚ñà      [0m| 206/500 [06:44<09:24,  1.92s/it]                                                           Episode 207	 reward: -7.28	 makespan: 720.65	 Mean_loss: 0.11107320,  training time: 1.98
progress:  41%|[34m‚ñà‚ñà‚ñà‚ñà      [0m| 206/500 [06:46<09:24,  1.92s/it]progress:  41%|[34m‚ñà‚ñà‚ñà‚ñà‚ñè     [0m| 207/500 [06:46<09:27,  1.94s/it]                                                           Episode 208	 reward: -7.35	 makespan: 727.95	 Mean_loss: 0.11537491,  training time: 1.92
progress:  41%|[34m‚ñà‚ñà‚ñà‚ñà‚ñè     [0m| 207/500 [06:48<09:27,  1.94s/it]progress:  42%|[34m‚ñà‚ñà‚ñà‚ñà‚ñè     [0m| 208/500 [06:48<09:24,  1.93s/it]                                                           Episode 209	 reward: -7.40	 makespan: 732.20	 Mean_loss: 0.14634548,  training time: 1.92
progress:  42%|[34m‚ñà‚ñà‚ñà‚ñà‚ñè     [0m| 208/500 [06:50<09:24,  1.93s/it]progress:  42%|[34m‚ñà‚ñà‚ñà‚ñà‚ñè     [0m| 209/500 [06:50<09:21,  1.93s/it]                                                           Episode 210	 reward: -7.38	 makespan: 730.70	 Mean_loss: 0.10469522,  training time: 1.86
progress:  42%|[34m‚ñà‚ñà‚ñà‚ñà‚ñè     [0m| 209/500 [06:52<09:21,  1.93s/it]progress:  42%|[34m‚ñà‚ñà‚ñà‚ñà‚ñè     [0m| 210/500 [06:52<09:14,  1.91s/it]                                                           Episode 211	 reward: -7.28	 makespan: 721.20	 Mean_loss: 0.10504123,  training time: 1.89
progress:  42%|[34m‚ñà‚ñà‚ñà‚ñà‚ñè     [0m| 210/500 [06:53<09:14,  1.91s/it]progress:  42%|[34m‚ñà‚ñà‚ñà‚ñà‚ñè     [0m| 211/500 [06:53<09:10,  1.91s/it]                                                           Episode 212	 reward: -7.25	 makespan: 718.05	 Mean_loss: 0.10545740,  training time: 1.90
progress:  42%|[34m‚ñà‚ñà‚ñà‚ñà‚ñè     [0m| 211/500 [06:55<09:10,  1.91s/it]progress:  42%|[34m‚ñà‚ñà‚ñà‚ñà‚ñè     [0m| 212/500 [06:55<09:08,  1.90s/it]                                                           Episode 213	 reward: -7.27	 makespan: 719.35	 Mean_loss: 0.10806629,  training time: 1.91
progress:  42%|[34m‚ñà‚ñà‚ñà‚ñà‚ñè     [0m| 212/500 [06:57<09:08,  1.90s/it]progress:  43%|[34m‚ñà‚ñà‚ñà‚ñà‚ñé     [0m| 213/500 [06:57<09:06,  1.91s/it]                                                           Episode 214	 reward: -7.24	 makespan: 716.85	 Mean_loss: 0.09547728,  training time: 1.94
progress:  43%|[34m‚ñà‚ñà‚ñà‚ñà‚ñé     [0m| 213/500 [06:59<09:06,  1.91s/it]progress:  43%|[34m‚ñà‚ñà‚ñà‚ñà‚ñé     [0m| 214/500 [06:59<09:07,  1.92s/it]                                                           Episode 215	 reward: -7.29	 makespan: 722.10	 Mean_loss: 0.08927845,  training time: 1.86
progress:  43%|[34m‚ñà‚ñà‚ñà‚ñà‚ñé     [0m| 214/500 [07:01<09:07,  1.92s/it]progress:  43%|[34m‚ñà‚ñà‚ñà‚ñà‚ñé     [0m| 215/500 [07:01<09:01,  1.90s/it]                                                           Episode 216	 reward: -7.29	 makespan: 721.35	 Mean_loss: 0.09270877,  training time: 1.90
progress:  43%|[34m‚ñà‚ñà‚ñà‚ñà‚ñé     [0m| 215/500 [07:03<09:01,  1.90s/it]progress:  43%|[34m‚ñà‚ñà‚ñà‚ñà‚ñé     [0m| 216/500 [07:03<08:59,  1.90s/it]                                                           Episode 217	 reward: -7.25	 makespan: 718.15	 Mean_loss: 0.10731557,  training time: 1.89
progress:  43%|[34m‚ñà‚ñà‚ñà‚ñà‚ñé     [0m| 216/500 [07:05<08:59,  1.90s/it]progress:  43%|[34m‚ñà‚ñà‚ñà‚ñà‚ñé     [0m| 217/500 [07:05<08:57,  1.90s/it]                                                           Episode 218	 reward: -7.20	 makespan: 712.60	 Mean_loss: 0.09667892,  training time: 1.89
progress:  43%|[34m‚ñà‚ñà‚ñà‚ñà‚ñé     [0m| 217/500 [07:07<08:57,  1.90s/it]progress:  44%|[34m‚ñà‚ñà‚ñà‚ñà‚ñé     [0m| 218/500 [07:07<08:55,  1.90s/it]                                                           Episode 219	 reward: -7.23	 makespan: 716.05	 Mean_loss: 0.09234919,  training time: 1.90
progress:  44%|[34m‚ñà‚ñà‚ñà‚ñà‚ñé     [0m| 218/500 [07:09<08:55,  1.90s/it]progress:  44%|[34m‚ñà‚ñà‚ñà‚ñà‚ñç     [0m| 219/500 [07:09<08:53,  1.90s/it]                                                           Episode 220	 reward: -7.29	 makespan: 722.15	 Mean_loss: 0.10878502,  training time: 2.00
progress:  44%|[34m‚ñà‚ñà‚ñà‚ñà‚ñç     [0m| 219/500 [07:11<08:53,  1.90s/it]progress:  44%|[34m‚ñà‚ñà‚ñà‚ñà‚ñç     [0m| 220/500 [07:11<09:00,  1.93s/it]                                                           Episode 221	 reward: -7.49	 makespan: 741.95	 Mean_loss: 0.12225769,  training time: 2.02
progress:  44%|[34m‚ñà‚ñà‚ñà‚ñà‚ñç     [0m| 220/500 [07:13<09:00,  1.93s/it]progress:  44%|[34m‚ñà‚ñà‚ñà‚ñà‚ñç     [0m| 221/500 [07:13<09:06,  1.96s/it]                                                           Episode 222	 reward: -7.50	 makespan: 742.05	 Mean_loss: 0.12596613,  training time: 1.95
progress:  44%|[34m‚ñà‚ñà‚ñà‚ñà‚ñç     [0m| 221/500 [07:15<09:06,  1.96s/it]progress:  44%|[34m‚ñà‚ñà‚ñà‚ñà‚ñç     [0m| 222/500 [07:15<09:03,  1.96s/it]                                                           Episode 223	 reward: -7.64	 makespan: 756.80	 Mean_loss: 0.11801254,  training time: 1.90
progress:  44%|[34m‚ñà‚ñà‚ñà‚ñà‚ñç     [0m| 222/500 [07:17<09:03,  1.96s/it]progress:  45%|[34m‚ñà‚ñà‚ñà‚ñà‚ñç     [0m| 223/500 [07:17<08:57,  1.94s/it]                                                           Episode 224	 reward: -7.48	 makespan: 740.35	 Mean_loss: 0.09627225,  training time: 1.93
progress:  45%|[34m‚ñà‚ñà‚ñà‚ñà‚ñç     [0m| 223/500 [07:18<08:57,  1.94s/it]progress:  45%|[34m‚ñà‚ñà‚ñà‚ñà‚ñç     [0m| 224/500 [07:18<08:55,  1.94s/it]                                                           Episode 225	 reward: -7.48	 makespan: 740.60	 Mean_loss: 0.11418118,  training time: 1.92
progress:  45%|[34m‚ñà‚ñà‚ñà‚ñà‚ñç     [0m| 224/500 [07:20<08:55,  1.94s/it]progress:  45%|[34m‚ñà‚ñà‚ñà‚ñà‚ñå     [0m| 225/500 [07:20<08:51,  1.93s/it]                                                           Episode 226	 reward: -7.53	 makespan: 745.90	 Mean_loss: 0.10643135,  training time: 1.92
progress:  45%|[34m‚ñà‚ñà‚ñà‚ñà‚ñå     [0m| 225/500 [07:22<08:51,  1.93s/it]progress:  45%|[34m‚ñà‚ñà‚ñà‚ñà‚ñå     [0m| 226/500 [07:22<08:48,  1.93s/it]                                                           Episode 227	 reward: -7.55	 makespan: 747.20	 Mean_loss: 0.11757668,  training time: 1.91
progress:  45%|[34m‚ñà‚ñà‚ñà‚ñà‚ñå     [0m| 226/500 [07:24<08:48,  1.93s/it]progress:  45%|[34m‚ñà‚ñà‚ñà‚ñà‚ñå     [0m| 227/500 [07:24<08:45,  1.92s/it]                                                           Episode 228	 reward: -7.49	 makespan: 741.25	 Mean_loss: 0.11775583,  training time: 2.01
progress:  45%|[34m‚ñà‚ñà‚ñà‚ñà‚ñå     [0m| 227/500 [07:26<08:45,  1.92s/it]progress:  46%|[34m‚ñà‚ñà‚ñà‚ñà‚ñå     [0m| 228/500 [07:26<08:50,  1.95s/it]                                                           Episode 229	 reward: -7.58	 makespan: 750.40	 Mean_loss: 0.13468757,  training time: 1.93
progress:  46%|[34m‚ñà‚ñà‚ñà‚ñà‚ñå     [0m| 228/500 [07:28<08:50,  1.95s/it]progress:  46%|[34m‚ñà‚ñà‚ñà‚ñà‚ñå     [0m| 229/500 [07:28<08:47,  1.95s/it]                                                           Episode 230	 reward: -7.45	 makespan: 737.85	 Mean_loss: 0.11337750,  training time: 1.95
progress:  46%|[34m‚ñà‚ñà‚ñà‚ñà‚ñå     [0m| 229/500 [07:30<08:47,  1.95s/it]progress:  46%|[34m‚ñà‚ñà‚ñà‚ñà‚ñå     [0m| 230/500 [07:30<08:45,  1.95s/it]                                                           Episode 231	 reward: -7.50	 makespan: 742.85	 Mean_loss: 0.11955486,  training time: 1.93
progress:  46%|[34m‚ñà‚ñà‚ñà‚ñà‚ñå     [0m| 230/500 [07:32<08:45,  1.95s/it]progress:  46%|[34m‚ñà‚ñà‚ñà‚ñà‚ñå     [0m| 231/500 [07:32<08:42,  1.94s/it]                                                           Episode 232	 reward: -7.38	 makespan: 730.45	 Mean_loss: 0.09633093,  training time: 1.90
progress:  46%|[34m‚ñà‚ñà‚ñà‚ñà‚ñå     [0m| 231/500 [07:34<08:42,  1.94s/it]progress:  46%|[34m‚ñà‚ñà‚ñà‚ñà‚ñã     [0m| 232/500 [07:34<08:37,  1.93s/it]                                                           Episode 233	 reward: -7.49	 makespan: 741.15	 Mean_loss: 0.12614378,  training time: 1.94
progress:  46%|[34m‚ñà‚ñà‚ñà‚ñà‚ñã     [0m| 232/500 [07:36<08:37,  1.93s/it]progress:  47%|[34m‚ñà‚ñà‚ñà‚ñà‚ñã     [0m| 233/500 [07:36<08:36,  1.93s/it]                                                           Episode 234	 reward: -7.42	 makespan: 734.80	 Mean_loss: 0.08633053,  training time: 1.90
progress:  47%|[34m‚ñà‚ñà‚ñà‚ñà‚ñã     [0m| 233/500 [07:38<08:36,  1.93s/it]progress:  47%|[34m‚ñà‚ñà‚ñà‚ñà‚ñã     [0m| 234/500 [07:38<08:31,  1.92s/it]                                                           Episode 235	 reward: -7.43	 makespan: 735.75	 Mean_loss: 0.10292462,  training time: 1.89
progress:  47%|[34m‚ñà‚ñà‚ñà‚ñà‚ñã     [0m| 234/500 [07:40<08:31,  1.92s/it]progress:  47%|[34m‚ñà‚ñà‚ñà‚ñà‚ñã     [0m| 235/500 [07:40<08:27,  1.92s/it]                                                           Episode 236	 reward: -7.52	 makespan: 744.20	 Mean_loss: 0.11582852,  training time: 1.91
progress:  47%|[34m‚ñà‚ñà‚ñà‚ñà‚ñã     [0m| 235/500 [07:42<08:27,  1.92s/it]progress:  47%|[34m‚ñà‚ñà‚ñà‚ñà‚ñã     [0m| 236/500 [07:42<08:24,  1.91s/it]                                                           Episode 237	 reward: -7.53	 makespan: 745.55	 Mean_loss: 0.10269566,  training time: 1.89
progress:  47%|[34m‚ñà‚ñà‚ñà‚ñà‚ñã     [0m| 236/500 [07:43<08:24,  1.91s/it]progress:  47%|[34m‚ñà‚ñà‚ñà‚ñà‚ñã     [0m| 237/500 [07:43<08:21,  1.91s/it]                                                           Episode 238	 reward: -7.54	 makespan: 746.00	 Mean_loss: 0.10686724,  training time: 1.91
progress:  47%|[34m‚ñà‚ñà‚ñà‚ñà‚ñã     [0m| 237/500 [07:45<08:21,  1.91s/it]progress:  48%|[34m‚ñà‚ñà‚ñà‚ñà‚ñä     [0m| 238/500 [07:45<08:19,  1.91s/it]                                                           Episode 239	 reward: -7.50	 makespan: 742.10	 Mean_loss: 0.12269010,  training time: 1.88
progress:  48%|[34m‚ñà‚ñà‚ñà‚ñà‚ñä     [0m| 238/500 [07:47<08:19,  1.91s/it]progress:  48%|[34m‚ñà‚ñà‚ñà‚ñà‚ñä     [0m| 239/500 [07:47<08:16,  1.90s/it]                                                           Episode 240	 reward: -7.43	 makespan: 736.00	 Mean_loss: 0.10945651,  training time: 1.93
progress:  48%|[34m‚ñà‚ñà‚ñà‚ñà‚ñä     [0m| 239/500 [07:49<08:16,  1.90s/it]progress:  48%|[34m‚ñà‚ñà‚ñà‚ñà‚ñä     [0m| 240/500 [07:49<08:16,  1.91s/it]                                                           Episode 241	 reward: -7.25	 makespan: 718.05	 Mean_loss: 0.11553099,  training time: 1.95
progress:  48%|[34m‚ñà‚ñà‚ñà‚ñà‚ñä     [0m| 240/500 [07:51<08:16,  1.91s/it]progress:  48%|[34m‚ñà‚ñà‚ñà‚ñà‚ñä     [0m| 241/500 [07:51<08:17,  1.92s/it]                                                           Episode 242	 reward: -7.31	 makespan: 724.15	 Mean_loss: 0.11527558,  training time: 1.90
progress:  48%|[34m‚ñà‚ñà‚ñà‚ñà‚ñä     [0m| 241/500 [07:53<08:17,  1.92s/it]progress:  48%|[34m‚ñà‚ñà‚ñà‚ñà‚ñä     [0m| 242/500 [07:53<08:14,  1.92s/it]                                                           Episode 243	 reward: -7.31	 makespan: 723.90	 Mean_loss: 0.15088290,  training time: 1.90
progress:  48%|[34m‚ñà‚ñà‚ñà‚ñà‚ñä     [0m| 242/500 [07:55<08:14,  1.92s/it]progress:  49%|[34m‚ñà‚ñà‚ñà‚ñà‚ñä     [0m| 243/500 [07:55<08:11,  1.91s/it]                                                           Episode 244	 reward: -7.23	 makespan: 715.95	 Mean_loss: 0.09993906,  training time: 1.91
progress:  49%|[34m‚ñà‚ñà‚ñà‚ñà‚ñä     [0m| 243/500 [07:57<08:11,  1.91s/it]progress:  49%|[34m‚ñà‚ñà‚ñà‚ñà‚ñâ     [0m| 244/500 [07:57<08:09,  1.91s/it]                                                           Episode 245	 reward: -7.23	 makespan: 715.70	 Mean_loss: 0.11364582,  training time: 1.89
progress:  49%|[34m‚ñà‚ñà‚ñà‚ñà‚ñâ     [0m| 244/500 [07:59<08:09,  1.91s/it]progress:  49%|[34m‚ñà‚ñà‚ñà‚ñà‚ñâ     [0m| 245/500 [07:59<08:06,  1.91s/it]                                                           Episode 246	 reward: -7.39	 makespan: 731.40	 Mean_loss: 0.11608236,  training time: 1.91
progress:  49%|[34m‚ñà‚ñà‚ñà‚ñà‚ñâ     [0m| 245/500 [08:01<08:06,  1.91s/it]progress:  49%|[34m‚ñà‚ñà‚ñà‚ñà‚ñâ     [0m| 246/500 [08:01<08:04,  1.91s/it]                                                           Episode 247	 reward: -7.24	 makespan: 717.00	 Mean_loss: 0.10974118,  training time: 1.89
progress:  49%|[34m‚ñà‚ñà‚ñà‚ñà‚ñâ     [0m| 246/500 [08:03<08:04,  1.91s/it]progress:  49%|[34m‚ñà‚ñà‚ñà‚ñà‚ñâ     [0m| 247/500 [08:03<08:01,  1.90s/it]                                                           Episode 248	 reward: -7.25	 makespan: 717.95	 Mean_loss: 0.09490005,  training time: 1.89
progress:  49%|[34m‚ñà‚ñà‚ñà‚ñà‚ñâ     [0m| 247/500 [08:04<08:01,  1.90s/it]progress:  50%|[34m‚ñà‚ñà‚ñà‚ñà‚ñâ     [0m| 248/500 [08:04<07:59,  1.90s/it]                                                           Episode 249	 reward: -7.35	 makespan: 727.20	 Mean_loss: 0.11122020,  training time: 1.92
progress:  50%|[34m‚ñà‚ñà‚ñà‚ñà‚ñâ     [0m| 248/500 [08:06<07:59,  1.90s/it]progress:  50%|[34m‚ñà‚ñà‚ñà‚ñà‚ñâ     [0m| 249/500 [08:06<07:58,  1.91s/it]                                                           Episode 250	 reward: -7.26	 makespan: 719.20	 Mean_loss: 0.14688498,  training time: 1.92
progress:  50%|[34m‚ñà‚ñà‚ñà‚ñà‚ñâ     [0m| 249/500 [08:08<07:58,  1.91s/it]progress:  50%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà     [0m| 250/500 [08:08<07:57,  1.91s/it]                                                           Episode 251	 reward: -7.21	 makespan: 713.90	 Mean_loss: 0.10619403,  training time: 1.90
progress:  50%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà     [0m| 250/500 [08:10<07:57,  1.91s/it]progress:  50%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà     [0m| 251/500 [08:10<07:55,  1.91s/it]                                                           Episode 252	 reward: -7.30	 makespan: 722.90	 Mean_loss: 0.10808887,  training time: 1.92
progress:  50%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà     [0m| 251/500 [08:12<07:55,  1.91s/it]progress:  50%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà     [0m| 252/500 [08:12<07:53,  1.91s/it]                                                           Episode 253	 reward: -7.32	 makespan: 724.50	 Mean_loss: 0.14038908,  training time: 1.90
progress:  50%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà     [0m| 252/500 [08:14<07:53,  1.91s/it]progress:  51%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà     [0m| 253/500 [08:14<07:51,  1.91s/it]                                                           Episode 254	 reward: -7.33	 makespan: 725.30	 Mean_loss: 0.11588492,  training time: 1.90
progress:  51%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà     [0m| 253/500 [08:16<07:51,  1.91s/it]progress:  51%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà     [0m| 254/500 [08:16<07:48,  1.91s/it]                                                           Episode 255	 reward: -7.21	 makespan: 714.25	 Mean_loss: 0.11118332,  training time: 1.90
progress:  51%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà     [0m| 254/500 [08:18<07:48,  1.91s/it]progress:  51%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà     [0m| 255/500 [08:18<07:46,  1.90s/it]                                                           Episode 256	 reward: -7.29	 makespan: 721.30	 Mean_loss: 0.09639838,  training time: 1.90
progress:  51%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà     [0m| 255/500 [08:20<07:46,  1.90s/it]progress:  51%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà     [0m| 256/500 [08:20<07:44,  1.91s/it]                                                           Episode 257	 reward: -7.24	 makespan: 717.10	 Mean_loss: 0.10372739,  training time: 1.90
progress:  51%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà     [0m| 256/500 [08:22<07:44,  1.91s/it]progress:  51%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    [0m| 257/500 [08:22<07:42,  1.91s/it]                                                           Episode 258	 reward: -7.35	 makespan: 727.30	 Mean_loss: 0.15569752,  training time: 1.90
progress:  51%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    [0m| 257/500 [08:24<07:42,  1.91s/it]progress:  52%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    [0m| 258/500 [08:24<07:40,  1.90s/it]                                                           Episode 259	 reward: -7.21	 makespan: 713.35	 Mean_loss: 0.11318898,  training time: 1.90
progress:  52%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    [0m| 258/500 [08:25<07:40,  1.90s/it]progress:  52%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    [0m| 259/500 [08:25<07:38,  1.90s/it]                                                           Episode 260	 reward: -7.20	 makespan: 712.35	 Mean_loss: 0.12357569,  training time: 1.93
progress:  52%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    [0m| 259/500 [08:27<07:38,  1.90s/it]progress:  52%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    [0m| 260/500 [08:27<07:38,  1.91s/it]                                                           Episode 261	 reward: -7.62	 makespan: 754.55	 Mean_loss: 0.11368130,  training time: 1.97
progress:  52%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    [0m| 260/500 [08:29<07:38,  1.91s/it]progress:  52%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    [0m| 261/500 [08:29<07:40,  1.93s/it]                                                           Episode 262	 reward: -7.44	 makespan: 736.50	 Mean_loss: 0.10833885,  training time: 2.04
progress:  52%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    [0m| 261/500 [08:31<07:40,  1.93s/it]progress:  52%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    [0m| 262/500 [08:31<07:47,  1.96s/it]                                                           Episode 263	 reward: -7.39	 makespan: 731.35	 Mean_loss: 0.13424550,  training time: 1.90
progress:  52%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    [0m| 262/500 [08:33<07:47,  1.96s/it]progress:  53%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    [0m| 263/500 [08:33<07:40,  1.95s/it]                                                           Episode 264	 reward: -7.47	 makespan: 739.35	 Mean_loss: 0.14373130,  training time: 1.87
progress:  53%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    [0m| 263/500 [08:35<07:40,  1.95s/it]progress:  53%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    [0m| 264/500 [08:35<07:33,  1.92s/it]                                                           Episode 265	 reward: -7.52	 makespan: 744.45	 Mean_loss: 0.11301878,  training time: 1.90
progress:  53%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    [0m| 264/500 [08:37<07:33,  1.92s/it]progress:  53%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    [0m| 265/500 [08:37<07:30,  1.92s/it]                                                           Episode 266	 reward: -7.54	 makespan: 746.90	 Mean_loss: 0.13205603,  training time: 1.96
progress:  53%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    [0m| 265/500 [08:39<07:30,  1.92s/it]progress:  53%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    [0m| 266/500 [08:39<07:31,  1.93s/it]                                                           Episode 267	 reward: -7.40	 makespan: 732.85	 Mean_loss: 0.10261513,  training time: 1.91
progress:  53%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    [0m| 266/500 [08:41<07:31,  1.93s/it]progress:  53%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    [0m| 267/500 [08:41<07:28,  1.92s/it]                                                           Episode 268	 reward: -7.59	 makespan: 751.70	 Mean_loss: 0.11679623,  training time: 1.90
progress:  53%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    [0m| 267/500 [08:43<07:28,  1.92s/it]progress:  54%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    [0m| 268/500 [08:43<07:24,  1.92s/it]                                                           Episode 269	 reward: -7.37	 makespan: 729.55	 Mean_loss: 0.10408133,  training time: 1.90
progress:  54%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    [0m| 268/500 [08:45<07:24,  1.92s/it]progress:  54%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    [0m| 269/500 [08:45<07:21,  1.91s/it]                                                           Episode 270	 reward: -7.42	 makespan: 734.55	 Mean_loss: 0.11094312,  training time: 1.90
progress:  54%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    [0m| 269/500 [08:47<07:21,  1.91s/it]progress:  54%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    [0m| 270/500 [08:47<07:19,  1.91s/it]                                                           Episode 271	 reward: -7.50	 makespan: 742.30	 Mean_loss: 0.11403849,  training time: 1.89
progress:  54%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    [0m| 270/500 [08:49<07:19,  1.91s/it]progress:  54%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    [0m| 271/500 [08:49<07:16,  1.90s/it]                                                           Episode 272	 reward: -7.45	 makespan: 737.15	 Mean_loss: 0.10606018,  training time: 1.91
progress:  54%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    [0m| 271/500 [08:50<07:16,  1.90s/it]progress:  54%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    [0m| 272/500 [08:50<07:14,  1.91s/it]                                                           Episode 273	 reward: -7.46	 makespan: 738.35	 Mean_loss: 0.10700685,  training time: 1.91
progress:  54%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    [0m| 272/500 [08:52<07:14,  1.91s/it]progress:  55%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    [0m| 273/500 [08:52<07:12,  1.91s/it]                                                           Episode 274	 reward: -7.52	 makespan: 744.10	 Mean_loss: 0.15462112,  training time: 1.90
progress:  55%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    [0m| 273/500 [08:54<07:12,  1.91s/it]progress:  55%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    [0m| 274/500 [08:54<07:10,  1.91s/it]                                                           Episode 275	 reward: -7.41	 makespan: 733.50	 Mean_loss: 0.09404727,  training time: 1.90
progress:  55%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    [0m| 274/500 [08:56<07:10,  1.91s/it]progress:  55%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    [0m| 275/500 [08:56<07:08,  1.90s/it]                                                           Episode 276	 reward: -7.34	 makespan: 726.65	 Mean_loss: 0.10512381,  training time: 1.90
progress:  55%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    [0m| 275/500 [08:58<07:08,  1.90s/it]progress:  55%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    [0m| 276/500 [08:58<07:06,  1.90s/it]                                                           Episode 277	 reward: -7.45	 makespan: 737.25	 Mean_loss: 0.11228758,  training time: 1.92
progress:  55%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    [0m| 276/500 [09:00<07:06,  1.90s/it]progress:  55%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    [0m| 277/500 [09:00<07:05,  1.91s/it]                                                           Episode 278	 reward: -7.42	 makespan: 734.50	 Mean_loss: 0.07974323,  training time: 1.90
progress:  55%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    [0m| 277/500 [09:02<07:05,  1.91s/it]progress:  56%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    [0m| 278/500 [09:02<07:03,  1.91s/it]                                                           Episode 279	 reward: -7.49	 makespan: 741.15	 Mean_loss: 0.08972460,  training time: 1.88
progress:  56%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    [0m| 278/500 [09:04<07:03,  1.91s/it]progress:  56%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    [0m| 279/500 [09:04<06:59,  1.90s/it]                                                           Episode 280	 reward: -7.46	 makespan: 738.05	 Mean_loss: 0.10687501,  training time: 1.91
progress:  56%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    [0m| 279/500 [09:06<06:59,  1.90s/it]progress:  56%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    [0m| 280/500 [09:06<06:58,  1.90s/it]                                                           Episode 281	 reward: -7.15	 makespan: 707.55	 Mean_loss: 0.12399410,  training time: 1.97
progress:  56%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    [0m| 280/500 [09:08<06:58,  1.90s/it]progress:  56%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    [0m| 281/500 [09:08<07:00,  1.92s/it]                                                           Episode 282	 reward: -7.07	 makespan: 699.65	 Mean_loss: 0.12878184,  training time: 1.92
progress:  56%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    [0m| 281/500 [09:10<07:00,  1.92s/it]progress:  56%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    [0m| 282/500 [09:10<06:58,  1.92s/it]                                                           Episode 283	 reward: -7.11	 makespan: 703.80	 Mean_loss: 0.11373312,  training time: 1.97
progress:  56%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    [0m| 282/500 [09:11<06:58,  1.92s/it]progress:  57%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    [0m| 283/500 [09:11<06:59,  1.94s/it]                                                           Episode 284	 reward: -7.16	 makespan: 709.15	 Mean_loss: 0.15971163,  training time: 1.91
progress:  57%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    [0m| 283/500 [09:13<06:59,  1.94s/it]progress:  57%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    [0m| 284/500 [09:13<06:56,  1.93s/it]                                                           Episode 285	 reward: -7.10	 makespan: 703.35	 Mean_loss: 0.14372911,  training time: 1.90
progress:  57%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    [0m| 284/500 [09:15<06:56,  1.93s/it]progress:  57%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    [0m| 285/500 [09:15<06:52,  1.92s/it]                                                           Episode 286	 reward: -7.09	 makespan: 702.00	 Mean_loss: 0.13345860,  training time: 1.90
progress:  57%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    [0m| 285/500 [09:17<06:52,  1.92s/it]progress:  57%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    [0m| 286/500 [09:17<06:49,  1.92s/it]                                                           Episode 287	 reward: -7.25	 makespan: 718.20	 Mean_loss: 0.13451628,  training time: 1.91
progress:  57%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    [0m| 286/500 [09:19<06:49,  1.92s/it]progress:  57%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    [0m| 287/500 [09:19<06:47,  1.91s/it]                                                           Episode 288	 reward: -7.11	 makespan: 703.70	 Mean_loss: 0.10261682,  training time: 1.89
progress:  57%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    [0m| 287/500 [09:21<06:47,  1.91s/it]progress:  58%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    [0m| 288/500 [09:21<06:44,  1.91s/it]                                                           Episode 289	 reward: -7.22	 makespan: 714.85	 Mean_loss: 0.09590761,  training time: 1.91
progress:  58%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    [0m| 288/500 [09:23<06:44,  1.91s/it]progress:  58%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    [0m| 289/500 [09:23<06:42,  1.91s/it]                                                           Episode 290	 reward: -7.21	 makespan: 714.10	 Mean_loss: 0.11532577,  training time: 1.91
progress:  58%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    [0m| 289/500 [09:25<06:42,  1.91s/it]progress:  58%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    [0m| 290/500 [09:25<06:40,  1.91s/it]                                                           Episode 291	 reward: -7.20	 makespan: 713.00	 Mean_loss: 0.13817622,  training time: 1.90
progress:  58%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    [0m| 290/500 [09:27<06:40,  1.91s/it]progress:  58%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    [0m| 291/500 [09:27<06:38,  1.91s/it]                                                           Episode 292	 reward: -7.19	 makespan: 711.55	 Mean_loss: 0.08711665,  training time: 1.91
progress:  58%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    [0m| 291/500 [09:29<06:38,  1.91s/it]progress:  58%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    [0m| 292/500 [09:29<06:36,  1.91s/it]                                                           Episode 293	 reward: -7.31	 makespan: 723.65	 Mean_loss: 0.09895909,  training time: 1.91
progress:  58%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    [0m| 292/500 [09:31<06:36,  1.91s/it]progress:  59%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    [0m| 293/500 [09:31<06:35,  1.91s/it]                                                           Episode 294	 reward: -7.28	 makespan: 720.60	 Mean_loss: 0.13925213,  training time: 1.91
progress:  59%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    [0m| 293/500 [09:32<06:35,  1.91s/it]progress:  59%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    [0m| 294/500 [09:32<06:33,  1.91s/it]                                                           Episode 295	 reward: -7.26	 makespan: 718.55	 Mean_loss: 0.12003251,  training time: 1.97
progress:  59%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    [0m| 294/500 [09:34<06:33,  1.91s/it]progress:  59%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    [0m| 295/500 [09:34<06:35,  1.93s/it]                                                           Episode 296	 reward: -7.13	 makespan: 706.25	 Mean_loss: 0.12071985,  training time: 1.94
progress:  59%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    [0m| 295/500 [09:36<06:35,  1.93s/it]progress:  59%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    [0m| 296/500 [09:36<06:34,  1.93s/it]                                                           Episode 297	 reward: -7.09	 makespan: 701.50	 Mean_loss: 0.11056203,  training time: 1.98
progress:  59%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    [0m| 296/500 [09:38<06:34,  1.93s/it]progress:  59%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    [0m| 297/500 [09:38<06:35,  1.95s/it]                                                           Episode 298	 reward: -7.30	 makespan: 722.60	 Mean_loss: 0.12066790,  training time: 1.90
progress:  59%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    [0m| 297/500 [09:40<06:35,  1.95s/it]progress:  60%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    [0m| 298/500 [09:40<06:30,  1.93s/it]                                                           Episode 299	 reward: -7.16	 makespan: 708.70	 Mean_loss: 0.10198726,  training time: 1.92
progress:  60%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    [0m| 298/500 [09:42<06:30,  1.93s/it]progress:  60%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    [0m| 299/500 [09:42<06:27,  1.93s/it]                                                           Episode 300	 reward: -7.23	 makespan: 715.50	 Mean_loss: 0.12096797,  training time: 1.92
progress:  60%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    [0m| 299/500 [09:44<06:27,  1.93s/it]progress:  60%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    [0m| 300/500 [09:44<06:25,  1.93s/it]                                                           Episode 301	 reward: -7.81	 makespan: 772.90	 Mean_loss: 0.13756867,  training time: 1.98
progress:  60%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    [0m| 300/500 [09:46<06:25,  1.93s/it]progress:  60%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    [0m| 301/500 [09:46<06:26,  1.94s/it]                                                           Episode 302	 reward: -7.74	 makespan: 766.70	 Mean_loss: 0.14081831,  training time: 1.90
progress:  60%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    [0m| 301/500 [09:48<06:26,  1.94s/it]progress:  60%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    [0m| 302/500 [09:48<06:22,  1.93s/it]                                                           Episode 303	 reward: -7.71	 makespan: 763.05	 Mean_loss: 0.12069829,  training time: 1.90
progress:  60%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    [0m| 302/500 [09:50<06:22,  1.93s/it]progress:  61%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    [0m| 303/500 [09:50<06:18,  1.92s/it]                                                           Episode 304	 reward: -7.89	 makespan: 781.20	 Mean_loss: 0.18344446,  training time: 1.90
progress:  61%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    [0m| 303/500 [09:52<06:18,  1.92s/it]progress:  61%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    [0m| 304/500 [09:52<06:15,  1.91s/it]                                                           Episode 305	 reward: -7.75	 makespan: 767.55	 Mean_loss: 0.14303681,  training time: 1.92
progress:  61%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    [0m| 304/500 [09:54<06:15,  1.91s/it]progress:  61%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    [0m| 305/500 [09:54<06:13,  1.92s/it]                                                           Episode 306	 reward: -7.74	 makespan: 766.35	 Mean_loss: 0.13915028,  training time: 1.91
progress:  61%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    [0m| 305/500 [09:56<06:13,  1.92s/it]progress:  61%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    [0m| 306/500 [09:56<06:11,  1.92s/it]                                                           Episode 307	 reward: -7.79	 makespan: 771.00	 Mean_loss: 0.12458844,  training time: 1.92
progress:  61%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    [0m| 306/500 [09:58<06:11,  1.92s/it]progress:  61%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   [0m| 307/500 [09:58<06:09,  1.92s/it]                                                           Episode 308	 reward: -7.78	 makespan: 770.50	 Mean_loss: 0.16919278,  training time: 1.92
progress:  61%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   [0m| 307/500 [09:59<06:09,  1.92s/it]progress:  62%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   [0m| 308/500 [09:59<06:08,  1.92s/it]                                                           Episode 309	 reward: -7.75	 makespan: 767.60	 Mean_loss: 0.13419248,  training time: 1.93
progress:  62%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   [0m| 308/500 [10:01<06:08,  1.92s/it]progress:  62%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   [0m| 309/500 [10:01<06:06,  1.92s/it]                                                           Episode 310	 reward: -7.74	 makespan: 765.80	 Mean_loss: 0.14558473,  training time: 1.93
progress:  62%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   [0m| 309/500 [10:03<06:06,  1.92s/it]progress:  62%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   [0m| 310/500 [10:03<06:05,  1.93s/it]                                                           Episode 311	 reward: -7.70	 makespan: 762.55	 Mean_loss: 0.14452088,  training time: 1.90
progress:  62%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   [0m| 310/500 [10:05<06:05,  1.93s/it]progress:  62%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   [0m| 311/500 [10:05<06:02,  1.92s/it]                                                           Episode 312	 reward: -7.68	 makespan: 760.45	 Mean_loss: 0.11304583,  training time: 1.90
progress:  62%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   [0m| 311/500 [10:07<06:02,  1.92s/it]progress:  62%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   [0m| 312/500 [10:07<05:59,  1.91s/it]                                                           Episode 313	 reward: -7.77	 makespan: 769.30	 Mean_loss: 0.14319326,  training time: 1.92
progress:  62%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   [0m| 312/500 [10:09<05:59,  1.91s/it]progress:  63%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   [0m| 313/500 [10:09<05:58,  1.92s/it]                                                           Episode 314	 reward: -7.73	 makespan: 765.00	 Mean_loss: 0.12904829,  training time: 1.91
progress:  63%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   [0m| 313/500 [10:11<05:58,  1.92s/it]progress:  63%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   [0m| 314/500 [10:11<05:56,  1.92s/it]                                                           Episode 315	 reward: -7.65	 makespan: 757.20	 Mean_loss: 0.11890110,  training time: 1.94
progress:  63%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   [0m| 314/500 [10:13<05:56,  1.92s/it]progress:  63%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   [0m| 315/500 [10:13<05:55,  1.92s/it]                                                           Episode 316	 reward: -7.69	 makespan: 761.25	 Mean_loss: 0.15306966,  training time: 1.91
progress:  63%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   [0m| 315/500 [10:15<05:55,  1.92s/it]progress:  63%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   [0m| 316/500 [10:15<05:53,  1.92s/it]                                                           Episode 317	 reward: -7.64	 makespan: 756.20	 Mean_loss: 0.10471632,  training time: 1.91
progress:  63%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   [0m| 316/500 [10:17<05:53,  1.92s/it]progress:  63%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   [0m| 317/500 [10:17<05:50,  1.92s/it]                                                           Episode 318	 reward: -7.64	 makespan: 756.80	 Mean_loss: 0.12312697,  training time: 1.92
progress:  63%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   [0m| 317/500 [10:19<05:50,  1.92s/it]progress:  64%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   [0m| 318/500 [10:19<05:49,  1.92s/it]                                                           Episode 319	 reward: -7.68	 makespan: 760.55	 Mean_loss: 0.11902879,  training time: 1.91
progress:  64%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   [0m| 318/500 [10:21<05:49,  1.92s/it]progress:  64%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   [0m| 319/500 [10:21<05:46,  1.92s/it]                                                           Episode 320	 reward: -7.69	 makespan: 761.40	 Mean_loss: 0.14928937,  training time: 1.91
progress:  64%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   [0m| 319/500 [10:22<05:46,  1.92s/it]progress:  64%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   [0m| 320/500 [10:22<05:44,  1.91s/it]                                                           Episode 321	 reward: -7.39	 makespan: 731.85	 Mean_loss: 0.10067432,  training time: 2.05
progress:  64%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   [0m| 320/500 [10:25<05:44,  1.91s/it]progress:  64%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   [0m| 321/500 [10:25<05:50,  1.96s/it]                                                           Episode 322	 reward: -7.21	 makespan: 713.30	 Mean_loss: 0.10643522,  training time: 1.93
progress:  64%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   [0m| 321/500 [10:26<05:50,  1.96s/it]progress:  64%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   [0m| 322/500 [10:26<05:47,  1.95s/it]                                                           Episode 323	 reward: -7.31	 makespan: 724.15	 Mean_loss: 0.10878753,  training time: 1.92
progress:  64%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   [0m| 322/500 [10:28<05:47,  1.95s/it]progress:  65%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   [0m| 323/500 [10:28<05:43,  1.94s/it]                                                           Episode 324	 reward: -7.47	 makespan: 739.65	 Mean_loss: 0.13017531,  training time: 1.94
progress:  65%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   [0m| 323/500 [10:30<05:43,  1.94s/it]progress:  65%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   [0m| 324/500 [10:30<05:41,  1.94s/it]                                                           Episode 325	 reward: -7.28	 makespan: 720.75	 Mean_loss: 0.11576428,  training time: 1.92
progress:  65%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   [0m| 324/500 [10:32<05:41,  1.94s/it]progress:  65%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   [0m| 325/500 [10:32<05:38,  1.93s/it]                                                           Episode 326	 reward: -7.26	 makespan: 718.45	 Mean_loss: 0.08688111,  training time: 1.90
progress:  65%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   [0m| 325/500 [10:34<05:38,  1.93s/it]progress:  65%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   [0m| 326/500 [10:34<05:34,  1.92s/it]                                                           Episode 327	 reward: -7.32	 makespan: 725.00	 Mean_loss: 0.11143684,  training time: 1.91
progress:  65%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   [0m| 326/500 [10:36<05:34,  1.92s/it]progress:  65%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   [0m| 327/500 [10:36<05:32,  1.92s/it]                                                           Episode 328	 reward: -7.27	 makespan: 719.80	 Mean_loss: 0.08629979,  training time: 1.91
progress:  65%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   [0m| 327/500 [10:38<05:32,  1.92s/it]progress:  66%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   [0m| 328/500 [10:38<05:29,  1.92s/it]                                                           Episode 329	 reward: -7.24	 makespan: 716.35	 Mean_loss: 0.08604656,  training time: 1.89
progress:  66%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   [0m| 328/500 [10:40<05:29,  1.92s/it]progress:  66%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   [0m| 329/500 [10:40<05:26,  1.91s/it]                                                           Episode 330	 reward: -7.40	 makespan: 733.05	 Mean_loss: 0.10100886,  training time: 1.92
progress:  66%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   [0m| 329/500 [10:42<05:26,  1.91s/it]progress:  66%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   [0m| 330/500 [10:42<05:25,  1.91s/it]                                                           Episode 331	 reward: -7.32	 makespan: 724.25	 Mean_loss: 0.09680782,  training time: 1.90
progress:  66%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   [0m| 330/500 [10:44<05:25,  1.91s/it]progress:  66%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   [0m| 331/500 [10:44<05:22,  1.91s/it]                                                           Episode 332	 reward: -7.36	 makespan: 728.50	 Mean_loss: 0.09590355,  training time: 1.90
progress:  66%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   [0m| 331/500 [10:46<05:22,  1.91s/it]progress:  66%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   [0m| 332/500 [10:46<05:20,  1.91s/it]                                                           Episode 333	 reward: -7.21	 makespan: 713.85	 Mean_loss: 0.09159184,  training time: 1.97
progress:  66%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   [0m| 332/500 [10:48<05:20,  1.91s/it]progress:  67%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   [0m| 333/500 [10:48<05:21,  1.93s/it]                                                           Episode 334	 reward: -7.22	 makespan: 714.35	 Mean_loss: 0.09289402,  training time: 2.01
progress:  67%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   [0m| 333/500 [10:50<05:21,  1.93s/it]progress:  67%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   [0m| 334/500 [10:50<05:24,  1.95s/it]                                                           Episode 335	 reward: -7.38	 makespan: 730.25	 Mean_loss: 0.10575289,  training time: 2.00
progress:  67%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   [0m| 334/500 [10:52<05:24,  1.95s/it]progress:  67%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   [0m| 335/500 [10:52<05:24,  1.97s/it]                                                           Episode 336	 reward: -7.20	 makespan: 712.95	 Mean_loss: 0.09226490,  training time: 1.92
progress:  67%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   [0m| 335/500 [10:53<05:24,  1.97s/it]progress:  67%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   [0m| 336/500 [10:53<05:20,  1.96s/it]                                                           Episode 337	 reward: -7.36	 makespan: 728.55	 Mean_loss: 0.10972106,  training time: 1.98
progress:  67%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   [0m| 336/500 [10:55<05:20,  1.96s/it]progress:  67%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   [0m| 337/500 [10:55<05:19,  1.96s/it]                                                           Episode 338	 reward: -7.20	 makespan: 713.25	 Mean_loss: 0.09208867,  training time: 1.92
progress:  67%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   [0m| 337/500 [10:57<05:19,  1.96s/it]progress:  68%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   [0m| 338/500 [10:57<05:15,  1.95s/it]                                                           Episode 339	 reward: -7.21	 makespan: 713.40	 Mean_loss: 0.12321036,  training time: 1.97
progress:  68%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   [0m| 338/500 [10:59<05:15,  1.95s/it]progress:  68%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   [0m| 339/500 [10:59<05:14,  1.96s/it]                                                           Episode 340	 reward: -7.28	 makespan: 720.25	 Mean_loss: 0.08369929,  training time: 1.94
progress:  68%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   [0m| 339/500 [11:01<05:14,  1.96s/it]progress:  68%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   [0m| 340/500 [11:01<05:12,  1.95s/it]                                                           Episode 341	 reward: -7.67	 makespan: 759.80	 Mean_loss: 0.10326432,  training time: 1.98
progress:  68%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   [0m| 340/500 [11:03<05:12,  1.95s/it]progress:  68%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   [0m| 341/500 [11:03<05:11,  1.96s/it]                                                           Episode 342	 reward: -7.43	 makespan: 735.40	 Mean_loss: 0.09115259,  training time: 1.92
progress:  68%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   [0m| 341/500 [11:05<05:11,  1.96s/it]progress:  68%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   [0m| 342/500 [11:05<05:07,  1.95s/it]                                                           Episode 343	 reward: -7.64	 makespan: 756.00	 Mean_loss: 0.10959018,  training time: 1.94
progress:  68%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   [0m| 342/500 [11:07<05:07,  1.95s/it]progress:  69%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   [0m| 343/500 [11:07<05:05,  1.95s/it]                                                           Episode 344	 reward: -7.54	 makespan: 746.15	 Mean_loss: 0.07218184,  training time: 1.92
progress:  69%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   [0m| 343/500 [11:09<05:05,  1.95s/it]progress:  69%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   [0m| 344/500 [11:09<05:02,  1.94s/it]                                                           Episode 345	 reward: -7.48	 makespan: 740.30	 Mean_loss: 0.11592168,  training time: 1.92
progress:  69%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   [0m| 344/500 [11:11<05:02,  1.94s/it]progress:  69%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   [0m| 345/500 [11:11<04:59,  1.93s/it]                                                           Episode 346	 reward: -7.38	 makespan: 731.00	 Mean_loss: 0.08699194,  training time: 1.94
progress:  69%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   [0m| 345/500 [11:13<04:59,  1.93s/it]progress:  69%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   [0m| 346/500 [11:13<04:58,  1.94s/it]                                                           Episode 347	 reward: -7.59	 makespan: 751.15	 Mean_loss: 0.11111742,  training time: 1.95
progress:  69%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   [0m| 346/500 [11:15<04:58,  1.94s/it]progress:  69%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   [0m| 347/500 [11:15<04:56,  1.94s/it]                                                           Episode 348	 reward: -7.67	 makespan: 759.35	 Mean_loss: 0.08583134,  training time: 1.96
progress:  69%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   [0m| 347/500 [11:17<04:56,  1.94s/it]progress:  70%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   [0m| 348/500 [11:17<04:55,  1.95s/it]                                                           Episode 349	 reward: -7.54	 makespan: 746.20	 Mean_loss: 0.13104641,  training time: 1.96
progress:  70%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   [0m| 348/500 [11:19<04:55,  1.95s/it]progress:  70%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   [0m| 349/500 [11:19<04:54,  1.95s/it]                                                           Episode 350	 reward: -7.45	 makespan: 737.85	 Mean_loss: 0.07494291,  training time: 1.92
progress:  70%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   [0m| 349/500 [11:21<04:54,  1.95s/it]progress:  70%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   [0m| 350/500 [11:21<04:51,  1.94s/it]                                                           Episode 351	 reward: -7.56	 makespan: 748.15	 Mean_loss: 0.08725363,  training time: 2.00
progress:  70%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   [0m| 350/500 [11:23<04:51,  1.94s/it]progress:  70%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   [0m| 351/500 [11:23<04:52,  1.96s/it]                                                           Episode 352	 reward: -7.40	 makespan: 732.20	 Mean_loss: 0.07185470,  training time: 1.92
progress:  70%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   [0m| 351/500 [11:25<04:52,  1.96s/it]progress:  70%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   [0m| 352/500 [11:25<04:48,  1.95s/it]                                                           Episode 353	 reward: -7.53	 makespan: 745.75	 Mean_loss: 0.09271739,  training time: 1.92
progress:  70%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   [0m| 352/500 [11:27<04:48,  1.95s/it]progress:  71%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   [0m| 353/500 [11:27<04:45,  1.94s/it]                                                           Episode 354	 reward: -7.49	 makespan: 741.45	 Mean_loss: 0.07586040,  training time: 1.93
progress:  71%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   [0m| 353/500 [11:28<04:45,  1.94s/it]progress:  71%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   [0m| 354/500 [11:28<04:42,  1.94s/it]                                                           Episode 355	 reward: -7.45	 makespan: 737.35	 Mean_loss: 0.09364787,  training time: 1.93
progress:  71%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   [0m| 354/500 [11:30<04:42,  1.94s/it]progress:  71%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   [0m| 355/500 [11:30<04:40,  1.93s/it]                                                           Episode 356	 reward: -7.57	 makespan: 749.85	 Mean_loss: 0.07845288,  training time: 1.92
progress:  71%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   [0m| 355/500 [11:32<04:40,  1.93s/it]progress:  71%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   [0m| 356/500 [11:32<04:37,  1.93s/it]                                                           Episode 357	 reward: -7.57	 makespan: 749.30	 Mean_loss: 0.08369858,  training time: 2.00
progress:  71%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   [0m| 356/500 [11:34<04:37,  1.93s/it]progress:  71%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  [0m| 357/500 [11:34<04:39,  1.95s/it]                                                           Episode 358	 reward: -7.47	 makespan: 739.45	 Mean_loss: 0.07413186,  training time: 1.92
progress:  71%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  [0m| 357/500 [11:36<04:39,  1.95s/it]progress:  72%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  [0m| 358/500 [11:36<04:35,  1.94s/it]                                                           Episode 359	 reward: -7.46	 makespan: 738.80	 Mean_loss: 0.07922881,  training time: 1.92
progress:  72%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  [0m| 358/500 [11:38<04:35,  1.94s/it]progress:  72%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  [0m| 359/500 [11:38<04:32,  1.94s/it]                                                           Episode 360	 reward: -7.61	 makespan: 753.40	 Mean_loss: 0.07196650,  training time: 1.93
progress:  72%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  [0m| 359/500 [11:40<04:32,  1.94s/it]progress:  72%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  [0m| 360/500 [11:40<04:30,  1.93s/it]                                                           Episode 361	 reward: -7.44	 makespan: 736.25	 Mean_loss: 0.08797105,  training time: 1.98
progress:  72%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  [0m| 360/500 [11:42<04:30,  1.93s/it]progress:  72%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  [0m| 361/500 [11:42<04:30,  1.95s/it]                                                           Episode 362	 reward: -7.47	 makespan: 739.35	 Mean_loss: 0.07837418,  training time: 1.91
progress:  72%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  [0m| 361/500 [11:44<04:30,  1.95s/it]progress:  72%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  [0m| 362/500 [11:44<04:27,  1.94s/it]                                                           Episode 363	 reward: -7.44	 makespan: 736.95	 Mean_loss: 0.09895368,  training time: 1.92
progress:  72%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  [0m| 362/500 [11:46<04:27,  1.94s/it]progress:  73%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  [0m| 363/500 [11:46<04:24,  1.93s/it]                                                           Episode 364	 reward: -7.46	 makespan: 738.80	 Mean_loss: 0.06993791,  training time: 1.92
progress:  73%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  [0m| 363/500 [11:48<04:24,  1.93s/it]progress:  73%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  [0m| 364/500 [11:48<04:22,  1.93s/it]                                                           Episode 365	 reward: -7.37	 makespan: 729.25	 Mean_loss: 0.08931273,  training time: 1.92
progress:  73%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  [0m| 364/500 [11:50<04:22,  1.93s/it]progress:  73%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  [0m| 365/500 [11:50<04:19,  1.93s/it]                                                           Episode 366	 reward: -7.41	 makespan: 734.05	 Mean_loss: 0.09689368,  training time: 1.90
progress:  73%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  [0m| 365/500 [11:52<04:19,  1.93s/it]progress:  73%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  [0m| 366/500 [11:52<04:17,  1.92s/it]                                                           Episode 367	 reward: -7.45	 makespan: 737.10	 Mean_loss: 0.09034016,  training time: 1.91
progress:  73%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  [0m| 366/500 [11:54<04:17,  1.92s/it]progress:  73%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  [0m| 367/500 [11:54<04:15,  1.92s/it]                                                           Episode 368	 reward: -7.30	 makespan: 722.55	 Mean_loss: 0.06180832,  training time: 1.94
progress:  73%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  [0m| 367/500 [11:56<04:15,  1.92s/it]progress:  74%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  [0m| 368/500 [11:56<04:14,  1.93s/it]                                                           Episode 369	 reward: -7.41	 makespan: 733.70	 Mean_loss: 0.07913133,  training time: 1.93
progress:  74%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  [0m| 368/500 [11:57<04:14,  1.93s/it]progress:  74%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  [0m| 369/500 [11:57<04:12,  1.93s/it]                                                           Episode 370	 reward: -7.43	 makespan: 735.90	 Mean_loss: 0.08312503,  training time: 1.97
progress:  74%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  [0m| 369/500 [11:59<04:12,  1.93s/it]progress:  74%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  [0m| 370/500 [11:59<04:12,  1.94s/it]                                                           Episode 371	 reward: -7.37	 makespan: 729.70	 Mean_loss: 0.09640255,  training time: 1.92
progress:  74%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  [0m| 370/500 [12:01<04:12,  1.94s/it]progress:  74%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  [0m| 371/500 [12:01<04:09,  1.94s/it]                                                           Episode 372	 reward: -7.32	 makespan: 724.75	 Mean_loss: 0.06012960,  training time: 1.92
progress:  74%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  [0m| 371/500 [12:03<04:09,  1.94s/it]progress:  74%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  [0m| 372/500 [12:03<04:07,  1.93s/it]                                                           Episode 373	 reward: -7.44	 makespan: 736.85	 Mean_loss: 0.07970890,  training time: 1.91
progress:  74%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  [0m| 372/500 [12:05<04:07,  1.93s/it]progress:  75%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  [0m| 373/500 [12:05<04:04,  1.92s/it]                                                           Episode 374	 reward: -7.31	 makespan: 724.15	 Mean_loss: 0.08057591,  training time: 1.91
progress:  75%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  [0m| 373/500 [12:07<04:04,  1.92s/it]progress:  75%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  [0m| 374/500 [12:07<04:01,  1.92s/it]                                                           Episode 375	 reward: -7.34	 makespan: 726.90	 Mean_loss: 0.05036571,  training time: 1.91
progress:  75%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  [0m| 374/500 [12:09<04:01,  1.92s/it]progress:  75%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  [0m| 375/500 [12:09<03:59,  1.92s/it]                                                           Episode 376	 reward: -7.38	 makespan: 730.55	 Mean_loss: 0.06200957,  training time: 1.92
progress:  75%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  [0m| 375/500 [12:11<03:59,  1.92s/it]progress:  75%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  [0m| 376/500 [12:11<03:57,  1.92s/it]                                                           Episode 377	 reward: -7.28	 makespan: 720.65	 Mean_loss: 0.06924073,  training time: 1.91
progress:  75%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  [0m| 376/500 [12:13<03:57,  1.92s/it]progress:  75%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  [0m| 377/500 [12:13<03:55,  1.91s/it]                                                           Episode 378	 reward: -7.35	 makespan: 727.80	 Mean_loss: 0.06529588,  training time: 1.91
progress:  75%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  [0m| 377/500 [12:15<03:55,  1.91s/it]progress:  76%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  [0m| 378/500 [12:15<03:53,  1.91s/it]                                                           Episode 379	 reward: -7.33	 makespan: 725.90	 Mean_loss: 0.08803733,  training time: 1.92
progress:  76%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  [0m| 378/500 [12:17<03:53,  1.91s/it]progress:  76%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  [0m| 379/500 [12:17<03:51,  1.91s/it]                                                           Episode 380	 reward: -7.39	 makespan: 731.65	 Mean_loss: 0.07315706,  training time: 1.93
progress:  76%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  [0m| 379/500 [12:19<03:51,  1.91s/it]progress:  76%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  [0m| 380/500 [12:19<03:50,  1.92s/it]                                                           Episode 381	 reward: -7.34	 makespan: 726.25	 Mean_loss: 0.09748714,  training time: 1.97
progress:  76%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  [0m| 380/500 [12:21<03:50,  1.92s/it]progress:  76%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  [0m| 381/500 [12:21<03:50,  1.93s/it]                                                           Episode 382	 reward: -7.16	 makespan: 708.70	 Mean_loss: 0.07994350,  training time: 1.88
progress:  76%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  [0m| 381/500 [12:22<03:50,  1.93s/it]progress:  76%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  [0m| 382/500 [12:22<03:46,  1.92s/it]                                                           Episode 383	 reward: -7.23	 makespan: 715.70	 Mean_loss: 0.12532003,  training time: 1.89
progress:  76%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  [0m| 382/500 [12:24<03:46,  1.92s/it]progress:  77%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  [0m| 383/500 [12:24<03:43,  1.91s/it]                                                           Episode 384	 reward: -7.18	 makespan: 710.95	 Mean_loss: 0.07860285,  training time: 1.91
progress:  77%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  [0m| 383/500 [12:26<03:43,  1.91s/it]progress:  77%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  [0m| 384/500 [12:26<03:41,  1.91s/it]                                                           Episode 385	 reward: -7.16	 makespan: 708.45	 Mean_loss: 0.08223338,  training time: 1.94
progress:  77%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  [0m| 384/500 [12:28<03:41,  1.91s/it]progress:  77%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  [0m| 385/500 [12:28<03:40,  1.92s/it]                                                           Episode 386	 reward: -7.31	 makespan: 723.55	 Mean_loss: 0.08763953,  training time: 1.91
progress:  77%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  [0m| 385/500 [12:30<03:40,  1.92s/it]progress:  77%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  [0m| 386/500 [12:30<03:38,  1.92s/it]                                                           Episode 387	 reward: -7.16	 makespan: 708.50	 Mean_loss: 0.09200875,  training time: 1.91
progress:  77%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  [0m| 386/500 [12:32<03:38,  1.92s/it]progress:  77%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  [0m| 387/500 [12:32<03:36,  1.92s/it]                                                           Episode 388	 reward: -7.23	 makespan: 715.70	 Mean_loss: 0.08386995,  training time: 1.87
progress:  77%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  [0m| 387/500 [12:34<03:36,  1.92s/it]progress:  78%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  [0m| 388/500 [12:34<03:33,  1.90s/it]                                                           Episode 389	 reward: -7.18	 makespan: 711.10	 Mean_loss: 0.07497527,  training time: 1.86
progress:  78%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  [0m| 388/500 [12:36<03:33,  1.90s/it]progress:  78%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  [0m| 389/500 [12:36<03:29,  1.89s/it]                                                           Episode 390	 reward: -7.24	 makespan: 716.95	 Mean_loss: 0.12748970,  training time: 1.91
progress:  78%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  [0m| 389/500 [12:38<03:29,  1.89s/it]progress:  78%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  [0m| 390/500 [12:38<03:28,  1.90s/it]                                                           Episode 391	 reward: -7.13	 makespan: 705.45	 Mean_loss: 0.09200311,  training time: 1.86
progress:  78%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  [0m| 390/500 [12:40<03:28,  1.90s/it]progress:  78%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  [0m| 391/500 [12:40<03:25,  1.89s/it]                                                           Episode 392	 reward: -7.20	 makespan: 712.80	 Mean_loss: 0.09791952,  training time: 1.88
progress:  78%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  [0m| 391/500 [12:41<03:25,  1.89s/it]progress:  78%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  [0m| 392/500 [12:41<03:23,  1.88s/it]                                                           Episode 393	 reward: -7.25	 makespan: 717.35	 Mean_loss: 0.09259634,  training time: 1.87
progress:  78%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  [0m| 392/500 [12:43<03:23,  1.88s/it]progress:  79%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  [0m| 393/500 [12:43<03:21,  1.88s/it]                                                           Episode 394	 reward: -7.04	 makespan: 696.90	 Mean_loss: 0.08355267,  training time: 1.88
progress:  79%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  [0m| 393/500 [12:45<03:21,  1.88s/it]progress:  79%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  [0m| 394/500 [12:45<03:19,  1.88s/it]                                                           Episode 395	 reward: -7.27	 makespan: 719.95	 Mean_loss: 0.11126450,  training time: 1.92
progress:  79%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  [0m| 394/500 [12:47<03:19,  1.88s/it]progress:  79%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  [0m| 395/500 [12:47<03:18,  1.89s/it]                                                           Episode 396	 reward: -7.24	 makespan: 717.10	 Mean_loss: 0.08091059,  training time: 1.88
progress:  79%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  [0m| 395/500 [12:49<03:18,  1.89s/it]progress:  79%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  [0m| 396/500 [12:49<03:16,  1.89s/it]                                                           Episode 397	 reward: -7.12	 makespan: 705.10	 Mean_loss: 0.08661833,  training time: 1.91
progress:  79%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  [0m| 396/500 [12:51<03:16,  1.89s/it]progress:  79%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  [0m| 397/500 [12:51<03:15,  1.90s/it]                                                           Episode 398	 reward: -7.09	 makespan: 702.25	 Mean_loss: 0.06929182,  training time: 1.93
progress:  79%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  [0m| 397/500 [12:53<03:15,  1.90s/it]progress:  80%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  [0m| 398/500 [12:53<03:14,  1.91s/it]                                                           Episode 399	 reward: -7.18	 makespan: 710.65	 Mean_loss: 0.08970898,  training time: 1.91
progress:  80%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  [0m| 398/500 [12:55<03:14,  1.91s/it]progress:  80%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  [0m| 399/500 [12:55<03:12,  1.91s/it]                                                           Episode 400	 reward: -7.11	 makespan: 704.00	 Mean_loss: 0.08470121,  training time: 1.94
progress:  80%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  [0m| 399/500 [12:57<03:12,  1.91s/it]progress:  80%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  [0m| 400/500 [12:57<03:11,  1.92s/it]                                                           Episode 401	 reward: -7.30	 makespan: 722.55	 Mean_loss: 0.08320301,  training time: 1.97
progress:  80%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  [0m| 400/500 [12:59<03:11,  1.92s/it]progress:  80%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  [0m| 401/500 [12:59<03:11,  1.93s/it]                                                           Episode 402	 reward: -7.35	 makespan: 727.75	 Mean_loss: 0.06061827,  training time: 1.95
progress:  80%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  [0m| 401/500 [13:01<03:11,  1.93s/it]progress:  80%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  [0m| 402/500 [13:01<03:09,  1.94s/it]                                                           Episode 403	 reward: -7.31	 makespan: 723.55	 Mean_loss: 0.09102606,  training time: 1.88
progress:  80%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  [0m| 402/500 [13:02<03:09,  1.94s/it]progress:  81%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  [0m| 403/500 [13:02<03:06,  1.92s/it]                                                           Episode 404	 reward: -7.25	 makespan: 718.10	 Mean_loss: 0.06528358,  training time: 1.86
progress:  81%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  [0m| 403/500 [13:04<03:06,  1.92s/it]progress:  81%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  [0m| 404/500 [13:04<03:02,  1.90s/it]                                                           Episode 405	 reward: -7.20	 makespan: 712.90	 Mean_loss: 0.06077798,  training time: 1.85
progress:  81%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  [0m| 404/500 [13:06<03:02,  1.90s/it]progress:  81%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  [0m| 405/500 [13:06<02:59,  1.89s/it]                                                           Episode 406	 reward: -7.35	 makespan: 727.45	 Mean_loss: 0.06518331,  training time: 1.89
progress:  81%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  [0m| 405/500 [13:08<02:59,  1.89s/it]progress:  81%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  [0m| 406/500 [13:08<02:57,  1.89s/it]                                                           Episode 407	 reward: -7.29	 makespan: 721.75	 Mean_loss: 0.12152594,  training time: 1.94
progress:  81%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  [0m| 406/500 [13:10<02:57,  1.89s/it]progress:  81%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè [0m| 407/500 [13:10<02:57,  1.90s/it]                                                           Episode 408	 reward: -7.28	 makespan: 720.55	 Mean_loss: 0.07403054,  training time: 1.91
progress:  81%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè [0m| 407/500 [13:12<02:57,  1.90s/it]progress:  82%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè [0m| 408/500 [13:12<02:55,  1.91s/it]                                                           Episode 409	 reward: -7.36	 makespan: 729.00	 Mean_loss: 0.07934912,  training time: 1.91
progress:  82%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè [0m| 408/500 [13:14<02:55,  1.91s/it]progress:  82%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè [0m| 409/500 [13:14<02:53,  1.91s/it]                                                           Episode 410	 reward: -7.29	 makespan: 722.10	 Mean_loss: 0.07170469,  training time: 1.92
progress:  82%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè [0m| 409/500 [13:16<02:53,  1.91s/it]progress:  82%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè [0m| 410/500 [13:16<02:52,  1.91s/it]                                                           Episode 411	 reward: -7.30	 makespan: 722.45	 Mean_loss: 0.08039874,  training time: 1.99
progress:  82%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè [0m| 410/500 [13:18<02:52,  1.91s/it]progress:  82%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè [0m| 411/500 [13:18<02:52,  1.93s/it]                                                           Episode 412	 reward: -7.31	 makespan: 723.40	 Mean_loss: 0.08052283,  training time: 1.92
progress:  82%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè [0m| 411/500 [13:20<02:52,  1.93s/it]progress:  82%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè [0m| 412/500 [13:20<02:49,  1.93s/it]                                                           Episode 413	 reward: -7.31	 makespan: 723.50	 Mean_loss: 0.08674469,  training time: 1.85
progress:  82%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè [0m| 412/500 [13:21<02:49,  1.93s/it]progress:  83%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé [0m| 413/500 [13:21<02:45,  1.91s/it]                                                           Episode 414	 reward: -7.32	 makespan: 724.40	 Mean_loss: 0.07930627,  training time: 1.97
progress:  83%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé [0m| 413/500 [13:23<02:45,  1.91s/it]progress:  83%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé [0m| 414/500 [13:23<02:45,  1.93s/it]                                                           Episode 415	 reward: -7.27	 makespan: 719.70	 Mean_loss: 0.08374342,  training time: 1.94
progress:  83%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé [0m| 414/500 [13:25<02:45,  1.93s/it]progress:  83%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé [0m| 415/500 [13:25<02:44,  1.93s/it]                                                           Episode 416	 reward: -7.19	 makespan: 711.55	 Mean_loss: 0.05697300,  training time: 1.86
progress:  83%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé [0m| 415/500 [13:27<02:44,  1.93s/it]progress:  83%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé [0m| 416/500 [13:27<02:40,  1.91s/it]                                                           Episode 417	 reward: -7.29	 makespan: 721.90	 Mean_loss: 0.06885570,  training time: 1.87
progress:  83%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé [0m| 416/500 [13:29<02:40,  1.91s/it]progress:  83%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé [0m| 417/500 [13:29<02:37,  1.90s/it]                                                           Episode 418	 reward: -7.24	 makespan: 717.20	 Mean_loss: 0.06388213,  training time: 1.91
progress:  83%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé [0m| 417/500 [13:31<02:37,  1.90s/it]progress:  84%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé [0m| 418/500 [13:31<02:35,  1.90s/it]                                                           Episode 419	 reward: -7.29	 makespan: 722.00	 Mean_loss: 0.06818993,  training time: 1.90
progress:  84%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé [0m| 418/500 [13:33<02:35,  1.90s/it]progress:  84%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç [0m| 419/500 [13:33<02:33,  1.90s/it]                                                           Episode 420	 reward: -7.15	 makespan: 707.45	 Mean_loss: 0.06991686,  training time: 1.94
progress:  84%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç [0m| 419/500 [13:35<02:33,  1.90s/it]progress:  84%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç [0m| 420/500 [13:35<02:33,  1.91s/it]                                                           Episode 421	 reward: -7.12	 makespan: 705.00	 Mean_loss: 0.07753599,  training time: 1.93
progress:  84%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç [0m| 420/500 [13:37<02:33,  1.91s/it]progress:  84%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç [0m| 421/500 [13:37<02:31,  1.92s/it]                                                           Episode 422	 reward: -7.14	 makespan: 706.45	 Mean_loss: 0.06456233,  training time: 1.90
progress:  84%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç [0m| 421/500 [13:39<02:31,  1.92s/it]progress:  84%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç [0m| 422/500 [13:39<02:29,  1.91s/it]                                                           Episode 423	 reward: -7.12	 makespan: 704.90	 Mean_loss: 0.07845665,  training time: 1.93
progress:  84%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç [0m| 422/500 [13:41<02:29,  1.91s/it]progress:  85%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç [0m| 423/500 [13:41<02:27,  1.92s/it]                                                           Episode 424	 reward: -7.25	 makespan: 717.65	 Mean_loss: 0.08238174,  training time: 1.84
progress:  85%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç [0m| 423/500 [13:42<02:27,  1.92s/it]progress:  85%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç [0m| 424/500 [13:42<02:24,  1.90s/it]                                                           Episode 425	 reward: -7.15	 makespan: 707.85	 Mean_loss: 0.08105309,  training time: 1.91
progress:  85%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç [0m| 424/500 [13:44<02:24,  1.90s/it]progress:  85%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå [0m| 425/500 [13:44<02:22,  1.90s/it]                                                           Episode 426	 reward: -7.14	 makespan: 706.85	 Mean_loss: 0.09022647,  training time: 1.94
progress:  85%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå [0m| 425/500 [13:46<02:22,  1.90s/it]progress:  85%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå [0m| 426/500 [13:46<02:21,  1.91s/it]                                                           Episode 427	 reward: -7.13	 makespan: 706.20	 Mean_loss: 0.06765133,  training time: 1.85
progress:  85%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå [0m| 426/500 [13:48<02:21,  1.91s/it]progress:  85%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå [0m| 427/500 [13:48<02:18,  1.89s/it]                                                           Episode 428	 reward: -7.09	 makespan: 701.50	 Mean_loss: 0.07664930,  training time: 1.91
progress:  85%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå [0m| 427/500 [13:50<02:18,  1.89s/it]progress:  86%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå [0m| 428/500 [13:50<02:16,  1.90s/it]                                                           Episode 429	 reward: -7.11	 makespan: 703.95	 Mean_loss: 0.07123718,  training time: 1.91
progress:  86%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå [0m| 428/500 [13:52<02:16,  1.90s/it]progress:  86%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå [0m| 429/500 [13:52<02:14,  1.90s/it]                                                           Episode 430	 reward: -7.06	 makespan: 698.90	 Mean_loss: 0.07641140,  training time: 1.92
progress:  86%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå [0m| 429/500 [13:54<02:14,  1.90s/it]progress:  86%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå [0m| 430/500 [13:54<02:13,  1.91s/it]                                                           Episode 431	 reward: -7.16	 makespan: 708.45	 Mean_loss: 0.06805514,  training time: 1.91
progress:  86%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå [0m| 430/500 [13:56<02:13,  1.91s/it]progress:  86%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå [0m| 431/500 [13:56<02:11,  1.91s/it]                                                           Episode 432	 reward: -7.26	 makespan: 718.60	 Mean_loss: 0.08139250,  training time: 1.86
progress:  86%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå [0m| 431/500 [13:58<02:11,  1.91s/it]progress:  86%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã [0m| 432/500 [13:58<02:08,  1.89s/it]                                                           Episode 433	 reward: -7.18	 makespan: 711.10	 Mean_loss: 0.09025252,  training time: 1.90
progress:  86%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã [0m| 432/500 [14:00<02:08,  1.89s/it]progress:  87%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã [0m| 433/500 [14:00<02:07,  1.90s/it]                                                           Episode 434	 reward: -7.03	 makespan: 695.75	 Mean_loss: 0.08918382,  training time: 1.85
progress:  87%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã [0m| 433/500 [14:01<02:07,  1.90s/it]progress:  87%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã [0m| 434/500 [14:01<02:04,  1.88s/it]                                                           Episode 435	 reward: -7.11	 makespan: 704.15	 Mean_loss: 0.06733733,  training time: 1.85
progress:  87%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã [0m| 434/500 [14:03<02:04,  1.88s/it]progress:  87%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã [0m| 435/500 [14:03<02:01,  1.87s/it]                                                           Episode 436	 reward: -7.11	 makespan: 703.75	 Mean_loss: 0.06849374,  training time: 1.88
progress:  87%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã [0m| 435/500 [14:05<02:01,  1.87s/it]progress:  87%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã [0m| 436/500 [14:05<02:00,  1.88s/it]                                                           Episode 437	 reward: -7.18	 makespan: 710.70	 Mean_loss: 0.06825878,  training time: 1.84
progress:  87%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã [0m| 436/500 [14:07<02:00,  1.88s/it]progress:  87%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã [0m| 437/500 [14:07<01:57,  1.87s/it]                                                           Episode 438	 reward: -7.19	 makespan: 712.15	 Mean_loss: 0.07305842,  training time: 1.88
progress:  87%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã [0m| 437/500 [14:09<01:57,  1.87s/it]progress:  88%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä [0m| 438/500 [14:09<01:55,  1.87s/it]                                                           Episode 439	 reward: -7.24	 makespan: 717.15	 Mean_loss: 0.09257584,  training time: 1.88
progress:  88%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä [0m| 438/500 [14:11<01:55,  1.87s/it]progress:  88%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä [0m| 439/500 [14:11<01:54,  1.87s/it]                                                           Episode 440	 reward: -7.11	 makespan: 703.70	 Mean_loss: 0.06976604,  training time: 1.90
progress:  88%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä [0m| 439/500 [14:13<01:54,  1.87s/it]progress:  88%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä [0m| 440/500 [14:13<01:52,  1.88s/it]                                                           Episode 441	 reward: -7.25	 makespan: 717.45	 Mean_loss: 0.09336296,  training time: 1.92
progress:  88%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä [0m| 440/500 [14:15<01:52,  1.88s/it]progress:  88%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä [0m| 441/500 [14:15<01:51,  1.89s/it]                                                           Episode 442	 reward: -7.46	 makespan: 738.10	 Mean_loss: 0.12217949,  training time: 1.92
progress:  88%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä [0m| 441/500 [14:17<01:51,  1.89s/it]progress:  88%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä [0m| 442/500 [14:17<01:50,  1.90s/it]                                                           Episode 443	 reward: -7.28	 makespan: 720.75	 Mean_loss: 0.08002961,  training time: 1.94
progress:  88%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä [0m| 442/500 [14:18<01:50,  1.90s/it]progress:  89%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä [0m| 443/500 [14:18<01:49,  1.91s/it]                                                           Episode 444	 reward: -7.42	 makespan: 734.30	 Mean_loss: 0.08677647,  training time: 1.93
progress:  89%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä [0m| 443/500 [14:20<01:49,  1.91s/it]progress:  89%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ [0m| 444/500 [14:20<01:47,  1.92s/it]                                                           Episode 445	 reward: -7.28	 makespan: 720.90	 Mean_loss: 0.10673721,  training time: 1.85
progress:  89%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ [0m| 444/500 [14:22<01:47,  1.92s/it]progress:  89%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ [0m| 445/500 [14:22<01:44,  1.90s/it]                                                           Episode 446	 reward: -7.38	 makespan: 730.40	 Mean_loss: 0.10327936,  training time: 1.88
progress:  89%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ [0m| 445/500 [14:24<01:44,  1.90s/it]progress:  89%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ [0m| 446/500 [14:24<01:42,  1.89s/it]                                                           Episode 447	 reward: -7.26	 makespan: 718.85	 Mean_loss: 0.07860613,  training time: 1.90
progress:  89%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ [0m| 446/500 [14:26<01:42,  1.89s/it]progress:  89%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ [0m| 447/500 [14:26<01:40,  1.90s/it]                                                           Episode 448	 reward: -7.29	 makespan: 721.95	 Mean_loss: 0.08547147,  training time: 1.90
progress:  89%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ [0m| 447/500 [14:28<01:40,  1.90s/it]progress:  90%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ [0m| 448/500 [14:28<01:38,  1.90s/it]                                                           Episode 449	 reward: -7.29	 makespan: 721.25	 Mean_loss: 0.09205196,  training time: 1.90
progress:  90%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ [0m| 448/500 [14:30<01:38,  1.90s/it]progress:  90%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ [0m| 449/500 [14:30<01:36,  1.90s/it]                                                           Episode 450	 reward: -7.18	 makespan: 710.75	 Mean_loss: 0.08197715,  training time: 1.85
progress:  90%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ [0m| 449/500 [14:32<01:36,  1.90s/it]progress:  90%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà [0m| 450/500 [14:32<01:34,  1.88s/it]                                                           Episode 451	 reward: -7.32	 makespan: 724.45	 Mean_loss: 0.07185807,  training time: 1.92
progress:  90%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà [0m| 450/500 [14:34<01:34,  1.88s/it]progress:  90%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà [0m| 451/500 [14:34<01:32,  1.90s/it]                                                           Episode 452	 reward: -7.40	 makespan: 732.40	 Mean_loss: 0.07801205,  training time: 1.88
progress:  90%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà [0m| 451/500 [14:35<01:32,  1.90s/it]progress:  90%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà [0m| 452/500 [14:35<01:30,  1.89s/it]                                                           Episode 453	 reward: -7.33	 makespan: 725.30	 Mean_loss: 0.08817299,  training time: 1.92
progress:  90%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà [0m| 452/500 [14:37<01:30,  1.89s/it]progress:  91%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà [0m| 453/500 [14:37<01:29,  1.90s/it]                                                           Episode 454	 reward: -7.21	 makespan: 713.95	 Mean_loss: 0.06445067,  training time: 1.90
progress:  91%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà [0m| 453/500 [14:39<01:29,  1.90s/it]progress:  91%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà [0m| 454/500 [14:39<01:27,  1.90s/it]                                                           Episode 455	 reward: -7.39	 makespan: 731.70	 Mean_loss: 0.08970778,  training time: 1.90
progress:  91%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà [0m| 454/500 [14:41<01:27,  1.90s/it]progress:  91%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà [0m| 455/500 [14:41<01:25,  1.90s/it]                                                           Episode 456	 reward: -7.23	 makespan: 716.05	 Mean_loss: 0.08020310,  training time: 1.84
progress:  91%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà [0m| 455/500 [14:43<01:25,  1.90s/it]progress:  91%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà [0m| 456/500 [14:43<01:22,  1.88s/it]                                                           Episode 457	 reward: -7.20	 makespan: 712.60	 Mean_loss: 0.06619737,  training time: 1.90
progress:  91%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà [0m| 456/500 [14:45<01:22,  1.88s/it]progress:  91%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè[0m| 457/500 [14:45<01:21,  1.89s/it]                                                           Episode 458	 reward: -7.31	 makespan: 723.85	 Mean_loss: 0.09234379,  training time: 1.93
progress:  91%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè[0m| 457/500 [14:47<01:21,  1.89s/it]progress:  92%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè[0m| 458/500 [14:47<01:19,  1.90s/it]                                                           Episode 459	 reward: -7.35	 makespan: 728.10	 Mean_loss: 0.07644573,  training time: 1.91
progress:  92%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè[0m| 458/500 [14:49<01:19,  1.90s/it]progress:  92%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè[0m| 459/500 [14:49<01:18,  1.90s/it]                                                           Episode 460	 reward: -7.23	 makespan: 715.35	 Mean_loss: 0.05938016,  training time: 1.93
progress:  92%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè[0m| 459/500 [14:51<01:18,  1.90s/it]progress:  92%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè[0m| 460/500 [14:51<01:16,  1.91s/it]                                                           Episode 461	 reward: -7.02	 makespan: 695.30	 Mean_loss: 0.07056081,  training time: 1.99
progress:  92%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè[0m| 460/500 [14:53<01:16,  1.91s/it]progress:  92%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè[0m| 461/500 [14:53<01:15,  1.94s/it]                                                           Episode 462	 reward: -6.97	 makespan: 689.95	 Mean_loss: 0.04890320,  training time: 1.90
progress:  92%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè[0m| 461/500 [14:55<01:15,  1.94s/it]progress:  92%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè[0m| 462/500 [14:55<01:13,  1.93s/it]                                                           Episode 463	 reward: -7.10	 makespan: 702.45	 Mean_loss: 0.07100540,  training time: 1.91
progress:  92%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè[0m| 462/500 [14:57<01:13,  1.93s/it]progress:  93%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé[0m| 463/500 [14:57<01:11,  1.92s/it]                                                           Episode 464	 reward: -7.06	 makespan: 698.50	 Mean_loss: 0.05788008,  training time: 1.94
progress:  93%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé[0m| 463/500 [14:58<01:11,  1.92s/it]progress:  93%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé[0m| 464/500 [14:58<01:09,  1.93s/it]                                                           Episode 465	 reward: -6.93	 makespan: 686.45	 Mean_loss: 0.06211530,  training time: 1.93
progress:  93%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé[0m| 464/500 [15:00<01:09,  1.93s/it]progress:  93%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé[0m| 465/500 [15:00<01:07,  1.93s/it]                                                           Episode 466	 reward: -7.04	 makespan: 697.25	 Mean_loss: 0.06054896,  training time: 1.94
progress:  93%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé[0m| 465/500 [15:02<01:07,  1.93s/it]progress:  93%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé[0m| 466/500 [15:02<01:05,  1.93s/it]                                                           Episode 467	 reward: -6.96	 makespan: 689.05	 Mean_loss: 0.04323037,  training time: 1.92
progress:  93%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé[0m| 466/500 [15:04<01:05,  1.93s/it]progress:  93%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé[0m| 467/500 [15:04<01:03,  1.93s/it]                                                           Episode 468	 reward: -7.00	 makespan: 692.70	 Mean_loss: 0.05430217,  training time: 1.94
progress:  93%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé[0m| 467/500 [15:06<01:03,  1.93s/it]progress:  94%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé[0m| 468/500 [15:06<01:01,  1.93s/it]                                                           Episode 469	 reward: -6.96	 makespan: 689.45	 Mean_loss: 0.04318377,  training time: 1.91
progress:  94%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé[0m| 468/500 [15:08<01:01,  1.93s/it]progress:  94%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç[0m| 469/500 [15:08<00:59,  1.93s/it]                                                           Episode 470	 reward: -6.98	 makespan: 691.45	 Mean_loss: 0.04236771,  training time: 1.92
progress:  94%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç[0m| 469/500 [15:10<00:59,  1.93s/it]progress:  94%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç[0m| 470/500 [15:10<00:57,  1.92s/it]                                                           Episode 471	 reward: -6.96	 makespan: 688.60	 Mean_loss: 0.04505246,  training time: 1.91
progress:  94%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç[0m| 470/500 [15:12<00:57,  1.92s/it]progress:  94%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç[0m| 471/500 [15:12<00:55,  1.92s/it]                                                           Episode 472	 reward: -7.03	 makespan: 696.45	 Mean_loss: 0.04210671,  training time: 1.91
progress:  94%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç[0m| 471/500 [15:14<00:55,  1.92s/it]progress:  94%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç[0m| 472/500 [15:14<00:53,  1.92s/it]                                                           Episode 473	 reward: -6.90	 makespan: 683.40	 Mean_loss: 0.05301458,  training time: 1.96
progress:  94%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç[0m| 472/500 [15:16<00:53,  1.92s/it]progress:  95%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç[0m| 473/500 [15:16<00:52,  1.93s/it]                                                           Episode 474	 reward: -7.00	 makespan: 693.35	 Mean_loss: 0.04146801,  training time: 1.92
progress:  95%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç[0m| 473/500 [15:18<00:52,  1.93s/it]progress:  95%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç[0m| 474/500 [15:18<00:50,  1.92s/it]                                                           Episode 475	 reward: -7.04	 makespan: 697.10	 Mean_loss: 0.04785378,  training time: 1.91
progress:  95%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç[0m| 474/500 [15:20<00:50,  1.92s/it]progress:  95%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå[0m| 475/500 [15:20<00:48,  1.92s/it]                                                           Episode 476	 reward: -7.00	 makespan: 693.20	 Mean_loss: 0.04571013,  training time: 1.92
progress:  95%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå[0m| 475/500 [15:22<00:48,  1.92s/it]progress:  95%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå[0m| 476/500 [15:22<00:46,  1.92s/it]                                                           Episode 477	 reward: -6.96	 makespan: 689.05	 Mean_loss: 0.03516866,  training time: 1.91
progress:  95%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå[0m| 476/500 [15:23<00:46,  1.92s/it]progress:  95%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå[0m| 477/500 [15:23<00:44,  1.92s/it]                                                           Episode 478	 reward: -7.03	 makespan: 696.00	 Mean_loss: 0.06191342,  training time: 1.90
progress:  95%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå[0m| 477/500 [15:25<00:44,  1.92s/it]progress:  96%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå[0m| 478/500 [15:25<00:42,  1.91s/it]                                                           Episode 479	 reward: -6.96	 makespan: 688.80	 Mean_loss: 0.04723152,  training time: 1.90
progress:  96%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå[0m| 478/500 [15:27<00:42,  1.91s/it]progress:  96%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå[0m| 479/500 [15:27<00:40,  1.91s/it]                                                           Episode 480	 reward: -6.93	 makespan: 686.00	 Mean_loss: 0.03448449,  training time: 1.93
progress:  96%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå[0m| 479/500 [15:29<00:40,  1.91s/it]progress:  96%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå[0m| 480/500 [15:29<00:38,  1.92s/it]                                                           Episode 481	 reward: -7.24	 makespan: 716.80	 Mean_loss: 0.09386617,  training time: 1.97
progress:  96%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå[0m| 480/500 [15:31<00:38,  1.92s/it]progress:  96%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå[0m| 481/500 [15:31<00:36,  1.93s/it]                                                           Episode 482	 reward: -7.32	 makespan: 724.25	 Mean_loss: 0.06150221,  training time: 1.91
progress:  96%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå[0m| 481/500 [15:33<00:36,  1.93s/it]progress:  96%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã[0m| 482/500 [15:33<00:34,  1.93s/it]                                                           Episode 483	 reward: -7.21	 makespan: 714.05	 Mean_loss: 0.05861548,  training time: 1.90
progress:  96%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã[0m| 482/500 [15:35<00:34,  1.93s/it]progress:  97%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã[0m| 483/500 [15:35<00:32,  1.92s/it]                                                           Episode 484	 reward: -7.17	 makespan: 710.05	 Mean_loss: 0.07436027,  training time: 1.91
progress:  97%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã[0m| 483/500 [15:37<00:32,  1.92s/it]progress:  97%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã[0m| 484/500 [15:37<00:30,  1.92s/it]                                                           Episode 485	 reward: -7.28	 makespan: 721.10	 Mean_loss: 0.08293364,  training time: 1.95
progress:  97%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã[0m| 484/500 [15:39<00:30,  1.92s/it]progress:  97%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã[0m| 485/500 [15:39<00:28,  1.93s/it]                                                           Episode 486	 reward: -7.30	 makespan: 722.60	 Mean_loss: 0.05663571,  training time: 1.92
progress:  97%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã[0m| 485/500 [15:41<00:28,  1.93s/it]progress:  97%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã[0m| 486/500 [15:41<00:26,  1.92s/it]                                                           Episode 487	 reward: -7.35	 makespan: 727.40	 Mean_loss: 0.06606799,  training time: 1.91
progress:  97%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã[0m| 486/500 [15:43<00:26,  1.92s/it]progress:  97%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã[0m| 487/500 [15:43<00:24,  1.92s/it]                                                           Episode 488	 reward: -7.32	 makespan: 724.20	 Mean_loss: 0.06730165,  training time: 1.93
progress:  97%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã[0m| 487/500 [15:45<00:24,  1.92s/it]progress:  98%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä[0m| 488/500 [15:45<00:23,  1.92s/it]                                                           Episode 489	 reward: -7.20	 makespan: 713.15	 Mean_loss: 0.07128270,  training time: 1.94
progress:  98%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä[0m| 488/500 [15:47<00:23,  1.92s/it]progress:  98%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä[0m| 489/500 [15:47<00:21,  1.93s/it]                                                           Episode 490	 reward: -7.21	 makespan: 714.10	 Mean_loss: 0.05290462,  training time: 1.94
progress:  98%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä[0m| 489/500 [15:49<00:21,  1.93s/it]progress:  98%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä[0m| 490/500 [15:49<00:19,  1.93s/it]                                                           Episode 491	 reward: -7.30	 makespan: 723.05	 Mean_loss: 0.06582643,  training time: 1.89
progress:  98%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä[0m| 490/500 [15:50<00:19,  1.93s/it]progress:  98%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä[0m| 491/500 [15:50<00:17,  1.92s/it]                                                           Episode 492	 reward: -7.24	 makespan: 716.80	 Mean_loss: 0.08591529,  training time: 1.85
progress:  98%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä[0m| 491/500 [15:52<00:17,  1.92s/it]progress:  98%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä[0m| 492/500 [15:52<00:15,  1.90s/it]                                                           Episode 493	 reward: -7.38	 makespan: 730.60	 Mean_loss: 0.07384180,  training time: 1.90
progress:  98%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä[0m| 492/500 [15:54<00:15,  1.90s/it]progress:  99%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä[0m| 493/500 [15:54<00:13,  1.90s/it]                                                           Episode 494	 reward: -7.34	 makespan: 726.25	 Mean_loss: 0.06823801,  training time: 1.89
progress:  99%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä[0m| 493/500 [15:56<00:13,  1.90s/it]progress:  99%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ[0m| 494/500 [15:56<00:11,  1.90s/it]                                                           Episode 495	 reward: -7.29	 makespan: 721.45	 Mean_loss: 0.07740451,  training time: 1.86
progress:  99%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ[0m| 494/500 [15:58<00:11,  1.90s/it]progress:  99%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ[0m| 495/500 [15:58<00:09,  1.89s/it]                                                           Episode 496	 reward: -7.24	 makespan: 716.85	 Mean_loss: 0.07046764,  training time: 1.90
progress:  99%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ[0m| 495/500 [16:00<00:09,  1.89s/it]progress:  99%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ[0m| 496/500 [16:00<00:07,  1.89s/it]                                                           Episode 497	 reward: -7.14	 makespan: 707.30	 Mean_loss: 0.06929325,  training time: 1.93
progress:  99%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ[0m| 496/500 [16:02<00:07,  1.89s/it]progress:  99%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ[0m| 497/500 [16:02<00:05,  1.90s/it]                                                           Episode 498	 reward: -7.19	 makespan: 711.60	 Mean_loss: 0.04671137,  training time: 1.96
progress:  99%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ[0m| 497/500 [16:04<00:05,  1.90s/it]progress: 100%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ[0m| 498/500 [16:04<00:03,  1.92s/it]                                                           Episode 499	 reward: -7.24	 makespan: 716.60	 Mean_loss: 0.06128410,  training time: 2.06
progress: 100%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ[0m| 498/500 [16:06<00:03,  1.92s/it]progress: 100%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ[0m| 499/500 [16:06<00:01,  1.96s/it]                                                           Episode 500	 reward: -7.21	 makespan: 713.70	 Mean_loss: 0.07536617,  training time: 1.94
progress: 100%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ[0m| 499/500 [16:08<00:01,  1.96s/it]progress: 100%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà[0m| 500/500 [16:08<00:00,  1.96s/it]progress: 100%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà[0m| 500/500 [16:08<00:00,  1.94s/it]
+ python train/DAN.py --n_j 20 --n_m 20 --op_per_job 5 --data_source SD2 --model_suffix SD2_exp18_3 --logdir ./runs/exp18/exp18-3/DAN/train_model/20X20X5 --max_updates 500
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x20+mix
save model name:  20x20+mix+SD2_exp18_3
-------------------------Training Setting-------------------------
source : SD2
model name :20x20+mix+SD2_exp18_3
vali data :./data/data_train_vali/SD2/20x20+mix


progress:   0%|[34m          [0m| 0/500 [00:00<?, ?it/s]                                                 Episode 1	 reward: -4.12	 makespan: 407.60	 Mean_loss: 0.69254524,  training time: 3.40
progress:   0%|[34m          [0m| 0/500 [00:03<?, ?it/s]progress:   0%|[34m          [0m| 1/500 [00:03<28:15,  3.40s/it]                                                         Episode 2	 reward: -4.03	 makespan: 399.35	 Mean_loss: 0.56548226,  training time: 2.38
progress:   0%|[34m          [0m| 1/500 [00:05<28:15,  3.40s/it]progress:   0%|[34m          [0m| 2/500 [00:05<23:13,  2.80s/it]                                                         Episode 3	 reward: -4.12	 makespan: 408.10	 Mean_loss: 0.55347472,  training time: 2.29
progress:   0%|[34m          [0m| 2/500 [00:08<23:13,  2.80s/it]progress:   1%|[34m          [0m| 3/500 [00:08<21:14,  2.56s/it]                                                         Episode 4	 reward: -3.96	 makespan: 392.10	 Mean_loss: 0.35500902,  training time: 2.40
progress:   1%|[34m          [0m| 3/500 [00:10<21:14,  2.56s/it]progress:   1%|[34m          [0m| 4/500 [00:10<20:40,  2.50s/it]                                                         Episode 5	 reward: -3.84	 makespan: 380.15	 Mean_loss: 0.23839906,  training time: 2.27
progress:   1%|[34m          [0m| 4/500 [00:12<20:40,  2.50s/it]progress:   1%|[34m          [0m| 5/500 [00:12<19:56,  2.42s/it]                                                         Episode 6	 reward: -3.99	 makespan: 395.30	 Mean_loss: 0.22464836,  training time: 2.26
progress:   1%|[34m          [0m| 5/500 [00:14<19:56,  2.42s/it]progress:   1%|[34m          [0m| 6/500 [00:14<19:28,  2.36s/it]                                                         Episode 7	 reward: -3.76	 makespan: 371.75	 Mean_loss: 0.21935579,  training time: 2.27
progress:   1%|[34m          [0m| 6/500 [00:17<19:28,  2.36s/it]progress:   1%|[34m‚ñè         [0m| 7/500 [00:17<19:10,  2.33s/it]                                                         Episode 8	 reward: -3.66	 makespan: 362.05	 Mean_loss: 0.27678287,  training time: 2.26
progress:   1%|[34m‚ñè         [0m| 7/500 [00:19<19:10,  2.33s/it]progress:   2%|[34m‚ñè         [0m| 8/500 [00:19<18:56,  2.31s/it]                                                         Episode 9	 reward: -3.71	 makespan: 367.60	 Mean_loss: 0.28393990,  training time: 2.26
progress:   2%|[34m‚ñè         [0m| 8/500 [00:21<18:56,  2.31s/it]progress:   2%|[34m‚ñè         [0m| 9/500 [00:21<18:47,  2.30s/it]                                                         Episode 10	 reward: -3.64	 makespan: 359.90	 Mean_loss: 0.21008380,  training time: 2.28
progress:   2%|[34m‚ñè         [0m| 9/500 [00:24<18:47,  2.30s/it]progress:   2%|[34m‚ñè         [0m| 10/500 [00:24<18:42,  2.29s/it]                                                          Episode 11	 reward: -3.63	 makespan: 358.95	 Mean_loss: 0.24044146,  training time: 2.34
progress:   2%|[34m‚ñè         [0m| 10/500 [00:26<18:42,  2.29s/it]progress:   2%|[34m‚ñè         [0m| 11/500 [00:26<18:47,  2.31s/it]                                                          Episode 12	 reward: -3.44	 makespan: 341.00	 Mean_loss: 0.19450448,  training time: 2.27
progress:   2%|[34m‚ñè         [0m| 11/500 [00:28<18:47,  2.31s/it]progress:   2%|[34m‚ñè         [0m| 12/500 [00:28<18:40,  2.30s/it]                                                          Episode 13	 reward: -3.63	 makespan: 359.70	 Mean_loss: 0.13724717,  training time: 2.29
progress:   2%|[34m‚ñè         [0m| 12/500 [00:30<18:40,  2.30s/it]progress:   3%|[34m‚ñé         [0m| 13/500 [00:30<18:36,  2.29s/it]                                                          Episode 14	 reward: -3.48	 makespan: 344.40	 Mean_loss: 0.11542643,  training time: 2.25
progress:   3%|[34m‚ñé         [0m| 13/500 [00:33<18:36,  2.29s/it]progress:   3%|[34m‚ñé         [0m| 14/500 [00:33<18:28,  2.28s/it]                                                          Episode 15	 reward: -3.66	 makespan: 362.40	 Mean_loss: 0.13233313,  training time: 2.26
progress:   3%|[34m‚ñé         [0m| 14/500 [00:35<18:28,  2.28s/it]progress:   3%|[34m‚ñé         [0m| 15/500 [00:35<18:23,  2.28s/it]                                                          Episode 16	 reward: -3.53	 makespan: 349.75	 Mean_loss: 0.12822200,  training time: 2.32
progress:   3%|[34m‚ñé         [0m| 15/500 [00:37<18:23,  2.28s/it]progress:   3%|[34m‚ñé         [0m| 16/500 [00:37<18:27,  2.29s/it]                                                          Episode 17	 reward: -3.41	 makespan: 337.40	 Mean_loss: 0.11682560,  training time: 2.28
progress:   3%|[34m‚ñé         [0m| 16/500 [00:40<18:27,  2.29s/it]progress:   3%|[34m‚ñé         [0m| 17/500 [00:40<18:23,  2.29s/it]                                                          Episode 18	 reward: -3.31	 makespan: 327.95	 Mean_loss: 0.10703003,  training time: 2.42
progress:   3%|[34m‚ñé         [0m| 17/500 [00:42<18:23,  2.29s/it]progress:   4%|[34m‚ñé         [0m| 18/500 [00:42<18:40,  2.33s/it]                                                          Episode 19	 reward: -3.39	 makespan: 335.85	 Mean_loss: 0.08035079,  training time: 2.48
progress:   4%|[34m‚ñé         [0m| 18/500 [00:44<18:40,  2.33s/it]progress:   4%|[34m‚ñç         [0m| 19/500 [00:44<19:01,  2.37s/it]                                                          Episode 20	 reward: -3.31	 makespan: 327.25	 Mean_loss: 0.08195715,  training time: 2.50
progress:   4%|[34m‚ñç         [0m| 19/500 [00:47<19:01,  2.37s/it]progress:   4%|[34m‚ñç         [0m| 20/500 [00:47<19:17,  2.41s/it]                                                          Episode 21	 reward: -3.45	 makespan: 341.40	 Mean_loss: 0.04861940,  training time: 2.42
progress:   4%|[34m‚ñç         [0m| 20/500 [00:49<19:17,  2.41s/it]progress:   4%|[34m‚ñç         [0m| 21/500 [00:49<19:16,  2.41s/it]                                                          Episode 22	 reward: -3.40	 makespan: 336.30	 Mean_loss: 0.06473656,  training time: 2.39
progress:   4%|[34m‚ñç         [0m| 21/500 [00:52<19:16,  2.41s/it]progress:   4%|[34m‚ñç         [0m| 22/500 [00:52<19:10,  2.41s/it]                                                          Episode 23	 reward: -3.31	 makespan: 327.45	 Mean_loss: 0.07070386,  training time: 2.40
progress:   4%|[34m‚ñç         [0m| 22/500 [00:54<19:10,  2.41s/it]progress:   5%|[34m‚ñç         [0m| 23/500 [00:54<19:07,  2.41s/it]                                                          Episode 24	 reward: -3.31	 makespan: 328.00	 Mean_loss: 0.05192362,  training time: 2.30
progress:   5%|[34m‚ñç         [0m| 23/500 [00:57<19:07,  2.41s/it]progress:   5%|[34m‚ñç         [0m| 24/500 [00:57<18:50,  2.38s/it]                                                          Episode 25	 reward: -3.22	 makespan: 318.60	 Mean_loss: 0.06227830,  training time: 2.30
progress:   5%|[34m‚ñç         [0m| 24/500 [00:59<18:50,  2.38s/it]progress:   5%|[34m‚ñå         [0m| 25/500 [00:59<18:37,  2.35s/it]                                                          Episode 26	 reward: -3.11	 makespan: 308.30	 Mean_loss: 0.06245911,  training time: 2.29
progress:   5%|[34m‚ñå         [0m| 25/500 [01:01<18:37,  2.35s/it]progress:   5%|[34m‚ñå         [0m| 26/500 [01:01<18:26,  2.33s/it]                                                          Episode 27	 reward: -2.99	 makespan: 295.65	 Mean_loss: 0.04480442,  training time: 2.49
progress:   5%|[34m‚ñå         [0m| 26/500 [01:04<18:26,  2.33s/it]progress:   5%|[34m‚ñå         [0m| 27/500 [01:04<18:46,  2.38s/it]                                                          Episode 28	 reward: -3.19	 makespan: 316.30	 Mean_loss: 0.05283126,  training time: 2.47
progress:   5%|[34m‚ñå         [0m| 27/500 [01:06<18:46,  2.38s/it]progress:   6%|[34m‚ñå         [0m| 28/500 [01:06<18:56,  2.41s/it]                                                          Episode 29	 reward: -2.88	 makespan: 285.00	 Mean_loss: 0.04563088,  training time: 2.30
progress:   6%|[34m‚ñå         [0m| 28/500 [01:08<18:56,  2.41s/it]progress:   6%|[34m‚ñå         [0m| 29/500 [01:08<18:38,  2.37s/it]                                                          Episode 30	 reward: -3.14	 makespan: 311.10	 Mean_loss: 0.04505103,  training time: 2.30
progress:   6%|[34m‚ñå         [0m| 29/500 [01:11<18:38,  2.37s/it]progress:   6%|[34m‚ñå         [0m| 30/500 [01:11<18:26,  2.35s/it]                                                          Episode 31	 reward: -3.01	 makespan: 297.80	 Mean_loss: 0.04960413,  training time: 2.29
progress:   6%|[34m‚ñå         [0m| 30/500 [01:13<18:26,  2.35s/it]progress:   6%|[34m‚ñå         [0m| 31/500 [01:13<18:15,  2.34s/it]                                                          Episode 32	 reward: -3.00	 makespan: 296.85	 Mean_loss: 0.03625504,  training time: 2.30
progress:   6%|[34m‚ñå         [0m| 31/500 [01:15<18:15,  2.34s/it]progress:   6%|[34m‚ñã         [0m| 32/500 [01:15<18:07,  2.32s/it]                                                          Episode 33	 reward: -2.94	 makespan: 290.75	 Mean_loss: 0.03783655,  training time: 2.39
progress:   6%|[34m‚ñã         [0m| 32/500 [01:18<18:07,  2.32s/it]progress:   7%|[34m‚ñã         [0m| 33/500 [01:18<18:14,  2.34s/it]                                                          Episode 34	 reward: -2.89	 makespan: 285.70	 Mean_loss: 0.02941658,  training time: 2.36
progress:   7%|[34m‚ñã         [0m| 33/500 [01:20<18:14,  2.34s/it]progress:   7%|[34m‚ñã         [0m| 34/500 [01:20<18:14,  2.35s/it]                                                          Episode 35	 reward: -2.91	 makespan: 288.30	 Mean_loss: 0.02901800,  training time: 2.35
progress:   7%|[34m‚ñã         [0m| 34/500 [01:22<18:14,  2.35s/it]progress:   7%|[34m‚ñã         [0m| 35/500 [01:22<18:11,  2.35s/it]                                                          Episode 36	 reward: -2.91	 makespan: 287.95	 Mean_loss: 0.03023341,  training time: 2.30
progress:   7%|[34m‚ñã         [0m| 35/500 [01:25<18:11,  2.35s/it]progress:   7%|[34m‚ñã         [0m| 36/500 [01:25<18:02,  2.33s/it]                                                          Episode 37	 reward: -2.89	 makespan: 285.70	 Mean_loss: 0.02385667,  training time: 2.33
progress:   7%|[34m‚ñã         [0m| 36/500 [01:27<18:02,  2.33s/it]progress:   7%|[34m‚ñã         [0m| 37/500 [01:27<18:00,  2.33s/it]                                                          Episode 38	 reward: -2.92	 makespan: 288.85	 Mean_loss: 0.03908844,  training time: 2.33
progress:   7%|[34m‚ñã         [0m| 37/500 [01:29<18:00,  2.33s/it]progress:   8%|[34m‚ñä         [0m| 38/500 [01:29<17:57,  2.33s/it]                                                          Episode 39	 reward: -2.89	 makespan: 285.80	 Mean_loss: 0.02324266,  training time: 2.44
progress:   8%|[34m‚ñä         [0m| 38/500 [01:32<17:57,  2.33s/it]progress:   8%|[34m‚ñä         [0m| 39/500 [01:32<18:10,  2.37s/it]                                                          Episode 40	 reward: -2.87	 makespan: 284.00	 Mean_loss: 0.04408468,  training time: 2.57
progress:   8%|[34m‚ñä         [0m| 39/500 [01:34<18:10,  2.37s/it]progress:   8%|[34m‚ñä         [0m| 40/500 [01:34<18:36,  2.43s/it]                                                          Episode 41	 reward: -2.78	 makespan: 275.70	 Mean_loss: 0.03163358,  training time: 2.48
progress:   8%|[34m‚ñä         [0m| 40/500 [01:37<18:36,  2.43s/it]progress:   8%|[34m‚ñä         [0m| 41/500 [01:37<18:41,  2.44s/it]                                                          Episode 42	 reward: -2.84	 makespan: 280.70	 Mean_loss: 0.03442632,  training time: 2.43
progress:   8%|[34m‚ñä         [0m| 41/500 [01:39<18:41,  2.44s/it]progress:   8%|[34m‚ñä         [0m| 42/500 [01:39<18:38,  2.44s/it]                                                          Episode 43	 reward: -2.88	 makespan: 284.75	 Mean_loss: 0.03073576,  training time: 2.28
progress:   8%|[34m‚ñä         [0m| 42/500 [01:42<18:38,  2.44s/it]progress:   9%|[34m‚ñä         [0m| 43/500 [01:42<18:14,  2.39s/it]                                                          Episode 44	 reward: -2.85	 makespan: 282.10	 Mean_loss: 0.03997634,  training time: 2.29
progress:   9%|[34m‚ñä         [0m| 43/500 [01:44<18:14,  2.39s/it]progress:   9%|[34m‚ñâ         [0m| 44/500 [01:44<17:57,  2.36s/it]                                                          Episode 45	 reward: -2.89	 makespan: 286.35	 Mean_loss: 0.04393514,  training time: 2.28
progress:   9%|[34m‚ñâ         [0m| 44/500 [01:46<17:57,  2.36s/it]progress:   9%|[34m‚ñâ         [0m| 45/500 [01:46<17:43,  2.34s/it]                                                          Episode 46	 reward: -2.87	 makespan: 283.75	 Mean_loss: 0.03605229,  training time: 2.28
progress:   9%|[34m‚ñâ         [0m| 45/500 [01:48<17:43,  2.34s/it]progress:   9%|[34m‚ñâ         [0m| 46/500 [01:48<17:34,  2.32s/it]                                                          Episode 47	 reward: -2.73	 makespan: 269.95	 Mean_loss: 0.03430814,  training time: 2.28
progress:   9%|[34m‚ñâ         [0m| 46/500 [01:51<17:34,  2.32s/it]progress:   9%|[34m‚ñâ         [0m| 47/500 [01:51<17:26,  2.31s/it]                                                          Episode 48	 reward: -2.84	 makespan: 281.50	 Mean_loss: 0.02575280,  training time: 2.30
progress:   9%|[34m‚ñâ         [0m| 47/500 [01:53<17:26,  2.31s/it]progress:  10%|[34m‚ñâ         [0m| 48/500 [01:53<17:23,  2.31s/it]                                                          Episode 49	 reward: -2.85	 makespan: 281.95	 Mean_loss: 0.03652772,  training time: 2.30
progress:  10%|[34m‚ñâ         [0m| 48/500 [01:55<17:23,  2.31s/it]progress:  10%|[34m‚ñâ         [0m| 49/500 [01:55<17:20,  2.31s/it]                                                          Episode 50	 reward: -2.95	 makespan: 292.30	 Mean_loss: 0.03039610,  training time: 2.30
progress:  10%|[34m‚ñâ         [0m| 49/500 [01:58<17:20,  2.31s/it]progress:  10%|[34m‚ñà         [0m| 50/500 [01:58<17:17,  2.30s/it]                                                          Episode 51	 reward: -2.79	 makespan: 276.25	 Mean_loss: 0.03641037,  training time: 2.28
progress:  10%|[34m‚ñà         [0m| 50/500 [02:00<17:17,  2.30s/it]progress:  10%|[34m‚ñà         [0m| 51/500 [02:00<17:11,  2.30s/it]                                                          Episode 52	 reward: -2.71	 makespan: 268.35	 Mean_loss: 0.03071020,  training time: 2.41
progress:  10%|[34m‚ñà         [0m| 51/500 [02:02<17:11,  2.30s/it]progress:  10%|[34m‚ñà         [0m| 52/500 [02:02<17:24,  2.33s/it]                                                          Episode 53	 reward: -2.75	 makespan: 271.90	 Mean_loss: 0.02353501,  training time: 2.36
progress:  10%|[34m‚ñà         [0m| 52/500 [02:05<17:24,  2.33s/it]progress:  11%|[34m‚ñà         [0m| 53/500 [02:05<17:26,  2.34s/it]                                                          Episode 54	 reward: -2.77	 makespan: 274.30	 Mean_loss: 0.02114213,  training time: 2.28
progress:  11%|[34m‚ñà         [0m| 53/500 [02:07<17:26,  2.34s/it]progress:  11%|[34m‚ñà         [0m| 54/500 [02:07<17:16,  2.32s/it]                                                          Episode 55	 reward: -2.71	 makespan: 268.75	 Mean_loss: 0.02628014,  training time: 2.45
progress:  11%|[34m‚ñà         [0m| 54/500 [02:09<17:16,  2.32s/it]progress:  11%|[34m‚ñà         [0m| 55/500 [02:09<17:32,  2.36s/it]                                                          Episode 56	 reward: -2.94	 makespan: 290.80	 Mean_loss: 0.03172497,  training time: 2.30
progress:  11%|[34m‚ñà         [0m| 55/500 [02:12<17:32,  2.36s/it]progress:  11%|[34m‚ñà         [0m| 56/500 [02:12<17:21,  2.35s/it]                                                          Episode 57	 reward: -2.75	 makespan: 272.05	 Mean_loss: 0.03337958,  training time: 2.33
progress:  11%|[34m‚ñà         [0m| 56/500 [02:14<17:21,  2.35s/it]progress:  11%|[34m‚ñà‚ñè        [0m| 57/500 [02:14<17:17,  2.34s/it]                                                          Episode 58	 reward: -2.77	 makespan: 274.35	 Mean_loss: 0.04587852,  training time: 2.30
progress:  11%|[34m‚ñà‚ñè        [0m| 57/500 [02:16<17:17,  2.34s/it]progress:  12%|[34m‚ñà‚ñè        [0m| 58/500 [02:16<17:10,  2.33s/it]                                                          Episode 59	 reward: -2.92	 makespan: 289.45	 Mean_loss: 0.03860681,  training time: 2.31
progress:  12%|[34m‚ñà‚ñè        [0m| 58/500 [02:19<17:10,  2.33s/it]progress:  12%|[34m‚ñà‚ñè        [0m| 59/500 [02:19<17:05,  2.33s/it]                                                          Episode 60	 reward: -2.67	 makespan: 264.70	 Mean_loss: 0.02517595,  training time: 2.34
progress:  12%|[34m‚ñà‚ñè        [0m| 59/500 [02:21<17:05,  2.33s/it]progress:  12%|[34m‚ñà‚ñè        [0m| 60/500 [02:21<17:05,  2.33s/it]                                                          Episode 61	 reward: -2.73	 makespan: 270.20	 Mean_loss: 0.03481099,  training time: 2.40
progress:  12%|[34m‚ñà‚ñè        [0m| 60/500 [02:23<17:05,  2.33s/it]progress:  12%|[34m‚ñà‚ñè        [0m| 61/500 [02:23<17:13,  2.35s/it]                                                          Episode 62	 reward: -2.63	 makespan: 260.75	 Mean_loss: 0.03621070,  training time: 2.35
progress:  12%|[34m‚ñà‚ñè        [0m| 61/500 [02:26<17:13,  2.35s/it]progress:  12%|[34m‚ñà‚ñè        [0m| 62/500 [02:26<17:11,  2.35s/it]                                                          Episode 63	 reward: -2.78	 makespan: 275.10	 Mean_loss: 0.03101321,  training time: 2.32
progress:  12%|[34m‚ñà‚ñè        [0m| 62/500 [02:28<17:11,  2.35s/it]progress:  13%|[34m‚ñà‚ñé        [0m| 63/500 [02:28<17:04,  2.34s/it]                                                          Episode 64	 reward: -2.76	 makespan: 273.25	 Mean_loss: 0.03224186,  training time: 2.33
progress:  13%|[34m‚ñà‚ñé        [0m| 63/500 [02:30<17:04,  2.34s/it]progress:  13%|[34m‚ñà‚ñé        [0m| 64/500 [02:30<17:00,  2.34s/it]                                                          Episode 65	 reward: -2.74	 makespan: 270.80	 Mean_loss: 0.03225684,  training time: 2.36
progress:  13%|[34m‚ñà‚ñé        [0m| 64/500 [02:33<17:00,  2.34s/it]progress:  13%|[34m‚ñà‚ñé        [0m| 65/500 [02:33<17:01,  2.35s/it]                                                          Episode 66	 reward: -2.71	 makespan: 268.00	 Mean_loss: 0.02565020,  training time: 2.44
progress:  13%|[34m‚ñà‚ñé        [0m| 65/500 [02:35<17:01,  2.35s/it]progress:  13%|[34m‚ñà‚ñé        [0m| 66/500 [02:35<17:10,  2.38s/it]                                                          Episode 67	 reward: -2.77	 makespan: 274.30	 Mean_loss: 0.02743256,  training time: 2.45
progress:  13%|[34m‚ñà‚ñé        [0m| 66/500 [02:38<17:10,  2.38s/it]progress:  13%|[34m‚ñà‚ñé        [0m| 67/500 [02:38<17:17,  2.40s/it]                                                          Episode 68	 reward: -2.71	 makespan: 267.95	 Mean_loss: 0.02792271,  training time: 2.27
progress:  13%|[34m‚ñà‚ñé        [0m| 67/500 [02:40<17:17,  2.40s/it]progress:  14%|[34m‚ñà‚ñé        [0m| 68/500 [02:40<16:58,  2.36s/it]                                                          Episode 69	 reward: -2.69	 makespan: 265.95	 Mean_loss: 0.03864387,  training time: 2.26
progress:  14%|[34m‚ñà‚ñé        [0m| 68/500 [02:42<16:58,  2.36s/it]progress:  14%|[34m‚ñà‚ñç        [0m| 69/500 [02:42<16:44,  2.33s/it]                                                          Episode 70	 reward: -2.66	 makespan: 263.35	 Mean_loss: 0.02929185,  training time: 2.28
progress:  14%|[34m‚ñà‚ñç        [0m| 69/500 [02:44<16:44,  2.33s/it]progress:  14%|[34m‚ñà‚ñç        [0m| 70/500 [02:44<16:36,  2.32s/it]                                                          Episode 71	 reward: -2.68	 makespan: 265.75	 Mean_loss: 0.02437214,  training time: 2.26
progress:  14%|[34m‚ñà‚ñç        [0m| 70/500 [02:47<16:36,  2.32s/it]progress:  14%|[34m‚ñà‚ñç        [0m| 71/500 [02:47<16:26,  2.30s/it]                                                          Episode 72	 reward: -2.72	 makespan: 269.60	 Mean_loss: 0.03610517,  training time: 2.29
progress:  14%|[34m‚ñà‚ñç        [0m| 71/500 [02:49<16:26,  2.30s/it]progress:  14%|[34m‚ñà‚ñç        [0m| 72/500 [02:49<16:23,  2.30s/it]                                                          Episode 73	 reward: -2.68	 makespan: 265.80	 Mean_loss: 0.02408029,  training time: 2.36
progress:  14%|[34m‚ñà‚ñç        [0m| 72/500 [02:51<16:23,  2.30s/it]progress:  15%|[34m‚ñà‚ñç        [0m| 73/500 [02:51<16:29,  2.32s/it]                                                          Episode 74	 reward: -2.74	 makespan: 271.40	 Mean_loss: 0.01956566,  training time: 2.28
progress:  15%|[34m‚ñà‚ñç        [0m| 73/500 [02:54<16:29,  2.32s/it]progress:  15%|[34m‚ñà‚ñç        [0m| 74/500 [02:54<16:22,  2.31s/it]                                                          Episode 75	 reward: -2.68	 makespan: 265.80	 Mean_loss: 0.03538890,  training time: 2.26
progress:  15%|[34m‚ñà‚ñç        [0m| 74/500 [02:56<16:22,  2.31s/it]progress:  15%|[34m‚ñà‚ñå        [0m| 75/500 [02:56<16:14,  2.29s/it]                                                          Episode 76	 reward: -2.70	 makespan: 267.50	 Mean_loss: 0.02962421,  training time: 2.35
progress:  15%|[34m‚ñà‚ñå        [0m| 75/500 [02:58<16:14,  2.29s/it]progress:  15%|[34m‚ñà‚ñå        [0m| 76/500 [02:58<16:20,  2.31s/it]                                                          Episode 77	 reward: -2.69	 makespan: 265.85	 Mean_loss: 0.03483753,  training time: 2.28
progress:  15%|[34m‚ñà‚ñå        [0m| 76/500 [03:01<16:20,  2.31s/it]progress:  15%|[34m‚ñà‚ñå        [0m| 77/500 [03:01<16:13,  2.30s/it]                                                          Episode 78	 reward: -2.68	 makespan: 265.15	 Mean_loss: 0.01997625,  training time: 2.27
progress:  15%|[34m‚ñà‚ñå        [0m| 77/500 [03:03<16:13,  2.30s/it]progress:  16%|[34m‚ñà‚ñå        [0m| 78/500 [03:03<16:07,  2.29s/it]                                                          Episode 79	 reward: -2.76	 makespan: 272.75	 Mean_loss: 0.02220555,  training time: 2.35
progress:  16%|[34m‚ñà‚ñå        [0m| 78/500 [03:05<16:07,  2.29s/it]progress:  16%|[34m‚ñà‚ñå        [0m| 79/500 [03:05<16:12,  2.31s/it]                                                          Episode 80	 reward: -2.59	 makespan: 256.15	 Mean_loss: 0.02245978,  training time: 2.31
progress:  16%|[34m‚ñà‚ñå        [0m| 79/500 [03:07<16:12,  2.31s/it]progress:  16%|[34m‚ñà‚ñå        [0m| 80/500 [03:07<16:09,  2.31s/it]                                                          Episode 81	 reward: -2.78	 makespan: 275.20	 Mean_loss: 0.03411105,  training time: 2.54
progress:  16%|[34m‚ñà‚ñå        [0m| 80/500 [03:10<16:09,  2.31s/it]progress:  16%|[34m‚ñà‚ñå        [0m| 81/500 [03:10<16:37,  2.38s/it]                                                          Episode 82	 reward: -2.78	 makespan: 275.65	 Mean_loss: 0.02649896,  training time: 2.27
progress:  16%|[34m‚ñà‚ñå        [0m| 81/500 [03:12<16:37,  2.38s/it]progress:  16%|[34m‚ñà‚ñã        [0m| 82/500 [03:12<16:21,  2.35s/it]                                                          Episode 83	 reward: -2.68	 makespan: 265.15	 Mean_loss: 0.02375340,  training time: 2.31
progress:  16%|[34m‚ñà‚ñã        [0m| 82/500 [03:15<16:21,  2.35s/it]progress:  17%|[34m‚ñà‚ñã        [0m| 83/500 [03:15<16:14,  2.34s/it]                                                          Episode 84	 reward: -2.67	 makespan: 264.20	 Mean_loss: 0.02547500,  training time: 2.32
progress:  17%|[34m‚ñà‚ñã        [0m| 83/500 [03:17<16:14,  2.34s/it]progress:  17%|[34m‚ñà‚ñã        [0m| 84/500 [03:17<16:10,  2.33s/it]                                                          Episode 85	 reward: -2.71	 makespan: 268.65	 Mean_loss: 0.02179536,  training time: 2.27
progress:  17%|[34m‚ñà‚ñã        [0m| 84/500 [03:19<16:10,  2.33s/it]progress:  17%|[34m‚ñà‚ñã        [0m| 85/500 [03:19<16:00,  2.31s/it]                                                          Episode 86	 reward: -2.76	 makespan: 273.15	 Mean_loss: 0.01660732,  training time: 2.29
progress:  17%|[34m‚ñà‚ñã        [0m| 85/500 [03:21<16:00,  2.31s/it]progress:  17%|[34m‚ñà‚ñã        [0m| 86/500 [03:21<15:54,  2.31s/it]                                                          Episode 87	 reward: -2.65	 makespan: 262.05	 Mean_loss: 0.01403110,  training time: 2.47
progress:  17%|[34m‚ñà‚ñã        [0m| 86/500 [03:24<15:54,  2.31s/it]progress:  17%|[34m‚ñà‚ñã        [0m| 87/500 [03:24<16:13,  2.36s/it]                                                          Episode 88	 reward: -2.75	 makespan: 272.00	 Mean_loss: 0.03198399,  training time: 2.27
progress:  17%|[34m‚ñà‚ñã        [0m| 87/500 [03:26<16:13,  2.36s/it]progress:  18%|[34m‚ñà‚ñä        [0m| 88/500 [03:26<15:59,  2.33s/it]                                                          Episode 89	 reward: -2.70	 makespan: 267.40	 Mean_loss: 0.02189574,  training time: 2.26
progress:  18%|[34m‚ñà‚ñä        [0m| 88/500 [03:28<15:59,  2.33s/it]progress:  18%|[34m‚ñà‚ñä        [0m| 89/500 [03:28<15:49,  2.31s/it]                                                          Episode 90	 reward: -2.72	 makespan: 268.80	 Mean_loss: 0.01941222,  training time: 2.31
progress:  18%|[34m‚ñà‚ñä        [0m| 89/500 [03:31<15:49,  2.31s/it]progress:  18%|[34m‚ñà‚ñä        [0m| 90/500 [03:31<15:47,  2.31s/it]                                                          Episode 91	 reward: -2.64	 makespan: 261.35	 Mean_loss: 0.01994228,  training time: 2.27
progress:  18%|[34m‚ñà‚ñä        [0m| 90/500 [03:33<15:47,  2.31s/it]progress:  18%|[34m‚ñà‚ñä        [0m| 91/500 [03:33<15:40,  2.30s/it]                                                          Episode 92	 reward: -2.72	 makespan: 269.55	 Mean_loss: 0.02405678,  training time: 2.26
progress:  18%|[34m‚ñà‚ñä        [0m| 91/500 [03:35<15:40,  2.30s/it]progress:  18%|[34m‚ñà‚ñä        [0m| 92/500 [03:35<15:34,  2.29s/it]                                                          Episode 93	 reward: -2.71	 makespan: 268.65	 Mean_loss: 0.02247876,  training time: 2.33
progress:  18%|[34m‚ñà‚ñä        [0m| 92/500 [03:38<15:34,  2.29s/it]progress:  19%|[34m‚ñà‚ñä        [0m| 93/500 [03:38<15:37,  2.30s/it]                                                          Episode 94	 reward: -2.70	 makespan: 267.45	 Mean_loss: 0.02231162,  training time: 2.49
progress:  19%|[34m‚ñà‚ñä        [0m| 93/500 [03:40<15:37,  2.30s/it]progress:  19%|[34m‚ñà‚ñâ        [0m| 94/500 [03:40<15:58,  2.36s/it]                                                          Episode 95	 reward: -2.67	 makespan: 264.65	 Mean_loss: 0.02323009,  training time: 2.33
progress:  19%|[34m‚ñà‚ñâ        [0m| 94/500 [03:42<15:58,  2.36s/it]progress:  19%|[34m‚ñà‚ñâ        [0m| 95/500 [03:42<15:51,  2.35s/it]                                                          Episode 96	 reward: -2.62	 makespan: 259.50	 Mean_loss: 0.01577356,  training time: 2.25
progress:  19%|[34m‚ñà‚ñâ        [0m| 95/500 [03:45<15:51,  2.35s/it]progress:  19%|[34m‚ñà‚ñâ        [0m| 96/500 [03:45<15:37,  2.32s/it]                                                          Episode 97	 reward: -2.60	 makespan: 257.80	 Mean_loss: 0.01572008,  training time: 2.26
progress:  19%|[34m‚ñà‚ñâ        [0m| 96/500 [03:47<15:37,  2.32s/it]progress:  19%|[34m‚ñà‚ñâ        [0m| 97/500 [03:47<15:27,  2.30s/it]                                                          Episode 98	 reward: -2.74	 makespan: 270.80	 Mean_loss: 0.02572331,  training time: 2.24
progress:  19%|[34m‚ñà‚ñâ        [0m| 97/500 [03:49<15:27,  2.30s/it]progress:  20%|[34m‚ñà‚ñâ        [0m| 98/500 [03:49<15:18,  2.28s/it]                                                          Episode 99	 reward: -2.69	 makespan: 266.45	 Mean_loss: 0.01885552,  training time: 2.28
progress:  20%|[34m‚ñà‚ñâ        [0m| 98/500 [03:52<15:18,  2.28s/it]progress:  20%|[34m‚ñà‚ñâ        [0m| 99/500 [03:52<15:15,  2.28s/it]                                                          Episode 100	 reward: -2.74	 makespan: 271.75	 Mean_loss: 0.02505781,  training time: 2.37
progress:  20%|[34m‚ñà‚ñâ        [0m| 99/500 [03:54<15:15,  2.28s/it]progress:  20%|[34m‚ñà‚ñà        [0m| 100/500 [03:54<15:23,  2.31s/it]                                                           Episode 101	 reward: -2.70	 makespan: 267.20	 Mean_loss: 0.01964492,  training time: 2.35
progress:  20%|[34m‚ñà‚ñà        [0m| 100/500 [03:56<15:23,  2.31s/it]progress:  20%|[34m‚ñà‚ñà        [0m| 101/500 [03:56<15:26,  2.32s/it]                                                           Episode 102	 reward: -2.64	 makespan: 261.10	 Mean_loss: 0.01077212,  training time: 2.27
progress:  20%|[34m‚ñà‚ñà        [0m| 101/500 [03:59<15:26,  2.32s/it]progress:  20%|[34m‚ñà‚ñà        [0m| 102/500 [03:59<15:18,  2.31s/it]                                                           Episode 103	 reward: -2.72	 makespan: 269.45	 Mean_loss: 0.01807856,  training time: 2.38
progress:  20%|[34m‚ñà‚ñà        [0m| 102/500 [04:01<15:18,  2.31s/it]progress:  21%|[34m‚ñà‚ñà        [0m| 103/500 [04:01<15:24,  2.33s/it]                                                           Episode 104	 reward: -2.76	 makespan: 272.75	 Mean_loss: 0.02024417,  training time: 2.32
progress:  21%|[34m‚ñà‚ñà        [0m| 103/500 [04:03<15:24,  2.33s/it]progress:  21%|[34m‚ñà‚ñà        [0m| 104/500 [04:03<15:20,  2.33s/it]                                                           Episode 105	 reward: -2.71	 makespan: 268.15	 Mean_loss: 0.01374428,  training time: 2.29
progress:  21%|[34m‚ñà‚ñà        [0m| 104/500 [04:06<15:20,  2.33s/it]progress:  21%|[34m‚ñà‚ñà        [0m| 105/500 [04:06<15:15,  2.32s/it]                                                           Episode 106	 reward: -2.77	 makespan: 274.30	 Mean_loss: 0.02148699,  training time: 2.37
progress:  21%|[34m‚ñà‚ñà        [0m| 105/500 [04:08<15:15,  2.32s/it]progress:  21%|[34m‚ñà‚ñà        [0m| 106/500 [04:08<15:18,  2.33s/it]                                                           Episode 107	 reward: -2.71	 makespan: 268.75	 Mean_loss: 0.01741350,  training time: 2.25
progress:  21%|[34m‚ñà‚ñà        [0m| 106/500 [04:10<15:18,  2.33s/it]progress:  21%|[34m‚ñà‚ñà‚ñè       [0m| 107/500 [04:10<15:06,  2.31s/it]                                                           Episode 108	 reward: -2.62	 makespan: 259.35	 Mean_loss: 0.01377666,  training time: 2.39
progress:  21%|[34m‚ñà‚ñà‚ñè       [0m| 107/500 [04:13<15:06,  2.31s/it]progress:  22%|[34m‚ñà‚ñà‚ñè       [0m| 108/500 [04:13<15:14,  2.33s/it]                                                           Episode 109	 reward: -2.68	 makespan: 265.10	 Mean_loss: 0.01591997,  training time: 2.45
progress:  22%|[34m‚ñà‚ñà‚ñè       [0m| 108/500 [04:15<15:14,  2.33s/it]progress:  22%|[34m‚ñà‚ñà‚ñè       [0m| 109/500 [04:15<15:26,  2.37s/it]                                                           Episode 110	 reward: -2.73	 makespan: 270.45	 Mean_loss: 0.01441107,  training time: 2.25
progress:  22%|[34m‚ñà‚ñà‚ñè       [0m| 109/500 [04:17<15:26,  2.37s/it]progress:  22%|[34m‚ñà‚ñà‚ñè       [0m| 110/500 [04:17<15:10,  2.34s/it]                                                           Episode 111	 reward: -2.66	 makespan: 263.60	 Mean_loss: 0.01555176,  training time: 2.27
progress:  22%|[34m‚ñà‚ñà‚ñè       [0m| 110/500 [04:20<15:10,  2.34s/it]progress:  22%|[34m‚ñà‚ñà‚ñè       [0m| 111/500 [04:20<15:01,  2.32s/it]                                                           Episode 112	 reward: -2.66	 makespan: 263.00	 Mean_loss: 0.02035974,  training time: 2.28
progress:  22%|[34m‚ñà‚ñà‚ñè       [0m| 111/500 [04:22<15:01,  2.32s/it]progress:  22%|[34m‚ñà‚ñà‚ñè       [0m| 112/500 [04:22<14:54,  2.31s/it]                                                           Episode 113	 reward: -2.77	 makespan: 274.35	 Mean_loss: 0.01559466,  training time: 2.28
progress:  22%|[34m‚ñà‚ñà‚ñè       [0m| 112/500 [04:24<14:54,  2.31s/it]progress:  23%|[34m‚ñà‚ñà‚ñé       [0m| 113/500 [04:24<14:49,  2.30s/it]                                                           Episode 114	 reward: -2.83	 makespan: 280.15	 Mean_loss: 0.01431200,  training time: 2.28
progress:  23%|[34m‚ñà‚ñà‚ñé       [0m| 113/500 [04:26<14:49,  2.30s/it]progress:  23%|[34m‚ñà‚ñà‚ñé       [0m| 114/500 [04:26<14:44,  2.29s/it]                                                           Episode 115	 reward: -2.75	 makespan: 272.60	 Mean_loss: 0.02625238,  training time: 2.35
progress:  23%|[34m‚ñà‚ñà‚ñé       [0m| 114/500 [04:29<14:44,  2.29s/it]progress:  23%|[34m‚ñà‚ñà‚ñé       [0m| 115/500 [04:29<14:49,  2.31s/it]                                                           Episode 116	 reward: -2.68	 makespan: 265.00	 Mean_loss: 0.01547692,  training time: 2.35
progress:  23%|[34m‚ñà‚ñà‚ñé       [0m| 115/500 [04:31<14:49,  2.31s/it]progress:  23%|[34m‚ñà‚ñà‚ñé       [0m| 116/500 [04:31<14:52,  2.32s/it]                                                           Episode 117	 reward: -2.68	 makespan: 265.00	 Mean_loss: 0.01465797,  training time: 2.33
progress:  23%|[34m‚ñà‚ñà‚ñé       [0m| 116/500 [04:33<14:52,  2.32s/it]progress:  23%|[34m‚ñà‚ñà‚ñé       [0m| 117/500 [04:33<14:50,  2.33s/it]                                                           Episode 118	 reward: -2.74	 makespan: 271.45	 Mean_loss: 0.01237564,  training time: 2.25
progress:  23%|[34m‚ñà‚ñà‚ñé       [0m| 117/500 [04:36<14:50,  2.33s/it]progress:  24%|[34m‚ñà‚ñà‚ñé       [0m| 118/500 [04:36<14:40,  2.31s/it]                                                           Episode 119	 reward: -2.62	 makespan: 259.15	 Mean_loss: 0.01801074,  training time: 2.28
progress:  24%|[34m‚ñà‚ñà‚ñé       [0m| 118/500 [04:38<14:40,  2.31s/it]progress:  24%|[34m‚ñà‚ñà‚ñç       [0m| 119/500 [04:38<14:35,  2.30s/it]                                                           Episode 120	 reward: -2.71	 makespan: 268.65	 Mean_loss: 0.02996636,  training time: 2.27
progress:  24%|[34m‚ñà‚ñà‚ñç       [0m| 119/500 [04:40<14:35,  2.30s/it]progress:  24%|[34m‚ñà‚ñà‚ñç       [0m| 120/500 [04:40<14:30,  2.29s/it]                                                           Episode 121	 reward: -2.53	 makespan: 250.60	 Mean_loss: 0.01470213,  training time: 2.34
progress:  24%|[34m‚ñà‚ñà‚ñç       [0m| 120/500 [04:43<14:30,  2.29s/it]progress:  24%|[34m‚ñà‚ñà‚ñç       [0m| 121/500 [04:43<14:33,  2.31s/it]                                                           Episode 122	 reward: -2.62	 makespan: 259.45	 Mean_loss: 0.01610990,  training time: 2.27
progress:  24%|[34m‚ñà‚ñà‚ñç       [0m| 121/500 [04:45<14:33,  2.31s/it]progress:  24%|[34m‚ñà‚ñà‚ñç       [0m| 122/500 [04:45<14:27,  2.30s/it]                                                           Episode 123	 reward: -2.67	 makespan: 264.45	 Mean_loss: 0.02801658,  training time: 2.27
progress:  24%|[34m‚ñà‚ñà‚ñç       [0m| 122/500 [04:47<14:27,  2.30s/it]progress:  25%|[34m‚ñà‚ñà‚ñç       [0m| 123/500 [04:47<14:23,  2.29s/it]                                                           Episode 124	 reward: -2.59	 makespan: 256.25	 Mean_loss: 0.01080819,  training time: 2.48
progress:  25%|[34m‚ñà‚ñà‚ñç       [0m| 123/500 [04:50<14:23,  2.29s/it]progress:  25%|[34m‚ñà‚ñà‚ñç       [0m| 124/500 [04:50<14:42,  2.35s/it]                                                           Episode 125	 reward: -2.63	 makespan: 260.20	 Mean_loss: 0.01656224,  training time: 2.35
progress:  25%|[34m‚ñà‚ñà‚ñç       [0m| 124/500 [04:52<14:42,  2.35s/it]progress:  25%|[34m‚ñà‚ñà‚ñå       [0m| 125/500 [04:52<14:40,  2.35s/it]                                                           Episode 126	 reward: -2.63	 makespan: 260.15	 Mean_loss: 0.00530968,  training time: 2.35
progress:  25%|[34m‚ñà‚ñà‚ñå       [0m| 125/500 [04:54<14:40,  2.35s/it]progress:  25%|[34m‚ñà‚ñà‚ñå       [0m| 126/500 [04:54<14:39,  2.35s/it]                                                           Episode 127	 reward: -2.69	 makespan: 266.05	 Mean_loss: 0.01865704,  training time: 2.42
progress:  25%|[34m‚ñà‚ñà‚ñå       [0m| 126/500 [04:57<14:39,  2.35s/it]progress:  25%|[34m‚ñà‚ñà‚ñå       [0m| 127/500 [04:57<14:44,  2.37s/it]                                                           Episode 128	 reward: -2.59	 makespan: 256.05	 Mean_loss: 0.01266566,  training time: 2.31
progress:  25%|[34m‚ñà‚ñà‚ñå       [0m| 127/500 [04:59<14:44,  2.37s/it]progress:  26%|[34m‚ñà‚ñà‚ñå       [0m| 128/500 [04:59<14:34,  2.35s/it]                                                           Episode 129	 reward: -2.66	 makespan: 263.00	 Mean_loss: 0.01591613,  training time: 2.28
progress:  26%|[34m‚ñà‚ñà‚ñå       [0m| 128/500 [05:01<14:34,  2.35s/it]progress:  26%|[34m‚ñà‚ñà‚ñå       [0m| 129/500 [05:01<14:25,  2.33s/it]                                                           Episode 130	 reward: -2.66	 makespan: 262.95	 Mean_loss: 0.01708761,  training time: 2.29
progress:  26%|[34m‚ñà‚ñà‚ñå       [0m| 129/500 [05:04<14:25,  2.33s/it]progress:  26%|[34m‚ñà‚ñà‚ñå       [0m| 130/500 [05:04<14:17,  2.32s/it]                                                           Episode 131	 reward: -2.62	 makespan: 259.75	 Mean_loss: 0.01910368,  training time: 2.31
progress:  26%|[34m‚ñà‚ñà‚ñå       [0m| 130/500 [05:06<14:17,  2.32s/it]progress:  26%|[34m‚ñà‚ñà‚ñå       [0m| 131/500 [05:06<14:14,  2.32s/it]                                                           Episode 132	 reward: -2.71	 makespan: 268.45	 Mean_loss: 0.01644620,  training time: 2.51
progress:  26%|[34m‚ñà‚ñà‚ñå       [0m| 131/500 [05:08<14:14,  2.32s/it]progress:  26%|[34m‚ñà‚ñà‚ñã       [0m| 132/500 [05:08<14:33,  2.37s/it]                                                           Episode 133	 reward: -2.73	 makespan: 269.90	 Mean_loss: 0.03290585,  training time: 2.37
progress:  26%|[34m‚ñà‚ñà‚ñã       [0m| 132/500 [05:11<14:33,  2.37s/it]progress:  27%|[34m‚ñà‚ñà‚ñã       [0m| 133/500 [05:11<14:31,  2.37s/it]                                                           Episode 134	 reward: -2.63	 makespan: 260.00	 Mean_loss: 0.01142252,  training time: 2.38
progress:  27%|[34m‚ñà‚ñà‚ñã       [0m| 133/500 [05:13<14:31,  2.37s/it]progress:  27%|[34m‚ñà‚ñà‚ñã       [0m| 134/500 [05:13<14:29,  2.38s/it]                                                           Episode 135	 reward: -2.66	 makespan: 262.95	 Mean_loss: 0.02005025,  training time: 2.26
progress:  27%|[34m‚ñà‚ñà‚ñã       [0m| 134/500 [05:15<14:29,  2.38s/it]progress:  27%|[34m‚ñà‚ñà‚ñã       [0m| 135/500 [05:15<14:14,  2.34s/it]                                                           Episode 136	 reward: -2.61	 makespan: 257.90	 Mean_loss: 0.01469463,  training time: 2.25
progress:  27%|[34m‚ñà‚ñà‚ñã       [0m| 135/500 [05:18<14:14,  2.34s/it]progress:  27%|[34m‚ñà‚ñà‚ñã       [0m| 136/500 [05:18<14:02,  2.31s/it]                                                           Episode 137	 reward: -2.69	 makespan: 266.15	 Mean_loss: 0.01721449,  training time: 2.26
progress:  27%|[34m‚ñà‚ñà‚ñã       [0m| 136/500 [05:20<14:02,  2.31s/it]progress:  27%|[34m‚ñà‚ñà‚ñã       [0m| 137/500 [05:20<13:54,  2.30s/it]                                                           Episode 138	 reward: -2.60	 makespan: 257.85	 Mean_loss: 0.01320034,  training time: 2.39
progress:  27%|[34m‚ñà‚ñà‚ñã       [0m| 137/500 [05:22<13:54,  2.30s/it]progress:  28%|[34m‚ñà‚ñà‚ñä       [0m| 138/500 [05:22<14:01,  2.32s/it]                                                           Episode 139	 reward: -2.56	 makespan: 253.75	 Mean_loss: 0.01192588,  training time: 2.26
progress:  28%|[34m‚ñà‚ñà‚ñä       [0m| 138/500 [05:25<14:01,  2.32s/it]progress:  28%|[34m‚ñà‚ñà‚ñä       [0m| 139/500 [05:25<13:52,  2.31s/it]                                                           Episode 140	 reward: -2.55	 makespan: 252.05	 Mean_loss: 0.02555616,  training time: 2.47
progress:  28%|[34m‚ñà‚ñà‚ñä       [0m| 139/500 [05:27<13:52,  2.31s/it]progress:  28%|[34m‚ñà‚ñà‚ñä       [0m| 140/500 [05:27<14:08,  2.36s/it]                                                           Episode 141	 reward: -2.69	 makespan: 265.95	 Mean_loss: 0.03020645,  training time: 2.32
progress:  28%|[34m‚ñà‚ñà‚ñä       [0m| 140/500 [05:29<14:08,  2.36s/it]progress:  28%|[34m‚ñà‚ñà‚ñä       [0m| 141/500 [05:29<14:02,  2.35s/it]                                                           Episode 142	 reward: -2.72	 makespan: 269.15	 Mean_loss: 0.02213004,  training time: 2.29
progress:  28%|[34m‚ñà‚ñà‚ñä       [0m| 141/500 [05:32<14:02,  2.35s/it]progress:  28%|[34m‚ñà‚ñà‚ñä       [0m| 142/500 [05:32<13:54,  2.33s/it]                                                           Episode 143	 reward: -2.74	 makespan: 271.25	 Mean_loss: 0.01402615,  training time: 2.29
progress:  28%|[34m‚ñà‚ñà‚ñä       [0m| 142/500 [05:34<13:54,  2.33s/it]progress:  29%|[34m‚ñà‚ñà‚ñä       [0m| 143/500 [05:34<13:47,  2.32s/it]                                                           Episode 144	 reward: -2.63	 makespan: 260.10	 Mean_loss: 0.01448473,  training time: 2.27
progress:  29%|[34m‚ñà‚ñà‚ñä       [0m| 143/500 [05:36<13:47,  2.32s/it]progress:  29%|[34m‚ñà‚ñà‚ñâ       [0m| 144/500 [05:36<13:39,  2.30s/it]                                                           Episode 145	 reward: -2.69	 makespan: 265.90	 Mean_loss: 0.02854463,  training time: 2.38
progress:  29%|[34m‚ñà‚ñà‚ñâ       [0m| 144/500 [05:39<13:39,  2.30s/it]progress:  29%|[34m‚ñà‚ñà‚ñâ       [0m| 145/500 [05:39<13:45,  2.33s/it]                                                           Episode 146	 reward: -2.72	 makespan: 269.20	 Mean_loss: 0.02082518,  training time: 2.28
progress:  29%|[34m‚ñà‚ñà‚ñâ       [0m| 145/500 [05:41<13:45,  2.33s/it]progress:  29%|[34m‚ñà‚ñà‚ñâ       [0m| 146/500 [05:41<13:38,  2.31s/it]                                                           Episode 147	 reward: -2.76	 makespan: 273.10	 Mean_loss: 0.02474650,  training time: 2.27
progress:  29%|[34m‚ñà‚ñà‚ñâ       [0m| 146/500 [05:43<13:38,  2.31s/it]progress:  29%|[34m‚ñà‚ñà‚ñâ       [0m| 147/500 [05:43<13:31,  2.30s/it]                                                           Episode 148	 reward: -2.74	 makespan: 271.00	 Mean_loss: 0.01758347,  training time: 2.38
progress:  29%|[34m‚ñà‚ñà‚ñâ       [0m| 147/500 [05:46<13:31,  2.30s/it]progress:  30%|[34m‚ñà‚ñà‚ñâ       [0m| 148/500 [05:46<13:38,  2.32s/it]                                                           Episode 149	 reward: -2.73	 makespan: 270.00	 Mean_loss: 0.02294650,  training time: 2.26
progress:  30%|[34m‚ñà‚ñà‚ñâ       [0m| 148/500 [05:48<13:38,  2.32s/it]progress:  30%|[34m‚ñà‚ñà‚ñâ       [0m| 149/500 [05:48<13:29,  2.31s/it]                                                           Episode 150	 reward: -2.69	 makespan: 266.10	 Mean_loss: 0.01586728,  training time: 2.26
progress:  30%|[34m‚ñà‚ñà‚ñâ       [0m| 149/500 [05:50<13:29,  2.31s/it]progress:  30%|[34m‚ñà‚ñà‚ñà       [0m| 150/500 [05:50<13:22,  2.29s/it]                                                           Episode 151	 reward: -2.72	 makespan: 269.15	 Mean_loss: 0.02144547,  training time: 2.33
progress:  30%|[34m‚ñà‚ñà‚ñà       [0m| 150/500 [05:52<13:22,  2.29s/it]progress:  30%|[34m‚ñà‚ñà‚ñà       [0m| 151/500 [05:52<13:24,  2.31s/it]                                                           Episode 152	 reward: -2.67	 makespan: 264.30	 Mean_loss: 0.01985227,  training time: 2.28
progress:  30%|[34m‚ñà‚ñà‚ñà       [0m| 151/500 [05:55<13:24,  2.31s/it]progress:  30%|[34m‚ñà‚ñà‚ñà       [0m| 152/500 [05:55<13:19,  2.30s/it]                                                           Episode 153	 reward: -2.65	 makespan: 262.05	 Mean_loss: 0.02137886,  training time: 2.32
progress:  30%|[34m‚ñà‚ñà‚ñà       [0m| 152/500 [05:57<13:19,  2.30s/it]progress:  31%|[34m‚ñà‚ñà‚ñà       [0m| 153/500 [05:57<13:20,  2.31s/it]                                                           Episode 154	 reward: -2.69	 makespan: 266.20	 Mean_loss: 0.01499379,  training time: 2.26
progress:  31%|[34m‚ñà‚ñà‚ñà       [0m| 153/500 [05:59<13:20,  2.31s/it]progress:  31%|[34m‚ñà‚ñà‚ñà       [0m| 154/500 [05:59<13:13,  2.29s/it]                                                           Episode 155	 reward: -2.74	 makespan: 271.45	 Mean_loss: 0.02880706,  training time: 2.28
progress:  31%|[34m‚ñà‚ñà‚ñà       [0m| 154/500 [06:02<13:13,  2.29s/it]progress:  31%|[34m‚ñà‚ñà‚ñà       [0m| 155/500 [06:02<13:09,  2.29s/it]                                                           Episode 156	 reward: -2.69	 makespan: 266.10	 Mean_loss: 0.02483765,  training time: 2.35
progress:  31%|[34m‚ñà‚ñà‚ñà       [0m| 155/500 [06:04<13:09,  2.29s/it]progress:  31%|[34m‚ñà‚ñà‚ñà       [0m| 156/500 [06:04<13:14,  2.31s/it]                                                           Episode 157	 reward: -2.70	 makespan: 267.20	 Mean_loss: 0.01658978,  training time: 2.33
progress:  31%|[34m‚ñà‚ñà‚ñà       [0m| 156/500 [06:06<13:14,  2.31s/it]progress:  31%|[34m‚ñà‚ñà‚ñà‚ñè      [0m| 157/500 [06:06<13:14,  2.32s/it]                                                           Episode 158	 reward: -2.71	 makespan: 268.00	 Mean_loss: 0.02065855,  training time: 2.35
progress:  31%|[34m‚ñà‚ñà‚ñà‚ñè      [0m| 157/500 [06:09<13:14,  2.32s/it]progress:  32%|[34m‚ñà‚ñà‚ñà‚ñè      [0m| 158/500 [06:09<13:15,  2.33s/it]                                                           Episode 159	 reward: -2.82	 makespan: 278.95	 Mean_loss: 0.02687138,  training time: 2.28
progress:  32%|[34m‚ñà‚ñà‚ñà‚ñè      [0m| 158/500 [06:11<13:15,  2.33s/it]progress:  32%|[34m‚ñà‚ñà‚ñà‚ñè      [0m| 159/500 [06:11<13:08,  2.31s/it]                                                           Episode 160	 reward: -2.70	 makespan: 267.30	 Mean_loss: 0.02526134,  training time: 2.33
progress:  32%|[34m‚ñà‚ñà‚ñà‚ñè      [0m| 159/500 [06:13<13:08,  2.31s/it]progress:  32%|[34m‚ñà‚ñà‚ñà‚ñè      [0m| 160/500 [06:13<13:07,  2.32s/it]                                                           Episode 161	 reward: -2.69	 makespan: 266.40	 Mean_loss: 0.02327027,  training time: 2.38
progress:  32%|[34m‚ñà‚ñà‚ñà‚ñè      [0m| 160/500 [06:16<13:07,  2.32s/it]progress:  32%|[34m‚ñà‚ñà‚ñà‚ñè      [0m| 161/500 [06:16<13:12,  2.34s/it]                                                           Episode 162	 reward: -2.64	 makespan: 261.00	 Mean_loss: 0.02310915,  training time: 2.43
progress:  32%|[34m‚ñà‚ñà‚ñà‚ñè      [0m| 161/500 [06:18<13:12,  2.34s/it]progress:  32%|[34m‚ñà‚ñà‚ñà‚ñè      [0m| 162/500 [06:18<13:20,  2.37s/it]                                                           Episode 163	 reward: -2.59	 makespan: 256.30	 Mean_loss: 0.01652602,  training time: 2.28
progress:  32%|[34m‚ñà‚ñà‚ñà‚ñè      [0m| 162/500 [06:20<13:20,  2.37s/it]progress:  33%|[34m‚ñà‚ñà‚ñà‚ñé      [0m| 163/500 [06:20<13:09,  2.34s/it]                                                           Episode 164	 reward: -2.65	 makespan: 262.35	 Mean_loss: 0.01725038,  training time: 2.33
progress:  33%|[34m‚ñà‚ñà‚ñà‚ñé      [0m| 163/500 [06:23<13:09,  2.34s/it]progress:  33%|[34m‚ñà‚ñà‚ñà‚ñé      [0m| 164/500 [06:23<13:05,  2.34s/it]                                                           Episode 165	 reward: -2.63	 makespan: 259.95	 Mean_loss: 0.01216123,  training time: 2.40
progress:  33%|[34m‚ñà‚ñà‚ñà‚ñé      [0m| 164/500 [06:25<13:05,  2.34s/it]progress:  33%|[34m‚ñà‚ñà‚ñà‚ñé      [0m| 165/500 [06:25<13:09,  2.36s/it]                                                           Episode 166	 reward: -2.63	 makespan: 260.45	 Mean_loss: 0.02046739,  training time: 2.25
progress:  33%|[34m‚ñà‚ñà‚ñà‚ñé      [0m| 165/500 [06:27<13:09,  2.36s/it]progress:  33%|[34m‚ñà‚ñà‚ñà‚ñé      [0m| 166/500 [06:27<12:56,  2.32s/it]                                                           Episode 167	 reward: -2.69	 makespan: 266.15	 Mean_loss: 0.02826001,  training time: 2.28
progress:  33%|[34m‚ñà‚ñà‚ñà‚ñé      [0m| 166/500 [06:30<12:56,  2.32s/it]progress:  33%|[34m‚ñà‚ñà‚ñà‚ñé      [0m| 167/500 [06:30<12:49,  2.31s/it]                                                           Episode 168	 reward: -2.68	 makespan: 265.10	 Mean_loss: 0.02041918,  training time: 2.28
progress:  33%|[34m‚ñà‚ñà‚ñà‚ñé      [0m| 167/500 [06:32<12:49,  2.31s/it]progress:  34%|[34m‚ñà‚ñà‚ñà‚ñé      [0m| 168/500 [06:32<12:44,  2.30s/it]                                                           Episode 169	 reward: -2.64	 makespan: 261.15	 Mean_loss: 0.01689654,  training time: 2.25
progress:  34%|[34m‚ñà‚ñà‚ñà‚ñé      [0m| 168/500 [06:34<12:44,  2.30s/it]progress:  34%|[34m‚ñà‚ñà‚ñà‚ñç      [0m| 169/500 [06:34<12:36,  2.29s/it]                                                           Episode 170	 reward: -2.61	 makespan: 258.30	 Mean_loss: 0.02155963,  training time: 2.36
progress:  34%|[34m‚ñà‚ñà‚ñà‚ñç      [0m| 169/500 [06:36<12:36,  2.29s/it]progress:  34%|[34m‚ñà‚ñà‚ñà‚ñç      [0m| 170/500 [06:36<12:42,  2.31s/it]                                                           Episode 171	 reward: -2.70	 makespan: 267.25	 Mean_loss: 0.01896688,  training time: 2.31
progress:  34%|[34m‚ñà‚ñà‚ñà‚ñç      [0m| 170/500 [06:39<12:42,  2.31s/it]progress:  34%|[34m‚ñà‚ñà‚ñà‚ñç      [0m| 171/500 [06:39<12:39,  2.31s/it]                                                           Episode 172	 reward: -2.62	 makespan: 259.75	 Mean_loss: 0.01342962,  training time: 2.53
progress:  34%|[34m‚ñà‚ñà‚ñà‚ñç      [0m| 171/500 [06:41<12:39,  2.31s/it]progress:  34%|[34m‚ñà‚ñà‚ñà‚ñç      [0m| 172/500 [06:41<12:58,  2.37s/it]                                                           Episode 173	 reward: -2.58	 makespan: 255.25	 Mean_loss: 0.01709205,  training time: 2.38
progress:  34%|[34m‚ñà‚ñà‚ñà‚ñç      [0m| 172/500 [06:44<12:58,  2.37s/it]progress:  35%|[34m‚ñà‚ñà‚ñà‚ñç      [0m| 173/500 [06:44<12:57,  2.38s/it]                                                           Episode 174	 reward: -2.65	 makespan: 262.00	 Mean_loss: 0.02606474,  training time: 2.45
progress:  35%|[34m‚ñà‚ñà‚ñà‚ñç      [0m| 173/500 [06:46<12:57,  2.38s/it]progress:  35%|[34m‚ñà‚ñà‚ñà‚ñç      [0m| 174/500 [06:46<13:02,  2.40s/it]                                                           Episode 175	 reward: -2.61	 makespan: 258.55	 Mean_loss: 0.01952865,  training time: 2.45
progress:  35%|[34m‚ñà‚ñà‚ñà‚ñç      [0m| 174/500 [06:49<13:02,  2.40s/it]progress:  35%|[34m‚ñà‚ñà‚ñà‚ñå      [0m| 175/500 [06:49<13:04,  2.41s/it]                                                           Episode 176	 reward: -2.61	 makespan: 258.25	 Mean_loss: 0.02859991,  training time: 2.30
progress:  35%|[34m‚ñà‚ñà‚ñà‚ñå      [0m| 175/500 [06:51<13:04,  2.41s/it]progress:  35%|[34m‚ñà‚ñà‚ñà‚ñå      [0m| 176/500 [06:51<12:51,  2.38s/it]                                                           Episode 177	 reward: -2.61	 makespan: 258.85	 Mean_loss: 0.01995473,  training time: 2.29
progress:  35%|[34m‚ñà‚ñà‚ñà‚ñå      [0m| 176/500 [06:53<12:51,  2.38s/it]progress:  35%|[34m‚ñà‚ñà‚ñà‚ñå      [0m| 177/500 [06:53<12:40,  2.35s/it]                                                           Episode 178	 reward: -2.65	 makespan: 262.50	 Mean_loss: 0.02247582,  training time: 2.39
progress:  35%|[34m‚ñà‚ñà‚ñà‚ñå      [0m| 177/500 [06:56<12:40,  2.35s/it]progress:  36%|[34m‚ñà‚ñà‚ñà‚ñå      [0m| 178/500 [06:56<12:41,  2.36s/it]                                                           Episode 179	 reward: -2.59	 makespan: 256.25	 Mean_loss: 0.01611470,  training time: 2.31
progress:  36%|[34m‚ñà‚ñà‚ñà‚ñå      [0m| 178/500 [06:58<12:41,  2.36s/it]progress:  36%|[34m‚ñà‚ñà‚ñà‚ñå      [0m| 179/500 [06:58<12:33,  2.35s/it]                                                           Episode 180	 reward: -2.62	 makespan: 259.15	 Mean_loss: 0.01361895,  training time: 2.28
progress:  36%|[34m‚ñà‚ñà‚ñà‚ñå      [0m| 179/500 [07:00<12:33,  2.35s/it]progress:  36%|[34m‚ñà‚ñà‚ñà‚ñå      [0m| 180/500 [07:00<12:24,  2.33s/it]                                                           Episode 181	 reward: -2.75	 makespan: 272.00	 Mean_loss: 0.03241822,  training time: 2.34
progress:  36%|[34m‚ñà‚ñà‚ñà‚ñå      [0m| 180/500 [07:02<12:24,  2.33s/it]progress:  36%|[34m‚ñà‚ñà‚ñà‚ñå      [0m| 181/500 [07:02<12:23,  2.33s/it]                                                           Episode 182	 reward: -2.71	 makespan: 268.55	 Mean_loss: 0.02403831,  training time: 2.35
progress:  36%|[34m‚ñà‚ñà‚ñà‚ñå      [0m| 181/500 [07:05<12:23,  2.33s/it]progress:  36%|[34m‚ñà‚ñà‚ñà‚ñã      [0m| 182/500 [07:05<12:22,  2.34s/it]                                                           Episode 183	 reward: -2.79	 makespan: 276.70	 Mean_loss: 0.02158580,  training time: 2.31
progress:  36%|[34m‚ñà‚ñà‚ñà‚ñã      [0m| 182/500 [07:07<12:22,  2.34s/it]progress:  37%|[34m‚ñà‚ñà‚ñà‚ñã      [0m| 183/500 [07:07<12:18,  2.33s/it]                                                           Episode 184	 reward: -2.73	 makespan: 270.15	 Mean_loss: 0.02178518,  training time: 2.48
progress:  37%|[34m‚ñà‚ñà‚ñà‚ñã      [0m| 183/500 [07:10<12:18,  2.33s/it]progress:  37%|[34m‚ñà‚ñà‚ñà‚ñã      [0m| 184/500 [07:10<12:30,  2.37s/it]                                                           Episode 185	 reward: -2.66	 makespan: 263.55	 Mean_loss: 0.01664074,  training time: 2.38
progress:  37%|[34m‚ñà‚ñà‚ñà‚ñã      [0m| 184/500 [07:12<12:30,  2.37s/it]progress:  37%|[34m‚ñà‚ñà‚ñà‚ñã      [0m| 185/500 [07:12<12:28,  2.38s/it]                                                           Episode 186	 reward: -2.73	 makespan: 270.55	 Mean_loss: 0.02131345,  training time: 2.36
progress:  37%|[34m‚ñà‚ñà‚ñà‚ñã      [0m| 185/500 [07:14<12:28,  2.38s/it]progress:  37%|[34m‚ñà‚ñà‚ñà‚ñã      [0m| 186/500 [07:14<12:25,  2.37s/it]                                                           Episode 187	 reward: -2.80	 makespan: 277.15	 Mean_loss: 0.02821292,  training time: 2.44
progress:  37%|[34m‚ñà‚ñà‚ñà‚ñã      [0m| 186/500 [07:17<12:25,  2.37s/it]progress:  37%|[34m‚ñà‚ñà‚ñà‚ñã      [0m| 187/500 [07:17<12:29,  2.39s/it]                                                           Episode 188	 reward: -2.66	 makespan: 263.35	 Mean_loss: 0.02570519,  training time: 2.37
progress:  37%|[34m‚ñà‚ñà‚ñà‚ñã      [0m| 187/500 [07:19<12:29,  2.39s/it]progress:  38%|[34m‚ñà‚ñà‚ñà‚ñä      [0m| 188/500 [07:19<12:24,  2.39s/it]                                                           Episode 189	 reward: -2.72	 makespan: 268.95	 Mean_loss: 0.02169847,  training time: 2.42
progress:  38%|[34m‚ñà‚ñà‚ñà‚ñä      [0m| 188/500 [07:22<12:24,  2.39s/it]progress:  38%|[34m‚ñà‚ñà‚ñà‚ñä      [0m| 189/500 [07:22<12:25,  2.40s/it]                                                           Episode 190	 reward: -2.69	 makespan: 266.70	 Mean_loss: 0.02420665,  training time: 2.37
progress:  38%|[34m‚ñà‚ñà‚ñà‚ñä      [0m| 189/500 [07:24<12:25,  2.40s/it]progress:  38%|[34m‚ñà‚ñà‚ñà‚ñä      [0m| 190/500 [07:24<12:20,  2.39s/it]                                                           Episode 191	 reward: -2.82	 makespan: 278.85	 Mean_loss: 0.03002258,  training time: 2.37
progress:  38%|[34m‚ñà‚ñà‚ñà‚ñä      [0m| 190/500 [07:26<12:20,  2.39s/it]progress:  38%|[34m‚ñà‚ñà‚ñà‚ñä      [0m| 191/500 [07:26<12:17,  2.39s/it]                                                           Episode 192	 reward: -2.74	 makespan: 270.85	 Mean_loss: 0.02599734,  training time: 2.36
progress:  38%|[34m‚ñà‚ñà‚ñà‚ñä      [0m| 191/500 [07:29<12:17,  2.39s/it]progress:  38%|[34m‚ñà‚ñà‚ñà‚ñä      [0m| 192/500 [07:29<12:12,  2.38s/it]                                                           Episode 193	 reward: -2.74	 makespan: 271.05	 Mean_loss: 0.02392787,  training time: 2.38
progress:  38%|[34m‚ñà‚ñà‚ñà‚ñä      [0m| 192/500 [07:31<12:12,  2.38s/it]progress:  39%|[34m‚ñà‚ñà‚ñà‚ñä      [0m| 193/500 [07:31<12:10,  2.38s/it]                                                           Episode 194	 reward: -2.69	 makespan: 266.60	 Mean_loss: 0.01896030,  training time: 2.29
progress:  39%|[34m‚ñà‚ñà‚ñà‚ñä      [0m| 193/500 [07:33<12:10,  2.38s/it]progress:  39%|[34m‚ñà‚ñà‚ñà‚ñâ      [0m| 194/500 [07:33<11:59,  2.35s/it]                                                           Episode 195	 reward: -2.66	 makespan: 263.10	 Mean_loss: 0.02323132,  training time: 2.29
progress:  39%|[34m‚ñà‚ñà‚ñà‚ñâ      [0m| 194/500 [07:36<11:59,  2.35s/it]progress:  39%|[34m‚ñà‚ñà‚ñà‚ñâ      [0m| 195/500 [07:36<11:51,  2.33s/it]                                                           Episode 196	 reward: -2.71	 makespan: 268.65	 Mean_loss: 0.02073834,  training time: 2.32
progress:  39%|[34m‚ñà‚ñà‚ñà‚ñâ      [0m| 195/500 [07:38<11:51,  2.33s/it]progress:  39%|[34m‚ñà‚ñà‚ñà‚ñâ      [0m| 196/500 [07:38<11:48,  2.33s/it]                                                           Episode 197	 reward: -2.76	 makespan: 273.30	 Mean_loss: 0.01969021,  training time: 2.33
progress:  39%|[34m‚ñà‚ñà‚ñà‚ñâ      [0m| 196/500 [07:40<11:48,  2.33s/it]progress:  39%|[34m‚ñà‚ñà‚ñà‚ñâ      [0m| 197/500 [07:40<11:45,  2.33s/it]                                                           Episode 198	 reward: -2.76	 makespan: 273.55	 Mean_loss: 0.01785444,  training time: 2.31
progress:  39%|[34m‚ñà‚ñà‚ñà‚ñâ      [0m| 197/500 [07:43<11:45,  2.33s/it]progress:  40%|[34m‚ñà‚ñà‚ñà‚ñâ      [0m| 198/500 [07:43<11:41,  2.32s/it]                                                           Episode 199	 reward: -2.76	 makespan: 273.30	 Mean_loss: 0.02279937,  training time: 2.30
progress:  40%|[34m‚ñà‚ñà‚ñà‚ñâ      [0m| 198/500 [07:45<11:41,  2.32s/it]progress:  40%|[34m‚ñà‚ñà‚ñà‚ñâ      [0m| 199/500 [07:45<11:37,  2.32s/it]                                                           Episode 200	 reward: -2.76	 makespan: 273.25	 Mean_loss: 0.01894964,  training time: 2.34
progress:  40%|[34m‚ñà‚ñà‚ñà‚ñâ      [0m| 199/500 [07:47<11:37,  2.32s/it]progress:  40%|[34m‚ñà‚ñà‚ñà‚ñà      [0m| 200/500 [07:47<11:37,  2.32s/it]                                                           Episode 201	 reward: -2.67	 makespan: 264.35	 Mean_loss: 0.01713028,  training time: 2.37
progress:  40%|[34m‚ñà‚ñà‚ñà‚ñà      [0m| 200/500 [07:50<11:37,  2.32s/it]progress:  40%|[34m‚ñà‚ñà‚ñà‚ñà      [0m| 201/500 [07:50<11:39,  2.34s/it]                                                           Episode 202	 reward: -2.50	 makespan: 247.80	 Mean_loss: 0.01525673,  training time: 2.30
progress:  40%|[34m‚ñà‚ñà‚ñà‚ñà      [0m| 201/500 [07:52<11:39,  2.34s/it]progress:  40%|[34m‚ñà‚ñà‚ñà‚ñà      [0m| 202/500 [07:52<11:33,  2.33s/it]                                                           Episode 203	 reward: -2.55	 makespan: 252.75	 Mean_loss: 0.01315025,  training time: 2.29
progress:  40%|[34m‚ñà‚ñà‚ñà‚ñà      [0m| 202/500 [07:54<11:33,  2.33s/it]progress:  41%|[34m‚ñà‚ñà‚ñà‚ñà      [0m| 203/500 [07:54<11:28,  2.32s/it]                                                           Episode 204	 reward: -2.58	 makespan: 255.80	 Mean_loss: 0.01721627,  training time: 2.37
progress:  41%|[34m‚ñà‚ñà‚ñà‚ñà      [0m| 203/500 [07:57<11:28,  2.32s/it]progress:  41%|[34m‚ñà‚ñà‚ñà‚ñà      [0m| 204/500 [07:57<11:30,  2.33s/it]                                                           Episode 205	 reward: -2.53	 makespan: 250.65	 Mean_loss: 0.01416206,  training time: 2.29
progress:  41%|[34m‚ñà‚ñà‚ñà‚ñà      [0m| 204/500 [07:59<11:30,  2.33s/it]progress:  41%|[34m‚ñà‚ñà‚ñà‚ñà      [0m| 205/500 [07:59<11:24,  2.32s/it]                                                           Episode 206	 reward: -2.64	 makespan: 261.85	 Mean_loss: 0.02006967,  training time: 2.29
progress:  41%|[34m‚ñà‚ñà‚ñà‚ñà      [0m| 205/500 [08:01<11:24,  2.32s/it]progress:  41%|[34m‚ñà‚ñà‚ñà‚ñà      [0m| 206/500 [08:01<11:19,  2.31s/it]                                                           Episode 207	 reward: -2.63	 makespan: 260.30	 Mean_loss: 0.01917509,  training time: 2.29
progress:  41%|[34m‚ñà‚ñà‚ñà‚ñà      [0m| 206/500 [08:03<11:19,  2.31s/it]progress:  41%|[34m‚ñà‚ñà‚ñà‚ñà‚ñè     [0m| 207/500 [08:03<11:15,  2.31s/it]                                                           Episode 208	 reward: -2.62	 makespan: 259.45	 Mean_loss: 0.02277462,  training time: 2.31
progress:  41%|[34m‚ñà‚ñà‚ñà‚ñà‚ñè     [0m| 207/500 [08:06<11:15,  2.31s/it]progress:  42%|[34m‚ñà‚ñà‚ñà‚ñà‚ñè     [0m| 208/500 [08:06<11:13,  2.31s/it]                                                           Episode 209	 reward: -2.60	 makespan: 257.25	 Mean_loss: 0.01358799,  training time: 2.29
progress:  42%|[34m‚ñà‚ñà‚ñà‚ñà‚ñè     [0m| 208/500 [08:08<11:13,  2.31s/it]progress:  42%|[34m‚ñà‚ñà‚ñà‚ñà‚ñè     [0m| 209/500 [08:08<11:10,  2.30s/it]                                                           Episode 210	 reward: -2.67	 makespan: 264.10	 Mean_loss: 0.01478033,  training time: 2.29
progress:  42%|[34m‚ñà‚ñà‚ñà‚ñà‚ñè     [0m| 209/500 [08:10<11:10,  2.30s/it]progress:  42%|[34m‚ñà‚ñà‚ñà‚ñà‚ñè     [0m| 210/500 [08:10<11:07,  2.30s/it]                                                           Episode 211	 reward: -2.60	 makespan: 257.35	 Mean_loss: 0.01178502,  training time: 2.29
progress:  42%|[34m‚ñà‚ñà‚ñà‚ñà‚ñè     [0m| 210/500 [08:13<11:07,  2.30s/it]progress:  42%|[34m‚ñà‚ñà‚ñà‚ñà‚ñè     [0m| 211/500 [08:13<11:03,  2.30s/it]                                                           Episode 212	 reward: -2.59	 makespan: 256.90	 Mean_loss: 0.02166350,  training time: 2.32
progress:  42%|[34m‚ñà‚ñà‚ñà‚ñà‚ñè     [0m| 211/500 [08:15<11:03,  2.30s/it]progress:  42%|[34m‚ñà‚ñà‚ñà‚ñà‚ñè     [0m| 212/500 [08:15<11:03,  2.30s/it]                                                           Episode 213	 reward: -2.66	 makespan: 263.45	 Mean_loss: 0.01372286,  training time: 2.30
progress:  42%|[34m‚ñà‚ñà‚ñà‚ñà‚ñè     [0m| 212/500 [08:17<11:03,  2.30s/it]progress:  43%|[34m‚ñà‚ñà‚ñà‚ñà‚ñé     [0m| 213/500 [08:17<11:00,  2.30s/it]                                                           Episode 214	 reward: -2.64	 makespan: 261.75	 Mean_loss: 0.02590458,  training time: 2.29
progress:  43%|[34m‚ñà‚ñà‚ñà‚ñà‚ñé     [0m| 213/500 [08:20<11:00,  2.30s/it]progress:  43%|[34m‚ñà‚ñà‚ñà‚ñà‚ñé     [0m| 214/500 [08:20<10:57,  2.30s/it]                                                           Episode 215	 reward: -2.67	 makespan: 264.15	 Mean_loss: 0.02335405,  training time: 2.31
progress:  43%|[34m‚ñà‚ñà‚ñà‚ñà‚ñé     [0m| 214/500 [08:22<10:57,  2.30s/it]progress:  43%|[34m‚ñà‚ñà‚ñà‚ñà‚ñé     [0m| 215/500 [08:22<10:56,  2.30s/it]                                                           Episode 216	 reward: -2.58	 makespan: 255.40	 Mean_loss: 0.01882255,  training time: 2.29
progress:  43%|[34m‚ñà‚ñà‚ñà‚ñà‚ñé     [0m| 215/500 [08:24<10:56,  2.30s/it]progress:  43%|[34m‚ñà‚ñà‚ñà‚ñà‚ñé     [0m| 216/500 [08:24<10:52,  2.30s/it]                                                           Episode 217	 reward: -2.59	 makespan: 256.35	 Mean_loss: 0.01053754,  training time: 2.29
progress:  43%|[34m‚ñà‚ñà‚ñà‚ñà‚ñé     [0m| 216/500 [08:26<10:52,  2.30s/it]progress:  43%|[34m‚ñà‚ñà‚ñà‚ñà‚ñé     [0m| 217/500 [08:26<10:50,  2.30s/it]                                                           Episode 218	 reward: -2.54	 makespan: 251.35	 Mean_loss: 0.01848214,  training time: 2.34
progress:  43%|[34m‚ñà‚ñà‚ñà‚ñà‚ñé     [0m| 217/500 [08:29<10:50,  2.30s/it]progress:  44%|[34m‚ñà‚ñà‚ñà‚ñà‚ñé     [0m| 218/500 [08:29<10:51,  2.31s/it]                                                           Episode 219	 reward: -2.63	 makespan: 259.95	 Mean_loss: 0.01458708,  training time: 2.33
progress:  44%|[34m‚ñà‚ñà‚ñà‚ñà‚ñé     [0m| 218/500 [08:31<10:51,  2.31s/it]progress:  44%|[34m‚ñà‚ñà‚ñà‚ñà‚ñç     [0m| 219/500 [08:31<10:51,  2.32s/it]                                                           Episode 220	 reward: -2.64	 makespan: 261.60	 Mean_loss: 0.01551797,  training time: 2.31
progress:  44%|[34m‚ñà‚ñà‚ñà‚ñà‚ñç     [0m| 219/500 [08:33<10:51,  2.32s/it]progress:  44%|[34m‚ñà‚ñà‚ñà‚ñà‚ñç     [0m| 220/500 [08:33<10:48,  2.31s/it]                                                           Episode 221	 reward: -2.58	 makespan: 254.95	 Mean_loss: 0.02043271,  training time: 2.45
progress:  44%|[34m‚ñà‚ñà‚ñà‚ñà‚ñç     [0m| 220/500 [08:36<10:48,  2.31s/it]progress:  44%|[34m‚ñà‚ñà‚ñà‚ñà‚ñç     [0m| 221/500 [08:36<10:57,  2.36s/it]                                                           Episode 222	 reward: -2.53	 makespan: 250.90	 Mean_loss: 0.01189025,  training time: 2.48
progress:  44%|[34m‚ñà‚ñà‚ñà‚ñà‚ñç     [0m| 221/500 [08:38<10:57,  2.36s/it]progress:  44%|[34m‚ñà‚ñà‚ñà‚ñà‚ñç     [0m| 222/500 [08:38<11:05,  2.39s/it]                                                           Episode 223	 reward: -2.58	 makespan: 255.50	 Mean_loss: 0.01223775,  training time: 2.40
progress:  44%|[34m‚ñà‚ñà‚ñà‚ñà‚ñç     [0m| 222/500 [08:41<11:05,  2.39s/it]progress:  45%|[34m‚ñà‚ñà‚ñà‚ñà‚ñç     [0m| 223/500 [08:41<11:03,  2.39s/it]                                                           Episode 224	 reward: -2.57	 makespan: 254.60	 Mean_loss: 0.01911585,  training time: 2.48
progress:  45%|[34m‚ñà‚ñà‚ñà‚ñà‚ñç     [0m| 223/500 [08:43<11:03,  2.39s/it]progress:  45%|[34m‚ñà‚ñà‚ñà‚ñà‚ñç     [0m| 224/500 [08:43<11:07,  2.42s/it]                                                           Episode 225	 reward: -2.62	 makespan: 259.45	 Mean_loss: 0.01821326,  training time: 2.28
progress:  45%|[34m‚ñà‚ñà‚ñà‚ñà‚ñç     [0m| 224/500 [08:46<11:07,  2.42s/it]progress:  45%|[34m‚ñà‚ñà‚ñà‚ñà‚ñå     [0m| 225/500 [08:46<10:53,  2.38s/it]                                                           Episode 226	 reward: -2.60	 makespan: 257.20	 Mean_loss: 0.01757573,  training time: 2.29
progress:  45%|[34m‚ñà‚ñà‚ñà‚ñà‚ñå     [0m| 225/500 [08:48<10:53,  2.38s/it]progress:  45%|[34m‚ñà‚ñà‚ñà‚ñà‚ñå     [0m| 226/500 [08:48<10:44,  2.35s/it]                                                           Episode 227	 reward: -2.62	 makespan: 258.90	 Mean_loss: 0.01569701,  training time: 2.27
progress:  45%|[34m‚ñà‚ñà‚ñà‚ñà‚ñå     [0m| 226/500 [08:50<10:44,  2.35s/it]progress:  45%|[34m‚ñà‚ñà‚ñà‚ñà‚ñå     [0m| 227/500 [08:50<10:35,  2.33s/it]                                                           Episode 228	 reward: -2.54	 makespan: 251.50	 Mean_loss: 0.01738621,  training time: 2.26
progress:  45%|[34m‚ñà‚ñà‚ñà‚ñà‚ñå     [0m| 227/500 [08:52<10:35,  2.33s/it]progress:  46%|[34m‚ñà‚ñà‚ñà‚ñà‚ñå     [0m| 228/500 [08:52<10:27,  2.31s/it]                                                           Episode 229	 reward: -2.75	 makespan: 272.05	 Mean_loss: 0.02785115,  training time: 2.27
progress:  46%|[34m‚ñà‚ñà‚ñà‚ñà‚ñå     [0m| 228/500 [08:55<10:27,  2.31s/it]progress:  46%|[34m‚ñà‚ñà‚ñà‚ñà‚ñå     [0m| 229/500 [08:55<10:21,  2.29s/it]                                                           Episode 230	 reward: -2.50	 makespan: 247.85	 Mean_loss: 0.01306163,  training time: 2.27
progress:  46%|[34m‚ñà‚ñà‚ñà‚ñà‚ñå     [0m| 229/500 [08:57<10:21,  2.29s/it]progress:  46%|[34m‚ñà‚ñà‚ñà‚ñà‚ñå     [0m| 230/500 [08:57<10:17,  2.29s/it]                                                           Episode 231	 reward: -2.56	 makespan: 253.40	 Mean_loss: 0.01564246,  training time: 2.28
progress:  46%|[34m‚ñà‚ñà‚ñà‚ñà‚ñå     [0m| 230/500 [08:59<10:17,  2.29s/it]progress:  46%|[34m‚ñà‚ñà‚ñà‚ñà‚ñå     [0m| 231/500 [08:59<10:14,  2.29s/it]                                                           Episode 232	 reward: -2.59	 makespan: 256.25	 Mean_loss: 0.01513608,  training time: 2.30
progress:  46%|[34m‚ñà‚ñà‚ñà‚ñà‚ñå     [0m| 231/500 [09:01<10:14,  2.29s/it]progress:  46%|[34m‚ñà‚ñà‚ñà‚ñà‚ñã     [0m| 232/500 [09:01<10:13,  2.29s/it]                                                           Episode 233	 reward: -2.55	 makespan: 252.05	 Mean_loss: 0.01297645,  training time: 2.30
progress:  46%|[34m‚ñà‚ñà‚ñà‚ñà‚ñã     [0m| 232/500 [09:04<10:13,  2.29s/it]progress:  47%|[34m‚ñà‚ñà‚ñà‚ñà‚ñã     [0m| 233/500 [09:04<10:12,  2.29s/it]                                                           Episode 234	 reward: -2.54	 makespan: 251.90	 Mean_loss: 0.00873080,  training time: 2.42
progress:  47%|[34m‚ñà‚ñà‚ñà‚ñà‚ñã     [0m| 233/500 [09:06<10:12,  2.29s/it]progress:  47%|[34m‚ñà‚ñà‚ñà‚ñà‚ñã     [0m| 234/500 [09:06<10:20,  2.33s/it]                                                           Episode 235	 reward: -2.67	 makespan: 264.45	 Mean_loss: 0.01570473,  training time: 2.30
progress:  47%|[34m‚ñà‚ñà‚ñà‚ñà‚ñã     [0m| 234/500 [09:08<10:20,  2.33s/it]progress:  47%|[34m‚ñà‚ñà‚ñà‚ñà‚ñã     [0m| 235/500 [09:08<10:15,  2.32s/it]                                                           Episode 236	 reward: -2.60	 makespan: 257.15	 Mean_loss: 0.02125317,  training time: 2.29
progress:  47%|[34m‚ñà‚ñà‚ñà‚ñà‚ñã     [0m| 235/500 [09:11<10:15,  2.32s/it]progress:  47%|[34m‚ñà‚ñà‚ñà‚ñà‚ñã     [0m| 236/500 [09:11<10:10,  2.31s/it]                                                           Episode 237	 reward: -2.58	 makespan: 255.65	 Mean_loss: 0.01905433,  training time: 2.28
progress:  47%|[34m‚ñà‚ñà‚ñà‚ñà‚ñã     [0m| 236/500 [09:13<10:10,  2.31s/it]progress:  47%|[34m‚ñà‚ñà‚ñà‚ñà‚ñã     [0m| 237/500 [09:13<10:05,  2.30s/it]                                                           Episode 238	 reward: -2.51	 makespan: 248.00	 Mean_loss: 0.00889892,  training time: 2.29
progress:  47%|[34m‚ñà‚ñà‚ñà‚ñà‚ñã     [0m| 237/500 [09:15<10:05,  2.30s/it]progress:  48%|[34m‚ñà‚ñà‚ñà‚ñà‚ñä     [0m| 238/500 [09:15<10:02,  2.30s/it]                                                           Episode 239	 reward: -2.58	 makespan: 254.95	 Mean_loss: 0.01475521,  training time: 2.31
progress:  48%|[34m‚ñà‚ñà‚ñà‚ñà‚ñä     [0m| 238/500 [09:18<10:02,  2.30s/it]progress:  48%|[34m‚ñà‚ñà‚ñà‚ñà‚ñä     [0m| 239/500 [09:18<10:00,  2.30s/it]                                                           Episode 240	 reward: -2.59	 makespan: 256.70	 Mean_loss: 0.01781965,  training time: 2.36
progress:  48%|[34m‚ñà‚ñà‚ñà‚ñà‚ñä     [0m| 239/500 [09:20<10:00,  2.30s/it]progress:  48%|[34m‚ñà‚ñà‚ñà‚ñà‚ñä     [0m| 240/500 [09:20<10:03,  2.32s/it]                                                           Episode 241	 reward: -2.60	 makespan: 256.95	 Mean_loss: 0.01849992,  training time: 2.33
progress:  48%|[34m‚ñà‚ñà‚ñà‚ñà‚ñä     [0m| 240/500 [09:22<10:03,  2.32s/it]progress:  48%|[34m‚ñà‚ñà‚ñà‚ñà‚ñä     [0m| 241/500 [09:22<10:01,  2.32s/it]                                                           Episode 242	 reward: -2.66	 makespan: 263.40	 Mean_loss: 0.01627587,  training time: 2.29
progress:  48%|[34m‚ñà‚ñà‚ñà‚ñà‚ñä     [0m| 241/500 [09:25<10:01,  2.32s/it]progress:  48%|[34m‚ñà‚ñà‚ñà‚ñà‚ñä     [0m| 242/500 [09:25<09:56,  2.31s/it]                                                           Episode 243	 reward: -2.64	 makespan: 260.90	 Mean_loss: 0.03379560,  training time: 2.30
progress:  48%|[34m‚ñà‚ñà‚ñà‚ñà‚ñä     [0m| 242/500 [09:27<09:56,  2.31s/it]progress:  49%|[34m‚ñà‚ñà‚ñà‚ñà‚ñä     [0m| 243/500 [09:27<09:53,  2.31s/it]                                                           Episode 244	 reward: -2.68	 makespan: 265.35	 Mean_loss: 0.02021717,  training time: 2.29
progress:  49%|[34m‚ñà‚ñà‚ñà‚ñà‚ñä     [0m| 243/500 [09:29<09:53,  2.31s/it]progress:  49%|[34m‚ñà‚ñà‚ñà‚ñà‚ñâ     [0m| 244/500 [09:29<09:49,  2.30s/it]                                                           Episode 245	 reward: -2.73	 makespan: 270.10	 Mean_loss: 0.02173831,  training time: 2.36
progress:  49%|[34m‚ñà‚ñà‚ñà‚ñà‚ñâ     [0m| 244/500 [09:32<09:49,  2.30s/it]progress:  49%|[34m‚ñà‚ñà‚ñà‚ñà‚ñâ     [0m| 245/500 [09:32<09:51,  2.32s/it]                                                           Episode 246	 reward: -2.69	 makespan: 266.65	 Mean_loss: 0.02085980,  training time: 2.38
progress:  49%|[34m‚ñà‚ñà‚ñà‚ñà‚ñâ     [0m| 245/500 [09:34<09:51,  2.32s/it]progress:  49%|[34m‚ñà‚ñà‚ñà‚ñà‚ñâ     [0m| 246/500 [09:34<09:53,  2.34s/it]                                                           Episode 247	 reward: -2.69	 makespan: 266.45	 Mean_loss: 0.01827808,  training time: 2.27
progress:  49%|[34m‚ñà‚ñà‚ñà‚ñà‚ñâ     [0m| 246/500 [09:36<09:53,  2.34s/it]progress:  49%|[34m‚ñà‚ñà‚ñà‚ñà‚ñâ     [0m| 247/500 [09:36<09:46,  2.32s/it]                                                           Episode 248	 reward: -2.65	 makespan: 262.40	 Mean_loss: 0.02279910,  training time: 2.29
progress:  49%|[34m‚ñà‚ñà‚ñà‚ñà‚ñâ     [0m| 247/500 [09:39<09:46,  2.32s/it]progress:  50%|[34m‚ñà‚ñà‚ñà‚ñà‚ñâ     [0m| 248/500 [09:39<09:42,  2.31s/it]                                                           Episode 249	 reward: -2.68	 makespan: 265.00	 Mean_loss: 0.02563517,  training time: 2.30
progress:  50%|[34m‚ñà‚ñà‚ñà‚ñà‚ñâ     [0m| 248/500 [09:41<09:42,  2.31s/it]progress:  50%|[34m‚ñà‚ñà‚ñà‚ñà‚ñâ     [0m| 249/500 [09:41<09:38,  2.31s/it]                                                           Episode 250	 reward: -2.52	 makespan: 249.05	 Mean_loss: 0.01285209,  training time: 2.36
progress:  50%|[34m‚ñà‚ñà‚ñà‚ñà‚ñâ     [0m| 249/500 [09:43<09:38,  2.31s/it]progress:  50%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà     [0m| 250/500 [09:43<09:40,  2.32s/it]                                                           Episode 251	 reward: -2.70	 makespan: 266.95	 Mean_loss: 0.02195793,  training time: 2.29
progress:  50%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà     [0m| 250/500 [09:45<09:40,  2.32s/it]progress:  50%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà     [0m| 251/500 [09:45<09:35,  2.31s/it]                                                           Episode 252	 reward: -2.65	 makespan: 262.15	 Mean_loss: 0.02015652,  training time: 2.41
progress:  50%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà     [0m| 251/500 [09:48<09:35,  2.31s/it]progress:  50%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà     [0m| 252/500 [09:48<09:40,  2.34s/it]                                                           Episode 253	 reward: -2.67	 makespan: 264.10	 Mean_loss: 0.02351810,  training time: 2.30
progress:  50%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà     [0m| 252/500 [09:50<09:40,  2.34s/it]progress:  51%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà     [0m| 253/500 [09:50<09:35,  2.33s/it]                                                           Episode 254	 reward: -2.62	 makespan: 259.85	 Mean_loss: 0.02907421,  training time: 2.28
progress:  51%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà     [0m| 253/500 [09:52<09:35,  2.33s/it]progress:  51%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà     [0m| 254/500 [09:52<09:29,  2.32s/it]                                                           Episode 255	 reward: -2.70	 makespan: 267.60	 Mean_loss: 0.02624479,  training time: 2.26
progress:  51%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà     [0m| 254/500 [09:55<09:29,  2.32s/it]progress:  51%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà     [0m| 255/500 [09:55<09:23,  2.30s/it]                                                           Episode 256	 reward: -2.58	 makespan: 254.95	 Mean_loss: 0.01647825,  training time: 2.27
progress:  51%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà     [0m| 255/500 [09:57<09:23,  2.30s/it]progress:  51%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà     [0m| 256/500 [09:57<09:18,  2.29s/it]                                                           Episode 257	 reward: -2.67	 makespan: 264.15	 Mean_loss: 0.02651703,  training time: 2.26
progress:  51%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà     [0m| 256/500 [09:59<09:18,  2.29s/it]progress:  51%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    [0m| 257/500 [09:59<09:14,  2.28s/it]                                                           Episode 258	 reward: -2.57	 makespan: 254.70	 Mean_loss: 0.01308050,  training time: 2.26
progress:  51%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    [0m| 257/500 [10:02<09:14,  2.28s/it]progress:  52%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    [0m| 258/500 [10:02<09:10,  2.28s/it]                                                           Episode 259	 reward: -2.64	 makespan: 261.85	 Mean_loss: 0.01944128,  training time: 2.30
progress:  52%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    [0m| 258/500 [10:04<09:10,  2.28s/it]progress:  52%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    [0m| 259/500 [10:04<09:10,  2.28s/it]                                                           Episode 260	 reward: -2.69	 makespan: 266.60	 Mean_loss: 0.02439430,  training time: 2.37
progress:  52%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    [0m| 259/500 [10:06<09:10,  2.28s/it]progress:  52%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    [0m| 260/500 [10:06<09:14,  2.31s/it]                                                           Episode 261	 reward: -2.58	 makespan: 254.95	 Mean_loss: 0.01554443,  training time: 2.41
progress:  52%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    [0m| 260/500 [10:09<09:14,  2.31s/it]progress:  52%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    [0m| 261/500 [10:09<09:19,  2.34s/it]                                                           Episode 262	 reward: -2.61	 makespan: 258.85	 Mean_loss: 0.01485203,  training time: 2.35
progress:  52%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    [0m| 261/500 [10:11<09:19,  2.34s/it]progress:  52%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    [0m| 262/500 [10:11<09:17,  2.34s/it]                                                           Episode 263	 reward: -2.60	 makespan: 257.45	 Mean_loss: 0.01454019,  training time: 2.27
progress:  52%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    [0m| 262/500 [10:13<09:17,  2.34s/it]progress:  53%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    [0m| 263/500 [10:13<09:09,  2.32s/it]                                                           Episode 264	 reward: -2.64	 makespan: 261.20	 Mean_loss: 0.01831500,  training time: 2.31
progress:  53%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    [0m| 263/500 [10:16<09:09,  2.32s/it]progress:  53%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    [0m| 264/500 [10:16<09:06,  2.32s/it]                                                           Episode 265	 reward: -2.59	 makespan: 255.95	 Mean_loss: 0.01675215,  training time: 2.49
progress:  53%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    [0m| 264/500 [10:18<09:06,  2.32s/it]progress:  53%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    [0m| 265/500 [10:18<09:16,  2.37s/it]                                                           Episode 266	 reward: -2.58	 makespan: 255.60	 Mean_loss: 0.01049745,  training time: 2.52
progress:  53%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    [0m| 265/500 [10:21<09:16,  2.37s/it]progress:  53%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    [0m| 266/500 [10:21<09:24,  2.41s/it]                                                           Episode 267	 reward: -2.65	 makespan: 262.50	 Mean_loss: 0.01097776,  training time: 2.48
progress:  53%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    [0m| 266/500 [10:23<09:24,  2.41s/it]progress:  53%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    [0m| 267/500 [10:23<09:26,  2.43s/it]                                                           Episode 268	 reward: -2.62	 makespan: 258.90	 Mean_loss: 0.01544032,  training time: 2.37
progress:  53%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    [0m| 267/500 [10:25<09:26,  2.43s/it]progress:  54%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    [0m| 268/500 [10:25<09:19,  2.41s/it]                                                           Episode 269	 reward: -2.67	 makespan: 264.00	 Mean_loss: 0.02354269,  training time: 2.29
progress:  54%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    [0m| 268/500 [10:28<09:19,  2.41s/it]progress:  54%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    [0m| 269/500 [10:28<09:08,  2.38s/it]                                                           Episode 270	 reward: -2.59	 makespan: 256.05	 Mean_loss: 0.00819376,  training time: 2.25
progress:  54%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    [0m| 269/500 [10:30<09:08,  2.38s/it]progress:  54%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    [0m| 270/500 [10:30<08:58,  2.34s/it]                                                           Episode 271	 reward: -2.66	 makespan: 263.35	 Mean_loss: 0.01517900,  training time: 2.33
progress:  54%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    [0m| 270/500 [10:32<08:58,  2.34s/it]progress:  54%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    [0m| 271/500 [10:32<08:55,  2.34s/it]                                                           Episode 272	 reward: -2.58	 makespan: 255.90	 Mean_loss: 0.01301990,  training time: 2.35
progress:  54%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    [0m| 271/500 [10:35<08:55,  2.34s/it]progress:  54%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    [0m| 272/500 [10:35<08:54,  2.34s/it]                                                           Episode 273	 reward: -2.58	 makespan: 255.40	 Mean_loss: 0.00927706,  training time: 2.29
progress:  54%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    [0m| 272/500 [10:37<08:54,  2.34s/it]progress:  55%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    [0m| 273/500 [10:37<08:48,  2.33s/it]                                                           Episode 274	 reward: -2.63	 makespan: 260.65	 Mean_loss: 0.01972263,  training time: 2.26
progress:  55%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    [0m| 273/500 [10:39<08:48,  2.33s/it]progress:  55%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    [0m| 274/500 [10:39<08:41,  2.31s/it]                                                           Episode 275	 reward: -2.54	 makespan: 251.85	 Mean_loss: 0.02306017,  training time: 2.28
progress:  55%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    [0m| 274/500 [10:41<08:41,  2.31s/it]progress:  55%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    [0m| 275/500 [10:41<08:37,  2.30s/it]                                                           Episode 276	 reward: -2.58	 makespan: 254.95	 Mean_loss: 0.02173890,  training time: 2.40
progress:  55%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    [0m| 275/500 [10:44<08:37,  2.30s/it]progress:  55%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    [0m| 276/500 [10:44<08:41,  2.33s/it]                                                           Episode 277	 reward: -2.66	 makespan: 262.90	 Mean_loss: 0.01610268,  training time: 2.25
progress:  55%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    [0m| 276/500 [10:46<08:41,  2.33s/it]progress:  55%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    [0m| 277/500 [10:46<08:34,  2.31s/it]                                                           Episode 278	 reward: -2.64	 makespan: 261.85	 Mean_loss: 0.01864822,  training time: 2.25
progress:  55%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    [0m| 277/500 [10:48<08:34,  2.31s/it]progress:  56%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    [0m| 278/500 [10:48<08:28,  2.29s/it]                                                           Episode 279	 reward: -2.56	 makespan: 253.20	 Mean_loss: 0.01570532,  training time: 2.25
progress:  56%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    [0m| 278/500 [10:51<08:28,  2.29s/it]progress:  56%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    [0m| 279/500 [10:51<08:23,  2.28s/it]                                                           Episode 280	 reward: -2.56	 makespan: 253.75	 Mean_loss: 0.01066670,  training time: 2.27
progress:  56%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    [0m| 279/500 [10:53<08:23,  2.28s/it]progress:  56%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    [0m| 280/500 [10:53<08:21,  2.28s/it]                                                           Episode 281	 reward: -2.52	 makespan: 249.25	 Mean_loss: 0.01020887,  training time: 2.30
progress:  56%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    [0m| 280/500 [10:55<08:21,  2.28s/it]progress:  56%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    [0m| 281/500 [10:55<08:20,  2.28s/it]                                                           Episode 282	 reward: -2.56	 makespan: 252.95	 Mean_loss: 0.01346278,  training time: 2.29
progress:  56%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    [0m| 281/500 [10:57<08:20,  2.28s/it]progress:  56%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    [0m| 282/500 [10:57<08:18,  2.29s/it]                                                           Episode 283	 reward: -2.57	 makespan: 254.10	 Mean_loss: 0.00952919,  training time: 2.28
progress:  56%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    [0m| 282/500 [11:00<08:18,  2.29s/it]progress:  57%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    [0m| 283/500 [11:00<08:15,  2.28s/it]                                                           Episode 284	 reward: -2.58	 makespan: 255.55	 Mean_loss: 0.01439598,  training time: 2.32
progress:  57%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    [0m| 283/500 [11:02<08:15,  2.28s/it]progress:  57%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    [0m| 284/500 [11:02<08:15,  2.29s/it]                                                           Episode 285	 reward: -2.58	 makespan: 255.85	 Mean_loss: 0.02713611,  training time: 2.27
progress:  57%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    [0m| 284/500 [11:04<08:15,  2.29s/it]progress:  57%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    [0m| 285/500 [11:04<08:11,  2.29s/it]                                                           Episode 286	 reward: -2.58	 makespan: 255.20	 Mean_loss: 0.01070929,  training time: 2.25
progress:  57%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    [0m| 285/500 [11:07<08:11,  2.29s/it]progress:  57%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    [0m| 286/500 [11:07<08:06,  2.28s/it]                                                           Episode 287	 reward: -2.52	 makespan: 249.05	 Mean_loss: 0.01341753,  training time: 2.26
progress:  57%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    [0m| 286/500 [11:09<08:06,  2.28s/it]progress:  57%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    [0m| 287/500 [11:09<08:03,  2.27s/it]                                                           Episode 288	 reward: -2.62	 makespan: 259.20	 Mean_loss: 0.02137310,  training time: 2.25
progress:  57%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    [0m| 287/500 [11:11<08:03,  2.27s/it]progress:  58%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    [0m| 288/500 [11:11<08:00,  2.27s/it]                                                           Episode 289	 reward: -2.46	 makespan: 243.40	 Mean_loss: 0.01086032,  training time: 2.35
progress:  58%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    [0m| 288/500 [11:13<08:00,  2.27s/it]progress:  58%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    [0m| 289/500 [11:13<08:03,  2.29s/it]                                                           Episode 290	 reward: -2.57	 makespan: 254.90	 Mean_loss: 0.01074944,  training time: 2.33
progress:  58%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    [0m| 289/500 [11:16<08:03,  2.29s/it]progress:  58%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    [0m| 290/500 [11:16<08:03,  2.30s/it]                                                           Episode 291	 reward: -2.63	 makespan: 260.70	 Mean_loss: 0.01860534,  training time: 2.32
progress:  58%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    [0m| 290/500 [11:18<08:03,  2.30s/it]progress:  58%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    [0m| 291/500 [11:18<08:02,  2.31s/it]                                                           Episode 292	 reward: -2.66	 makespan: 263.70	 Mean_loss: 0.01599590,  training time: 2.30
progress:  58%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    [0m| 291/500 [11:20<08:02,  2.31s/it]progress:  58%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    [0m| 292/500 [11:20<07:59,  2.31s/it]                                                           Episode 293	 reward: -2.56	 makespan: 253.50	 Mean_loss: 0.01701778,  training time: 2.30
progress:  58%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    [0m| 292/500 [11:23<07:59,  2.31s/it]progress:  59%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    [0m| 293/500 [11:23<07:56,  2.30s/it]                                                           Episode 294	 reward: -2.56	 makespan: 253.25	 Mean_loss: 0.01689356,  training time: 2.28
progress:  59%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    [0m| 293/500 [11:25<07:56,  2.30s/it]progress:  59%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    [0m| 294/500 [11:25<07:53,  2.30s/it]                                                           Episode 295	 reward: -2.61	 makespan: 258.85	 Mean_loss: 0.01782559,  training time: 2.29
progress:  59%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    [0m| 294/500 [11:27<07:53,  2.30s/it]progress:  59%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    [0m| 295/500 [11:27<07:50,  2.29s/it]                                                           Episode 296	 reward: -2.58	 makespan: 255.70	 Mean_loss: 0.02061656,  training time: 2.29
progress:  59%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    [0m| 295/500 [11:30<07:50,  2.29s/it]progress:  59%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    [0m| 296/500 [11:30<07:47,  2.29s/it]                                                           Episode 297	 reward: -2.60	 makespan: 257.55	 Mean_loss: 0.01007592,  training time: 2.29
progress:  59%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    [0m| 296/500 [11:32<07:47,  2.29s/it]progress:  59%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    [0m| 297/500 [11:32<07:44,  2.29s/it]                                                           Episode 298	 reward: -2.49	 makespan: 246.45	 Mean_loss: 0.01361626,  training time: 2.36
progress:  59%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    [0m| 297/500 [11:34<07:44,  2.29s/it]progress:  60%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    [0m| 298/500 [11:34<07:47,  2.31s/it]                                                           Episode 299	 reward: -2.53	 makespan: 250.75	 Mean_loss: 0.01870263,  training time: 2.26
progress:  60%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    [0m| 298/500 [11:36<07:47,  2.31s/it]progress:  60%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    [0m| 299/500 [11:36<07:41,  2.30s/it]                                                           Episode 300	 reward: -2.52	 makespan: 249.70	 Mean_loss: 0.01118523,  training time: 2.34
progress:  60%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    [0m| 299/500 [11:39<07:41,  2.30s/it]progress:  60%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    [0m| 300/500 [11:39<07:42,  2.31s/it]                                                           Episode 301	 reward: -2.75	 makespan: 272.40	 Mean_loss: 0.01937068,  training time: 2.42
progress:  60%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    [0m| 300/500 [11:41<07:42,  2.31s/it]progress:  60%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    [0m| 301/500 [11:41<07:46,  2.35s/it]                                                           Episode 302	 reward: -2.73	 makespan: 269.95	 Mean_loss: 0.01700100,  training time: 2.33
progress:  60%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    [0m| 301/500 [11:44<07:46,  2.35s/it]progress:  60%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    [0m| 302/500 [11:44<07:43,  2.34s/it]                                                           Episode 303	 reward: -2.66	 makespan: 263.25	 Mean_loss: 0.02122419,  training time: 2.31
progress:  60%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    [0m| 302/500 [11:46<07:43,  2.34s/it]progress:  61%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    [0m| 303/500 [11:46<07:39,  2.33s/it]                                                           Episode 304	 reward: -2.60	 makespan: 257.65	 Mean_loss: 0.02061618,  training time: 2.39
progress:  61%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    [0m| 303/500 [11:48<07:39,  2.33s/it]progress:  61%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    [0m| 304/500 [11:48<07:40,  2.35s/it]                                                           Episode 305	 reward: -2.69	 makespan: 266.65	 Mean_loss: 0.01087306,  training time: 2.27
progress:  61%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    [0m| 304/500 [11:51<07:40,  2.35s/it]progress:  61%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    [0m| 305/500 [11:51<07:33,  2.33s/it]                                                           Episode 306	 reward: -2.63	 makespan: 260.00	 Mean_loss: 0.01513157,  training time: 2.39
progress:  61%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    [0m| 305/500 [11:53<07:33,  2.33s/it]progress:  61%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    [0m| 306/500 [11:53<07:34,  2.35s/it]                                                           Episode 307	 reward: -2.76	 makespan: 273.65	 Mean_loss: 0.02102058,  training time: 2.39
progress:  61%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    [0m| 306/500 [11:55<07:34,  2.35s/it]progress:  61%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   [0m| 307/500 [11:55<07:35,  2.36s/it]                                                           Episode 308	 reward: -2.67	 makespan: 264.05	 Mean_loss: 0.01413420,  training time: 2.29
progress:  61%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   [0m| 307/500 [11:58<07:35,  2.36s/it]progress:  62%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   [0m| 308/500 [11:58<07:28,  2.34s/it]                                                           Episode 309	 reward: -2.70	 makespan: 267.05	 Mean_loss: 0.01763431,  training time: 2.26
progress:  62%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   [0m| 308/500 [12:00<07:28,  2.34s/it]progress:  62%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   [0m| 309/500 [12:00<07:22,  2.31s/it]                                                           Episode 310	 reward: -2.77	 makespan: 274.70	 Mean_loss: 0.01277618,  training time: 2.32
progress:  62%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   [0m| 309/500 [12:02<07:22,  2.31s/it]progress:  62%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   [0m| 310/500 [12:02<07:20,  2.32s/it]                                                           Episode 311	 reward: -2.57	 makespan: 254.60	 Mean_loss: 0.01370138,  training time: 2.52
progress:  62%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   [0m| 310/500 [12:05<07:20,  2.32s/it]progress:  62%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   [0m| 311/500 [12:05<07:29,  2.38s/it]                                                           Episode 312	 reward: -2.71	 makespan: 268.55	 Mean_loss: 0.01701504,  training time: 2.31
progress:  62%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   [0m| 311/500 [12:07<07:29,  2.38s/it]progress:  62%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   [0m| 312/500 [12:07<07:23,  2.36s/it]                                                           Episode 313	 reward: -2.62	 makespan: 259.60	 Mean_loss: 0.02021324,  training time: 2.28
progress:  62%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   [0m| 312/500 [12:09<07:23,  2.36s/it]progress:  63%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   [0m| 313/500 [12:09<07:16,  2.34s/it]                                                           Episode 314	 reward: -2.63	 makespan: 260.80	 Mean_loss: 0.02096350,  training time: 2.30
progress:  63%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   [0m| 313/500 [12:12<07:16,  2.34s/it]progress:  63%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   [0m| 314/500 [12:12<07:12,  2.33s/it]                                                           Episode 315	 reward: -2.63	 makespan: 260.65	 Mean_loss: 0.01467471,  training time: 2.32
progress:  63%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   [0m| 314/500 [12:14<07:12,  2.33s/it]progress:  63%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   [0m| 315/500 [12:14<07:09,  2.32s/it]                                                           Episode 316	 reward: -2.61	 makespan: 258.75	 Mean_loss: 0.01592205,  training time: 2.28
progress:  63%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   [0m| 315/500 [12:16<07:09,  2.32s/it]progress:  63%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   [0m| 316/500 [12:16<07:05,  2.31s/it]                                                           Episode 317	 reward: -2.62	 makespan: 259.20	 Mean_loss: 0.01677492,  training time: 2.27
progress:  63%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   [0m| 316/500 [12:18<07:05,  2.31s/it]progress:  63%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   [0m| 317/500 [12:18<07:00,  2.30s/it]                                                           Episode 318	 reward: -2.68	 makespan: 265.25	 Mean_loss: 0.01756967,  training time: 2.27
progress:  63%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   [0m| 317/500 [12:21<07:00,  2.30s/it]progress:  64%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   [0m| 318/500 [12:21<06:56,  2.29s/it]                                                           Episode 319	 reward: -2.62	 makespan: 259.35	 Mean_loss: 0.01413664,  training time: 2.36
progress:  64%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   [0m| 318/500 [12:23<06:56,  2.29s/it]progress:  64%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   [0m| 319/500 [12:23<06:58,  2.31s/it]                                                           Episode 320	 reward: -2.62	 makespan: 259.60	 Mean_loss: 0.00896243,  training time: 2.45
progress:  64%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   [0m| 319/500 [12:26<06:58,  2.31s/it]progress:  64%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   [0m| 320/500 [12:26<07:03,  2.35s/it]                                                           Episode 321	 reward: -2.58	 makespan: 255.75	 Mean_loss: 0.02782001,  training time: 2.42
progress:  64%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   [0m| 320/500 [12:28<07:03,  2.35s/it]progress:  64%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   [0m| 321/500 [12:28<07:05,  2.38s/it]                                                           Episode 322	 reward: -2.53	 makespan: 250.75	 Mean_loss: 0.00894899,  training time: 2.28
progress:  64%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   [0m| 321/500 [12:30<07:05,  2.38s/it]progress:  64%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   [0m| 322/500 [12:30<06:57,  2.35s/it]                                                           Episode 323	 reward: -2.57	 makespan: 254.25	 Mean_loss: 0.00707512,  training time: 2.25
progress:  64%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   [0m| 322/500 [12:33<06:57,  2.35s/it]progress:  65%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   [0m| 323/500 [12:33<06:50,  2.32s/it]                                                           Episode 324	 reward: -2.55	 makespan: 252.25	 Mean_loss: 0.01117988,  training time: 2.24
progress:  65%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   [0m| 323/500 [12:35<06:50,  2.32s/it]progress:  65%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   [0m| 324/500 [12:35<06:43,  2.29s/it]                                                           Episode 325	 reward: -2.69	 makespan: 266.15	 Mean_loss: 0.01822007,  training time: 2.41
progress:  65%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   [0m| 324/500 [12:37<06:43,  2.29s/it]progress:  65%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   [0m| 325/500 [12:37<06:47,  2.33s/it]                                                           Episode 326	 reward: -2.61	 makespan: 258.60	 Mean_loss: 0.01197171,  training time: 2.30
progress:  65%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   [0m| 325/500 [12:39<06:47,  2.33s/it]progress:  65%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   [0m| 326/500 [12:39<06:43,  2.32s/it]                                                           Episode 327	 reward: -2.67	 makespan: 264.00	 Mean_loss: 0.01987281,  training time: 2.27
progress:  65%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   [0m| 326/500 [12:42<06:43,  2.32s/it]progress:  65%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   [0m| 327/500 [12:42<06:38,  2.30s/it]                                                           Episode 328	 reward: -2.53	 makespan: 250.75	 Mean_loss: 0.00626519,  training time: 2.26
progress:  65%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   [0m| 327/500 [12:44<06:38,  2.30s/it]progress:  66%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   [0m| 328/500 [12:44<06:34,  2.29s/it]                                                           Episode 329	 reward: -2.66	 makespan: 263.15	 Mean_loss: 0.01966862,  training time: 2.31
progress:  66%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   [0m| 328/500 [12:46<06:34,  2.29s/it]progress:  66%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   [0m| 329/500 [12:46<06:32,  2.30s/it]                                                           Episode 330	 reward: -2.54	 makespan: 251.00	 Mean_loss: 0.00678226,  training time: 2.32
progress:  66%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   [0m| 329/500 [12:49<06:32,  2.30s/it]progress:  66%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   [0m| 330/500 [12:49<06:31,  2.31s/it]                                                           Episode 331	 reward: -2.61	 makespan: 258.85	 Mean_loss: 0.01091643,  training time: 2.27
progress:  66%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   [0m| 330/500 [12:51<06:31,  2.31s/it]progress:  66%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   [0m| 331/500 [12:51<06:27,  2.29s/it]                                                           Episode 332	 reward: -2.57	 makespan: 254.50	 Mean_loss: 0.01747010,  training time: 2.25
progress:  66%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   [0m| 331/500 [12:53<06:27,  2.29s/it]progress:  66%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   [0m| 332/500 [12:53<06:23,  2.28s/it]                                                           Episode 333	 reward: -2.53	 makespan: 250.80	 Mean_loss: 0.01089386,  training time: 2.27
progress:  66%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   [0m| 332/500 [12:55<06:23,  2.28s/it]progress:  67%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   [0m| 333/500 [12:55<06:20,  2.28s/it]                                                           Episode 334	 reward: -2.53	 makespan: 250.50	 Mean_loss: 0.01387258,  training time: 2.25
progress:  67%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   [0m| 333/500 [12:58<06:20,  2.28s/it]progress:  67%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   [0m| 334/500 [12:58<06:17,  2.27s/it]                                                           Episode 335	 reward: -2.61	 makespan: 258.35	 Mean_loss: 0.01073316,  training time: 2.27
progress:  67%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   [0m| 334/500 [13:00<06:17,  2.27s/it]progress:  67%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   [0m| 335/500 [13:00<06:15,  2.27s/it]                                                           Episode 336	 reward: -2.66	 makespan: 263.00	 Mean_loss: 0.01166932,  training time: 2.26
progress:  67%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   [0m| 335/500 [13:02<06:15,  2.27s/it]progress:  67%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   [0m| 336/500 [13:02<06:12,  2.27s/it]                                                           Episode 337	 reward: -2.65	 makespan: 262.15	 Mean_loss: 0.02162535,  training time: 2.33
progress:  67%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   [0m| 336/500 [13:05<06:12,  2.27s/it]progress:  67%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   [0m| 337/500 [13:05<06:12,  2.29s/it]                                                           Episode 338	 reward: -2.65	 makespan: 261.95	 Mean_loss: 0.01908336,  training time: 2.35
progress:  67%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   [0m| 337/500 [13:07<06:12,  2.29s/it]progress:  68%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   [0m| 338/500 [13:07<06:13,  2.31s/it]                                                           Episode 339	 reward: -2.59	 makespan: 256.30	 Mean_loss: 0.00799771,  training time: 2.33
progress:  68%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   [0m| 338/500 [13:09<06:13,  2.31s/it]progress:  68%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   [0m| 339/500 [13:09<06:12,  2.31s/it]                                                           Episode 340	 reward: -2.57	 makespan: 254.10	 Mean_loss: 0.01105480,  training time: 2.29
progress:  68%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   [0m| 339/500 [13:12<06:12,  2.31s/it]progress:  68%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   [0m| 340/500 [13:12<06:09,  2.31s/it]                                                           Episode 341	 reward: -2.57	 makespan: 254.35	 Mean_loss: 0.01539441,  training time: 2.33
progress:  68%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   [0m| 340/500 [13:14<06:09,  2.31s/it]progress:  68%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   [0m| 341/500 [13:14<06:07,  2.31s/it]                                                           Episode 342	 reward: -2.58	 makespan: 255.15	 Mean_loss: 0.02337768,  training time: 2.27
progress:  68%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   [0m| 341/500 [13:16<06:07,  2.31s/it]progress:  68%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   [0m| 342/500 [13:16<06:03,  2.30s/it]                                                           Episode 343	 reward: -2.53	 makespan: 250.80	 Mean_loss: 0.01733852,  training time: 2.37
progress:  68%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   [0m| 342/500 [13:18<06:03,  2.30s/it]progress:  69%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   [0m| 343/500 [13:18<06:04,  2.32s/it]                                                           Episode 344	 reward: -2.53	 makespan: 250.15	 Mean_loss: 0.01135243,  training time: 2.27
progress:  69%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   [0m| 343/500 [13:21<06:04,  2.32s/it]progress:  69%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   [0m| 344/500 [13:21<05:59,  2.31s/it]                                                           Episode 345	 reward: -2.54	 makespan: 251.35	 Mean_loss: 0.01505312,  training time: 2.24
progress:  69%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   [0m| 344/500 [13:23<05:59,  2.31s/it]progress:  69%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   [0m| 345/500 [13:23<05:54,  2.29s/it]                                                           Episode 346	 reward: -2.66	 makespan: 263.30	 Mean_loss: 0.03421202,  training time: 2.28
progress:  69%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   [0m| 345/500 [13:25<05:54,  2.29s/it]progress:  69%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   [0m| 346/500 [13:25<05:51,  2.29s/it]                                                           Episode 347	 reward: -2.56	 makespan: 253.30	 Mean_loss: 0.01828350,  training time: 2.24
progress:  69%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   [0m| 346/500 [13:28<05:51,  2.29s/it]progress:  69%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   [0m| 347/500 [13:28<05:47,  2.27s/it]                                                           Episode 348	 reward: -2.54	 makespan: 251.35	 Mean_loss: 0.01907634,  training time: 2.29
progress:  69%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   [0m| 347/500 [13:30<05:47,  2.27s/it]progress:  70%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   [0m| 348/500 [13:30<05:46,  2.28s/it]                                                           Episode 349	 reward: -2.58	 makespan: 255.55	 Mean_loss: 0.02075129,  training time: 2.27
progress:  70%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   [0m| 348/500 [13:32<05:46,  2.28s/it]progress:  70%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   [0m| 349/500 [13:32<05:43,  2.28s/it]                                                           Episode 350	 reward: -2.63	 makespan: 260.25	 Mean_loss: 0.01911628,  training time: 2.26
progress:  70%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   [0m| 349/500 [13:34<05:43,  2.28s/it]progress:  70%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   [0m| 350/500 [13:34<05:40,  2.27s/it]                                                           Episode 351	 reward: -2.48	 makespan: 245.15	 Mean_loss: 0.01611179,  training time: 2.28
progress:  70%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   [0m| 350/500 [13:37<05:40,  2.27s/it]progress:  70%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   [0m| 351/500 [13:37<05:38,  2.27s/it]                                                           Episode 352	 reward: -2.52	 makespan: 249.90	 Mean_loss: 0.01221472,  training time: 2.24
progress:  70%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   [0m| 351/500 [13:39<05:38,  2.27s/it]progress:  70%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   [0m| 352/500 [13:39<05:35,  2.26s/it]                                                           Episode 353	 reward: -2.58	 makespan: 255.15	 Mean_loss: 0.01203734,  training time: 2.25
progress:  70%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   [0m| 352/500 [13:41<05:35,  2.26s/it]progress:  71%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   [0m| 353/500 [13:41<05:32,  2.26s/it]                                                           Episode 354	 reward: -2.60	 makespan: 257.40	 Mean_loss: 0.01606745,  training time: 2.26
progress:  71%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   [0m| 353/500 [13:43<05:32,  2.26s/it]progress:  71%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   [0m| 354/500 [13:43<05:29,  2.26s/it]                                                           Episode 355	 reward: -2.63	 makespan: 260.40	 Mean_loss: 0.01602099,  training time: 2.38
progress:  71%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   [0m| 354/500 [13:46<05:29,  2.26s/it]progress:  71%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   [0m| 355/500 [13:46<05:32,  2.30s/it]                                                           Episode 356	 reward: -2.54	 makespan: 251.70	 Mean_loss: 0.01362000,  training time: 2.31
progress:  71%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   [0m| 355/500 [13:48<05:32,  2.30s/it]progress:  71%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   [0m| 356/500 [13:48<05:31,  2.30s/it]                                                           Episode 357	 reward: -2.64	 makespan: 261.00	 Mean_loss: 0.01405660,  training time: 2.30
progress:  71%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   [0m| 356/500 [13:50<05:31,  2.30s/it]progress:  71%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  [0m| 357/500 [13:50<05:28,  2.30s/it]                                                           Episode 358	 reward: -2.56	 makespan: 253.80	 Mean_loss: 0.01930824,  training time: 2.40
progress:  71%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  [0m| 357/500 [13:53<05:28,  2.30s/it]progress:  72%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  [0m| 358/500 [13:53<05:30,  2.33s/it]                                                           Episode 359	 reward: -2.51	 makespan: 248.25	 Mean_loss: 0.02260411,  training time: 2.33
progress:  72%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  [0m| 358/500 [13:55<05:30,  2.33s/it]progress:  72%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  [0m| 359/500 [13:55<05:28,  2.33s/it]                                                           Episode 360	 reward: -2.55	 makespan: 252.45	 Mean_loss: 0.01352749,  training time: 2.39
progress:  72%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  [0m| 359/500 [13:57<05:28,  2.33s/it]progress:  72%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  [0m| 360/500 [13:57<05:28,  2.35s/it]                                                           Episode 361	 reward: -2.65	 makespan: 262.75	 Mean_loss: 0.01427516,  training time: 2.34
progress:  72%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  [0m| 360/500 [14:00<05:28,  2.35s/it]progress:  72%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  [0m| 361/500 [14:00<05:26,  2.35s/it]                                                           Episode 362	 reward: -2.64	 makespan: 261.55	 Mean_loss: 0.01421589,  training time: 2.28
progress:  72%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  [0m| 361/500 [14:02<05:26,  2.35s/it]progress:  72%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  [0m| 362/500 [14:02<05:21,  2.33s/it]                                                           Episode 363	 reward: -2.65	 makespan: 262.55	 Mean_loss: 0.01597541,  training time: 2.33
progress:  72%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  [0m| 362/500 [14:04<05:21,  2.33s/it]progress:  73%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  [0m| 363/500 [14:04<05:18,  2.33s/it]                                                           Episode 364	 reward: -2.56	 makespan: 253.75	 Mean_loss: 0.01307939,  training time: 2.26
progress:  73%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  [0m| 363/500 [14:07<05:18,  2.33s/it]progress:  73%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  [0m| 364/500 [14:07<05:13,  2.31s/it]                                                           Episode 365	 reward: -2.64	 makespan: 260.95	 Mean_loss: 0.02362683,  training time: 2.26
progress:  73%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  [0m| 364/500 [14:09<05:13,  2.31s/it]progress:  73%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  [0m| 365/500 [14:09<05:09,  2.29s/it]                                                           Episode 366	 reward: -2.51	 makespan: 248.60	 Mean_loss: 0.01729637,  training time: 2.27
progress:  73%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  [0m| 365/500 [14:11<05:09,  2.29s/it]progress:  73%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  [0m| 366/500 [14:11<05:06,  2.28s/it]                                                           Episode 367	 reward: -2.61	 makespan: 258.85	 Mean_loss: 0.01743462,  training time: 2.25
progress:  73%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  [0m| 366/500 [14:13<05:06,  2.28s/it]progress:  73%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  [0m| 367/500 [14:13<05:02,  2.27s/it]                                                           Episode 368	 reward: -2.66	 makespan: 263.25	 Mean_loss: 0.02352851,  training time: 2.25
progress:  73%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  [0m| 367/500 [14:16<05:02,  2.27s/it]progress:  74%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  [0m| 368/500 [14:16<04:59,  2.27s/it]                                                           Episode 369	 reward: -2.68	 makespan: 265.75	 Mean_loss: 0.02729124,  training time: 2.26
progress:  74%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  [0m| 368/500 [14:18<04:59,  2.27s/it]progress:  74%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  [0m| 369/500 [14:18<04:56,  2.27s/it]                                                           Episode 370	 reward: -2.69	 makespan: 266.15	 Mean_loss: 0.02306509,  training time: 2.47
progress:  74%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  [0m| 369/500 [14:20<04:56,  2.27s/it]progress:  74%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  [0m| 370/500 [14:20<05:02,  2.33s/it]                                                           Episode 371	 reward: -2.61	 makespan: 258.80	 Mean_loss: 0.02021578,  training time: 2.40
progress:  74%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  [0m| 370/500 [14:23<05:02,  2.33s/it]progress:  74%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  [0m| 371/500 [14:23<05:03,  2.35s/it]                                                           Episode 372	 reward: -2.63	 makespan: 260.30	 Mean_loss: 0.01700362,  training time: 2.44
progress:  74%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  [0m| 371/500 [14:25<05:03,  2.35s/it]progress:  74%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  [0m| 372/500 [14:25<05:04,  2.38s/it]                                                           Episode 373	 reward: -2.67	 makespan: 263.95	 Mean_loss: 0.01743340,  training time: 2.43
progress:  74%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  [0m| 372/500 [14:28<05:04,  2.38s/it]progress:  75%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  [0m| 373/500 [14:28<05:03,  2.39s/it]                                                           Episode 374	 reward: -2.63	 makespan: 260.45	 Mean_loss: 0.02559320,  training time: 2.31
progress:  75%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  [0m| 373/500 [14:30<05:03,  2.39s/it]progress:  75%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  [0m| 374/500 [14:30<04:58,  2.37s/it]                                                           Episode 375	 reward: -2.64	 makespan: 261.85	 Mean_loss: 0.02046670,  training time: 2.30
progress:  75%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  [0m| 374/500 [14:32<04:58,  2.37s/it]progress:  75%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  [0m| 375/500 [14:32<04:53,  2.35s/it]                                                           Episode 376	 reward: -2.60	 makespan: 257.20	 Mean_loss: 0.01698614,  training time: 2.29
progress:  75%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  [0m| 375/500 [14:35<04:53,  2.35s/it]progress:  75%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  [0m| 376/500 [14:35<04:49,  2.33s/it]                                                           Episode 377	 reward: -2.67	 makespan: 264.25	 Mean_loss: 0.03342591,  training time: 2.27
progress:  75%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  [0m| 376/500 [14:37<04:49,  2.33s/it]progress:  75%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  [0m| 377/500 [14:37<04:44,  2.31s/it]                                                           Episode 378	 reward: -2.57	 makespan: 254.30	 Mean_loss: 0.01459175,  training time: 2.32
progress:  75%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  [0m| 377/500 [14:39<04:44,  2.31s/it]progress:  76%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  [0m| 378/500 [14:39<04:42,  2.32s/it]                                                           Episode 379	 reward: -2.73	 makespan: 270.15	 Mean_loss: 0.03679550,  training time: 2.28
progress:  76%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  [0m| 378/500 [14:42<04:42,  2.32s/it]progress:  76%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  [0m| 379/500 [14:42<04:38,  2.31s/it]                                                           Episode 380	 reward: -2.62	 makespan: 259.85	 Mean_loss: 0.02175170,  training time: 2.30
progress:  76%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  [0m| 379/500 [14:44<04:38,  2.31s/it]progress:  76%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  [0m| 380/500 [14:44<04:36,  2.30s/it]                                                           Episode 381	 reward: -2.57	 makespan: 254.20	 Mean_loss: 0.01628410,  training time: 2.35
progress:  76%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  [0m| 380/500 [14:46<04:36,  2.30s/it]progress:  76%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  [0m| 381/500 [14:46<04:35,  2.32s/it]                                                           Episode 382	 reward: -2.48	 makespan: 245.15	 Mean_loss: 0.01505832,  training time: 2.28
progress:  76%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  [0m| 381/500 [14:48<04:35,  2.32s/it]progress:  76%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  [0m| 382/500 [14:48<04:32,  2.31s/it]                                                           Episode 383	 reward: -2.47	 makespan: 244.65	 Mean_loss: 0.01037816,  training time: 2.30
progress:  76%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  [0m| 382/500 [14:51<04:32,  2.31s/it]progress:  77%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  [0m| 383/500 [14:51<04:29,  2.30s/it]                                                           Episode 384	 reward: -2.59	 makespan: 256.65	 Mean_loss: 0.03128543,  training time: 2.29
progress:  77%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  [0m| 383/500 [14:53<04:29,  2.30s/it]progress:  77%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  [0m| 384/500 [14:53<04:26,  2.30s/it]                                                           Episode 385	 reward: -2.48	 makespan: 245.35	 Mean_loss: 0.01152428,  training time: 2.28
progress:  77%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  [0m| 384/500 [14:55<04:26,  2.30s/it]progress:  77%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  [0m| 385/500 [14:55<04:23,  2.29s/it]                                                           Episode 386	 reward: -2.45	 makespan: 243.00	 Mean_loss: 0.01229890,  training time: 2.29
progress:  77%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  [0m| 385/500 [14:58<04:23,  2.29s/it]progress:  77%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  [0m| 386/500 [14:58<04:21,  2.29s/it]                                                           Episode 387	 reward: -2.65	 makespan: 262.05	 Mean_loss: 0.02990132,  training time: 2.28
progress:  77%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  [0m| 386/500 [15:00<04:21,  2.29s/it]progress:  77%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  [0m| 387/500 [15:00<04:18,  2.29s/it]                                                           Episode 388	 reward: -2.54	 makespan: 251.00	 Mean_loss: 0.01052309,  training time: 2.29
progress:  77%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  [0m| 387/500 [15:02<04:18,  2.29s/it]progress:  78%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  [0m| 388/500 [15:02<04:16,  2.29s/it]                                                           Episode 389	 reward: -2.57	 makespan: 254.65	 Mean_loss: 0.01309296,  training time: 2.33
progress:  78%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  [0m| 388/500 [15:04<04:16,  2.29s/it]progress:  78%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  [0m| 389/500 [15:04<04:15,  2.30s/it]                                                           Episode 390	 reward: -2.57	 makespan: 254.50	 Mean_loss: 0.02543866,  training time: 2.34
progress:  78%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  [0m| 389/500 [15:07<04:15,  2.30s/it]progress:  78%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  [0m| 390/500 [15:07<04:14,  2.31s/it]                                                           Episode 391	 reward: -2.46	 makespan: 243.35	 Mean_loss: 0.01505111,  training time: 2.37
progress:  78%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  [0m| 390/500 [15:09<04:14,  2.31s/it]progress:  78%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  [0m| 391/500 [15:09<04:14,  2.33s/it]                                                           Episode 392	 reward: -2.54	 makespan: 251.50	 Mean_loss: 0.01583491,  training time: 2.32
progress:  78%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  [0m| 391/500 [15:12<04:14,  2.33s/it]progress:  78%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  [0m| 392/500 [15:12<04:11,  2.33s/it]                                                           Episode 393	 reward: -2.54	 makespan: 251.30	 Mean_loss: 0.01860726,  training time: 2.28
progress:  78%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  [0m| 392/500 [15:14<04:11,  2.33s/it]progress:  79%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  [0m| 393/500 [15:14<04:07,  2.32s/it]                                                           Episode 394	 reward: -2.54	 makespan: 251.45	 Mean_loss: 0.01260866,  training time: 2.40
progress:  79%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  [0m| 393/500 [15:16<04:07,  2.32s/it]progress:  79%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  [0m| 394/500 [15:16<04:08,  2.34s/it]                                                           Episode 395	 reward: -2.59	 makespan: 256.90	 Mean_loss: 0.01323698,  training time: 2.27
progress:  79%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  [0m| 394/500 [15:18<04:08,  2.34s/it]progress:  79%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  [0m| 395/500 [15:18<04:03,  2.32s/it]                                                           Episode 396	 reward: -2.44	 makespan: 241.30	 Mean_loss: 0.01491953,  training time: 2.31
progress:  79%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  [0m| 395/500 [15:21<04:03,  2.32s/it]progress:  79%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  [0m| 396/500 [15:21<04:01,  2.32s/it]                                                           Episode 397	 reward: -2.47	 makespan: 244.50	 Mean_loss: 0.01056352,  training time: 2.33
progress:  79%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  [0m| 396/500 [15:23<04:01,  2.32s/it]progress:  79%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  [0m| 397/500 [15:23<03:59,  2.32s/it]                                                           Episode 398	 reward: -2.52	 makespan: 249.25	 Mean_loss: 0.00962861,  training time: 2.26
progress:  79%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  [0m| 397/500 [15:25<03:59,  2.32s/it]progress:  80%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  [0m| 398/500 [15:25<03:55,  2.30s/it]                                                           Episode 399	 reward: -2.49	 makespan: 246.50	 Mean_loss: 0.01577642,  training time: 2.27
progress:  80%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  [0m| 398/500 [15:28<03:55,  2.30s/it]progress:  80%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  [0m| 399/500 [15:28<03:51,  2.29s/it]                                                           Episode 400	 reward: -2.51	 makespan: 248.55	 Mean_loss: 0.00802458,  training time: 2.28
progress:  80%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  [0m| 399/500 [15:30<03:51,  2.29s/it]progress:  80%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  [0m| 400/500 [15:30<03:48,  2.29s/it]                                                           Episode 401	 reward: -2.48	 makespan: 245.85	 Mean_loss: 0.01725899,  training time: 2.33
progress:  80%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  [0m| 400/500 [15:32<03:48,  2.29s/it]progress:  80%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  [0m| 401/500 [15:32<03:47,  2.30s/it]                                                           Episode 402	 reward: -2.50	 makespan: 247.10	 Mean_loss: 0.01997348,  training time: 2.29
progress:  80%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  [0m| 401/500 [15:35<03:47,  2.30s/it]progress:  80%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  [0m| 402/500 [15:35<03:45,  2.30s/it]                                                           Episode 403	 reward: -2.70	 makespan: 267.05	 Mean_loss: 0.02973004,  training time: 2.51
progress:  80%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  [0m| 402/500 [15:37<03:45,  2.30s/it]progress:  81%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  [0m| 403/500 [15:37<03:49,  2.36s/it]                                                           Episode 404	 reward: -2.50	 makespan: 247.15	 Mean_loss: 0.01559887,  training time: 2.39
progress:  81%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  [0m| 403/500 [15:39<03:49,  2.36s/it]progress:  81%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  [0m| 404/500 [15:39<03:47,  2.37s/it]                                                           Episode 405	 reward: -2.54	 makespan: 251.80	 Mean_loss: 0.01003830,  training time: 2.29
progress:  81%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  [0m| 404/500 [15:42<03:47,  2.37s/it]progress:  81%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  [0m| 405/500 [15:42<03:43,  2.35s/it]                                                           Episode 406	 reward: -2.49	 makespan: 246.10	 Mean_loss: 0.02126053,  training time: 2.52
progress:  81%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  [0m| 405/500 [15:44<03:43,  2.35s/it]progress:  81%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  [0m| 406/500 [15:44<03:45,  2.40s/it]                                                           Episode 407	 reward: -2.49	 makespan: 246.15	 Mean_loss: 0.01770071,  training time: 2.38
progress:  81%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  [0m| 406/500 [15:47<03:45,  2.40s/it]progress:  81%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè [0m| 407/500 [15:47<03:42,  2.40s/it]                                                           Episode 408	 reward: -2.60	 makespan: 257.40	 Mean_loss: 0.01457325,  training time: 2.34
progress:  81%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè [0m| 407/500 [15:49<03:42,  2.40s/it]progress:  82%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè [0m| 408/500 [15:49<03:38,  2.38s/it]                                                           Episode 409	 reward: -2.51	 makespan: 248.65	 Mean_loss: 0.02538667,  training time: 2.26
progress:  82%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè [0m| 408/500 [15:51<03:38,  2.38s/it]progress:  82%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè [0m| 409/500 [15:51<03:33,  2.34s/it]                                                           Episode 410	 reward: -2.48	 makespan: 245.70	 Mean_loss: 0.01530566,  training time: 2.31
progress:  82%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè [0m| 409/500 [15:54<03:33,  2.34s/it]progress:  82%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè [0m| 410/500 [15:54<03:29,  2.33s/it]                                                           Episode 411	 reward: -2.57	 makespan: 254.00	 Mean_loss: 0.01803134,  training time: 2.27
progress:  82%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè [0m| 410/500 [15:56<03:29,  2.33s/it]progress:  82%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè [0m| 411/500 [15:56<03:25,  2.31s/it]                                                           Episode 412	 reward: -2.62	 makespan: 259.00	 Mean_loss: 0.02383437,  training time: 2.26
progress:  82%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè [0m| 411/500 [15:58<03:25,  2.31s/it]progress:  82%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè [0m| 412/500 [15:58<03:22,  2.30s/it]                                                           Episode 413	 reward: -2.60	 makespan: 257.45	 Mean_loss: 0.01796122,  training time: 2.31
progress:  82%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè [0m| 412/500 [16:00<03:22,  2.30s/it]progress:  83%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé [0m| 413/500 [16:00<03:20,  2.30s/it]                                                           Episode 414	 reward: -2.52	 makespan: 249.45	 Mean_loss: 0.01509679,  training time: 2.28
progress:  83%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé [0m| 413/500 [16:03<03:20,  2.30s/it]progress:  83%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé [0m| 414/500 [16:03<03:17,  2.30s/it]                                                           Episode 415	 reward: -2.58	 makespan: 255.00	 Mean_loss: 0.01901419,  training time: 2.35
progress:  83%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé [0m| 414/500 [16:05<03:17,  2.30s/it]progress:  83%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé [0m| 415/500 [16:05<03:16,  2.31s/it]                                                           Episode 416	 reward: -2.60	 makespan: 257.05	 Mean_loss: 0.01644134,  training time: 2.40
progress:  83%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé [0m| 415/500 [16:07<03:16,  2.31s/it]progress:  83%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé [0m| 416/500 [16:07<03:16,  2.34s/it]                                                           Episode 417	 reward: -2.54	 makespan: 251.60	 Mean_loss: 0.01764946,  training time: 2.41
progress:  83%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé [0m| 416/500 [16:10<03:16,  2.34s/it]progress:  83%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé [0m| 417/500 [16:10<03:15,  2.36s/it]                                                           Episode 418	 reward: -2.51	 makespan: 248.35	 Mean_loss: 0.01156094,  training time: 2.45
progress:  83%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé [0m| 417/500 [16:12<03:15,  2.36s/it]progress:  84%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé [0m| 418/500 [16:12<03:15,  2.39s/it]                                                           Episode 419	 reward: -2.54	 makespan: 251.00	 Mean_loss: 0.00671418,  training time: 2.27
progress:  84%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé [0m| 418/500 [16:15<03:15,  2.39s/it]progress:  84%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç [0m| 419/500 [16:15<03:10,  2.35s/it]                                                           Episode 420	 reward: -2.57	 makespan: 254.55	 Mean_loss: 0.02093160,  training time: 2.26
progress:  84%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç [0m| 419/500 [16:17<03:10,  2.35s/it]progress:  84%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç [0m| 420/500 [16:17<03:06,  2.33s/it]                                                           Episode 421	 reward: -2.61	 makespan: 258.50	 Mean_loss: 0.01524770,  training time: 2.31
progress:  84%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç [0m| 420/500 [16:19<03:06,  2.33s/it]progress:  84%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç [0m| 421/500 [16:19<03:03,  2.32s/it]                                                           Episode 422	 reward: -2.60	 makespan: 257.25	 Mean_loss: 0.00992616,  training time: 2.27
progress:  84%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç [0m| 421/500 [16:21<03:03,  2.32s/it]progress:  84%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç [0m| 422/500 [16:21<02:59,  2.31s/it]                                                           Episode 423	 reward: -2.57	 makespan: 254.60	 Mean_loss: 0.01638339,  training time: 2.28
progress:  84%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç [0m| 422/500 [16:24<02:59,  2.31s/it]progress:  85%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç [0m| 423/500 [16:24<02:57,  2.30s/it]                                                           Episode 424	 reward: -2.52	 makespan: 249.65	 Mean_loss: 0.01349263,  training time: 2.26
progress:  85%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç [0m| 423/500 [16:26<02:57,  2.30s/it]progress:  85%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç [0m| 424/500 [16:26<02:53,  2.29s/it]                                                           Episode 425	 reward: -2.62	 makespan: 259.00	 Mean_loss: 0.02257326,  training time: 2.26
progress:  85%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç [0m| 424/500 [16:28<02:53,  2.29s/it]progress:  85%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå [0m| 425/500 [16:28<02:51,  2.28s/it]                                                           Episode 426	 reward: -2.47	 makespan: 245.00	 Mean_loss: 0.01402817,  training time: 2.37
progress:  85%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå [0m| 425/500 [16:31<02:51,  2.28s/it]progress:  85%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå [0m| 426/500 [16:31<02:50,  2.31s/it]                                                           Episode 427	 reward: -2.66	 makespan: 263.10	 Mean_loss: 0.02323408,  training time: 2.26
progress:  85%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå [0m| 426/500 [16:33<02:50,  2.31s/it]progress:  85%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå [0m| 427/500 [16:33<02:47,  2.29s/it]                                                           Episode 428	 reward: -2.55	 makespan: 252.15	 Mean_loss: 0.01078561,  training time: 2.26
progress:  85%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå [0m| 427/500 [16:35<02:47,  2.29s/it]progress:  86%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå [0m| 428/500 [16:35<02:44,  2.28s/it]                                                           Episode 429	 reward: -2.62	 makespan: 259.80	 Mean_loss: 0.01219694,  training time: 2.32
progress:  86%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå [0m| 428/500 [16:37<02:44,  2.28s/it]progress:  86%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå [0m| 429/500 [16:37<02:42,  2.29s/it]                                                           Episode 430	 reward: -2.54	 makespan: 251.50	 Mean_loss: 0.00791086,  training time: 2.38
progress:  86%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå [0m| 429/500 [16:40<02:42,  2.29s/it]progress:  86%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå [0m| 430/500 [16:40<02:42,  2.32s/it]                                                           Episode 431	 reward: -2.65	 makespan: 262.15	 Mean_loss: 0.01770732,  training time: 2.29
progress:  86%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå [0m| 430/500 [16:42<02:42,  2.32s/it]progress:  86%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå [0m| 431/500 [16:42<02:39,  2.31s/it]                                                           Episode 432	 reward: -2.73	 makespan: 270.40	 Mean_loss: 0.01837267,  training time: 2.33
progress:  86%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå [0m| 431/500 [16:44<02:39,  2.31s/it]progress:  86%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã [0m| 432/500 [16:44<02:37,  2.32s/it]                                                           Episode 433	 reward: -2.64	 makespan: 261.60	 Mean_loss: 0.01702846,  training time: 2.30
progress:  86%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã [0m| 432/500 [16:47<02:37,  2.32s/it]progress:  87%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã [0m| 433/500 [16:47<02:34,  2.31s/it]                                                           Episode 434	 reward: -2.66	 makespan: 263.10	 Mean_loss: 0.01885397,  training time: 2.27
progress:  87%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã [0m| 433/500 [16:49<02:34,  2.31s/it]progress:  87%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã [0m| 434/500 [16:49<02:31,  2.30s/it]                                                           Episode 435	 reward: -2.60	 makespan: 257.80	 Mean_loss: 0.00940814,  training time: 2.28
progress:  87%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã [0m| 434/500 [16:51<02:31,  2.30s/it]progress:  87%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã [0m| 435/500 [16:51<02:29,  2.29s/it]                                                           Episode 436	 reward: -2.68	 makespan: 265.60	 Mean_loss: 0.01778796,  training time: 2.29
progress:  87%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã [0m| 435/500 [16:54<02:29,  2.29s/it]progress:  87%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã [0m| 436/500 [16:54<02:26,  2.29s/it]                                                           Episode 437	 reward: -2.63	 makespan: 260.30	 Mean_loss: 0.01333253,  training time: 2.28
progress:  87%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã [0m| 436/500 [16:56<02:26,  2.29s/it]progress:  87%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã [0m| 437/500 [16:56<02:24,  2.29s/it]                                                           Episode 438	 reward: -2.61	 makespan: 258.55	 Mean_loss: 0.01023626,  training time: 2.49
progress:  87%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã [0m| 437/500 [16:58<02:24,  2.29s/it]progress:  88%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä [0m| 438/500 [16:58<02:25,  2.35s/it]                                                           Episode 439	 reward: -2.67	 makespan: 263.90	 Mean_loss: 0.01358850,  training time: 2.28
progress:  88%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä [0m| 438/500 [17:01<02:25,  2.35s/it]progress:  88%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä [0m| 439/500 [17:01<02:22,  2.33s/it]                                                           Episode 440	 reward: -2.68	 makespan: 265.75	 Mean_loss: 0.01333035,  training time: 2.31
progress:  88%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä [0m| 439/500 [17:03<02:22,  2.33s/it]progress:  88%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä [0m| 440/500 [17:03<02:19,  2.32s/it]                                                           Episode 441	 reward: -2.64	 makespan: 261.15	 Mean_loss: 0.01929049,  training time: 2.38
progress:  88%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä [0m| 440/500 [17:05<02:19,  2.32s/it]progress:  88%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä [0m| 441/500 [17:05<02:18,  2.34s/it]                                                           Episode 442	 reward: -2.63	 makespan: 260.45	 Mean_loss: 0.01128005,  training time: 2.40
progress:  88%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä [0m| 441/500 [17:08<02:18,  2.34s/it]progress:  88%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä [0m| 442/500 [17:08<02:16,  2.36s/it]                                                           Episode 443	 reward: -2.63	 makespan: 260.65	 Mean_loss: 0.03007329,  training time: 2.30
progress:  88%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä [0m| 442/500 [17:10<02:16,  2.36s/it]progress:  89%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä [0m| 443/500 [17:10<02:13,  2.34s/it]                                                           Episode 444	 reward: -2.66	 makespan: 263.35	 Mean_loss: 0.01524583,  training time: 2.25
progress:  89%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä [0m| 443/500 [17:12<02:13,  2.34s/it]progress:  89%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ [0m| 444/500 [17:12<02:09,  2.32s/it]                                                           Episode 445	 reward: -2.63	 makespan: 260.80	 Mean_loss: 0.02286972,  training time: 2.25
progress:  89%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ [0m| 444/500 [17:15<02:09,  2.32s/it]progress:  89%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ [0m| 445/500 [17:15<02:06,  2.30s/it]                                                           Episode 446	 reward: -2.57	 makespan: 253.95	 Mean_loss: 0.01222066,  training time: 2.36
progress:  89%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ [0m| 445/500 [17:17<02:06,  2.30s/it]progress:  89%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ [0m| 446/500 [17:17<02:05,  2.32s/it]                                                           Episode 447	 reward: -2.55	 makespan: 252.80	 Mean_loss: 0.01131209,  training time: 2.26
progress:  89%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ [0m| 446/500 [17:19<02:05,  2.32s/it]progress:  89%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ [0m| 447/500 [17:19<02:01,  2.30s/it]                                                           Episode 448	 reward: -2.52	 makespan: 249.25	 Mean_loss: 0.01190560,  training time: 2.27
progress:  89%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ [0m| 447/500 [17:21<02:01,  2.30s/it]progress:  90%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ [0m| 448/500 [17:21<01:59,  2.29s/it]                                                           Episode 449	 reward: -2.59	 makespan: 255.95	 Mean_loss: 0.01476471,  training time: 2.29
progress:  90%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ [0m| 448/500 [17:24<01:59,  2.29s/it]progress:  90%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ [0m| 449/500 [17:24<01:56,  2.29s/it]                                                           Episode 450	 reward: -2.51	 makespan: 248.80	 Mean_loss: 0.01204767,  training time: 2.29
progress:  90%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ [0m| 449/500 [17:26<01:56,  2.29s/it]progress:  90%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà [0m| 450/500 [17:26<01:54,  2.29s/it]                                                           Episode 451	 reward: -2.54	 makespan: 251.10	 Mean_loss: 0.01592427,  training time: 2.32
progress:  90%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà [0m| 450/500 [17:28<01:54,  2.29s/it]progress:  90%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà [0m| 451/500 [17:28<01:52,  2.30s/it]                                                           Episode 452	 reward: -2.60	 makespan: 257.60	 Mean_loss: 0.01513285,  training time: 2.30
progress:  90%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà [0m| 451/500 [17:31<01:52,  2.30s/it]progress:  90%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà [0m| 452/500 [17:31<01:50,  2.30s/it]                                                           Episode 453	 reward: -2.64	 makespan: 261.00	 Mean_loss: 0.01026626,  training time: 2.30
progress:  90%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà [0m| 452/500 [17:33<01:50,  2.30s/it]progress:  91%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà [0m| 453/500 [17:33<01:48,  2.30s/it]                                                           Episode 454	 reward: -2.59	 makespan: 256.90	 Mean_loss: 0.02098963,  training time: 2.32
progress:  91%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà [0m| 453/500 [17:35<01:48,  2.30s/it]progress:  91%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà [0m| 454/500 [17:35<01:46,  2.31s/it]                                                           Episode 455	 reward: -2.49	 makespan: 246.40	 Mean_loss: 0.01462342,  training time: 2.34
progress:  91%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà [0m| 454/500 [17:38<01:46,  2.31s/it]progress:  91%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà [0m| 455/500 [17:38<01:44,  2.32s/it]                                                           Episode 456	 reward: -2.62	 makespan: 259.00	 Mean_loss: 0.02003837,  training time: 2.27
progress:  91%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà [0m| 455/500 [17:40<01:44,  2.32s/it]progress:  91%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà [0m| 456/500 [17:40<01:41,  2.30s/it]                                                           Episode 457	 reward: -2.56	 makespan: 253.50	 Mean_loss: 0.01437757,  training time: 2.30
progress:  91%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà [0m| 456/500 [17:42<01:41,  2.30s/it]progress:  91%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè[0m| 457/500 [17:42<01:38,  2.30s/it]                                                           Episode 458	 reward: -2.57	 makespan: 254.10	 Mean_loss: 0.01029112,  training time: 2.31
progress:  91%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè[0m| 457/500 [17:45<01:38,  2.30s/it]progress:  92%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè[0m| 458/500 [17:45<01:36,  2.30s/it]                                                           Episode 459	 reward: -2.56	 makespan: 253.25	 Mean_loss: 0.01435260,  training time: 2.38
progress:  92%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè[0m| 458/500 [17:47<01:36,  2.30s/it]progress:  92%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè[0m| 459/500 [17:47<01:35,  2.33s/it]                                                           Episode 460	 reward: -2.58	 makespan: 255.85	 Mean_loss: 0.01568153,  training time: 2.37
progress:  92%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè[0m| 459/500 [17:49<01:35,  2.33s/it]progress:  92%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè[0m| 460/500 [17:49<01:33,  2.34s/it]                                                           Episode 461	 reward: -2.55	 makespan: 252.55	 Mean_loss: 0.02099720,  training time: 2.42
progress:  92%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè[0m| 460/500 [17:52<01:33,  2.34s/it]progress:  92%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè[0m| 461/500 [17:52<01:32,  2.36s/it]                                                           Episode 462	 reward: -2.53	 makespan: 250.80	 Mean_loss: 0.02187932,  training time: 2.25
progress:  92%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè[0m| 461/500 [17:54<01:32,  2.36s/it]progress:  92%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè[0m| 462/500 [17:54<01:28,  2.33s/it]                                                           Episode 463	 reward: -2.68	 makespan: 265.25	 Mean_loss: 0.02181020,  training time: 2.25
progress:  92%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè[0m| 462/500 [17:56<01:28,  2.33s/it]progress:  93%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé[0m| 463/500 [17:56<01:25,  2.31s/it]                                                           Episode 464	 reward: -2.57	 makespan: 254.75	 Mean_loss: 0.01209773,  training time: 2.25
progress:  93%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé[0m| 463/500 [17:58<01:25,  2.31s/it]progress:  93%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé[0m| 464/500 [17:58<01:22,  2.29s/it]                                                           Episode 465	 reward: -2.67	 makespan: 264.65	 Mean_loss: 0.02557318,  training time: 2.26
progress:  93%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé[0m| 464/500 [18:01<01:22,  2.29s/it]progress:  93%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé[0m| 465/500 [18:01<01:19,  2.28s/it]                                                           Episode 466	 reward: -2.60	 makespan: 257.70	 Mean_loss: 0.02020910,  training time: 2.33
progress:  93%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé[0m| 465/500 [18:03<01:19,  2.28s/it]progress:  93%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé[0m| 466/500 [18:03<01:18,  2.30s/it]                                                           Episode 467	 reward: -2.61	 makespan: 258.45	 Mean_loss: 0.01810014,  training time: 2.27
progress:  93%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé[0m| 466/500 [18:05<01:18,  2.30s/it]progress:  93%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé[0m| 467/500 [18:05<01:15,  2.29s/it]                                                           Episode 468	 reward: -2.68	 makespan: 265.05	 Mean_loss: 0.02030613,  training time: 2.24
progress:  93%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé[0m| 467/500 [18:08<01:15,  2.29s/it]progress:  94%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé[0m| 468/500 [18:08<01:12,  2.27s/it]                                                           Episode 469	 reward: -2.54	 makespan: 251.50	 Mean_loss: 0.01540927,  training time: 2.30
progress:  94%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé[0m| 468/500 [18:10<01:12,  2.27s/it]progress:  94%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç[0m| 469/500 [18:10<01:10,  2.28s/it]                                                           Episode 470	 reward: -2.59	 makespan: 256.85	 Mean_loss: 0.03366254,  training time: 2.29
progress:  94%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç[0m| 469/500 [18:12<01:10,  2.28s/it]progress:  94%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç[0m| 470/500 [18:12<01:08,  2.29s/it]                                                           Episode 471	 reward: -2.66	 makespan: 263.10	 Mean_loss: 0.01685037,  training time: 2.25
progress:  94%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç[0m| 470/500 [18:14<01:08,  2.29s/it]progress:  94%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç[0m| 471/500 [18:14<01:05,  2.27s/it]                                                           Episode 472	 reward: -2.61	 makespan: 258.25	 Mean_loss: 0.01867212,  training time: 2.24
progress:  94%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç[0m| 471/500 [18:17<01:05,  2.27s/it]progress:  94%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç[0m| 472/500 [18:17<01:03,  2.26s/it]                                                           Episode 473	 reward: -2.66	 makespan: 262.85	 Mean_loss: 0.02433378,  training time: 2.26
progress:  94%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç[0m| 472/500 [18:19<01:03,  2.26s/it]progress:  95%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç[0m| 473/500 [18:19<01:01,  2.26s/it]                                                           Episode 474	 reward: -2.62	 makespan: 259.10	 Mean_loss: 0.01304787,  training time: 2.46
progress:  95%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç[0m| 473/500 [18:21<01:01,  2.26s/it]progress:  95%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç[0m| 474/500 [18:21<01:00,  2.32s/it]                                                           Episode 475	 reward: -2.69	 makespan: 266.35	 Mean_loss: 0.02522350,  training time: 2.34
progress:  95%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç[0m| 474/500 [18:24<01:00,  2.32s/it]progress:  95%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå[0m| 475/500 [18:24<00:58,  2.33s/it]                                                           Episode 476	 reward: -2.69	 makespan: 266.70	 Mean_loss: 0.01612961,  training time: 2.34
progress:  95%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå[0m| 475/500 [18:26<00:58,  2.33s/it]progress:  95%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå[0m| 476/500 [18:26<00:55,  2.33s/it]                                                           Episode 477	 reward: -2.64	 makespan: 261.45	 Mean_loss: 0.02902406,  training time: 2.36
progress:  95%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå[0m| 476/500 [18:28<00:55,  2.33s/it]progress:  95%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå[0m| 477/500 [18:28<00:53,  2.34s/it]                                                           Episode 478	 reward: -2.68	 makespan: 265.30	 Mean_loss: 0.01206471,  training time: 2.38
progress:  95%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå[0m| 477/500 [18:31<00:53,  2.34s/it]progress:  96%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå[0m| 478/500 [18:31<00:51,  2.35s/it]                                                           Episode 479	 reward: -2.62	 makespan: 259.65	 Mean_loss: 0.03051099,  training time: 2.29
progress:  96%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå[0m| 478/500 [18:33<00:51,  2.35s/it]progress:  96%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå[0m| 479/500 [18:33<00:49,  2.34s/it]                                                           Episode 480	 reward: -2.66	 makespan: 263.50	 Mean_loss: 0.02211573,  training time: 2.26
progress:  96%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå[0m| 479/500 [18:35<00:49,  2.34s/it]progress:  96%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå[0m| 480/500 [18:35<00:46,  2.31s/it]                                                           Episode 481	 reward: -2.67	 makespan: 264.75	 Mean_loss: 0.00940541,  training time: 2.32
progress:  96%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå[0m| 480/500 [18:38<00:46,  2.31s/it]progress:  96%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå[0m| 481/500 [18:38<00:43,  2.31s/it]                                                           Episode 482	 reward: -2.60	 makespan: 257.10	 Mean_loss: 0.01334641,  training time: 2.45
progress:  96%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå[0m| 481/500 [18:40<00:43,  2.31s/it]progress:  96%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã[0m| 482/500 [18:40<00:42,  2.36s/it]                                                           Episode 483	 reward: -2.67	 makespan: 264.30	 Mean_loss: 0.01905875,  training time: 2.30
progress:  96%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã[0m| 482/500 [18:42<00:42,  2.36s/it]progress:  97%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã[0m| 483/500 [18:42<00:39,  2.34s/it]                                                           Episode 484	 reward: -2.69	 makespan: 266.30	 Mean_loss: 0.01970059,  training time: 2.25
progress:  97%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã[0m| 483/500 [18:45<00:39,  2.34s/it]progress:  97%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã[0m| 484/500 [18:45<00:37,  2.31s/it]                                                           Episode 485	 reward: -2.62	 makespan: 259.65	 Mean_loss: 0.01339057,  training time: 2.42
progress:  97%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã[0m| 484/500 [18:47<00:37,  2.31s/it]progress:  97%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã[0m| 485/500 [18:47<00:35,  2.35s/it]                                                           Episode 486	 reward: -2.67	 makespan: 264.20	 Mean_loss: 0.01583247,  training time: 2.27
progress:  97%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã[0m| 485/500 [18:49<00:35,  2.35s/it]progress:  97%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã[0m| 486/500 [18:49<00:32,  2.32s/it]                                                           Episode 487	 reward: -2.66	 makespan: 263.00	 Mean_loss: 0.02482316,  training time: 2.28
progress:  97%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã[0m| 486/500 [18:52<00:32,  2.32s/it]progress:  97%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã[0m| 487/500 [18:52<00:30,  2.31s/it]                                                           Episode 488	 reward: -2.73	 makespan: 270.35	 Mean_loss: 0.01484356,  training time: 2.30
progress:  97%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã[0m| 487/500 [18:54<00:30,  2.31s/it]progress:  98%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä[0m| 488/500 [18:54<00:27,  2.31s/it]                                                           Episode 489	 reward: -2.62	 makespan: 259.25	 Mean_loss: 0.01529497,  training time: 2.32
progress:  98%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä[0m| 488/500 [18:56<00:27,  2.31s/it]progress:  98%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä[0m| 489/500 [18:56<00:25,  2.31s/it]                                                           Episode 490	 reward: -2.68	 makespan: 265.55	 Mean_loss: 0.01876388,  training time: 2.31
progress:  98%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä[0m| 489/500 [18:59<00:25,  2.31s/it]progress:  98%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä[0m| 490/500 [18:59<00:23,  2.31s/it]                                                           Episode 491	 reward: -2.66	 makespan: 263.40	 Mean_loss: 0.02167514,  training time: 2.31
progress:  98%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä[0m| 490/500 [19:01<00:23,  2.31s/it]progress:  98%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä[0m| 491/500 [19:01<00:20,  2.31s/it]                                                           Episode 492	 reward: -2.65	 makespan: 262.10	 Mean_loss: 0.01626620,  training time: 2.35
progress:  98%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä[0m| 491/500 [19:03<00:20,  2.31s/it]progress:  98%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä[0m| 492/500 [19:03<00:18,  2.33s/it]                                                           Episode 493	 reward: -2.62	 makespan: 259.15	 Mean_loss: 0.01272098,  training time: 2.35
progress:  98%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä[0m| 492/500 [19:06<00:18,  2.33s/it]progress:  99%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä[0m| 493/500 [19:06<00:16,  2.33s/it]                                                           Episode 494	 reward: -2.59	 makespan: 256.75	 Mean_loss: 0.01371523,  training time: 2.31
progress:  99%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä[0m| 493/500 [19:08<00:16,  2.33s/it]progress:  99%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ[0m| 494/500 [19:08<00:13,  2.32s/it]                                                           Episode 495	 reward: -2.65	 makespan: 262.00	 Mean_loss: 0.00912179,  training time: 2.30
progress:  99%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ[0m| 494/500 [19:10<00:13,  2.32s/it]progress:  99%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ[0m| 495/500 [19:10<00:11,  2.32s/it]                                                           Episode 496	 reward: -2.66	 makespan: 263.35	 Mean_loss: 0.01539960,  training time: 2.32
progress:  99%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ[0m| 495/500 [19:13<00:11,  2.32s/it]progress:  99%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ[0m| 496/500 [19:13<00:09,  2.32s/it]                                                           Episode 497	 reward: -2.63	 makespan: 260.05	 Mean_loss: 0.01693620,  training time: 2.31
progress:  99%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ[0m| 496/500 [19:15<00:09,  2.32s/it]progress:  99%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ[0m| 497/500 [19:15<00:06,  2.32s/it]                                                           Episode 498	 reward: -2.62	 makespan: 259.65	 Mean_loss: 0.01184363,  training time: 2.32
progress:  99%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ[0m| 497/500 [19:17<00:06,  2.32s/it]progress: 100%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ[0m| 498/500 [19:17<00:04,  2.32s/it]                                                           Episode 499	 reward: -2.69	 makespan: 266.05	 Mean_loss: 0.01814466,  training time: 2.46
progress: 100%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ[0m| 498/500 [19:20<00:04,  2.32s/it]progress: 100%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ[0m| 499/500 [19:20<00:02,  2.36s/it]                                                           Episode 500	 reward: -2.56	 makespan: 253.70	 Mean_loss: 0.02236563,  training time: 2.71
progress: 100%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ[0m| 499/500 [19:22<00:02,  2.36s/it]progress: 100%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà[0m| 500/500 [19:22<00:00,  2.46s/it]progress: 100%|[34m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà[0m| 500/500 [19:22<00:00,  2.33s/it]
+ n_j=20
+ for model in 20x15+mix+SD2_exp18_3 20X20+mix+SD2_exp18_3
+ for n_m in '$n_m_options'
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20x15+mix+SD2_exp18_3/20x5 --model_suffix free --finetuning_model 20x15+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 5 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x5+mix
save model name:  20x5+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 11, in __init__
    self.ppo.policy.load_state_dict(torch.load(self.finetuning_model, map_location='cuda'))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/serialization.py", line 986, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/serialization.py", line 435, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/serialization.py", line 416, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: './trained_network/SD2/20x15+mix+SD2_exp18_3.pth'
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20x15+mix+SD2_exp18_3/20x5 --model_suffix free --finetuning_model 20x15+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 5 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x5+mix
save model name:  20x5+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 11, in __init__
    self.ppo.policy.load_state_dict(torch.load(self.finetuning_model, map_location='cuda'))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/serialization.py", line 986, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/serialization.py", line 435, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/serialization.py", line 416, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: './trained_network/SD2/20x15+mix+SD2_exp18_3.pth'
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20x15+mix+SD2_exp18_3/20x5 --model_suffix free --finetuning_model 20x15+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 5 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x5+mix
save model name:  20x5+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 11, in __init__
    self.ppo.policy.load_state_dict(torch.load(self.finetuning_model, map_location='cuda'))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/serialization.py", line 986, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/serialization.py", line 435, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/serialization.py", line 416, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: './trained_network/SD2/20x15+mix+SD2_exp18_3.pth'
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20x15+mix+SD2_exp18_3/20x5 --model_suffix free --finetuning_model 20x15+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 5 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x5+mix
save model name:  20x5+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 11, in __init__
    self.ppo.policy.load_state_dict(torch.load(self.finetuning_model, map_location='cuda'))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/serialization.py", line 986, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/serialization.py", line 435, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/serialization.py", line 416, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: './trained_network/SD2/20x15+mix+SD2_exp18_3.pth'
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20x15+mix+SD2_exp18_3/20x5 --model_suffix free --finetuning_model 20x15+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 5 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x5+mix
save model name:  20x5+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 11, in __init__
    self.ppo.policy.load_state_dict(torch.load(self.finetuning_model, map_location='cuda'))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/serialization.py", line 986, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/serialization.py", line 435, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/serialization.py", line 416, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: './trained_network/SD2/20x15+mix+SD2_exp18_3.pth'
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20x15+mix+SD2_exp18_3/20x5 --model_suffix free --finetuning_model 20x15+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 5 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x5+mix
save model name:  20x5+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 11, in __init__
    self.ppo.policy.load_state_dict(torch.load(self.finetuning_model, map_location='cuda'))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/serialization.py", line 986, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/serialization.py", line 435, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/serialization.py", line 416, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: './trained_network/SD2/20x15+mix+SD2_exp18_3.pth'
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for n_m in '$n_m_options'
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20x15+mix+SD2_exp18_3/20x7 --model_suffix free --finetuning_model 20x15+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 7 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x7+mix
save model name:  20x7+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 11, in __init__
    self.ppo.policy.load_state_dict(torch.load(self.finetuning_model, map_location='cuda'))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/serialization.py", line 986, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/serialization.py", line 435, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/serialization.py", line 416, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: './trained_network/SD2/20x15+mix+SD2_exp18_3.pth'
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20x15+mix+SD2_exp18_3/20x7 --model_suffix free --finetuning_model 20x15+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 7 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x7+mix
save model name:  20x7+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 11, in __init__
    self.ppo.policy.load_state_dict(torch.load(self.finetuning_model, map_location='cuda'))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/serialization.py", line 986, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/serialization.py", line 435, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/serialization.py", line 416, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: './trained_network/SD2/20x15+mix+SD2_exp18_3.pth'
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20x15+mix+SD2_exp18_3/20x7 --model_suffix free --finetuning_model 20x15+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 7 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x7+mix
save model name:  20x7+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 11, in __init__
    self.ppo.policy.load_state_dict(torch.load(self.finetuning_model, map_location='cuda'))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/serialization.py", line 986, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/serialization.py", line 435, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/serialization.py", line 416, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: './trained_network/SD2/20x15+mix+SD2_exp18_3.pth'
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20x15+mix+SD2_exp18_3/20x7 --model_suffix free --finetuning_model 20x15+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 7 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x7+mix
save model name:  20x7+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 11, in __init__
    self.ppo.policy.load_state_dict(torch.load(self.finetuning_model, map_location='cuda'))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/serialization.py", line 986, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/serialization.py", line 435, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/serialization.py", line 416, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: './trained_network/SD2/20x15+mix+SD2_exp18_3.pth'
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20x15+mix+SD2_exp18_3/20x7 --model_suffix free --finetuning_model 20x15+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 7 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x7+mix
save model name:  20x7+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 11, in __init__
    self.ppo.policy.load_state_dict(torch.load(self.finetuning_model, map_location='cuda'))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/serialization.py", line 986, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/serialization.py", line 435, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/serialization.py", line 416, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: './trained_network/SD2/20x15+mix+SD2_exp18_3.pth'
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20x15+mix+SD2_exp18_3/20x7 --model_suffix free --finetuning_model 20x15+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 7 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x7+mix
save model name:  20x7+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 11, in __init__
    self.ppo.policy.load_state_dict(torch.load(self.finetuning_model, map_location='cuda'))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/serialization.py", line 986, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/serialization.py", line 435, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/serialization.py", line 416, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: './trained_network/SD2/20x15+mix+SD2_exp18_3.pth'
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for n_m in '$n_m_options'
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20x15+mix+SD2_exp18_3/20x10 --model_suffix free --finetuning_model 20x15+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 10 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x10+mix
save model name:  20x10+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 11, in __init__
    self.ppo.policy.load_state_dict(torch.load(self.finetuning_model, map_location='cuda'))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/serialization.py", line 986, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/serialization.py", line 435, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/serialization.py", line 416, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: './trained_network/SD2/20x15+mix+SD2_exp18_3.pth'
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20x15+mix+SD2_exp18_3/20x10 --model_suffix free --finetuning_model 20x15+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 10 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x10+mix
save model name:  20x10+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 11, in __init__
    self.ppo.policy.load_state_dict(torch.load(self.finetuning_model, map_location='cuda'))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/serialization.py", line 986, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/serialization.py", line 435, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/serialization.py", line 416, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: './trained_network/SD2/20x15+mix+SD2_exp18_3.pth'
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20x15+mix+SD2_exp18_3/20x10 --model_suffix free --finetuning_model 20x15+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 10 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x10+mix
save model name:  20x10+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 11, in __init__
    self.ppo.policy.load_state_dict(torch.load(self.finetuning_model, map_location='cuda'))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/serialization.py", line 986, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/serialization.py", line 435, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/serialization.py", line 416, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: './trained_network/SD2/20x15+mix+SD2_exp18_3.pth'
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20x15+mix+SD2_exp18_3/20x10 --model_suffix free --finetuning_model 20x15+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 10 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x10+mix
save model name:  20x10+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20x15+mix+SD2_exp18_3/20x10 --model_suffix free --finetuning_model 20x15+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 10 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x10+mix
save model name:  20x10+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20x15+mix+SD2_exp18_3/20x10 --model_suffix free --finetuning_model 20x15+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 10 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x10+mix
save model name:  20x10+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for n_m in '$n_m_options'
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20x15+mix+SD2_exp18_3/20x13 --model_suffix free --finetuning_model 20x15+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 13 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x13+mix
save model name:  20x13+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20x15+mix+SD2_exp18_3/20x13 --model_suffix free --finetuning_model 20x15+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 13 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x13+mix
save model name:  20x13+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20x15+mix+SD2_exp18_3/20x13 --model_suffix free --finetuning_model 20x15+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 13 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x13+mix
save model name:  20x13+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20x15+mix+SD2_exp18_3/20x13 --model_suffix free --finetuning_model 20x15+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 13 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x13+mix
save model name:  20x13+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20x15+mix+SD2_exp18_3/20x13 --model_suffix free --finetuning_model 20x15+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 13 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x13+mix
save model name:  20x13+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20x15+mix+SD2_exp18_3/20x13 --model_suffix free --finetuning_model 20x15+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 13 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x13+mix
save model name:  20x13+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for n_m in '$n_m_options'
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20x15+mix+SD2_exp18_3/20x15 --model_suffix free --finetuning_model 20x15+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 15 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x15+mix
save model name:  20x15+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20x15+mix+SD2_exp18_3/20x15 --model_suffix free --finetuning_model 20x15+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 15 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x15+mix
save model name:  20x15+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20x15+mix+SD2_exp18_3/20x15 --model_suffix free --finetuning_model 20x15+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 15 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x15+mix
save model name:  20x15+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20x15+mix+SD2_exp18_3/20x15 --model_suffix free --finetuning_model 20x15+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 15 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x15+mix
save model name:  20x15+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20x15+mix+SD2_exp18_3/20x15 --model_suffix free --finetuning_model 20x15+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 15 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x15+mix
save model name:  20x15+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20x15+mix+SD2_exp18_3/20x15 --model_suffix free --finetuning_model 20x15+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 15 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x15+mix
save model name:  20x15+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for n_m in '$n_m_options'
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20x15+mix+SD2_exp18_3/20x20 --model_suffix free --finetuning_model 20x15+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 20 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x20+mix
save model name:  20x20+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20x15+mix+SD2_exp18_3/20x20 --model_suffix free --finetuning_model 20x15+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 20 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x20+mix
save model name:  20x20+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20x15+mix+SD2_exp18_3/20x20 --model_suffix free --finetuning_model 20x15+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 20 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x20+mix
save model name:  20x20+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20x15+mix+SD2_exp18_3/20x20 --model_suffix free --finetuning_model 20x15+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 20 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x20+mix
save model name:  20x20+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20x15+mix+SD2_exp18_3/20x20 --model_suffix free --finetuning_model 20x15+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 20 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x20+mix
save model name:  20x20+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20x15+mix+SD2_exp18_3/20x20 --model_suffix free --finetuning_model 20x15+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 20 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x20+mix
save model name:  20x20+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for model in 20x15+mix+SD2_exp18_3 20X20+mix+SD2_exp18_3
+ for n_m in '$n_m_options'
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20X20+mix+SD2_exp18_3/20x5 --model_suffix free --finetuning_model 20X20+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 5 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x5+mix
save model name:  20x5+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20X20+mix+SD2_exp18_3/20x5 --model_suffix free --finetuning_model 20X20+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 5 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x5+mix
save model name:  20x5+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20X20+mix+SD2_exp18_3/20x5 --model_suffix free --finetuning_model 20X20+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 5 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x5+mix
save model name:  20x5+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20X20+mix+SD2_exp18_3/20x5 --model_suffix free --finetuning_model 20X20+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 5 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x5+mix
save model name:  20x5+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20X20+mix+SD2_exp18_3/20x5 --model_suffix free --finetuning_model 20X20+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 5 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x5+mix
save model name:  20x5+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20X20+mix+SD2_exp18_3/20x5 --model_suffix free --finetuning_model 20X20+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 5 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x5+mix
save model name:  20x5+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for n_m in '$n_m_options'
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20X20+mix+SD2_exp18_3/20x7 --model_suffix free --finetuning_model 20X20+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 7 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x7+mix
save model name:  20x7+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20X20+mix+SD2_exp18_3/20x7 --model_suffix free --finetuning_model 20X20+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 7 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x7+mix
save model name:  20x7+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20X20+mix+SD2_exp18_3/20x7 --model_suffix free --finetuning_model 20X20+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 7 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x7+mix
save model name:  20x7+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20X20+mix+SD2_exp18_3/20x7 --model_suffix free --finetuning_model 20X20+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 7 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x7+mix
save model name:  20x7+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20X20+mix+SD2_exp18_3/20x7 --model_suffix free --finetuning_model 20X20+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 7 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x7+mix
save model name:  20x7+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20X20+mix+SD2_exp18_3/20x7 --model_suffix free --finetuning_model 20X20+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 7 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x7+mix
save model name:  20x7+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for n_m in '$n_m_options'
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20X20+mix+SD2_exp18_3/20x10 --model_suffix free --finetuning_model 20X20+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 10 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x10+mix
save model name:  20x10+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20X20+mix+SD2_exp18_3/20x10 --model_suffix free --finetuning_model 20X20+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 10 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x10+mix
save model name:  20x10+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20X20+mix+SD2_exp18_3/20x10 --model_suffix free --finetuning_model 20X20+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 10 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x10+mix
save model name:  20x10+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20X20+mix+SD2_exp18_3/20x10 --model_suffix free --finetuning_model 20X20+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 10 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x10+mix
save model name:  20x10+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20X20+mix+SD2_exp18_3/20x10 --model_suffix free --finetuning_model 20X20+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 10 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x10+mix
save model name:  20x10+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20X20+mix+SD2_exp18_3/20x10 --model_suffix free --finetuning_model 20X20+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 10 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x10+mix
save model name:  20x10+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for n_m in '$n_m_options'
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20X20+mix+SD2_exp18_3/20x13 --model_suffix free --finetuning_model 20X20+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 13 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x13+mix
save model name:  20x13+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20X20+mix+SD2_exp18_3/20x13 --model_suffix free --finetuning_model 20X20+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 13 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x13+mix
save model name:  20x13+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20X20+mix+SD2_exp18_3/20x13 --model_suffix free --finetuning_model 20X20+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 13 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x13+mix
save model name:  20x13+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20X20+mix+SD2_exp18_3/20x13 --model_suffix free --finetuning_model 20X20+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 13 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x13+mix
save model name:  20x13+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20X20+mix+SD2_exp18_3/20x13 --model_suffix free --finetuning_model 20X20+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 13 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x13+mix
save model name:  20x13+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20X20+mix+SD2_exp18_3/20x13 --model_suffix free --finetuning_model 20X20+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 13 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x13+mix
save model name:  20x13+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for n_m in '$n_m_options'
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20X20+mix+SD2_exp18_3/20x15 --model_suffix free --finetuning_model 20X20+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 15 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x15+mix
save model name:  20x15+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20X20+mix+SD2_exp18_3/20x15 --model_suffix free --finetuning_model 20X20+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 15 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x15+mix
save model name:  20x15+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20X20+mix+SD2_exp18_3/20x15 --model_suffix free --finetuning_model 20X20+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 15 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x15+mix
save model name:  20x15+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20X20+mix+SD2_exp18_3/20x15 --model_suffix free --finetuning_model 20X20+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 15 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x15+mix
save model name:  20x15+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20X20+mix+SD2_exp18_3/20x15 --model_suffix free --finetuning_model 20X20+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 15 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x15+mix
save model name:  20x15+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20X20+mix+SD2_exp18_3/20x15 --model_suffix free --finetuning_model 20X20+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 15 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x15+mix
save model name:  20x15+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for n_m in '$n_m_options'
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20X20+mix+SD2_exp18_3/20x20 --model_suffix free --finetuning_model 20X20+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 20 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x20+mix
save model name:  20x20+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20X20+mix+SD2_exp18_3/20x20 --model_suffix free --finetuning_model 20X20+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 20 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x20+mix
save model name:  20x20+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20X20+mix+SD2_exp18_3/20x20 --model_suffix free --finetuning_model 20X20+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 20 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x20+mix
save model name:  20x20+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20X20+mix+SD2_exp18_3/20x20 --model_suffix free --finetuning_model 20X20+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 20 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x20+mix
save model name:  20x20+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20X20+mix+SD2_exp18_3/20x20 --model_suffix free --finetuning_model 20X20+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 20 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x20+mix
save model name:  20x20+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/DAN/finetuning/20X20+mix+SD2_exp18_3/20x20 --model_suffix free --finetuning_model 20X20+mix+SD2_exp18_3 --max_updates 21 --n_j 20 --n_m 20 --op_per_job 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x20+mix
save model name:  20x20+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ --num_envs 10 --lr 0.003
+ local runcnf=1
+ local retval=127
+ [[ hxB =~ i ]]
+ runcnf=0
+ [[ ! -S /run/dbus/system_bus_socket ]]
+ [[ ! -x /usr/libexec/packagekitd ]]
+ [[ -n '' ]]
+ '[' 0 -eq 1 ']'
+ [[ -n 4.2.46(2)-release ]]
+ printf 'bash: %scommand not found\n' '--num_envs: '
bash: --num_envs: command not found
+ return 127
+ model_suffix=exp18_203_512_3
+ logdir_maml=./runs/exp18/exp18-3/maml
+ python train/multi_task_maml_exp18.py --logdir ./runs/exp18/exp18-3/maml/train_model/model_suffix --model_suffix exp18_203_512_3 --maml_model True --meta_iterations 203 --num_tasks 6 --max_updates 500 --num_envs 10 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --n_j_options 20 20 20 20 20 20 --n_m_options 5 7 10 13 15 20 --op_per_job_options 5 5 5 5 5 5
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/10x5+mix
save model name:  maml+exp18_203_512_3
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/multi_task_maml_exp18.py", line 97, in <module>
    trainer = MultiTaskTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/multi_task_maml_exp18.py", line 14, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ n_j=20
+ for model in 'maml+$model_suffix'
+ for n_m in '$n_m_options'
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/maml/finetuning/maml+exp18_203_512_3/20x5_5 --model_suffix free --finetuning_model maml+exp18_203_512_3 --max_updates 21 --n_j 20 --n_m 5 --op_per_job 5 --num_envs 10 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x5+mix
save model name:  20x5+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/maml/finetuning/maml+exp18_203_512_3/20x5_5 --model_suffix free --finetuning_model maml+exp18_203_512_3 --max_updates 21 --n_j 20 --n_m 5 --op_per_job 5 --num_envs 10 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x5+mix
save model name:  20x5+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/maml/finetuning/maml+exp18_203_512_3/20x5_5 --model_suffix free --finetuning_model maml+exp18_203_512_3 --max_updates 21 --n_j 20 --n_m 5 --op_per_job 5 --num_envs 10 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x5+mix
save model name:  20x5+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/maml/finetuning/maml+exp18_203_512_3/20x5_5 --model_suffix free --finetuning_model maml+exp18_203_512_3 --max_updates 21 --n_j 20 --n_m 5 --op_per_job 5 --num_envs 10 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x5+mix
save model name:  20x5+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/maml/finetuning/maml+exp18_203_512_3/20x5_5 --model_suffix free --finetuning_model maml+exp18_203_512_3 --max_updates 21 --n_j 20 --n_m 5 --op_per_job 5 --num_envs 10 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x5+mix
save model name:  20x5+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/maml/finetuning/maml+exp18_203_512_3/20x5_5 --model_suffix free --finetuning_model maml+exp18_203_512_3 --max_updates 21 --n_j 20 --n_m 5 --op_per_job 5 --num_envs 10 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x5+mix
save model name:  20x5+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ for n_m in '$n_m_options'
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/maml/finetuning/maml+exp18_203_512_3/20x7_5 --model_suffix free --finetuning_model maml+exp18_203_512_3 --max_updates 21 --n_j 20 --n_m 7 --op_per_job 5 --num_envs 10 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x7+mix
save model name:  20x7+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/maml/finetuning/maml+exp18_203_512_3/20x7_5 --model_suffix free --finetuning_model maml+exp18_203_512_3 --max_updates 21 --n_j 20 --n_m 7 --op_per_job 5 --num_envs 10 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x7+mix
save model name:  20x7+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/maml/finetuning/maml+exp18_203_512_3/20x7_5 --model_suffix free --finetuning_model maml+exp18_203_512_3 --max_updates 21 --n_j 20 --n_m 7 --op_per_job 5 --num_envs 10 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x7+mix
save model name:  20x7+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/maml/finetuning/maml+exp18_203_512_3/20x7_5 --model_suffix free --finetuning_model maml+exp18_203_512_3 --max_updates 21 --n_j 20 --n_m 7 --op_per_job 5 --num_envs 10 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x7+mix
save model name:  20x7+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/maml/finetuning/maml+exp18_203_512_3/20x7_5 --model_suffix free --finetuning_model maml+exp18_203_512_3 --max_updates 21 --n_j 20 --n_m 7 --op_per_job 5 --num_envs 10 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x7+mix
save model name:  20x7+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/maml/finetuning/maml+exp18_203_512_3/20x7_5 --model_suffix free --finetuning_model maml+exp18_203_512_3 --max_updates 21 --n_j 20 --n_m 7 --op_per_job 5 --num_envs 10 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x7+mix
save model name:  20x7+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ for n_m in '$n_m_options'
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/maml/finetuning/maml+exp18_203_512_3/20x10_5 --model_suffix free --finetuning_model maml+exp18_203_512_3 --max_updates 21 --n_j 20 --n_m 10 --op_per_job 5 --num_envs 10 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x10+mix
save model name:  20x10+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/maml/finetuning/maml+exp18_203_512_3/20x10_5 --model_suffix free --finetuning_model maml+exp18_203_512_3 --max_updates 21 --n_j 20 --n_m 10 --op_per_job 5 --num_envs 10 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x10+mix
save model name:  20x10+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/maml/finetuning/maml+exp18_203_512_3/20x10_5 --model_suffix free --finetuning_model maml+exp18_203_512_3 --max_updates 21 --n_j 20 --n_m 10 --op_per_job 5 --num_envs 10 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x10+mix
save model name:  20x10+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/maml/finetuning/maml+exp18_203_512_3/20x10_5 --model_suffix free --finetuning_model maml+exp18_203_512_3 --max_updates 21 --n_j 20 --n_m 10 --op_per_job 5 --num_envs 10 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x10+mix
save model name:  20x10+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/maml/finetuning/maml+exp18_203_512_3/20x10_5 --model_suffix free --finetuning_model maml+exp18_203_512_3 --max_updates 21 --n_j 20 --n_m 10 --op_per_job 5 --num_envs 10 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x10+mix
save model name:  20x10+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/maml/finetuning/maml+exp18_203_512_3/20x10_5 --model_suffix free --finetuning_model maml+exp18_203_512_3 --max_updates 21 --n_j 20 --n_m 10 --op_per_job 5 --num_envs 10 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x10+mix
save model name:  20x10+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ for n_m in '$n_m_options'
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/maml/finetuning/maml+exp18_203_512_3/20x13_5 --model_suffix free --finetuning_model maml+exp18_203_512_3 --max_updates 21 --n_j 20 --n_m 13 --op_per_job 5 --num_envs 10 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x13+mix
save model name:  20x13+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/maml/finetuning/maml+exp18_203_512_3/20x13_5 --model_suffix free --finetuning_model maml+exp18_203_512_3 --max_updates 21 --n_j 20 --n_m 13 --op_per_job 5 --num_envs 10 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x13+mix
save model name:  20x13+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/maml/finetuning/maml+exp18_203_512_3/20x13_5 --model_suffix free --finetuning_model maml+exp18_203_512_3 --max_updates 21 --n_j 20 --n_m 13 --op_per_job 5 --num_envs 10 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x13+mix
save model name:  20x13+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/maml/finetuning/maml+exp18_203_512_3/20x13_5 --model_suffix free --finetuning_model maml+exp18_203_512_3 --max_updates 21 --n_j 20 --n_m 13 --op_per_job 5 --num_envs 10 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x13+mix
save model name:  20x13+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/maml/finetuning/maml+exp18_203_512_3/20x13_5 --model_suffix free --finetuning_model maml+exp18_203_512_3 --max_updates 21 --n_j 20 --n_m 13 --op_per_job 5 --num_envs 10 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x13+mix
save model name:  20x13+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/maml/finetuning/maml+exp18_203_512_3/20x13_5 --model_suffix free --finetuning_model maml+exp18_203_512_3 --max_updates 21 --n_j 20 --n_m 13 --op_per_job 5 --num_envs 10 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x13+mix
save model name:  20x13+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ for n_m in '$n_m_options'
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/maml/finetuning/maml+exp18_203_512_3/20x15_5 --model_suffix free --finetuning_model maml+exp18_203_512_3 --max_updates 21 --n_j 20 --n_m 15 --op_per_job 5 --num_envs 10 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x15+mix
save model name:  20x15+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/maml/finetuning/maml+exp18_203_512_3/20x15_5 --model_suffix free --finetuning_model maml+exp18_203_512_3 --max_updates 21 --n_j 20 --n_m 15 --op_per_job 5 --num_envs 10 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x15+mix
save model name:  20x15+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/maml/finetuning/maml+exp18_203_512_3/20x15_5 --model_suffix free --finetuning_model maml+exp18_203_512_3 --max_updates 21 --n_j 20 --n_m 15 --op_per_job 5 --num_envs 10 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x15+mix
save model name:  20x15+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/maml/finetuning/maml+exp18_203_512_3/20x15_5 --model_suffix free --finetuning_model maml+exp18_203_512_3 --max_updates 21 --n_j 20 --n_m 15 --op_per_job 5 --num_envs 10 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x15+mix
save model name:  20x15+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/maml/finetuning/maml+exp18_203_512_3/20x15_5 --model_suffix free --finetuning_model maml+exp18_203_512_3 --max_updates 21 --n_j 20 --n_m 15 --op_per_job 5 --num_envs 10 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x15+mix
save model name:  20x15+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/maml/finetuning/maml+exp18_203_512_3/20x15_5 --model_suffix free --finetuning_model maml+exp18_203_512_3 --max_updates 21 --n_j 20 --n_m 15 --op_per_job 5 --num_envs 10 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x15+mix
save model name:  20x15+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ for n_m in '$n_m_options'
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/maml/finetuning/maml+exp18_203_512_3/20x20_5 --model_suffix free --finetuning_model maml+exp18_203_512_3 --max_updates 21 --n_j 20 --n_m 20 --op_per_job 5 --num_envs 10 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x20+mix
save model name:  20x20+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/maml/finetuning/maml+exp18_203_512_3/20x20_5 --model_suffix free --finetuning_model maml+exp18_203_512_3 --max_updates 21 --n_j 20 --n_m 20 --op_per_job 5 --num_envs 10 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x20+mix
save model name:  20x20+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/maml/finetuning/maml+exp18_203_512_3/20x20_5 --model_suffix free --finetuning_model maml+exp18_203_512_3 --max_updates 21 --n_j 20 --n_m 20 --op_per_job 5 --num_envs 10 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x20+mix
save model name:  20x20+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/maml/finetuning/maml+exp18_203_512_3/20x20_5 --model_suffix free --finetuning_model maml+exp18_203_512_3 --max_updates 21 --n_j 20 --n_m 20 --op_per_job 5 --num_envs 10 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x20+mix
save model name:  20x20+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/maml/finetuning/maml+exp18_203_512_3/20x20_5 --model_suffix free --finetuning_model maml+exp18_203_512_3 --max_updates 21 --n_j 20 --n_m 20 --op_per_job 5 --num_envs 10 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x20+mix
save model name:  20x20+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
+ for op_per_job in '$op_per_job_options'
+ python train/DAN_finetuning.py --logdir ./runs/exp18/exp18-3/maml/finetuning/maml+exp18_203_512_3/20x20_5 --model_suffix free --finetuning_model maml+exp18_203_512_3 --max_updates 21 --n_j 20 --n_m 20 --op_per_job 5 --num_envs 10 --hidden_dim_actor 512 --hidden_dim_critic 512 --num_mlp_layers_actor 3 --num_mlp_layers_critic 3 --lr 0.003
/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
vali_data = ./data/data_train_vali/SD2/20x20+mix
save model name:  20x20+mix+free
Traceback (most recent call last):
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 132, in <module>
    main()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 127, in main
    trainer = DANTrainer(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/train/DAN_finetuning.py", line 10, in __init__
    self.ppo = PPO_initialize()
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 531, in PPO_initialize
    ppo = PPO(configs)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/PPO.py", line 160, in __init__
    self.policy = DANIEL(config)
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 121, in __init__
    self.feature_exact = DualAttentionNetwork(config).to(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/main_model.py", line 46, in __init__
    MultiHeadOpAttnBlock(
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 76, in __init__
    self.attentions = [
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 77, in <listcomp>
    SingleOpAttnBlock(input_dim, output_dim, dropout_prob) for
  File "/work/home/lxx_hzau/project/FJSP-DRL-main/model/attention_layer.py", line 20, in __init__
    self.W = nn.Parameter(torch.empty(size=(input_dim, output_dim)))
  File "/work/home/lxx_hzau/miniconda3/envs/RL-torch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 298, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
